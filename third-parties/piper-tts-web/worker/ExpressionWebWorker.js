var I_ = Object.defineProperty;
var rf = ($e) => {
  throw TypeError($e);
};
var F_ = ($e, $, r) => $ in $e ? I_($e, $, { enumerable: !0, configurable: !0, writable: !0, value: r }) : $e[$] = r;
var fe = ($e, $, r) => F_($e, typeof $ != "symbol" ? $ + "" : $, r), nf = ($e, $, r) => $.has($e) || rf("Cannot " + r);
var Dc = ($e, $, r) => (nf($e, $, "read from private field"), r ? r.call($e) : $.get($e)), Gp = ($e, $, r) => $.has($e) ? rf("Cannot add the same private member more than once") : $ instanceof WeakSet ? $.add($e) : $.set($e, r), Lc = ($e, $, r, _) => (nf($e, $, "write to private field"), _ ? _.call($e, r) : $.set($e, r), r);
var af = {
  /***/
  "./node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm": (
    /*!****************************************************************************!*\
      !*** ./node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm ***!
      \****************************************************************************/
    /***/
    ($e, $, r) => {
      $e.exports = r.p + "ort-wasm-simd-threaded.jsep.wasm";
    }
  ),
  /***/
  "./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs?46eb": (
    /*!**************************************************************!*\
      !*** ./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs ***!
      \**************************************************************/
    /***/
    ($e, $, r) => {
      $e.exports = r.p + "ort.bundle.min.mjs";
    }
  ),
  /***/
  "?2ce3": (
    /*!**********************************!*\
      !*** onnxruntime-node (ignored) ***!
      \**********************************/
    /***/
    () => {
    }
  ),
  /***/
  "?7a2c": (
    /*!********************!*\
      !*** fs (ignored) ***!
      \********************/
    /***/
    () => {
    }
  ),
  /***/
  "?a42a": (
    /*!**********************!*\
      !*** path (ignored) ***!
      \**********************/
    /***/
    () => {
    }
  ),
  /***/
  "?2b25": (
    /*!***********************!*\
      !*** sharp (ignored) ***!
      \***********************/
    /***/
    () => {
    }
  ),
  /***/
  "?569f": (
    /*!********************!*\
      !*** fs (ignored) ***!
      \********************/
    /***/
    () => {
    }
  ),
  /***/
  "?3f59": (
    /*!**********************!*\
      !*** path (ignored) ***!
      \**********************/
    /***/
    () => {
    }
  ),
  /***/
  "?154a": (
    /*!*********************!*\
      !*** url (ignored) ***!
      \*********************/
    /***/
    () => {
    }
  ),
  /***/
  "./node_modules/@huggingface/jinja/dist/index.js": (
    /*!*******************************************************!*\
      !*** ./node_modules/@huggingface/jinja/dist/index.js ***!
      \*******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Environment: () => (
          /* binding */
          st
        ),
        /* harmony export */
        Interpreter: () => (
          /* binding */
          dt
        ),
        /* harmony export */
        Template: () => (
          /* binding */
          ht
        ),
        /* harmony export */
        parse: () => (
          /* binding */
          ye
        ),
        /* harmony export */
        tokenize: () => (
          /* binding */
          y
        )
        /* harmony export */
      });
      var _ = Object.freeze({
        Text: "Text",
        // The text between Jinja statements or expressions
        NumericLiteral: "NumericLiteral",
        // e.g., 123
        BooleanLiteral: "BooleanLiteral",
        // true or false
        NullLiteral: "NullLiteral",
        // none
        StringLiteral: "StringLiteral",
        // 'string'
        Identifier: "Identifier",
        // Variables, functions, etc.
        Equals: "Equals",
        // =
        OpenParen: "OpenParen",
        // (
        CloseParen: "CloseParen",
        // )
        OpenStatement: "OpenStatement",
        // {%
        CloseStatement: "CloseStatement",
        // %}
        OpenExpression: "OpenExpression",
        // {{
        CloseExpression: "CloseExpression",
        // }}
        OpenSquareBracket: "OpenSquareBracket",
        // [
        CloseSquareBracket: "CloseSquareBracket",
        // ]
        OpenCurlyBracket: "OpenCurlyBracket",
        // {
        CloseCurlyBracket: "CloseCurlyBracket",
        // }
        Comma: "Comma",
        // ,
        Dot: "Dot",
        // .
        Colon: "Colon",
        // :
        Pipe: "Pipe",
        // |
        CallOperator: "CallOperator",
        // ()
        AdditiveBinaryOperator: "AdditiveBinaryOperator",
        // + -
        MultiplicativeBinaryOperator: "MultiplicativeBinaryOperator",
        // * / %
        ComparisonBinaryOperator: "ComparisonBinaryOperator",
        // < > <= >= == !=
        UnaryOperator: "UnaryOperator",
        // ! - +
        // Keywords
        Set: "Set",
        If: "If",
        For: "For",
        In: "In",
        Is: "Is",
        NotIn: "NotIn",
        Else: "Else",
        EndIf: "EndIf",
        ElseIf: "ElseIf",
        EndFor: "EndFor",
        And: "And",
        Or: "Or",
        Not: "UnaryOperator",
        Macro: "Macro",
        EndMacro: "EndMacro"
      }), D = Object.freeze({
        set: _.Set,
        for: _.For,
        in: _.In,
        is: _.Is,
        if: _.If,
        else: _.Else,
        endif: _.EndIf,
        elif: _.ElseIf,
        endfor: _.EndFor,
        and: _.And,
        or: _.Or,
        not: _.Not,
        "not in": _.NotIn,
        macro: _.Macro,
        endmacro: _.EndMacro,
        // Literals
        true: _.BooleanLiteral,
        false: _.BooleanLiteral,
        none: _.NullLiteral,
        // NOTE: According to the Jinja docs: The special constants true, false, and none are indeed lowercase.
        // Because that caused confusion in the past, (True used to expand to an undefined variable that was considered false),
        // all three can now also be written in title case (True, False, and None). However, for consistency, (all Jinja identifiers are lowercase)
        // you should use the lowercase versions.
        True: _.BooleanLiteral,
        False: _.BooleanLiteral,
        None: _.NullLiteral
      }), U = class {
        /**
         * Constructs a new Token.
         * @param {string} value The raw value as seen inside the source code.
         * @param {TokenType} type The type of token.
         */
        constructor(L, oe) {
          this.value = L, this.type = oe;
        }
      };
      function Y(L) {
        return /\w/.test(L);
      }
      function R(L) {
        return /[0-9]/.test(L);
      }
      var g = [
        // Control sequences
        ["{%", _.OpenStatement],
        ["%}", _.CloseStatement],
        ["{{", _.OpenExpression],
        ["}}", _.CloseExpression],
        // Single character tokens
        ["(", _.OpenParen],
        [")", _.CloseParen],
        ["{", _.OpenCurlyBracket],
        ["}", _.CloseCurlyBracket],
        ["[", _.OpenSquareBracket],
        ["]", _.CloseSquareBracket],
        [",", _.Comma],
        [".", _.Dot],
        [":", _.Colon],
        ["|", _.Pipe],
        // Comparison operators
        ["<=", _.ComparisonBinaryOperator],
        [">=", _.ComparisonBinaryOperator],
        ["==", _.ComparisonBinaryOperator],
        ["!=", _.ComparisonBinaryOperator],
        ["<", _.ComparisonBinaryOperator],
        [">", _.ComparisonBinaryOperator],
        // Arithmetic operators
        ["+", _.AdditiveBinaryOperator],
        ["-", _.AdditiveBinaryOperator],
        ["*", _.MultiplicativeBinaryOperator],
        ["/", _.MultiplicativeBinaryOperator],
        ["%", _.MultiplicativeBinaryOperator],
        // Assignment operator
        ["=", _.Equals]
      ], v = /* @__PURE__ */ new Map([
        ["n", `
`],
        // New line
        ["t", "	"],
        // Horizontal tab
        ["r", "\r"],
        // Carriage return
        ["b", "\b"],
        // Backspace
        ["f", "\f"],
        // Form feed
        ["v", "\v"],
        // Vertical tab
        ["'", "'"],
        // Single quote
        ['"', '"'],
        // Double quote
        ["\\", "\\"]
        // Backslash
      ]);
      function M(L, oe = {}) {
        return L.endsWith(`
`) && (L = L.slice(0, -1)), L = L.replace(/{#.*?#}/gs, "{##}"), oe.lstrip_blocks && (L = L.replace(/^[ \t]*({[#%])/gm, "$1")), oe.trim_blocks && (L = L.replace(/([#%]})\n/g, "$1")), L.replace(/{##}/g, "").replace(/-%}\s*/g, "%}").replace(/\s*{%-/g, "{%").replace(/-}}\s*/g, "}}").replace(/\s*{{-/g, "{{");
      }
      function y(L, oe = {}) {
        var Je, ut, mt;
        const H = [], me = M(L, oe);
        let Ae = 0;
        const We = (vt) => {
          let kt = "";
          for (; vt(me[Ae]); ) {
            if (me[Ae] === "\\") {
              if (++Ae, Ae >= me.length)
                throw new SyntaxError("Unexpected end of input");
              const At = me[Ae++], is = v.get(At);
              if (is === void 0)
                throw new SyntaxError(`Unexpected escaped character: ${At}`);
              kt += is;
              continue;
            }
            if (kt += me[Ae++], Ae >= me.length)
              throw new SyntaxError("Unexpected end of input");
          }
          return kt;
        };
        e:
          for (; Ae < me.length; ) {
            const vt = (Je = H.at(-1)) == null ? void 0 : Je.type;
            if (vt === void 0 || vt === _.CloseStatement || vt === _.CloseExpression) {
              let At = "";
              for (; Ae < me.length && // Keep going until we hit the next Jinja statement or expression
              !(me[Ae] === "{" && (me[Ae + 1] === "%" || me[Ae + 1] === "{")); )
                At += me[Ae++];
              if (At.length > 0) {
                H.push(new U(At, _.Text));
                continue;
              }
            }
            We((At) => /\s/.test(At));
            const kt = me[Ae];
            if (kt === "-" || kt === "+") {
              const At = (ut = H.at(-1)) == null ? void 0 : ut.type;
              if (At === _.Text || At === void 0)
                throw new SyntaxError(`Unexpected character: ${kt}`);
              switch (At) {
                case _.Identifier:
                case _.NumericLiteral:
                case _.BooleanLiteral:
                case _.NullLiteral:
                case _.StringLiteral:
                case _.CloseParen:
                case _.CloseSquareBracket:
                  break;
                default: {
                  ++Ae;
                  const is = We(R);
                  H.push(
                    new U(`${kt}${is}`, is.length > 0 ? _.NumericLiteral : _.UnaryOperator)
                  );
                  continue;
                }
              }
            }
            for (const [At, is] of g)
              if (me.slice(Ae, Ae + At.length) === At) {
                H.push(new U(At, is)), Ae += At.length;
                continue e;
              }
            if (kt === "'" || kt === '"') {
              ++Ae;
              const At = We((is) => is !== kt);
              H.push(new U(At, _.StringLiteral)), ++Ae;
              continue;
            }
            if (R(kt)) {
              const At = We(R);
              H.push(new U(At, _.NumericLiteral));
              continue;
            }
            if (Y(kt)) {
              const At = We(Y), is = Object.hasOwn(D, At) ? D[At] : _.Identifier;
              is === _.In && ((mt = H.at(-1)) == null ? void 0 : mt.type) === _.Not ? (H.pop(), H.push(new U("not in", _.NotIn))) : H.push(new U(At, is));
              continue;
            }
            throw new SyntaxError(`Unexpected character: ${kt}`);
          }
        return H;
      }
      var b = class {
        constructor() {
          fe(this, "type", "Statement");
        }
      }, I = class extends b {
        constructor(oe) {
          super();
          fe(this, "type", "Program");
          this.body = oe;
        }
      }, K = class extends b {
        constructor(oe, H, me) {
          super();
          fe(this, "type", "If");
          this.test = oe, this.body = H, this.alternate = me;
        }
      }, se = class extends b {
        constructor(oe, H, me, Ae) {
          super();
          fe(this, "type", "For");
          this.loopvar = oe, this.iterable = H, this.body = me, this.defaultBlock = Ae;
        }
      }, ie = class extends b {
        constructor(oe, H) {
          super();
          fe(this, "type", "Set");
          this.assignee = oe, this.value = H;
        }
      }, W = class extends b {
        constructor(oe, H, me) {
          super();
          fe(this, "type", "Macro");
          this.name = oe, this.args = H, this.body = me;
        }
      }, j = class extends b {
        constructor() {
          super(...arguments);
          fe(this, "type", "Expression");
        }
      }, q = class extends j {
        constructor(oe, H, me) {
          super();
          fe(this, "type", "MemberExpression");
          this.object = oe, this.property = H, this.computed = me;
        }
      }, A = class extends j {
        constructor(oe, H) {
          super();
          fe(this, "type", "CallExpression");
          this.callee = oe, this.args = H;
        }
      }, S = class extends j {
        /**
         * @param {string} value The name of the identifier
         */
        constructor(oe) {
          super();
          fe(this, "type", "Identifier");
          this.value = oe;
        }
      }, w = class extends j {
        constructor(oe) {
          super();
          fe(this, "type", "Literal");
          this.value = oe;
        }
      }, x = class extends w {
        constructor() {
          super(...arguments);
          fe(this, "type", "NumericLiteral");
        }
      }, F = class extends w {
        constructor() {
          super(...arguments);
          fe(this, "type", "StringLiteral");
        }
      }, le = class extends w {
        constructor() {
          super(...arguments);
          fe(this, "type", "BooleanLiteral");
        }
      }, ne = class extends w {
        constructor() {
          super(...arguments);
          fe(this, "type", "NullLiteral");
        }
      }, be = class extends w {
        constructor() {
          super(...arguments);
          fe(this, "type", "ArrayLiteral");
        }
      }, _e = class extends w {
        constructor() {
          super(...arguments);
          fe(this, "type", "TupleLiteral");
        }
      }, re = class extends w {
        constructor() {
          super(...arguments);
          fe(this, "type", "ObjectLiteral");
        }
      }, xe = class extends j {
        constructor(oe, H, me) {
          super();
          fe(this, "type", "BinaryExpression");
          this.operator = oe, this.left = H, this.right = me;
        }
      }, ce = class extends j {
        constructor(oe, H) {
          super();
          fe(this, "type", "FilterExpression");
          this.operand = oe, this.filter = H;
        }
      }, ke = class extends j {
        constructor(oe, H) {
          super();
          fe(this, "type", "SelectExpression");
          this.iterable = oe, this.test = H;
        }
      }, Fe = class extends j {
        constructor(oe, H, me) {
          super();
          fe(this, "type", "TestExpression");
          this.operand = oe, this.negate = H, this.test = me;
        }
      }, Ee = class extends j {
        constructor(oe, H) {
          super();
          fe(this, "type", "UnaryExpression");
          this.operator = oe, this.argument = H;
        }
      }, tt = class extends j {
        constructor(oe = void 0, H = void 0, me = void 0) {
          super();
          fe(this, "type", "SliceExpression");
          this.start = oe, this.stop = H, this.step = me;
        }
      }, Ge = class extends j {
        constructor(oe, H) {
          super();
          fe(this, "type", "KeywordArgumentExpression");
          this.key = oe, this.value = H;
        }
      };
      function ye(L) {
        const oe = new I([]);
        let H = 0;
        function me(it, Tt) {
          const Dt = L[H++];
          if (!Dt || Dt.type !== it)
            throw new Error(`Parser Error: ${Tt}. ${Dt.type} !== ${it}.`);
          return Dt;
        }
        function Ae() {
          switch (L[H].type) {
            case _.Text:
              return ut();
            case _.OpenStatement:
              return mt();
            case _.OpenExpression:
              return vt();
            default:
              throw new SyntaxError(`Unexpected token type: ${L[H].type}`);
          }
        }
        function We(...it) {
          return H + it.length <= L.length && it.some((Tt, Dt) => Tt !== L[H + Dt].type);
        }
        function Je(...it) {
          return H + it.length <= L.length && it.every((Tt, Dt) => Tt === L[H + Dt].type);
        }
        function ut() {
          return new F(me(_.Text, "Expected text token").value);
        }
        function mt() {
          me(_.OpenStatement, "Expected opening statement token");
          let it;
          switch (L[H].type) {
            case _.Set:
              ++H, it = kt(), me(_.CloseStatement, "Expected closing statement token");
              break;
            case _.If:
              ++H, it = At(), me(_.OpenStatement, "Expected {% token"), me(_.EndIf, "Expected endif token"), me(_.CloseStatement, "Expected %} token");
              break;
            case _.Macro:
              ++H, it = is(), me(_.OpenStatement, "Expected {% token"), me(_.EndMacro, "Expected endmacro token"), me(_.CloseStatement, "Expected %} token");
              break;
            case _.For:
              ++H, it = Cs(), me(_.OpenStatement, "Expected {% token"), me(_.EndFor, "Expected endfor token"), me(_.CloseStatement, "Expected %} token");
              break;
            default:
              throw new SyntaxError(`Unknown statement type: ${L[H].type}`);
          }
          return it;
        }
        function vt() {
          me(_.OpenExpression, "Expected opening expression token");
          const it = Ds();
          return me(_.CloseExpression, "Expected closing expression token"), it;
        }
        function kt() {
          const it = Ds();
          if (Je(_.Equals)) {
            ++H;
            const Tt = kt();
            return new ie(it, Tt);
          }
          return it;
        }
        function At() {
          var Vs, Nr, Ir, Ms, lr, Fs, Pr, es;
          const it = Ds();
          me(_.CloseStatement, "Expected closing statement token");
          const Tt = [], Dt = [];
          for (; !(((Vs = L[H]) == null ? void 0 : Vs.type) === _.OpenStatement && (((Nr = L[H + 1]) == null ? void 0 : Nr.type) === _.ElseIf || ((Ir = L[H + 1]) == null ? void 0 : Ir.type) === _.Else || ((Ms = L[H + 1]) == null ? void 0 : Ms.type) === _.EndIf)); )
            Tt.push(Ae());
          if (((lr = L[H]) == null ? void 0 : lr.type) === _.OpenStatement && ((Fs = L[H + 1]) == null ? void 0 : Fs.type) !== _.EndIf)
            if (++H, Je(_.ElseIf))
              me(_.ElseIf, "Expected elseif token"), Dt.push(At());
            else
              for (me(_.Else, "Expected else token"), me(_.CloseStatement, "Expected closing statement token"); !(((Pr = L[H]) == null ? void 0 : Pr.type) === _.OpenStatement && ((es = L[H + 1]) == null ? void 0 : es.type) === _.EndIf); )
                Dt.push(Ae());
          return new K(it, Tt, Dt);
        }
        function is() {
          const it = ar();
          if (it.type !== "Identifier")
            throw new SyntaxError("Expected identifier following macro statement");
          const Tt = $r();
          me(_.CloseStatement, "Expected closing statement token");
          const Dt = [];
          for (; We(_.OpenStatement, _.EndMacro); )
            Dt.push(Ae());
          return new W(it, Tt, Dt);
        }
        function ys(it = !1) {
          const Tt = it ? ar : Ds, Dt = [Tt()], Vs = Je(_.Comma);
          for (; Vs && (++H, Dt.push(Tt()), !!Je(_.Comma)); )
            ;
          return Vs ? new _e(Dt) : Dt[0];
        }
        function Cs() {
          const it = ys(!0);
          if (!(it instanceof S || it instanceof _e))
            throw new SyntaxError(`Expected identifier/tuple for the loop variable, got ${it.type} instead`);
          me(_.In, "Expected `in` keyword following loop variable");
          const Tt = Ds();
          me(_.CloseStatement, "Expected closing statement token");
          const Dt = [];
          for (; We(_.OpenStatement, _.EndFor) && We(_.OpenStatement, _.Else); )
            Dt.push(Ae());
          const Vs = [];
          if (Je(_.OpenStatement, _.Else))
            for (++H, ++H, me(_.CloseStatement, "Expected closing statement token"); We(_.OpenStatement, _.EndFor); )
              Vs.push(Ae());
          return new se(it, Tt, Dt, Vs);
        }
        function Ds() {
          return sr();
        }
        function sr() {
          const it = kr();
          if (Je(_.If)) {
            ++H;
            const Tt = kr();
            if (Je(_.Else)) {
              ++H;
              const Dt = kr();
              return new K(Tt, [it], [Dt]);
            } else
              return new ke(it, Tt);
          }
          return it;
        }
        function kr() {
          let it = Qr();
          for (; Je(_.Or); ) {
            const Tt = L[H];
            ++H;
            const Dt = Qr();
            it = new xe(Tt, it, Dt);
          }
          return it;
        }
        function Qr() {
          let it = Us();
          for (; Je(_.And); ) {
            const Tt = L[H];
            ++H;
            const Dt = Us();
            it = new xe(Tt, it, Dt);
          }
          return it;
        }
        function Us() {
          let it;
          for (; Je(_.Not); ) {
            const Tt = L[H];
            ++H;
            const Dt = Us();
            it = new Ee(Tt, Dt);
          }
          return it ?? Tr();
        }
        function Tr() {
          let it = Nt();
          for (; Je(_.ComparisonBinaryOperator) || Je(_.In) || Je(_.NotIn); ) {
            const Tt = L[H];
            ++H;
            const Dt = Nt();
            it = new xe(Tt, it, Dt);
          }
          return it;
        }
        function Nt() {
          let it = Ar();
          for (; Je(_.AdditiveBinaryOperator); ) {
            const Tt = L[H];
            ++H;
            const Dt = Ar();
            it = new xe(Tt, it, Dt);
          }
          return it;
        }
        function Xr() {
          const it = Jr();
          return Je(_.OpenParen) ? Sr(it) : it;
        }
        function Sr(it) {
          let Tt = new A(it, $r());
          return Je(_.OpenParen) && (Tt = Sr(Tt)), Tt;
        }
        function $r() {
          me(_.OpenParen, "Expected opening parenthesis for arguments list");
          const it = Yr();
          return me(_.CloseParen, "Expected closing parenthesis for arguments list"), it;
        }
        function Yr() {
          const it = [];
          for (; !Je(_.CloseParen); ) {
            let Tt = Ds();
            if (Je(_.Equals)) {
              if (++H, !(Tt instanceof S))
                throw new SyntaxError("Expected identifier for keyword argument");
              const Dt = Ds();
              Tt = new Ge(Tt, Dt);
            }
            it.push(Tt), Je(_.Comma) && ++H;
          }
          return it;
        }
        function dr() {
          const it = [];
          let Tt = !1;
          for (; !Je(_.CloseSquareBracket); )
            Je(_.Colon) ? (it.push(void 0), ++H, Tt = !0) : (it.push(Ds()), Je(_.Colon) && (++H, Tt = !0));
          if (it.length === 0)
            throw new SyntaxError("Expected at least one argument for member/slice expression");
          if (Tt) {
            if (it.length > 3)
              throw new SyntaxError("Expected 0-3 arguments for slice expression");
            return new tt(...it);
          }
          return it[0];
        }
        function Jr() {
          let it = ar();
          for (; Je(_.Dot) || Je(_.OpenSquareBracket); ) {
            const Tt = L[H];
            ++H;
            let Dt;
            const Vs = Tt.type !== _.Dot;
            if (Vs)
              Dt = dr(), me(_.CloseSquareBracket, "Expected closing square bracket");
            else if (Dt = ar(), Dt.type !== "Identifier")
              throw new SyntaxError("Expected identifier following dot operator");
            it = new q(it, Dt, Vs);
          }
          return it;
        }
        function Ar() {
          let it = Br();
          for (; Je(_.MultiplicativeBinaryOperator); ) {
            const Tt = L[H];
            ++H;
            const Dt = Br();
            it = new xe(Tt, it, Dt);
          }
          return it;
        }
        function Br() {
          let it = Rr();
          for (; Je(_.Is); ) {
            ++H;
            const Tt = Je(_.Not);
            Tt && ++H;
            let Dt = ar();
            if (Dt instanceof le ? Dt = new S(Dt.value.toString()) : Dt instanceof ne && (Dt = new S("none")), !(Dt instanceof S))
              throw new SyntaxError("Expected identifier for the test");
            it = new Fe(it, Tt, Dt);
          }
          return it;
        }
        function Rr() {
          let it = Xr();
          for (; Je(_.Pipe); ) {
            ++H;
            let Tt = ar();
            if (!(Tt instanceof S))
              throw new SyntaxError("Expected identifier for the filter");
            Je(_.OpenParen) && (Tt = Sr(Tt)), it = new ce(it, Tt);
          }
          return it;
        }
        function ar() {
          const it = L[H];
          switch (it.type) {
            case _.NumericLiteral:
              return ++H, new x(Number(it.value));
            case _.StringLiteral:
              return ++H, new F(it.value);
            case _.BooleanLiteral:
              return ++H, new le(it.value.toLowerCase() === "true");
            case _.NullLiteral:
              return ++H, new ne(null);
            case _.Identifier:
              return ++H, new S(it.value);
            case _.OpenParen: {
              ++H;
              const Tt = ys();
              if (L[H].type !== _.CloseParen)
                throw new SyntaxError(`Expected closing parenthesis, got ${L[H].type} instead`);
              return ++H, Tt;
            }
            case _.OpenSquareBracket: {
              ++H;
              const Tt = [];
              for (; !Je(_.CloseSquareBracket); )
                Tt.push(Ds()), Je(_.Comma) && ++H;
              return ++H, new be(Tt);
            }
            case _.OpenCurlyBracket: {
              ++H;
              const Tt = /* @__PURE__ */ new Map();
              for (; !Je(_.CloseCurlyBracket); ) {
                const Dt = Ds();
                me(_.Colon, "Expected colon between key and value in object literal");
                const Vs = Ds();
                Tt.set(Dt, Vs), Je(_.Comma) && ++H;
              }
              return ++H, new re(Tt);
            }
            default:
              throw new SyntaxError(`Unexpected token: ${it.type}`);
          }
        }
        for (; H < L.length; )
          oe.body.push(Ae());
        return oe;
      }
      function J(L, oe, H = 1) {
        oe === void 0 && (oe = L, L = 0);
        const me = [];
        for (let Ae = L; Ae < oe; Ae += H)
          me.push(Ae);
        return me;
      }
      function de(L, oe, H, me = 1) {
        const Ae = Math.sign(me);
        Ae >= 0 ? (oe = (oe ?? (oe = 0)) < 0 ? Math.max(L.length + oe, 0) : Math.min(oe, L.length), H = (H ?? (H = L.length)) < 0 ? Math.max(L.length + H, 0) : Math.min(H, L.length)) : (oe = (oe ?? (oe = L.length - 1)) < 0 ? Math.max(L.length + oe, -1) : Math.min(oe, L.length - 1), H = (H ?? (H = -1)) < -1 ? Math.max(L.length + H, -1) : Math.min(H, L.length - 1));
        const We = [];
        for (let Je = oe; Ae * Je < Ae * H; Je += me)
          We.push(L[Je]);
        return We;
      }
      function Ce(L) {
        return L.replace(/\b\w/g, (oe) => oe.toUpperCase());
      }
      var Be = class {
        /**
         * Creates a new RuntimeValue.
         */
        constructor(L = void 0) {
          fe(this, "type", "RuntimeValue");
          fe(this, "value");
          /**
           * A collection of built-in functions for this type.
           */
          fe(this, "builtins", /* @__PURE__ */ new Map());
          this.value = L;
        }
        /**
         * Determines truthiness or falsiness of the runtime value.
         * This function should be overridden by subclasses if it has custom truthiness criteria.
         * @returns {BooleanValue} BooleanValue(true) if the value is truthy, BooleanValue(false) otherwise.
         */
        __bool__() {
          return new Ke(!!this.value);
        }
      }, Ze = class extends Be {
        constructor() {
          super(...arguments);
          fe(this, "type", "NumericValue");
        }
      }, te = class extends Be {
        constructor() {
          super(...arguments);
          fe(this, "type", "StringValue");
          fe(this, "builtins", /* @__PURE__ */ new Map([
            [
              "upper",
              new Ve(() => new te(this.value.toUpperCase()))
            ],
            [
              "lower",
              new Ve(() => new te(this.value.toLowerCase()))
            ],
            [
              "strip",
              new Ve(() => new te(this.value.trim()))
            ],
            [
              "title",
              new Ve(() => new te(Ce(this.value)))
            ],
            ["length", new Ze(this.value.length)],
            [
              "rstrip",
              new Ve(() => new te(this.value.trimEnd()))
            ],
            [
              "lstrip",
              new Ve(() => new te(this.value.trimStart()))
            ]
          ]));
        }
      }, Ke = class extends Be {
        constructor() {
          super(...arguments);
          fe(this, "type", "BooleanValue");
        }
      }, je = class extends Be {
        constructor() {
          super(...arguments);
          fe(this, "type", "ObjectValue");
          fe(this, "builtins", /* @__PURE__ */ new Map([
            [
              "get",
              new Ve(([oe, H]) => {
                if (!(oe instanceof te))
                  throw new Error(`Object key must be a string: got ${oe.type}`);
                return this.value.get(oe.value) ?? H ?? new Ne();
              })
            ],
            [
              "items",
              new Ve(() => new Te(
                Array.from(this.value.entries()).map(([oe, H]) => new Te([new te(oe), H]))
              ))
            ]
          ]));
        }
        /**
         * NOTE: necessary to override since all JavaScript arrays are considered truthy,
         * while only non-empty Python arrays are consider truthy.
         *
         * e.g.,
         *  - JavaScript:  {} && 5 -> 5
         *  - Python:      {} and 5 -> {}
         */
        __bool__() {
          return new Ke(this.value.size > 0);
        }
      }, ae = class extends je {
        constructor() {
          super(...arguments);
          fe(this, "type", "KeywordArgumentsValue");
        }
      }, Te = class extends Be {
        constructor() {
          super(...arguments);
          fe(this, "type", "ArrayValue");
          fe(this, "builtins", /* @__PURE__ */ new Map([["length", new Ze(this.value.length)]]));
        }
        /**
         * NOTE: necessary to override since all JavaScript arrays are considered truthy,
         * while only non-empty Python arrays are consider truthy.
         *
         * e.g.,
         *  - JavaScript:  [] && 5 -> 5
         *  - Python:      [] and 5 -> []
         */
        __bool__() {
          return new Ke(this.value.length > 0);
        }
      }, Ue = class extends Te {
        constructor() {
          super(...arguments);
          fe(this, "type", "TupleValue");
        }
      }, Ve = class extends Be {
        constructor() {
          super(...arguments);
          fe(this, "type", "FunctionValue");
        }
      }, Ne = class extends Be {
        constructor() {
          super(...arguments);
          fe(this, "type", "NullValue");
        }
      }, Re = class extends Be {
        constructor() {
          super(...arguments);
          fe(this, "type", "UndefinedValue");
        }
      }, st = class {
        constructor(L) {
          /**
           * The variables declared in this environment.
           */
          fe(this, "variables", /* @__PURE__ */ new Map([
            [
              "namespace",
              new Ve((L) => {
                if (L.length === 0)
                  return new je(/* @__PURE__ */ new Map());
                if (L.length !== 1 || !(L[0] instanceof je))
                  throw new Error("`namespace` expects either zero arguments or a single object argument");
                return L[0];
              })
            ]
          ]));
          /**
           * The tests available in this environment.
           */
          fe(this, "tests", /* @__PURE__ */ new Map([
            ["boolean", (L) => L.type === "BooleanValue"],
            ["callable", (L) => L instanceof Ve],
            [
              "odd",
              (L) => {
                if (L.type !== "NumericValue")
                  throw new Error(`Cannot apply test "odd" to type: ${L.type}`);
                return L.value % 2 !== 0;
              }
            ],
            [
              "even",
              (L) => {
                if (L.type !== "NumericValue")
                  throw new Error(`Cannot apply test "even" to type: ${L.type}`);
                return L.value % 2 === 0;
              }
            ],
            ["false", (L) => L.type === "BooleanValue" && !L.value],
            ["true", (L) => L.type === "BooleanValue" && L.value],
            ["none", (L) => L.type === "NullValue"],
            ["string", (L) => L.type === "StringValue"],
            ["number", (L) => L.type === "NumericValue"],
            ["integer", (L) => L.type === "NumericValue" && Number.isInteger(L.value)],
            ["iterable", (L) => L.type === "ArrayValue" || L.type === "StringValue"],
            ["mapping", (L) => L.type === "ObjectValue"],
            [
              "lower",
              (L) => {
                const oe = L.value;
                return L.type === "StringValue" && oe === oe.toLowerCase();
              }
            ],
            [
              "upper",
              (L) => {
                const oe = L.value;
                return L.type === "StringValue" && oe === oe.toUpperCase();
              }
            ],
            ["none", (L) => L.type === "NullValue"],
            ["defined", (L) => L.type !== "UndefinedValue"],
            ["undefined", (L) => L.type === "UndefinedValue"],
            ["equalto", (L, oe) => L.value === oe.value],
            ["eq", (L, oe) => L.value === oe.value]
          ]));
          this.parent = L;
        }
        /**
         * Set the value of a variable in the current environment.
         */
        set(L, oe) {
          return this.declareVariable(L, ct(oe));
        }
        declareVariable(L, oe) {
          if (this.variables.has(L))
            throw new SyntaxError(`Variable already declared: ${L}`);
          return this.variables.set(L, oe), oe;
        }
        // private assignVariable(name: string, value: AnyRuntimeValue): AnyRuntimeValue {
        // 	const env = this.resolve(name);
        // 	env.variables.set(name, value);
        // 	return value;
        // }
        /**
         * Set variable in the current scope.
         * See https://jinja.palletsprojects.com/en/3.0.x/templates/#assignments for more information.
         */
        setVariable(L, oe) {
          return this.variables.set(L, oe), oe;
        }
        /**
         * Resolve the environment in which the variable is declared.
         * @param {string} name The name of the variable.
         * @returns {Environment} The environment in which the variable is declared.
         */
        resolve(L) {
          if (this.variables.has(L))
            return this;
          if (this.parent)
            return this.parent.resolve(L);
          throw new Error(`Unknown variable: ${L}`);
        }
        lookupVariable(L) {
          try {
            return this.resolve(L).variables.get(L) ?? new Re();
          } catch {
            return new Re();
          }
        }
      }, dt = class {
        constructor(L) {
          fe(this, "global");
          this.global = L ?? new st();
        }
        /**
         * Run the program.
         */
        run(L) {
          return this.evaluate(L, this.global);
        }
        /**
         * Evaluates expressions following the binary operation type.
         */
        evaluateBinaryExpression(L, oe) {
          const H = this.evaluate(L.left, oe);
          switch (L.operator.value) {
            case "and":
              return H.__bool__().value ? this.evaluate(L.right, oe) : H;
            case "or":
              return H.__bool__().value ? H : this.evaluate(L.right, oe);
          }
          const me = this.evaluate(L.right, oe);
          switch (L.operator.value) {
            case "==":
              return new Ke(H.value == me.value);
            case "!=":
              return new Ke(H.value != me.value);
          }
          if (H instanceof Re || me instanceof Re)
            throw new Error("Cannot perform operation on undefined values");
          if (H instanceof Ne || me instanceof Ne)
            throw new Error("Cannot perform operation on null values");
          if (H instanceof Ze && me instanceof Ze)
            switch (L.operator.value) {
              case "+":
                return new Ze(H.value + me.value);
              case "-":
                return new Ze(H.value - me.value);
              case "*":
                return new Ze(H.value * me.value);
              case "/":
                return new Ze(H.value / me.value);
              case "%":
                return new Ze(H.value % me.value);
              case "<":
                return new Ke(H.value < me.value);
              case ">":
                return new Ke(H.value > me.value);
              case ">=":
                return new Ke(H.value >= me.value);
              case "<=":
                return new Ke(H.value <= me.value);
            }
          else if (H instanceof Te && me instanceof Te)
            switch (L.operator.value) {
              case "+":
                return new Te(H.value.concat(me.value));
            }
          else if (me instanceof Te) {
            const Ae = me.value.find((We) => We.value === H.value) !== void 0;
            switch (L.operator.value) {
              case "in":
                return new Ke(Ae);
              case "not in":
                return new Ke(!Ae);
            }
          }
          if (H instanceof te || me instanceof te)
            switch (L.operator.value) {
              case "+":
                return new te(H.value.toString() + me.value.toString());
            }
          if (H instanceof te && me instanceof te)
            switch (L.operator.value) {
              case "in":
                return new Ke(me.value.includes(H.value));
              case "not in":
                return new Ke(!me.value.includes(H.value));
            }
          if (H instanceof te && me instanceof je)
            switch (L.operator.value) {
              case "in":
                return new Ke(me.value.has(H.value));
              case "not in":
                return new Ke(!me.value.has(H.value));
            }
          throw new SyntaxError(`Unknown operator "${L.operator.value}" between ${H.type} and ${me.type}`);
        }
        evaluateArguments(L, oe) {
          const H = [], me = /* @__PURE__ */ new Map();
          for (const Ae of L)
            if (Ae.type === "KeywordArgumentExpression") {
              const We = Ae;
              me.set(We.key.value, this.evaluate(We.value, oe));
            } else {
              if (me.size > 0)
                throw new Error("Positional arguments must come before keyword arguments");
              H.push(this.evaluate(Ae, oe));
            }
          return [H, me];
        }
        /**
         * Evaluates expressions following the filter operation type.
         */
        evaluateFilterExpression(L, oe) {
          const H = this.evaluate(L.operand, oe);
          if (L.filter.type === "Identifier") {
            const me = L.filter;
            if (me.value === "tojson")
              return new te(lt(H));
            if (H instanceof Te)
              switch (me.value) {
                case "list":
                  return H;
                case "first":
                  return H.value[0];
                case "last":
                  return H.value[H.value.length - 1];
                case "length":
                  return new Ze(H.value.length);
                case "reverse":
                  return new Te(H.value.reverse());
                case "sort":
                  return new Te(
                    H.value.sort((Ae, We) => {
                      if (Ae.type !== We.type)
                        throw new Error(`Cannot compare different types: ${Ae.type} and ${We.type}`);
                      switch (Ae.type) {
                        case "NumericValue":
                          return Ae.value - We.value;
                        case "StringValue":
                          return Ae.value.localeCompare(We.value);
                        default:
                          throw new Error(`Cannot compare type: ${Ae.type}`);
                      }
                    })
                  );
                default:
                  throw new Error(`Unknown ArrayValue filter: ${me.value}`);
              }
            else if (H instanceof te)
              switch (me.value) {
                case "length":
                  return new Ze(H.value.length);
                case "upper":
                  return new te(H.value.toUpperCase());
                case "lower":
                  return new te(H.value.toLowerCase());
                case "title":
                  return new te(Ce(H.value));
                case "capitalize":
                  return new te(H.value.charAt(0).toUpperCase() + H.value.slice(1));
                case "trim":
                  return new te(H.value.trim());
                case "indent":
                  return new te(
                    H.value.split(`
`).map(
                      (Ae, We) => (
                        // By default, don't indent the first line or empty lines
                        We === 0 || Ae.length === 0 ? Ae : "    " + Ae
                      )
                    ).join(`
`)
                  );
                case "string":
                  return H;
                default:
                  throw new Error(`Unknown StringValue filter: ${me.value}`);
              }
            else if (H instanceof Ze)
              switch (me.value) {
                case "abs":
                  return new Ze(Math.abs(H.value));
                default:
                  throw new Error(`Unknown NumericValue filter: ${me.value}`);
              }
            else if (H instanceof je)
              switch (me.value) {
                case "items":
                  return new Te(
                    Array.from(H.value.entries()).map(([Ae, We]) => new Te([new te(Ae), We]))
                  );
                case "length":
                  return new Ze(H.value.size);
                default:
                  throw new Error(`Unknown ObjectValue filter: ${me.value}`);
              }
            throw new Error(`Cannot apply filter "${me.value}" to type: ${H.type}`);
          } else if (L.filter.type === "CallExpression") {
            const me = L.filter;
            if (me.callee.type !== "Identifier")
              throw new Error(`Unknown filter: ${me.callee.type}`);
            const Ae = me.callee.value;
            if (Ae === "tojson") {
              const [, We] = this.evaluateArguments(me.args, oe), Je = We.get("indent") ?? new Ne();
              if (!(Je instanceof Ze || Je instanceof Ne))
                throw new Error("If set, indent must be a number");
              return new te(lt(H, Je.value));
            }
            if (H instanceof Te) {
              switch (Ae) {
                case "selectattr":
                case "rejectattr": {
                  const We = Ae === "selectattr";
                  if (H.value.some((At) => !(At instanceof je)))
                    throw new Error(`\`${Ae}\` can only be applied to array of objects`);
                  if (me.args.some((At) => At.type !== "StringLiteral"))
                    throw new Error(`arguments of \`${Ae}\` must be strings`);
                  const [Je, ut, mt] = me.args.map((At) => this.evaluate(At, oe));
                  let vt;
                  if (ut) {
                    const At = oe.tests.get(ut.value);
                    if (!At)
                      throw new Error(`Unknown test: ${ut.value}`);
                    vt = At;
                  } else
                    vt = (...At) => At[0].__bool__().value;
                  const kt = H.value.filter((At) => {
                    const is = At.value.get(Je.value), ys = is ? vt(is, mt) : !1;
                    return We ? ys : !ys;
                  });
                  return new Te(kt);
                }
                case "map": {
                  const [, We] = this.evaluateArguments(me.args, oe);
                  if (We.has("attribute")) {
                    const Je = We.get("attribute");
                    if (!(Je instanceof te))
                      throw new Error("attribute must be a string");
                    const ut = We.get("default"), mt = H.value.map((vt) => {
                      if (!(vt instanceof je))
                        throw new Error("items in map must be an object");
                      return vt.value.get(Je.value) ?? ut ?? new Re();
                    });
                    return new Te(mt);
                  } else
                    throw new Error("`map` expressions without `attribute` set are not currently supported.");
                }
              }
              throw new Error(`Unknown ArrayValue filter: ${Ae}`);
            } else if (H instanceof te) {
              switch (Ae) {
                case "indent": {
                  const [We, Je] = this.evaluateArguments(me.args, oe), ut = We.at(0) ?? Je.get("width") ?? new Ze(4);
                  if (!(ut instanceof Ze))
                    throw new Error("width must be a number");
                  const mt = We.at(1) ?? Je.get("first") ?? new Ke(!1), vt = We.at(2) ?? Je.get("blank") ?? new Ke(!1), kt = H.value.split(`
`), At = " ".repeat(ut.value), is = kt.map(
                    (ys, Cs) => !mt.value && Cs === 0 || !vt.value && ys.length === 0 ? ys : At + ys
                  );
                  return new te(is.join(`
`));
                }
              }
              throw new Error(`Unknown StringValue filter: ${Ae}`);
            } else
              throw new Error(`Cannot apply filter "${Ae}" to type: ${H.type}`);
          }
          throw new Error(`Unknown filter: ${L.filter.type}`);
        }
        /**
         * Evaluates expressions following the test operation type.
         */
        evaluateTestExpression(L, oe) {
          const H = this.evaluate(L.operand, oe), me = oe.tests.get(L.test.value);
          if (!me)
            throw new Error(`Unknown test: ${L.test.value}`);
          const Ae = me(H);
          return new Ke(L.negate ? !Ae : Ae);
        }
        /**
         * Evaluates expressions following the unary operation type.
         */
        evaluateUnaryExpression(L, oe) {
          const H = this.evaluate(L.argument, oe);
          switch (L.operator.value) {
            case "not":
              return new Ke(!H.value);
            default:
              throw new SyntaxError(`Unknown operator: ${L.operator.value}`);
          }
        }
        evalProgram(L, oe) {
          return this.evaluateBlock(L.body, oe);
        }
        evaluateBlock(L, oe) {
          let H = "";
          for (const me of L) {
            const Ae = this.evaluate(me, oe);
            Ae.type !== "NullValue" && Ae.type !== "UndefinedValue" && (H += Ae.value);
          }
          return new te(H);
        }
        evaluateIdentifier(L, oe) {
          return oe.lookupVariable(L.value);
        }
        evaluateCallExpression(L, oe) {
          const [H, me] = this.evaluateArguments(L.args, oe);
          me.size > 0 && H.push(new ae(me));
          const Ae = this.evaluate(L.callee, oe);
          if (Ae.type !== "FunctionValue")
            throw new Error(`Cannot call something that is not a function: got ${Ae.type}`);
          return Ae.value(H, oe);
        }
        evaluateSliceExpression(L, oe, H) {
          if (!(L instanceof Te || L instanceof te))
            throw new Error("Slice object must be an array or string");
          const me = this.evaluate(oe.start, H), Ae = this.evaluate(oe.stop, H), We = this.evaluate(oe.step, H);
          if (!(me instanceof Ze || me instanceof Re))
            throw new Error("Slice start must be numeric or undefined");
          if (!(Ae instanceof Ze || Ae instanceof Re))
            throw new Error("Slice stop must be numeric or undefined");
          if (!(We instanceof Ze || We instanceof Re))
            throw new Error("Slice step must be numeric or undefined");
          return L instanceof Te ? new Te(de(L.value, me.value, Ae.value, We.value)) : new te(de(Array.from(L.value), me.value, Ae.value, We.value).join(""));
        }
        evaluateMemberExpression(L, oe) {
          const H = this.evaluate(L.object, oe);
          let me;
          if (L.computed) {
            if (L.property.type === "SliceExpression")
              return this.evaluateSliceExpression(H, L.property, oe);
            me = this.evaluate(L.property, oe);
          } else
            me = new te(L.property.value);
          let Ae;
          if (H instanceof je) {
            if (!(me instanceof te))
              throw new Error(`Cannot access property with non-string: got ${me.type}`);
            Ae = H.value.get(me.value) ?? H.builtins.get(me.value);
          } else if (H instanceof Te || H instanceof te)
            if (me instanceof Ze)
              Ae = H.value.at(me.value), H instanceof te && (Ae = new te(H.value.at(me.value)));
            else if (me instanceof te)
              Ae = H.builtins.get(me.value);
            else
              throw new Error(`Cannot access property with non-string/non-number: got ${me.type}`);
          else {
            if (!(me instanceof te))
              throw new Error(`Cannot access property with non-string: got ${me.type}`);
            Ae = H.builtins.get(me.value);
          }
          return Ae instanceof Be ? Ae : new Re();
        }
        evaluateSet(L, oe) {
          const H = this.evaluate(L.value, oe);
          if (L.assignee.type === "Identifier") {
            const me = L.assignee.value;
            oe.setVariable(me, H);
          } else if (L.assignee.type === "MemberExpression") {
            const me = L.assignee, Ae = this.evaluate(me.object, oe);
            if (!(Ae instanceof je))
              throw new Error("Cannot assign to member of non-object");
            if (me.property.type !== "Identifier")
              throw new Error("Cannot assign to member with non-identifier property");
            Ae.value.set(me.property.value, H);
          } else
            throw new Error(`Invalid LHS inside assignment expression: ${JSON.stringify(L.assignee)}`);
          return new Ne();
        }
        evaluateIf(L, oe) {
          const H = this.evaluate(L.test, oe);
          return this.evaluateBlock(H.__bool__().value ? L.body : L.alternate, oe);
        }
        evaluateFor(L, oe) {
          const H = new st(oe);
          let me, Ae;
          if (L.iterable.type === "SelectExpression") {
            const vt = L.iterable;
            Ae = this.evaluate(vt.iterable, H), me = vt.test;
          } else
            Ae = this.evaluate(L.iterable, H);
          if (!(Ae instanceof Te))
            throw new Error(`Expected iterable type in for loop: got ${Ae.type}`);
          const We = [], Je = [];
          for (let vt = 0; vt < Ae.value.length; ++vt) {
            const kt = new st(H), At = Ae.value[vt];
            let is;
            if (L.loopvar.type === "Identifier")
              is = (ys) => ys.setVariable(L.loopvar.value, At);
            else if (L.loopvar.type === "TupleLiteral") {
              const ys = L.loopvar;
              if (At.type !== "ArrayValue")
                throw new Error(`Cannot unpack non-iterable type: ${At.type}`);
              const Cs = At;
              if (ys.value.length !== Cs.value.length)
                throw new Error(`Too ${ys.value.length > Cs.value.length ? "few" : "many"} items to unpack`);
              is = (Ds) => {
                for (let sr = 0; sr < ys.value.length; ++sr) {
                  if (ys.value[sr].type !== "Identifier")
                    throw new Error(`Cannot unpack non-identifier type: ${ys.value[sr].type}`);
                  Ds.setVariable(ys.value[sr].value, Cs.value[sr]);
                }
              };
            } else
              throw new Error(`Invalid loop variable(s): ${L.loopvar.type}`);
            me && (is(kt), !this.evaluate(me, kt).__bool__().value) || (We.push(At), Je.push(is));
          }
          let ut = "", mt = !0;
          for (let vt = 0; vt < We.length; ++vt) {
            const kt = /* @__PURE__ */ new Map([
              ["index", new Ze(vt + 1)],
              ["index0", new Ze(vt)],
              ["revindex", new Ze(We.length - vt)],
              ["revindex0", new Ze(We.length - vt - 1)],
              ["first", new Ke(vt === 0)],
              ["last", new Ke(vt === We.length - 1)],
              ["length", new Ze(We.length)],
              ["previtem", vt > 0 ? We[vt - 1] : new Re()],
              ["nextitem", vt < We.length - 1 ? We[vt + 1] : new Re()]
            ]);
            H.setVariable("loop", new je(kt)), Je[vt](H);
            const At = this.evaluateBlock(L.body, H);
            ut += At.value, mt = !1;
          }
          if (mt) {
            const vt = this.evaluateBlock(L.defaultBlock, H);
            ut += vt.value;
          }
          return new te(ut);
        }
        /**
         * See https://jinja.palletsprojects.com/en/3.1.x/templates/#macros for more information.
         */
        evaluateMacro(L, oe) {
          return oe.setVariable(
            L.name.value,
            new Ve((H, me) => {
              var Je;
              const Ae = new st(me);
              H = H.slice();
              let We;
              ((Je = H.at(-1)) == null ? void 0 : Je.type) === "KeywordArgumentsValue" && (We = H.pop());
              for (let ut = 0; ut < L.args.length; ++ut) {
                const mt = L.args[ut], vt = H[ut];
                if (mt.type === "Identifier") {
                  const kt = mt;
                  if (!vt)
                    throw new Error(`Missing positional argument: ${kt.value}`);
                  Ae.setVariable(kt.value, vt);
                } else if (mt.type === "KeywordArgumentExpression") {
                  const kt = mt, At = vt ?? // Try positional arguments first
                  (We == null ? void 0 : We.value.get(kt.key.value)) ?? // Look in user-passed kwargs
                  this.evaluate(kt.value, Ae);
                  Ae.setVariable(kt.key.value, At);
                } else
                  throw new Error(`Unknown argument type: ${mt.type}`);
              }
              return this.evaluateBlock(L.body, Ae);
            })
          ), new Ne();
        }
        evaluate(L, oe) {
          if (L === void 0)
            return new Re();
          switch (L.type) {
            case "Program":
              return this.evalProgram(L, oe);
            case "Set":
              return this.evaluateSet(L, oe);
            case "If":
              return this.evaluateIf(L, oe);
            case "For":
              return this.evaluateFor(L, oe);
            case "Macro":
              return this.evaluateMacro(L, oe);
            case "NumericLiteral":
              return new Ze(Number(L.value));
            case "StringLiteral":
              return new te(L.value);
            case "BooleanLiteral":
              return new Ke(L.value);
            case "NullLiteral":
              return new Ne(L.value);
            case "ArrayLiteral":
              return new Te(L.value.map((H) => this.evaluate(H, oe)));
            case "TupleLiteral":
              return new Ue(L.value.map((H) => this.evaluate(H, oe)));
            case "ObjectLiteral": {
              const H = /* @__PURE__ */ new Map();
              for (const [me, Ae] of L.value) {
                const We = this.evaluate(me, oe);
                if (!(We instanceof te))
                  throw new Error(`Object keys must be strings: got ${We.type}`);
                H.set(We.value, this.evaluate(Ae, oe));
              }
              return new je(H);
            }
            case "Identifier":
              return this.evaluateIdentifier(L, oe);
            case "CallExpression":
              return this.evaluateCallExpression(L, oe);
            case "MemberExpression":
              return this.evaluateMemberExpression(L, oe);
            case "UnaryExpression":
              return this.evaluateUnaryExpression(L, oe);
            case "BinaryExpression":
              return this.evaluateBinaryExpression(L, oe);
            case "FilterExpression":
              return this.evaluateFilterExpression(L, oe);
            case "TestExpression":
              return this.evaluateTestExpression(L, oe);
            default:
              throw new SyntaxError(`Unknown node type: ${L.type}`);
          }
        }
      };
      function ct(L) {
        switch (typeof L) {
          case "number":
            return new Ze(L);
          case "string":
            return new te(L);
          case "boolean":
            return new Ke(L);
          case "undefined":
            return new Re();
          case "object":
            return L === null ? new Ne() : Array.isArray(L) ? new Te(L.map(ct)) : new je(
              new Map(Object.entries(L).map(([oe, H]) => [oe, ct(H)]))
            );
          case "function":
            return new Ve((oe, H) => {
              const me = L(...oe.map((Ae) => Ae.value)) ?? null;
              return ct(me);
            });
          default:
            throw new Error(`Cannot convert to runtime value: ${L}`);
        }
      }
      function lt(L, oe, H) {
        const me = H ?? 0;
        switch (L.type) {
          case "NullValue":
          case "UndefinedValue":
            return "null";
          case "NumericValue":
          case "StringValue":
          case "BooleanValue":
            return JSON.stringify(L.value);
          case "ArrayValue":
          case "ObjectValue": {
            const Ae = oe ? " ".repeat(oe) : "", We = `
` + Ae.repeat(me), Je = We + Ae;
            if (L.type === "ArrayValue") {
              const ut = L.value.map((mt) => lt(mt, oe, me + 1));
              return oe ? `[${Je}${ut.join(`,${Je}`)}${We}]` : `[${ut.join(", ")}]`;
            } else {
              const ut = Array.from(L.value.entries()).map(([mt, vt]) => {
                const kt = `"${mt}": ${lt(vt, oe, me + 1)}`;
                return oe ? `${Je}${kt}` : kt;
              });
              return oe ? `{${ut.join(",")}${We}}` : `{${ut.join(", ")}}`;
            }
          }
          default:
            throw new Error(`Cannot convert to JSON: ${L.type}`);
        }
      }
      var ht = class {
        /**
         * @param {string} template The template string
         */
        constructor(L) {
          fe(this, "parsed");
          const oe = y(L, {
            lstrip_blocks: !0,
            trim_blocks: !0
          });
          this.parsed = ye(oe);
        }
        render(L) {
          const oe = new st();
          oe.set("false", !1), oe.set("true", !0), oe.set("raise_exception", (Ae) => {
            throw new Error(Ae);
          }), oe.set("range", J);
          for (const [Ae, We] of Object.entries(L))
            oe.set(Ae, We);
          return new dt(oe).run(this.parsed).value;
        }
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/backend-impl.js": (
    /*!******************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/backend-impl.js ***!
      \******************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        registerBackend: () => (
          /* binding */
          U
        ),
        /* harmony export */
        resolveBackendAndExecutionProviders: () => (
          /* binding */
          R
        )
        /* harmony export */
      });
      const _ = /* @__PURE__ */ new Map(), D = [], U = (g, v, M) => {
        if (v && typeof v.init == "function" && typeof v.createInferenceSessionHandler == "function") {
          const y = _.get(g);
          if (y === void 0)
            _.set(g, { backend: v, priority: M });
          else {
            if (y.priority > M)
              return;
            if (y.priority === M && y.backend !== v)
              throw new Error(`cannot register backend "${g}" using priority ${M}`);
          }
          if (M >= 0) {
            const b = D.indexOf(g);
            b !== -1 && D.splice(b, 1);
            for (let I = 0; I < D.length; I++)
              if (_.get(D[I]).priority <= M) {
                D.splice(I, 0, g);
                return;
              }
            D.push(g);
          }
          return;
        }
        throw new TypeError("not a valid backend");
      }, Y = async (g) => {
        const v = _.get(g);
        if (!v)
          return "backend not found.";
        if (v.initialized)
          return v.backend;
        if (v.aborted)
          return v.error;
        {
          const M = !!v.initPromise;
          try {
            return M || (v.initPromise = v.backend.init(g)), await v.initPromise, v.initialized = !0, v.backend;
          } catch (y) {
            return M || (v.error = `${y}`, v.aborted = !0), v.error;
          } finally {
            delete v.initPromise;
          }
        }
      }, R = async (g) => {
        const v = g.executionProviders || [], M = v.map((ie) => typeof ie == "string" ? ie : ie.name), y = M.length === 0 ? D : M;
        let b;
        const I = [], K = /* @__PURE__ */ new Set();
        for (const ie of y) {
          const W = await Y(ie);
          typeof W == "string" ? I.push({ name: ie, err: W }) : (b || (b = W), b === W && K.add(ie));
        }
        if (!b)
          throw new Error(`no available backend found. ERR: ${I.map((ie) => `[${ie.name}] ${ie.err}`).join(", ")}`);
        for (const { name: ie, err: W } of I)
          M.includes(ie) && console.warn(`removing requested execution provider "${ie}" from session options because it is not available: ${W}`);
        const se = v.filter((ie) => K.has(typeof ie == "string" ? ie : ie.name));
        return [
          b,
          new Proxy(g, {
            get: (ie, W) => W === "executionProviders" ? se : Reflect.get(ie, W)
          })
        ];
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/backend.js": (
    /*!*************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/backend.js ***!
      \*************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        registerBackend: () => (
          /* reexport safe */
          _.registerBackend
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./backend-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/backend-impl.js"
      );
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/env-impl.js": (
    /*!**************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/env-impl.js ***!
      \**************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        env: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./version.js */
        "./node_modules/onnxruntime-common/dist/esm/version.js"
      );
      let D = "warning";
      const U = {
        wasm: {},
        webgl: {},
        webgpu: {},
        versions: { common: _.version },
        set logLevel(Y) {
          if (Y !== void 0) {
            if (typeof Y != "string" || ["verbose", "info", "warning", "error", "fatal"].indexOf(Y) === -1)
              throw new Error(`Unsupported logging level: ${Y}`);
            D = Y;
          }
        },
        get logLevel() {
          return D;
        }
      };
      Object.defineProperty(U, "logLevel", { enumerable: !0 });
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/env.js": (
    /*!*********************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/env.js ***!
      \*********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        env: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./env-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/env-impl.js"
      );
      const D = _.env;
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/index.js": (
    /*!***********************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/index.js ***!
      \***********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        InferenceSession: () => (
          /* reexport safe */
          U.InferenceSession
        ),
        /* harmony export */
        TRACE: () => (
          /* reexport safe */
          R.TRACE
        ),
        /* harmony export */
        TRACE_FUNC_BEGIN: () => (
          /* reexport safe */
          R.TRACE_FUNC_BEGIN
        ),
        /* harmony export */
        TRACE_FUNC_END: () => (
          /* reexport safe */
          R.TRACE_FUNC_END
        ),
        /* harmony export */
        Tensor: () => (
          /* reexport safe */
          Y.Tensor
        ),
        /* harmony export */
        TrainingSession: () => (
          /* reexport safe */
          g.TrainingSession
        ),
        /* harmony export */
        env: () => (
          /* reexport safe */
          D.env
        ),
        /* harmony export */
        registerBackend: () => (
          /* reexport safe */
          _.registerBackend
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./backend.js */
        "./node_modules/onnxruntime-common/dist/esm/backend.js"
      ), D = r(
        /*! ./env.js */
        "./node_modules/onnxruntime-common/dist/esm/env.js"
      ), U = r(
        /*! ./inference-session.js */
        "./node_modules/onnxruntime-common/dist/esm/inference-session.js"
      ), Y = r(
        /*! ./tensor.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor.js"
      );
      r(
        /*! ./tensor-conversion.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-conversion.js"
      ), r(
        /*! ./tensor-factory.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-factory.js"
      );
      var R = r(
        /*! ./trace.js */
        "./node_modules/onnxruntime-common/dist/esm/trace.js"
      );
      r(
        /*! ./onnx-model.js */
        "./node_modules/onnxruntime-common/dist/esm/onnx-model.js"
      ), r(
        /*! ./onnx-value.js */
        "./node_modules/onnxruntime-common/dist/esm/onnx-value.js"
      );
      var g = r(
        /*! ./training-session.js */
        "./node_modules/onnxruntime-common/dist/esm/training-session.js"
      );
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/inference-session-impl.js": (
    /*!****************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/inference-session-impl.js ***!
      \****************************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        InferenceSession: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./backend-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/backend-impl.js"
      ), D = r(
        /*! ./tensor.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor.js"
      ), U = r(
        /*! ./trace.js */
        "./node_modules/onnxruntime-common/dist/esm/trace.js"
      );
      class Y {
        constructor(g) {
          this.handler = g;
        }
        async run(g, v, M) {
          (0, U.TRACE_FUNC_BEGIN)();
          const y = {};
          let b = {};
          if (typeof g != "object" || g === null || g instanceof D.Tensor || Array.isArray(g))
            throw new TypeError("'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.");
          let I = !0;
          if (typeof v == "object") {
            if (v === null)
              throw new TypeError("Unexpected argument[1]: cannot be null.");
            if (v instanceof D.Tensor)
              throw new TypeError("'fetches' cannot be a Tensor");
            if (Array.isArray(v)) {
              if (v.length === 0)
                throw new TypeError("'fetches' cannot be an empty array.");
              I = !1;
              for (const ie of v) {
                if (typeof ie != "string")
                  throw new TypeError("'fetches' must be a string array or an object.");
                if (this.outputNames.indexOf(ie) === -1)
                  throw new RangeError(`'fetches' contains invalid output name: ${ie}.`);
                y[ie] = null;
              }
              if (typeof M == "object" && M !== null)
                b = M;
              else if (typeof M < "u")
                throw new TypeError("'options' must be an object.");
            } else {
              let ie = !1;
              const W = Object.getOwnPropertyNames(v);
              for (const j of this.outputNames)
                if (W.indexOf(j) !== -1) {
                  const q = v[j];
                  (q === null || q instanceof D.Tensor) && (ie = !0, I = !1, y[j] = q);
                }
              if (ie) {
                if (typeof M == "object" && M !== null)
                  b = M;
                else if (typeof M < "u")
                  throw new TypeError("'options' must be an object.");
              } else
                b = v;
            }
          } else if (typeof v < "u")
            throw new TypeError("Unexpected argument[1]: must be 'fetches' or 'options'.");
          for (const ie of this.inputNames)
            if (typeof g[ie] > "u")
              throw new Error(`input '${ie}' is missing in 'feeds'.`);
          if (I)
            for (const ie of this.outputNames)
              y[ie] = null;
          const K = await this.handler.run(g, y, b), se = {};
          for (const ie in K)
            if (Object.hasOwnProperty.call(K, ie)) {
              const W = K[ie];
              W instanceof D.Tensor ? se[ie] = W : se[ie] = new D.Tensor(W.type, W.data, W.dims);
            }
          return (0, U.TRACE_FUNC_END)(), se;
        }
        async release() {
          return this.handler.dispose();
        }
        static async create(g, v, M, y) {
          (0, U.TRACE_FUNC_BEGIN)();
          let b, I = {};
          if (typeof g == "string") {
            if (b = g, typeof v == "object" && v !== null)
              I = v;
            else if (typeof v < "u")
              throw new TypeError("'options' must be an object.");
          } else if (g instanceof Uint8Array) {
            if (b = g, typeof v == "object" && v !== null)
              I = v;
            else if (typeof v < "u")
              throw new TypeError("'options' must be an object.");
          } else if (g instanceof ArrayBuffer || typeof SharedArrayBuffer < "u" && g instanceof SharedArrayBuffer) {
            const W = g;
            let j = 0, q = g.byteLength;
            if (typeof v == "object" && v !== null)
              I = v;
            else if (typeof v == "number") {
              if (j = v, !Number.isSafeInteger(j))
                throw new RangeError("'byteOffset' must be an integer.");
              if (j < 0 || j >= W.byteLength)
                throw new RangeError(`'byteOffset' is out of range [0, ${W.byteLength}).`);
              if (q = g.byteLength - j, typeof M == "number") {
                if (q = M, !Number.isSafeInteger(q))
                  throw new RangeError("'byteLength' must be an integer.");
                if (q <= 0 || j + q > W.byteLength)
                  throw new RangeError(`'byteLength' is out of range (0, ${W.byteLength - j}].`);
                if (typeof y == "object" && y !== null)
                  I = y;
                else if (typeof y < "u")
                  throw new TypeError("'options' must be an object.");
              } else if (typeof M < "u")
                throw new TypeError("'byteLength' must be a number.");
            } else if (typeof v < "u")
              throw new TypeError("'options' must be an object.");
            b = new Uint8Array(W, j, q);
          } else
            throw new TypeError("Unexpected argument[0]: must be 'path' or 'buffer'.");
          const [K, se] = await (0, _.resolveBackendAndExecutionProviders)(I), ie = await K.createInferenceSessionHandler(b, se);
          return (0, U.TRACE_FUNC_END)(), new Y(ie);
        }
        startProfiling() {
          this.handler.startProfiling();
        }
        endProfiling() {
          this.handler.endProfiling();
        }
        get inputNames() {
          return this.handler.inputNames;
        }
        get outputNames() {
          return this.handler.outputNames;
        }
      }
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/inference-session.js": (
    /*!***********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/inference-session.js ***!
      \***********************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        InferenceSession: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./inference-session-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/inference-session-impl.js"
      );
      const D = _.InferenceSession;
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/onnx-model.js": (
    /*!****************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/onnx-model.js ***!
      \****************************************************************/
    /***/
    ($e, $, r) => {
      r.r($);
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/onnx-value.js": (
    /*!****************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/onnx-value.js ***!
      \****************************************************************/
    /***/
    ($e, $, r) => {
      r.r($);
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-conversion-impl.js": (
    /*!****************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-conversion-impl.js ***!
      \****************************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        tensorToDataURL: () => (
          /* binding */
          _
        ),
        /* harmony export */
        tensorToImageData: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      const _ = (U, Y) => {
        const R = typeof document < "u" ? document.createElement("canvas") : new OffscreenCanvas(1, 1);
        R.width = U.dims[3], R.height = U.dims[2];
        const g = R.getContext("2d");
        if (g != null) {
          let v, M;
          (Y == null ? void 0 : Y.tensorLayout) !== void 0 && Y.tensorLayout === "NHWC" ? (v = U.dims[2], M = U.dims[3]) : (v = U.dims[3], M = U.dims[2]);
          const y = (Y == null ? void 0 : Y.format) !== void 0 ? Y.format : "RGB", b = Y == null ? void 0 : Y.norm;
          let I, K;
          b === void 0 || b.mean === void 0 ? I = [255, 255, 255, 255] : typeof b.mean == "number" ? I = [b.mean, b.mean, b.mean, b.mean] : (I = [b.mean[0], b.mean[1], b.mean[2], 0], b.mean[3] !== void 0 && (I[3] = b.mean[3])), b === void 0 || b.bias === void 0 ? K = [0, 0, 0, 0] : typeof b.bias == "number" ? K = [b.bias, b.bias, b.bias, b.bias] : (K = [b.bias[0], b.bias[1], b.bias[2], 0], b.bias[3] !== void 0 && (K[3] = b.bias[3]));
          const se = M * v;
          let ie = 0, W = se, j = se * 2, q = -1;
          y === "RGBA" ? (ie = 0, W = se, j = se * 2, q = se * 3) : y === "RGB" ? (ie = 0, W = se, j = se * 2) : y === "RBG" && (ie = 0, j = se, W = se * 2);
          for (let A = 0; A < M; A++)
            for (let S = 0; S < v; S++) {
              const w = (U.data[ie++] - K[0]) * I[0], x = (U.data[W++] - K[1]) * I[1], F = (U.data[j++] - K[2]) * I[2], le = q === -1 ? 255 : (U.data[q++] - K[3]) * I[3];
              g.fillStyle = "rgba(" + w + "," + x + "," + F + "," + le + ")", g.fillRect(S, A, 1, 1);
            }
          if ("toDataURL" in R)
            return R.toDataURL();
          throw new Error("toDataURL is not supported");
        } else
          throw new Error("Can not access image data");
      }, D = (U, Y) => {
        const R = typeof document < "u" ? document.createElement("canvas").getContext("2d") : new OffscreenCanvas(1, 1).getContext("2d");
        let g;
        if (R != null) {
          let v, M, y;
          (Y == null ? void 0 : Y.tensorLayout) !== void 0 && Y.tensorLayout === "NHWC" ? (v = U.dims[2], M = U.dims[1], y = U.dims[3]) : (v = U.dims[3], M = U.dims[2], y = U.dims[1]);
          const b = Y !== void 0 && Y.format !== void 0 ? Y.format : "RGB", I = Y == null ? void 0 : Y.norm;
          let K, se;
          I === void 0 || I.mean === void 0 ? K = [255, 255, 255, 255] : typeof I.mean == "number" ? K = [I.mean, I.mean, I.mean, I.mean] : (K = [I.mean[0], I.mean[1], I.mean[2], 255], I.mean[3] !== void 0 && (K[3] = I.mean[3])), I === void 0 || I.bias === void 0 ? se = [0, 0, 0, 0] : typeof I.bias == "number" ? se = [I.bias, I.bias, I.bias, I.bias] : (se = [I.bias[0], I.bias[1], I.bias[2], 0], I.bias[3] !== void 0 && (se[3] = I.bias[3]));
          const ie = M * v;
          if (Y !== void 0 && (Y.format !== void 0 && y === 4 && Y.format !== "RGBA" || y === 3 && Y.format !== "RGB" && Y.format !== "BGR"))
            throw new Error("Tensor format doesn't match input tensor dims");
          const W = 4;
          let j = 0, q = 1, A = 2, S = 3, w = 0, x = ie, F = ie * 2, le = -1;
          b === "RGBA" ? (w = 0, x = ie, F = ie * 2, le = ie * 3) : b === "RGB" ? (w = 0, x = ie, F = ie * 2) : b === "RBG" && (w = 0, F = ie, x = ie * 2), g = R.createImageData(v, M);
          for (let ne = 0; ne < M * v; j += W, q += W, A += W, S += W, ne++)
            g.data[j] = (U.data[w++] - se[0]) * K[0], g.data[q] = (U.data[x++] - se[1]) * K[1], g.data[A] = (U.data[F++] - se[2]) * K[2], g.data[S] = le === -1 ? 255 : (U.data[le++] - se[3]) * K[3];
        } else
          throw new Error("Can not access image data");
        return g;
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-conversion.js": (
    /*!***********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-conversion.js ***!
      \***********************************************************************/
    /***/
    ($e, $, r) => {
      r.r($);
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-factory-impl.js": (
    /*!*************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-factory-impl.js ***!
      \*************************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        bufferToTensor: () => (
          /* binding */
          D
        ),
        /* harmony export */
        tensorFromGpuBuffer: () => (
          /* binding */
          R
        ),
        /* harmony export */
        tensorFromImage: () => (
          /* binding */
          U
        ),
        /* harmony export */
        tensorFromMLTensor: () => (
          /* binding */
          g
        ),
        /* harmony export */
        tensorFromPinnedBuffer: () => (
          /* binding */
          v
        ),
        /* harmony export */
        tensorFromTexture: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./tensor-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-impl.js"
      );
      const D = (M, y) => {
        if (M === void 0)
          throw new Error("Image buffer must be defined");
        if (y.height === void 0 || y.width === void 0)
          throw new Error("Image height and width must be defined");
        if (y.tensorLayout === "NHWC")
          throw new Error("NHWC Tensor layout is not supported yet");
        const { height: b, width: I } = y, K = y.norm ?? { mean: 255, bias: 0 };
        let se, ie;
        typeof K.mean == "number" ? se = [K.mean, K.mean, K.mean, K.mean] : se = [K.mean[0], K.mean[1], K.mean[2], K.mean[3] ?? 255], typeof K.bias == "number" ? ie = [K.bias, K.bias, K.bias, K.bias] : ie = [K.bias[0], K.bias[1], K.bias[2], K.bias[3] ?? 0];
        const W = y.format !== void 0 ? y.format : "RGBA", j = y.tensorFormat !== void 0 && y.tensorFormat !== void 0 ? y.tensorFormat : "RGB", q = b * I, A = j === "RGBA" ? new Float32Array(q * 4) : new Float32Array(q * 3);
        let S = 4, w = 0, x = 1, F = 2, le = 3, ne = 0, be = q, _e = q * 2, re = -1;
        W === "RGB" && (S = 3, w = 0, x = 1, F = 2, le = -1), j === "RGBA" ? re = q * 3 : j === "RBG" ? (ne = 0, _e = q, be = q * 2) : j === "BGR" && (_e = 0, be = q, ne = q * 2);
        for (let ce = 0; ce < q; ce++, w += S, F += S, x += S, le += S)
          A[ne++] = (M[w] + ie[0]) / se[0], A[be++] = (M[x] + ie[1]) / se[1], A[_e++] = (M[F] + ie[2]) / se[2], re !== -1 && le !== -1 && (A[re++] = (M[le] + ie[3]) / se[3]);
        return j === "RGBA" ? new _.Tensor("float32", A, [1, 4, b, I]) : new _.Tensor("float32", A, [1, 3, b, I]);
      }, U = async (M, y) => {
        const b = typeof HTMLImageElement < "u" && M instanceof HTMLImageElement, I = typeof ImageData < "u" && M instanceof ImageData, K = typeof ImageBitmap < "u" && M instanceof ImageBitmap, se = typeof M == "string";
        let ie, W = y ?? {};
        const j = () => {
          if (typeof document < "u")
            return document.createElement("canvas");
          if (typeof OffscreenCanvas < "u")
            return new OffscreenCanvas(1, 1);
          throw new Error("Canvas is not supported");
        }, q = (A) => typeof HTMLCanvasElement < "u" && A instanceof HTMLCanvasElement || A instanceof OffscreenCanvas ? A.getContext("2d") : null;
        if (b) {
          const A = j();
          A.width = M.width, A.height = M.height;
          const S = q(A);
          if (S != null) {
            let w = M.height, x = M.width;
            if (y !== void 0 && y.resizedHeight !== void 0 && y.resizedWidth !== void 0 && (w = y.resizedHeight, x = y.resizedWidth), y !== void 0) {
              if (W = y, y.tensorFormat !== void 0)
                throw new Error("Image input config format must be RGBA for HTMLImageElement");
              W.tensorFormat = "RGBA", W.height = w, W.width = x;
            } else
              W.tensorFormat = "RGBA", W.height = w, W.width = x;
            S.drawImage(M, 0, 0), ie = S.getImageData(0, 0, x, w).data;
          } else
            throw new Error("Can not access image data");
        } else if (I) {
          let A, S;
          if (y !== void 0 && y.resizedWidth !== void 0 && y.resizedHeight !== void 0 ? (A = y.resizedHeight, S = y.resizedWidth) : (A = M.height, S = M.width), y !== void 0 && (W = y), W.format = "RGBA", W.height = A, W.width = S, y !== void 0) {
            const w = j();
            w.width = S, w.height = A;
            const x = q(w);
            if (x != null)
              x.putImageData(M, 0, 0), ie = x.getImageData(0, 0, S, A).data;
            else
              throw new Error("Can not access image data");
          } else
            ie = M.data;
        } else if (K) {
          if (y === void 0)
            throw new Error("Please provide image config with format for Imagebitmap");
          const A = j();
          A.width = M.width, A.height = M.height;
          const S = q(A);
          if (S != null) {
            const w = M.height, x = M.width;
            return S.drawImage(M, 0, 0, x, w), ie = S.getImageData(0, 0, x, w).data, W.height = w, W.width = x, D(ie, W);
          } else
            throw new Error("Can not access image data");
        } else {
          if (se)
            return new Promise((A, S) => {
              const w = j(), x = q(w);
              if (!M || !x)
                return S();
              const F = new Image();
              F.crossOrigin = "Anonymous", F.src = M, F.onload = () => {
                w.width = F.width, w.height = F.height, x.drawImage(F, 0, 0, w.width, w.height);
                const le = x.getImageData(0, 0, w.width, w.height);
                W.height = w.height, W.width = w.width, A(D(le.data, W));
              };
            });
          throw new Error("Input data provided is not supported - aborted tensor creation");
        }
        if (ie !== void 0)
          return D(ie, W);
        throw new Error("Input data provided is not supported - aborted tensor creation");
      }, Y = (M, y) => {
        const { width: b, height: I, download: K, dispose: se } = y, ie = [1, I, b, 4];
        return new _.Tensor({ location: "texture", type: "float32", texture: M, dims: ie, download: K, dispose: se });
      }, R = (M, y) => {
        const { dataType: b, dims: I, download: K, dispose: se } = y;
        return new _.Tensor({ location: "gpu-buffer", type: b ?? "float32", gpuBuffer: M, dims: I, download: K, dispose: se });
      }, g = (M, y) => {
        const { dataType: b, dims: I, download: K, dispose: se } = y;
        return new _.Tensor({ location: "ml-tensor", type: b ?? "float32", mlTensor: M, dims: I, download: K, dispose: se });
      }, v = (M, y, b) => new _.Tensor({ location: "cpu-pinned", type: M, data: y, dims: b ?? [y.length] });
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-factory.js": (
    /*!********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-factory.js ***!
      \********************************************************************/
    /***/
    ($e, $, r) => {
      r.r($);
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-impl-type-mapping.js": (
    /*!******************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-impl-type-mapping.js ***!
      \******************************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        NUMERIC_TENSOR_TYPEDARRAY_TO_TYPE_MAP: () => (
          /* binding */
          D
        ),
        /* harmony export */
        NUMERIC_TENSOR_TYPE_TO_TYPEDARRAY_MAP: () => (
          /* binding */
          _
        ),
        /* harmony export */
        checkTypedArray: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      const _ = /* @__PURE__ */ new Map([
        ["float32", Float32Array],
        ["uint8", Uint8Array],
        ["int8", Int8Array],
        ["uint16", Uint16Array],
        ["int16", Int16Array],
        ["int32", Int32Array],
        ["bool", Uint8Array],
        ["float64", Float64Array],
        ["uint32", Uint32Array],
        ["int4", Uint8Array],
        ["uint4", Uint8Array]
      ]), D = /* @__PURE__ */ new Map([
        [Float32Array, "float32"],
        [Uint8Array, "uint8"],
        [Int8Array, "int8"],
        [Uint16Array, "uint16"],
        [Int16Array, "int16"],
        [Int32Array, "int32"],
        [Float64Array, "float64"],
        [Uint32Array, "uint32"]
      ]);
      let U = !1;
      const Y = () => {
        if (!U) {
          U = !0;
          const R = typeof BigInt64Array < "u" && BigInt64Array.from, g = typeof BigUint64Array < "u" && BigUint64Array.from, v = typeof Float16Array < "u" && Float16Array.from;
          R && (_.set("int64", BigInt64Array), D.set(BigInt64Array, "int64")), g && (_.set("uint64", BigUint64Array), D.set(BigUint64Array, "uint64")), v ? (_.set("float16", Float16Array), D.set(Float16Array, "float16")) : _.set("float16", Uint16Array);
        }
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-impl.js": (
    /*!*****************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-impl.js ***!
      \*****************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Tensor: () => (
          /* binding */
          R
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./tensor-conversion-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-conversion-impl.js"
      ), D = r(
        /*! ./tensor-factory-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-factory-impl.js"
      ), U = r(
        /*! ./tensor-impl-type-mapping.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-impl-type-mapping.js"
      ), Y = r(
        /*! ./tensor-utils-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-utils-impl.js"
      );
      class R {
        /**
         * implementation.
         */
        constructor(v, M, y) {
          (0, U.checkTypedArray)();
          let b, I;
          if (typeof v == "object" && "location" in v)
            switch (this.dataLocation = v.location, b = v.type, I = v.dims, v.location) {
              case "cpu-pinned": {
                const se = U.NUMERIC_TENSOR_TYPE_TO_TYPEDARRAY_MAP.get(b);
                if (!se)
                  throw new TypeError(`unsupported type "${b}" to create tensor from pinned buffer`);
                if (!(v.data instanceof se))
                  throw new TypeError(`buffer should be of type ${se.name}`);
                this.cpuData = v.data;
                break;
              }
              case "texture": {
                if (b !== "float32")
                  throw new TypeError(`unsupported type "${b}" to create tensor from texture`);
                this.gpuTextureData = v.texture, this.downloader = v.download, this.disposer = v.dispose;
                break;
              }
              case "gpu-buffer": {
                if (b !== "float32" && b !== "float16" && b !== "int32" && b !== "int64" && b !== "uint32" && b !== "uint8" && b !== "bool" && b !== "uint4" && b !== "int4")
                  throw new TypeError(`unsupported type "${b}" to create tensor from gpu buffer`);
                this.gpuBufferData = v.gpuBuffer, this.downloader = v.download, this.disposer = v.dispose;
                break;
              }
              case "ml-tensor": {
                if (b !== "float32" && b !== "float16" && b !== "int32" && b !== "int64" && b !== "uint32" && b !== "uint64" && b !== "int8" && b !== "uint8" && b !== "bool")
                  throw new TypeError(`unsupported type "${b}" to create tensor from MLTensor`);
                this.mlTensorData = v.mlTensor, this.downloader = v.download, this.disposer = v.dispose;
                break;
              }
              default:
                throw new Error(`Tensor constructor: unsupported location '${this.dataLocation}'`);
            }
          else {
            let se, ie;
            if (typeof v == "string")
              if (b = v, ie = y, v === "string") {
                if (!Array.isArray(M))
                  throw new TypeError("A string tensor's data must be a string array.");
                se = M;
              } else {
                const W = U.NUMERIC_TENSOR_TYPE_TO_TYPEDARRAY_MAP.get(v);
                if (W === void 0)
                  throw new TypeError(`Unsupported tensor type: ${v}.`);
                if (Array.isArray(M)) {
                  if (v === "float16" && W === Uint16Array || v === "uint4" || v === "int4")
                    throw new TypeError(`Creating a ${v} tensor from number array is not supported. Please use ${W.name} as data.`);
                  v === "uint64" || v === "int64" ? se = W.from(M, BigInt) : se = W.from(M);
                } else if (M instanceof W)
                  se = M;
                else if (M instanceof Uint8ClampedArray)
                  if (v === "uint8")
                    se = Uint8Array.from(M);
                  else
                    throw new TypeError("A Uint8ClampedArray tensor's data must be type of uint8");
                else
                  throw new TypeError(`A ${b} tensor's data must be type of ${W}`);
              }
            else if (ie = M, Array.isArray(v)) {
              if (v.length === 0)
                throw new TypeError("Tensor type cannot be inferred from an empty array.");
              const W = typeof v[0];
              if (W === "string")
                b = "string", se = v;
              else if (W === "boolean")
                b = "bool", se = Uint8Array.from(v);
              else
                throw new TypeError(`Invalid element type of data array: ${W}.`);
            } else if (v instanceof Uint8ClampedArray)
              b = "uint8", se = Uint8Array.from(v);
            else {
              const W = U.NUMERIC_TENSOR_TYPEDARRAY_TO_TYPE_MAP.get(v.constructor);
              if (W === void 0)
                throw new TypeError(`Unsupported type for tensor data: ${v.constructor}.`);
              b = W, se = v;
            }
            if (ie === void 0)
              ie = [se.length];
            else if (!Array.isArray(ie))
              throw new TypeError("A tensor's dims must be a number array");
            I = ie, this.cpuData = se, this.dataLocation = "cpu";
          }
          const K = (0, Y.calculateSize)(I);
          if (this.cpuData && K !== this.cpuData.length && !((b === "uint4" || b === "int4") && Math.ceil(K / 2) === this.cpuData.length))
            throw new Error(`Tensor's size(${K}) does not match data length(${this.cpuData.length}).`);
          this.type = b, this.dims = I, this.size = K;
        }
        // #endregion
        // #region factory
        static async fromImage(v, M) {
          return (0, D.tensorFromImage)(v, M);
        }
        static fromTexture(v, M) {
          return (0, D.tensorFromTexture)(v, M);
        }
        static fromGpuBuffer(v, M) {
          return (0, D.tensorFromGpuBuffer)(v, M);
        }
        static fromMLTensor(v, M) {
          return (0, D.tensorFromMLTensor)(v, M);
        }
        static fromPinnedBuffer(v, M, y) {
          return (0, D.tensorFromPinnedBuffer)(v, M, y);
        }
        // #endregion
        // #region conversions
        toDataURL(v) {
          return (0, _.tensorToDataURL)(this, v);
        }
        toImageData(v) {
          return (0, _.tensorToImageData)(this, v);
        }
        // #endregion
        // #region properties
        get data() {
          if (this.ensureValid(), !this.cpuData)
            throw new Error("The data is not on CPU. Use `getData()` to download GPU data to CPU, or use `texture` or `gpuBuffer` property to access the GPU data directly.");
          return this.cpuData;
        }
        get location() {
          return this.dataLocation;
        }
        get texture() {
          if (this.ensureValid(), !this.gpuTextureData)
            throw new Error("The data is not stored as a WebGL texture.");
          return this.gpuTextureData;
        }
        get gpuBuffer() {
          if (this.ensureValid(), !this.gpuBufferData)
            throw new Error("The data is not stored as a WebGPU buffer.");
          return this.gpuBufferData;
        }
        get mlTensor() {
          if (this.ensureValid(), !this.mlTensorData)
            throw new Error("The data is not stored as a WebNN MLTensor.");
          return this.mlTensorData;
        }
        // #endregion
        // #region methods
        async getData(v) {
          switch (this.ensureValid(), this.dataLocation) {
            case "cpu":
            case "cpu-pinned":
              return this.data;
            case "texture":
            case "gpu-buffer":
            case "ml-tensor": {
              if (!this.downloader)
                throw new Error("The current tensor is not created with a specified data downloader.");
              if (this.isDownloading)
                throw new Error("The current tensor is being downloaded.");
              try {
                this.isDownloading = !0;
                const M = await this.downloader();
                return this.downloader = void 0, this.dataLocation = "cpu", this.cpuData = M, v && this.disposer && (this.disposer(), this.disposer = void 0), M;
              } finally {
                this.isDownloading = !1;
              }
            }
            default:
              throw new Error(`cannot get data from location: ${this.dataLocation}`);
          }
        }
        dispose() {
          if (this.isDownloading)
            throw new Error("The current tensor is being downloaded.");
          this.disposer && (this.disposer(), this.disposer = void 0), this.cpuData = void 0, this.gpuTextureData = void 0, this.gpuBufferData = void 0, this.mlTensorData = void 0, this.downloader = void 0, this.isDownloading = void 0, this.dataLocation = "none";
        }
        // #endregion
        // #region tensor utilities
        ensureValid() {
          if (this.dataLocation === "none")
            throw new Error("The tensor is disposed.");
        }
        reshape(v) {
          if (this.ensureValid(), this.downloader || this.disposer)
            throw new Error("Cannot reshape a tensor that owns GPU resource.");
          return (0, Y.tensorReshape)(this, v);
        }
      }
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-utils-impl.js": (
    /*!***********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-utils-impl.js ***!
      \***********************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        calculateSize: () => (
          /* binding */
          D
        ),
        /* harmony export */
        tensorReshape: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./tensor-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-impl.js"
      );
      const D = (Y) => {
        let R = 1;
        for (let g = 0; g < Y.length; g++) {
          const v = Y[g];
          if (typeof v != "number" || !Number.isSafeInteger(v))
            throw new TypeError(`dims[${g}] must be an integer, got: ${v}`);
          if (v < 0)
            throw new RangeError(`dims[${g}] must be a non-negative integer, got: ${v}`);
          R *= v;
        }
        return R;
      }, U = (Y, R) => {
        switch (Y.location) {
          case "cpu":
            return new _.Tensor(Y.type, Y.data, R);
          case "cpu-pinned":
            return new _.Tensor({
              location: "cpu-pinned",
              data: Y.data,
              type: Y.type,
              dims: R
            });
          case "texture":
            return new _.Tensor({
              location: "texture",
              texture: Y.texture,
              type: Y.type,
              dims: R
            });
          case "gpu-buffer":
            return new _.Tensor({
              location: "gpu-buffer",
              gpuBuffer: Y.gpuBuffer,
              type: Y.type,
              dims: R
            });
          case "ml-tensor":
            return new _.Tensor({
              location: "ml-tensor",
              mlTensor: Y.mlTensor,
              type: Y.type,
              dims: R
            });
          default:
            throw new Error(`tensorReshape: tensor location ${Y.location} is not supported`);
        }
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor.js": (
    /*!************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor.js ***!
      \************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Tensor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./tensor-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-impl.js"
      );
      const D = _.Tensor;
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/trace.js": (
    /*!***********************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/trace.js ***!
      \***********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        TRACE: () => (
          /* binding */
          D
        ),
        /* harmony export */
        TRACE_FUNC_BEGIN: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        TRACE_FUNC_END: () => (
          /* binding */
          R
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./env-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/env-impl.js"
      );
      const D = (g, v) => {
        (typeof _.env.trace > "u" ? !_.env.wasm.trace : !_.env.trace) || console.timeStamp(`${g}::ORT::${v}`);
      }, U = (g, v) => {
        var b;
        const M = ((b = new Error().stack) == null ? void 0 : b.split(/\r\n|\r|\n/g)) || [];
        let y = !1;
        for (let I = 0; I < M.length; I++) {
          if (y && !M[I].includes("TRACE_FUNC")) {
            let K = `FUNC_${g}::${M[I].trim().split(" ")[1]}`;
            v && (K += `::${v}`), D("CPU", K);
            return;
          }
          M[I].includes("TRACE_FUNC") && (y = !0);
        }
      }, Y = (g) => {
        (typeof _.env.trace > "u" ? !_.env.wasm.trace : !_.env.trace) || U("BEGIN", g);
      }, R = (g) => {
        (typeof _.env.trace > "u" ? !_.env.wasm.trace : !_.env.trace) || U("END", g);
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/training-session-impl.js": (
    /*!***************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/training-session-impl.js ***!
      \***************************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        TrainingSession: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./backend-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/backend-impl.js"
      ), D = r(
        /*! ./tensor.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor.js"
      );
      const U = "Training backend could not be resolved. Make sure you're using the correct configuration & WebAssembly files.";
      class Y {
        constructor(g, v, M) {
          this.handler = g, this.hasOptimizerModel = v, this.hasEvalModel = M;
        }
        get trainingInputNames() {
          return this.handler.inputNames;
        }
        get trainingOutputNames() {
          return this.handler.outputNames;
        }
        get evalInputNames() {
          if (this.hasEvalModel)
            return this.handler.evalInputNames;
          throw new Error("This training session has no evalModel loaded.");
        }
        get evalOutputNames() {
          if (this.hasEvalModel)
            return this.handler.evalOutputNames;
          throw new Error("This training session has no evalModel loaded.");
        }
        static async create(g, v) {
          const M = g.evalModel || "", y = g.optimizerModel || "", b = v || {}, [I, K] = await (0, _.resolveBackendAndExecutionProviders)(b);
          if (I.createTrainingSessionHandler) {
            const se = await I.createTrainingSessionHandler(g.checkpointState, g.trainModel, M, y, K);
            return new Y(se, !!g.optimizerModel, !!g.evalModel);
          } else
            throw new Error(U);
        }
        /**
         * Helper function for runTrainStep and future runStep methods that handles the type-narrowing conversion from
         * the given parameters to SessionHandler.FetchesType and RunOptions.
         *
         * @param inputNames the feeds object is checked that they contain all input names in the provided list of input
         * names.
         * @param outputNames the fetches object is checked that their keys match up with valid names in the list of output
         * names.
         * @param feeds the required input
         * @param arg1 narrowed & converted into the SessionHandler.FetchesType or RunOptions object
         * @param arg2 optional RunOptions object.
         * @returns
         */
        typeNarrowingForRunStep(g, v, M, y, b) {
          const I = {};
          let K = {};
          if (typeof M != "object" || M === null || M instanceof D.Tensor || Array.isArray(M))
            throw new TypeError("'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.");
          let se = !0;
          if (typeof y == "object") {
            if (y === null)
              throw new TypeError("Unexpected argument[1]: cannot be null.");
            if (y instanceof D.Tensor)
              throw new TypeError("'fetches' cannot be a Tensor");
            if (Array.isArray(y)) {
              if (y.length === 0)
                throw new TypeError("'fetches' cannot be an empty array.");
              se = !1;
              for (const ie of y) {
                if (typeof ie != "string")
                  throw new TypeError("'fetches' must be a string array or an object.");
                if (v.indexOf(ie) === -1)
                  throw new RangeError(`'fetches' contains invalid output name: ${ie}.`);
                I[ie] = null;
              }
              if (typeof b == "object" && b !== null)
                K = b;
              else if (typeof b < "u")
                throw new TypeError("'options' must be an object.");
            } else {
              let ie = !1;
              const W = Object.getOwnPropertyNames(y);
              for (const j of v)
                if (W.indexOf(j) !== -1) {
                  const q = y[j];
                  (q === null || q instanceof D.Tensor) && (ie = !0, se = !1, I[j] = q);
                }
              if (ie) {
                if (typeof b == "object" && b !== null)
                  K = b;
                else if (typeof b < "u")
                  throw new TypeError("'options' must be an object.");
              } else
                K = y;
            }
          } else if (typeof y < "u")
            throw new TypeError("Unexpected argument[1]: must be 'fetches' or 'options'.");
          for (const ie of g)
            if (typeof M[ie] > "u")
              throw new Error(`input '${ie}' is missing in 'feeds'.`);
          if (se)
            for (const ie of v)
              I[ie] = null;
          return [I, K];
        }
        /**
         * Helper method for runTrainStep and any other runStep methods. Takes the ReturnType result from the SessionHandler
         * and changes it into a map of Tensors.
         *
         * @param results
         * @returns
         */
        convertHandlerReturnTypeToMapOfTensors(g) {
          const v = {};
          for (const M in g)
            if (Object.hasOwnProperty.call(g, M)) {
              const y = g[M];
              y instanceof D.Tensor ? v[M] = y : v[M] = new D.Tensor(y.type, y.data, y.dims);
            }
          return v;
        }
        async lazyResetGrad() {
          await this.handler.lazyResetGrad();
        }
        async runTrainStep(g, v, M) {
          const [y, b] = this.typeNarrowingForRunStep(this.trainingInputNames, this.trainingOutputNames, g, v, M), I = await this.handler.runTrainStep(g, y, b);
          return this.convertHandlerReturnTypeToMapOfTensors(I);
        }
        async runOptimizerStep(g) {
          if (this.hasOptimizerModel)
            await this.handler.runOptimizerStep(g || {});
          else
            throw new Error("This TrainingSession has no OptimizerModel loaded.");
        }
        async runEvalStep(g, v, M) {
          if (this.hasEvalModel) {
            const [y, b] = this.typeNarrowingForRunStep(this.evalInputNames, this.evalOutputNames, g, v, M), I = await this.handler.runEvalStep(g, y, b);
            return this.convertHandlerReturnTypeToMapOfTensors(I);
          } else
            throw new Error("This TrainingSession has no EvalModel loaded.");
        }
        async getParametersSize(g = !0) {
          return this.handler.getParametersSize(g);
        }
        async loadParametersBuffer(g, v = !0) {
          const M = await this.getParametersSize(v);
          if (g.length !== 4 * M)
            throw new Error("Size of the buffer passed into loadParametersBuffer must match the number of parameters in the model. Please use getParametersSize method to check.");
          return this.handler.loadParametersBuffer(g, v);
        }
        async getContiguousParameters(g = !0) {
          return this.handler.getContiguousParameters(g);
        }
        async release() {
          return this.handler.dispose();
        }
      }
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/training-session.js": (
    /*!**********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/training-session.js ***!
      \**********************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        TrainingSession: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./training-session-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/training-session-impl.js"
      );
      const D = _.TrainingSession;
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/version.js": (
    /*!*************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/version.js ***!
      \*************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        version: () => (
          /* binding */
          _
        )
        /* harmony export */
      });
      const _ = "1.20.1";
    }
  ),
  /***/
  "./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs?3a96": (
    /*!**************************************************************!*\
      !*** ./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs ***!
      \**************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        InferenceSession: () => (
          /* binding */
          lt
        ),
        /* harmony export */
        TRACE: () => (
          /* binding */
          Ue
        ),
        /* harmony export */
        TRACE_FUNC_BEGIN: () => (
          /* binding */
          Ne
        ),
        /* harmony export */
        TRACE_FUNC_END: () => (
          /* binding */
          Re
        ),
        /* harmony export */
        Tensor: () => (
          /* binding */
          ae
        ),
        /* harmony export */
        default: () => (
          /* binding */
          hf
        ),
        /* harmony export */
        env: () => (
          /* binding */
          F
        ),
        /* harmony export */
        registerBackend: () => (
          /* binding */
          K
        )
        /* harmony export */
      });
      /*!
       * ONNX Runtime Web v1.21.0-dev.20250114-228dd16893
       * Copyright (c) Microsoft Corporation. All rights reserved.
       * Licensed under the MIT License.
       */
      var _ = Object.defineProperty, D = Object.getOwnPropertyDescriptor, U = Object.getOwnPropertyNames, Y = Object.prototype.hasOwnProperty, R = ((e) => typeof require < "u" ? require : typeof Proxy < "u" ? new Proxy(e, { get: (t, s) => (typeof require < "u" ? require : t)[s] }) : e)(function(e) {
        if (typeof require < "u") return require.apply(this, arguments);
        throw Error('Dynamic require of "' + e + '" is not supported');
      }), g = (e, t) => () => (e && (t = e(e = 0)), t), v = (e, t) => {
        for (var s in t) _(e, s, { get: t[s], enumerable: !0 });
      }, M = (e, t, s, n) => {
        if (t && typeof t == "object" || typeof t == "function") for (let i of U(t)) !Y.call(e, i) && i !== s && _(e, i, { get: () => t[i], enumerable: !(n = D(t, i)) || n.enumerable });
        return e;
      }, y = (e) => M(_({}, "__esModule", { value: !0 }), e), b, I, K, se, ie, W = g(() => {
        b = /* @__PURE__ */ new Map(), I = [], K = (e, t, s) => {
          if (t && typeof t.init == "function" && typeof t.createInferenceSessionHandler == "function") {
            let n = b.get(e);
            if (n === void 0) b.set(e, { backend: t, priority: s });
            else {
              if (n.priority > s) return;
              if (n.priority === s && n.backend !== t) throw new Error(`cannot register backend "${e}" using priority ${s}`);
            }
            if (s >= 0) {
              let i = I.indexOf(e);
              i !== -1 && I.splice(i, 1);
              for (let a = 0; a < I.length; a++) if (b.get(I[a]).priority <= s) {
                I.splice(a, 0, e);
                return;
              }
              I.push(e);
            }
            return;
          }
          throw new TypeError("not a valid backend");
        }, se = async (e) => {
          let t = b.get(e);
          if (!t) return "backend not found.";
          if (t.initialized) return t.backend;
          if (t.aborted) return t.error;
          {
            let s = !!t.initPromise;
            try {
              return s || (t.initPromise = t.backend.init(e)), await t.initPromise, t.initialized = !0, t.backend;
            } catch (n) {
              return s || (t.error = `${n}`, t.aborted = !0), t.error;
            } finally {
              delete t.initPromise;
            }
          }
        }, ie = async (e) => {
          let t = e.executionProviders || [], s = t.map((p) => typeof p == "string" ? p : p.name), n = s.length === 0 ? I : s, i, a = [], o = /* @__PURE__ */ new Set();
          for (let p of n) {
            let h = await se(p);
            typeof h == "string" ? a.push({ name: p, err: h }) : (i || (i = h), i === h && o.add(p));
          }
          if (!i) throw new Error(`no available backend found. ERR: ${a.map((p) => `[${p.name}] ${p.err}`).join(", ")}`);
          for (let { name: p, err: h } of a) s.includes(p) && console.warn(`removing requested execution provider "${p}" from session options because it is not available: ${h}`);
          let u = t.filter((p) => o.has(typeof p == "string" ? p : p.name));
          return [i, new Proxy(e, { get: (p, h) => h === "executionProviders" ? u : Reflect.get(p, h) })];
        };
      }), j = g(() => {
        W();
      }), q, A = g(() => {
        q = "1.21.0-dev.20241212-1f88284f96";
      }), S, w, x = g(() => {
        A(), S = "warning", w = { wasm: {}, webgl: {}, webgpu: {}, versions: { common: q }, set logLevel(e) {
          if (e !== void 0) {
            if (typeof e != "string" || ["verbose", "info", "warning", "error", "fatal"].indexOf(e) === -1) throw new Error(`Unsupported logging level: ${e}`);
            S = e;
          }
        }, get logLevel() {
          return S;
        } }, Object.defineProperty(w, "logLevel", { enumerable: !0 });
      }), F, le = g(() => {
        x(), F = w;
      }), ne, be, _e = g(() => {
        ne = (e, t) => {
          let s = typeof document < "u" ? document.createElement("canvas") : new OffscreenCanvas(1, 1);
          s.width = e.dims[3], s.height = e.dims[2];
          let n = s.getContext("2d");
          if (n != null) {
            let i, a;
            (t == null ? void 0 : t.tensorLayout) !== void 0 && t.tensorLayout === "NHWC" ? (i = e.dims[2], a = e.dims[3]) : (i = e.dims[3], a = e.dims[2]);
            let o = (t == null ? void 0 : t.format) !== void 0 ? t.format : "RGB", u = t == null ? void 0 : t.norm, p, h;
            u === void 0 || u.mean === void 0 ? p = [255, 255, 255, 255] : typeof u.mean == "number" ? p = [u.mean, u.mean, u.mean, u.mean] : (p = [u.mean[0], u.mean[1], u.mean[2], 0], u.mean[3] !== void 0 && (p[3] = u.mean[3])), u === void 0 || u.bias === void 0 ? h = [0, 0, 0, 0] : typeof u.bias == "number" ? h = [u.bias, u.bias, u.bias, u.bias] : (h = [u.bias[0], u.bias[1], u.bias[2], 0], u.bias[3] !== void 0 && (h[3] = u.bias[3]));
            let k = a * i, C = 0, d = k, z = k * 2, B = -1;
            o === "RGBA" ? (C = 0, d = k, z = k * 2, B = k * 3) : o === "RGB" ? (C = 0, d = k, z = k * 2) : o === "RBG" && (C = 0, z = k, d = k * 2);
            for (let V = 0; V < a; V++) for (let Z = 0; Z < i; Z++) {
              let ee = (e.data[C++] - h[0]) * p[0], X = (e.data[d++] - h[1]) * p[1], he = (e.data[z++] - h[2]) * p[2], pe = B === -1 ? 255 : (e.data[B++] - h[3]) * p[3];
              n.fillStyle = "rgba(" + ee + "," + X + "," + he + "," + pe + ")", n.fillRect(Z, V, 1, 1);
            }
            if ("toDataURL" in s) return s.toDataURL();
            throw new Error("toDataURL is not supported");
          } else throw new Error("Can not access image data");
        }, be = (e, t) => {
          let s = typeof document < "u" ? document.createElement("canvas").getContext("2d") : new OffscreenCanvas(1, 1).getContext("2d"), n;
          if (s != null) {
            let i, a, o;
            (t == null ? void 0 : t.tensorLayout) !== void 0 && t.tensorLayout === "NHWC" ? (i = e.dims[2], a = e.dims[1], o = e.dims[3]) : (i = e.dims[3], a = e.dims[2], o = e.dims[1]);
            let u = t !== void 0 && t.format !== void 0 ? t.format : "RGB", p = t == null ? void 0 : t.norm, h, k;
            p === void 0 || p.mean === void 0 ? h = [255, 255, 255, 255] : typeof p.mean == "number" ? h = [p.mean, p.mean, p.mean, p.mean] : (h = [p.mean[0], p.mean[1], p.mean[2], 255], p.mean[3] !== void 0 && (h[3] = p.mean[3])), p === void 0 || p.bias === void 0 ? k = [0, 0, 0, 0] : typeof p.bias == "number" ? k = [p.bias, p.bias, p.bias, p.bias] : (k = [p.bias[0], p.bias[1], p.bias[2], 0], p.bias[3] !== void 0 && (k[3] = p.bias[3]));
            let C = a * i;
            if (t !== void 0 && (t.format !== void 0 && o === 4 && t.format !== "RGBA" || o === 3 && t.format !== "RGB" && t.format !== "BGR")) throw new Error("Tensor format doesn't match input tensor dims");
            let d = 4, z = 0, B = 1, V = 2, Z = 3, ee = 0, X = C, he = C * 2, pe = -1;
            u === "RGBA" ? (ee = 0, X = C, he = C * 2, pe = C * 3) : u === "RGB" ? (ee = 0, X = C, he = C * 2) : u === "RBG" && (ee = 0, he = C, X = C * 2), n = s.createImageData(i, a);
            for (let Me = 0; Me < a * i; z += d, B += d, V += d, Z += d, Me++) n.data[z] = (e.data[ee++] - k[0]) * h[0], n.data[B] = (e.data[X++] - k[1]) * h[1], n.data[V] = (e.data[he++] - k[2]) * h[2], n.data[Z] = pe === -1 ? 255 : (e.data[pe++] - k[3]) * h[3];
          } else throw new Error("Can not access image data");
          return n;
        };
      }), re, xe, ce, ke, Fe, Ee, tt = g(() => {
        je(), re = (e, t) => {
          if (e === void 0) throw new Error("Image buffer must be defined");
          if (t.height === void 0 || t.width === void 0) throw new Error("Image height and width must be defined");
          if (t.tensorLayout === "NHWC") throw new Error("NHWC Tensor layout is not supported yet");
          let { height: s, width: n } = t, i = t.norm ?? { mean: 255, bias: 0 }, a, o;
          typeof i.mean == "number" ? a = [i.mean, i.mean, i.mean, i.mean] : a = [i.mean[0], i.mean[1], i.mean[2], i.mean[3] ?? 255], typeof i.bias == "number" ? o = [i.bias, i.bias, i.bias, i.bias] : o = [i.bias[0], i.bias[1], i.bias[2], i.bias[3] ?? 0];
          let u = t.format !== void 0 ? t.format : "RGBA", p = t.tensorFormat !== void 0 && t.tensorFormat !== void 0 ? t.tensorFormat : "RGB", h = s * n, k = p === "RGBA" ? new Float32Array(h * 4) : new Float32Array(h * 3), C = 4, d = 0, z = 1, B = 2, V = 3, Z = 0, ee = h, X = h * 2, he = -1;
          u === "RGB" && (C = 3, d = 0, z = 1, B = 2, V = -1), p === "RGBA" ? he = h * 3 : p === "RBG" ? (Z = 0, X = h, ee = h * 2) : p === "BGR" && (X = 0, ee = h, Z = h * 2);
          for (let pe = 0; pe < h; pe++, d += C, B += C, z += C, V += C) k[Z++] = (e[d] + o[0]) / a[0], k[ee++] = (e[z] + o[1]) / a[1], k[X++] = (e[B] + o[2]) / a[2], he !== -1 && V !== -1 && (k[he++] = (e[V] + o[3]) / a[3]);
          return p === "RGBA" ? new Ke("float32", k, [1, 4, s, n]) : new Ke("float32", k, [1, 3, s, n]);
        }, xe = async (e, t) => {
          let s = typeof HTMLImageElement < "u" && e instanceof HTMLImageElement, n = typeof ImageData < "u" && e instanceof ImageData, i = typeof ImageBitmap < "u" && e instanceof ImageBitmap, a = typeof e == "string", o, u = t ?? {}, p = () => {
            if (typeof document < "u") return document.createElement("canvas");
            if (typeof OffscreenCanvas < "u") return new OffscreenCanvas(1, 1);
            throw new Error("Canvas is not supported");
          }, h = (k) => typeof HTMLCanvasElement < "u" && k instanceof HTMLCanvasElement || k instanceof OffscreenCanvas ? k.getContext("2d") : null;
          if (s) {
            let k = p();
            k.width = e.width, k.height = e.height;
            let C = h(k);
            if (C != null) {
              let d = e.height, z = e.width;
              if (t !== void 0 && t.resizedHeight !== void 0 && t.resizedWidth !== void 0 && (d = t.resizedHeight, z = t.resizedWidth), t !== void 0) {
                if (u = t, t.tensorFormat !== void 0) throw new Error("Image input config format must be RGBA for HTMLImageElement");
                u.tensorFormat = "RGBA", u.height = d, u.width = z;
              } else u.tensorFormat = "RGBA", u.height = d, u.width = z;
              C.drawImage(e, 0, 0), o = C.getImageData(0, 0, z, d).data;
            } else throw new Error("Can not access image data");
          } else if (n) {
            let k, C;
            if (t !== void 0 && t.resizedWidth !== void 0 && t.resizedHeight !== void 0 ? (k = t.resizedHeight, C = t.resizedWidth) : (k = e.height, C = e.width), t !== void 0 && (u = t), u.format = "RGBA", u.height = k, u.width = C, t !== void 0) {
              let d = p();
              d.width = C, d.height = k;
              let z = h(d);
              if (z != null) z.putImageData(e, 0, 0), o = z.getImageData(0, 0, C, k).data;
              else throw new Error("Can not access image data");
            } else o = e.data;
          } else if (i) {
            if (t === void 0) throw new Error("Please provide image config with format for Imagebitmap");
            let k = p();
            k.width = e.width, k.height = e.height;
            let C = h(k);
            if (C != null) {
              let d = e.height, z = e.width;
              return C.drawImage(e, 0, 0, z, d), o = C.getImageData(0, 0, z, d).data, u.height = d, u.width = z, re(o, u);
            } else throw new Error("Can not access image data");
          } else {
            if (a) return new Promise((k, C) => {
              let d = p(), z = h(d);
              if (!e || !z) return C();
              let B = new Image();
              B.crossOrigin = "Anonymous", B.src = e, B.onload = () => {
                d.width = B.width, d.height = B.height, z.drawImage(B, 0, 0, d.width, d.height);
                let V = z.getImageData(0, 0, d.width, d.height);
                u.height = d.height, u.width = d.width, k(re(V.data, u));
              };
            });
            throw new Error("Input data provided is not supported - aborted tensor creation");
          }
          if (o !== void 0) return re(o, u);
          throw new Error("Input data provided is not supported - aborted tensor creation");
        }, ce = (e, t) => {
          let { width: s, height: n, download: i, dispose: a } = t, o = [1, n, s, 4];
          return new Ke({ location: "texture", type: "float32", texture: e, dims: o, download: i, dispose: a });
        }, ke = (e, t) => {
          let { dataType: s, dims: n, download: i, dispose: a } = t;
          return new Ke({ location: "gpu-buffer", type: s ?? "float32", gpuBuffer: e, dims: n, download: i, dispose: a });
        }, Fe = (e, t) => {
          let { dataType: s, dims: n, download: i, dispose: a } = t;
          return new Ke({ location: "ml-tensor", type: s ?? "float32", mlTensor: e, dims: n, download: i, dispose: a });
        }, Ee = (e, t, s) => new Ke({ location: "cpu-pinned", type: e, data: t, dims: s ?? [t.length] });
      }), Ge, ye, J, de, Ce = g(() => {
        Ge = /* @__PURE__ */ new Map([["float32", Float32Array], ["uint8", Uint8Array], ["int8", Int8Array], ["uint16", Uint16Array], ["int16", Int16Array], ["int32", Int32Array], ["bool", Uint8Array], ["float64", Float64Array], ["uint32", Uint32Array], ["int4", Uint8Array], ["uint4", Uint8Array]]), ye = /* @__PURE__ */ new Map([[Float32Array, "float32"], [Uint8Array, "uint8"], [Int8Array, "int8"], [Uint16Array, "uint16"], [Int16Array, "int16"], [Int32Array, "int32"], [Float64Array, "float64"], [Uint32Array, "uint32"]]), J = !1, de = () => {
          if (!J) {
            J = !0;
            let e = typeof BigInt64Array < "u" && BigInt64Array.from, t = typeof BigUint64Array < "u" && BigUint64Array.from, s = typeof Float16Array < "u" && Float16Array.from;
            e && (Ge.set("int64", BigInt64Array), ye.set(BigInt64Array, "int64")), t && (Ge.set("uint64", BigUint64Array), ye.set(BigUint64Array, "uint64")), s ? (Ge.set("float16", Float16Array), ye.set(Float16Array, "float16")) : Ge.set("float16", Uint16Array);
          }
        };
      }), Be, Ze, te = g(() => {
        je(), Be = (e) => {
          let t = 1;
          for (let s = 0; s < e.length; s++) {
            let n = e[s];
            if (typeof n != "number" || !Number.isSafeInteger(n)) throw new TypeError(`dims[${s}] must be an integer, got: ${n}`);
            if (n < 0) throw new RangeError(`dims[${s}] must be a non-negative integer, got: ${n}`);
            t *= n;
          }
          return t;
        }, Ze = (e, t) => {
          switch (e.location) {
            case "cpu":
              return new Ke(e.type, e.data, t);
            case "cpu-pinned":
              return new Ke({ location: "cpu-pinned", data: e.data, type: e.type, dims: t });
            case "texture":
              return new Ke({ location: "texture", texture: e.texture, type: e.type, dims: t });
            case "gpu-buffer":
              return new Ke({ location: "gpu-buffer", gpuBuffer: e.gpuBuffer, type: e.type, dims: t });
            case "ml-tensor":
              return new Ke({ location: "ml-tensor", mlTensor: e.mlTensor, type: e.type, dims: t });
            default:
              throw new Error(`tensorReshape: tensor location ${e.location} is not supported`);
          }
        };
      }), Ke, je = g(() => {
        _e(), tt(), Ce(), te(), Ke = class {
          constructor(e, t, s) {
            de();
            let n, i;
            if (typeof e == "object" && "location" in e) switch (this.dataLocation = e.location, n = e.type, i = e.dims, e.location) {
              case "cpu-pinned": {
                let o = Ge.get(n);
                if (!o) throw new TypeError(`unsupported type "${n}" to create tensor from pinned buffer`);
                if (!(e.data instanceof o)) throw new TypeError(`buffer should be of type ${o.name}`);
                this.cpuData = e.data;
                break;
              }
              case "texture": {
                if (n !== "float32") throw new TypeError(`unsupported type "${n}" to create tensor from texture`);
                this.gpuTextureData = e.texture, this.downloader = e.download, this.disposer = e.dispose;
                break;
              }
              case "gpu-buffer": {
                if (n !== "float32" && n !== "float16" && n !== "int32" && n !== "int64" && n !== "uint32" && n !== "uint8" && n !== "bool" && n !== "uint4" && n !== "int4") throw new TypeError(`unsupported type "${n}" to create tensor from gpu buffer`);
                this.gpuBufferData = e.gpuBuffer, this.downloader = e.download, this.disposer = e.dispose;
                break;
              }
              case "ml-tensor": {
                if (n !== "float32" && n !== "float16" && n !== "int32" && n !== "int64" && n !== "uint32" && n !== "uint64" && n !== "int8" && n !== "uint8" && n !== "bool" && n !== "uint4" && n !== "int4") throw new TypeError(`unsupported type "${n}" to create tensor from MLTensor`);
                this.mlTensorData = e.mlTensor, this.downloader = e.download, this.disposer = e.dispose;
                break;
              }
              default:
                throw new Error(`Tensor constructor: unsupported location '${this.dataLocation}'`);
            }
            else {
              let o, u;
              if (typeof e == "string") if (n = e, u = s, e === "string") {
                if (!Array.isArray(t)) throw new TypeError("A string tensor's data must be a string array.");
                o = t;
              } else {
                let p = Ge.get(e);
                if (p === void 0) throw new TypeError(`Unsupported tensor type: ${e}.`);
                if (Array.isArray(t)) {
                  if (e === "float16" && p === Uint16Array || e === "uint4" || e === "int4") throw new TypeError(`Creating a ${e} tensor from number array is not supported. Please use ${p.name} as data.`);
                  e === "uint64" || e === "int64" ? o = p.from(t, BigInt) : o = p.from(t);
                } else if (t instanceof p) o = t;
                else if (t instanceof Uint8ClampedArray) if (e === "uint8") o = Uint8Array.from(t);
                else throw new TypeError("A Uint8ClampedArray tensor's data must be type of uint8");
                else throw new TypeError(`A ${n} tensor's data must be type of ${p}`);
              }
              else if (u = t, Array.isArray(e)) {
                if (e.length === 0) throw new TypeError("Tensor type cannot be inferred from an empty array.");
                let p = typeof e[0];
                if (p === "string") n = "string", o = e;
                else if (p === "boolean") n = "bool", o = Uint8Array.from(e);
                else throw new TypeError(`Invalid element type of data array: ${p}.`);
              } else if (e instanceof Uint8ClampedArray) n = "uint8", o = Uint8Array.from(e);
              else {
                let p = ye.get(e.constructor);
                if (p === void 0) throw new TypeError(`Unsupported type for tensor data: ${e.constructor}.`);
                n = p, o = e;
              }
              if (u === void 0) u = [o.length];
              else if (!Array.isArray(u)) throw new TypeError("A tensor's dims must be a number array");
              i = u, this.cpuData = o, this.dataLocation = "cpu";
            }
            let a = Be(i);
            if (this.cpuData && a !== this.cpuData.length && !((n === "uint4" || n === "int4") && Math.ceil(a / 2) === this.cpuData.length)) throw new Error(`Tensor's size(${a}) does not match data length(${this.cpuData.length}).`);
            this.type = n, this.dims = i, this.size = a;
          }
          static async fromImage(e, t) {
            return xe(e, t);
          }
          static fromTexture(e, t) {
            return ce(e, t);
          }
          static fromGpuBuffer(e, t) {
            return ke(e, t);
          }
          static fromMLTensor(e, t) {
            return Fe(e, t);
          }
          static fromPinnedBuffer(e, t, s) {
            return Ee(e, t, s);
          }
          toDataURL(e) {
            return ne(this, e);
          }
          toImageData(e) {
            return be(this, e);
          }
          get data() {
            if (this.ensureValid(), !this.cpuData) throw new Error("The data is not on CPU. Use `getData()` to download GPU data to CPU, or use `texture` or `gpuBuffer` property to access the GPU data directly.");
            return this.cpuData;
          }
          get location() {
            return this.dataLocation;
          }
          get texture() {
            if (this.ensureValid(), !this.gpuTextureData) throw new Error("The data is not stored as a WebGL texture.");
            return this.gpuTextureData;
          }
          get gpuBuffer() {
            if (this.ensureValid(), !this.gpuBufferData) throw new Error("The data is not stored as a WebGPU buffer.");
            return this.gpuBufferData;
          }
          get mlTensor() {
            if (this.ensureValid(), !this.mlTensorData) throw new Error("The data is not stored as a WebNN MLTensor.");
            return this.mlTensorData;
          }
          async getData(e) {
            switch (this.ensureValid(), this.dataLocation) {
              case "cpu":
              case "cpu-pinned":
                return this.data;
              case "texture":
              case "gpu-buffer":
              case "ml-tensor": {
                if (!this.downloader) throw new Error("The current tensor is not created with a specified data downloader.");
                if (this.isDownloading) throw new Error("The current tensor is being downloaded.");
                try {
                  this.isDownloading = !0;
                  let t = await this.downloader();
                  return this.downloader = void 0, this.dataLocation = "cpu", this.cpuData = t, e && this.disposer && (this.disposer(), this.disposer = void 0), t;
                } finally {
                  this.isDownloading = !1;
                }
              }
              default:
                throw new Error(`cannot get data from location: ${this.dataLocation}`);
            }
          }
          dispose() {
            if (this.isDownloading) throw new Error("The current tensor is being downloaded.");
            this.disposer && (this.disposer(), this.disposer = void 0), this.cpuData = void 0, this.gpuTextureData = void 0, this.gpuBufferData = void 0, this.mlTensorData = void 0, this.downloader = void 0, this.isDownloading = void 0, this.dataLocation = "none";
          }
          ensureValid() {
            if (this.dataLocation === "none") throw new Error("The tensor is disposed.");
          }
          reshape(e) {
            if (this.ensureValid(), this.downloader || this.disposer) throw new Error("Cannot reshape a tensor that owns GPU resource.");
            return Ze(this, e);
          }
        };
      }), ae, Te = g(() => {
        je(), ae = Ke;
      }), Ue, Ve, Ne, Re, st = g(() => {
        x(), Ue = (e, t) => {
          (typeof w.trace > "u" ? !w.wasm.trace : !w.trace) || console.timeStamp(`${e}::ORT::${t}`);
        }, Ve = (e, t) => {
          var i;
          let s = ((i = new Error().stack) == null ? void 0 : i.split(/\r\n|\r|\n/g)) || [], n = !1;
          for (let a = 0; a < s.length; a++) {
            if (n && !s[a].includes("TRACE_FUNC")) {
              let o = `FUNC_${e}::${s[a].trim().split(" ")[1]}`;
              t && (o += `::${t}`), Ue("CPU", o);
              return;
            }
            s[a].includes("TRACE_FUNC") && (n = !0);
          }
        }, Ne = (e) => {
          (typeof w.trace > "u" ? !w.wasm.trace : !w.trace) || Ve("BEGIN", e);
        }, Re = (e) => {
          (typeof w.trace > "u" ? !w.wasm.trace : !w.trace) || Ve("END", e);
        };
      }), dt, ct = g(() => {
        W(), Te(), st(), dt = class lf {
          constructor(t) {
            this.handler = t;
          }
          async run(t, s, n) {
            Ne();
            let i = {}, a = {};
            if (typeof t != "object" || t === null || t instanceof ae || Array.isArray(t)) throw new TypeError("'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.");
            let o = !0;
            if (typeof s == "object") {
              if (s === null) throw new TypeError("Unexpected argument[1]: cannot be null.");
              if (s instanceof ae) throw new TypeError("'fetches' cannot be a Tensor");
              if (Array.isArray(s)) {
                if (s.length === 0) throw new TypeError("'fetches' cannot be an empty array.");
                o = !1;
                for (let h of s) {
                  if (typeof h != "string") throw new TypeError("'fetches' must be a string array or an object.");
                  if (this.outputNames.indexOf(h) === -1) throw new RangeError(`'fetches' contains invalid output name: ${h}.`);
                  i[h] = null;
                }
                if (typeof n == "object" && n !== null) a = n;
                else if (typeof n < "u") throw new TypeError("'options' must be an object.");
              } else {
                let h = !1, k = Object.getOwnPropertyNames(s);
                for (let C of this.outputNames) if (k.indexOf(C) !== -1) {
                  let d = s[C];
                  (d === null || d instanceof ae) && (h = !0, o = !1, i[C] = d);
                }
                if (h) {
                  if (typeof n == "object" && n !== null) a = n;
                  else if (typeof n < "u") throw new TypeError("'options' must be an object.");
                } else a = s;
              }
            } else if (typeof s < "u") throw new TypeError("Unexpected argument[1]: must be 'fetches' or 'options'.");
            for (let h of this.inputNames) if (typeof t[h] > "u") throw new Error(`input '${h}' is missing in 'feeds'.`);
            if (o) for (let h of this.outputNames) i[h] = null;
            let u = await this.handler.run(t, i, a), p = {};
            for (let h in u) if (Object.hasOwnProperty.call(u, h)) {
              let k = u[h];
              k instanceof ae ? p[h] = k : p[h] = new ae(k.type, k.data, k.dims);
            }
            return Re(), p;
          }
          async release() {
            return this.handler.dispose();
          }
          static async create(t, s, n, i) {
            Ne();
            let a, o = {};
            if (typeof t == "string") {
              if (a = t, typeof s == "object" && s !== null) o = s;
              else if (typeof s < "u") throw new TypeError("'options' must be an object.");
            } else if (t instanceof Uint8Array) {
              if (a = t, typeof s == "object" && s !== null) o = s;
              else if (typeof s < "u") throw new TypeError("'options' must be an object.");
            } else if (t instanceof ArrayBuffer || typeof SharedArrayBuffer < "u" && t instanceof SharedArrayBuffer) {
              let k = t, C = 0, d = t.byteLength;
              if (typeof s == "object" && s !== null) o = s;
              else if (typeof s == "number") {
                if (C = s, !Number.isSafeInteger(C)) throw new RangeError("'byteOffset' must be an integer.");
                if (C < 0 || C >= k.byteLength) throw new RangeError(`'byteOffset' is out of range [0, ${k.byteLength}).`);
                if (d = t.byteLength - C, typeof n == "number") {
                  if (d = n, !Number.isSafeInteger(d)) throw new RangeError("'byteLength' must be an integer.");
                  if (d <= 0 || C + d > k.byteLength) throw new RangeError(`'byteLength' is out of range (0, ${k.byteLength - C}].`);
                  if (typeof i == "object" && i !== null) o = i;
                  else if (typeof i < "u") throw new TypeError("'options' must be an object.");
                } else if (typeof n < "u") throw new TypeError("'byteLength' must be a number.");
              } else if (typeof s < "u") throw new TypeError("'options' must be an object.");
              a = new Uint8Array(k, C, d);
            } else throw new TypeError("Unexpected argument[0]: must be 'path' or 'buffer'.");
            let [u, p] = await ie(o), h = await u.createInferenceSessionHandler(a, p);
            return Re(), new lf(h);
          }
          startProfiling() {
            this.handler.startProfiling();
          }
          endProfiling() {
            this.handler.endProfiling();
          }
          get inputNames() {
            return this.handler.inputNames;
          }
          get outputNames() {
            return this.handler.outputNames;
          }
        };
      }), lt, ht = g(() => {
        ct(), lt = dt;
      }), L = g(() => {
      }), oe = g(() => {
      }), H = g(() => {
      }), me = g(() => {
      }), Ae = {};
      v(Ae, { InferenceSession: () => lt, TRACE: () => Ue, TRACE_FUNC_BEGIN: () => Ne, TRACE_FUNC_END: () => Re, Tensor: () => ae, env: () => F, registerBackend: () => K });
      var We = g(() => {
        j(), le(), ht(), Te(), L(), oe(), st(), H(), me();
      }), Je = g(() => {
      }), ut = {};
      v(ut, { default: () => kt });
      var mt, vt, kt, At = g(() => {
        var e;
        Th(), lr(), Rr(), mt = "ort-wasm-proxy-worker", vt = ((e = globalThis.self) == null ? void 0 : e.name) === mt, vt && (self.onmessage = (t) => {
          let { type: s, in: n } = t.data;
          try {
            switch (s) {
              case "init-wasm":
                Ir(n.wasm).then(() => {
                  Ai(n).then(() => {
                    postMessage({ type: s });
                  }, (i) => {
                    postMessage({ type: s, err: i });
                  });
                }, (i) => {
                  postMessage({ type: s, err: i });
                });
                break;
              case "init-ep": {
                let { epName: i, env: a } = n;
                Kn(a, i).then(() => {
                  postMessage({ type: s });
                }, (o) => {
                  postMessage({ type: s, err: o });
                });
                break;
              }
              case "copy-from": {
                let { buffer: i } = n, a = Hn(i);
                postMessage({ type: s, out: a });
                break;
              }
              case "create": {
                let { model: i, options: a } = n;
                qn(i, a).then((o) => {
                  postMessage({ type: s, out: o });
                }, (o) => {
                  postMessage({ type: s, err: o });
                });
                break;
              }
              case "release":
                Ec(n), postMessage({ type: s });
                break;
              case "run": {
                let { sessionId: i, inputIndices: a, inputs: o, outputIndices: u, options: p } = n;
                kc(i, a, o, u, new Array(u.length).fill(null), p).then((h) => {
                  h.some((k) => k[3] !== "cpu") ? postMessage({ type: s, err: "Proxy does not support non-cpu tensor location." }) : postMessage({ type: s, out: h }, hr([...o, ...h]));
                }, (h) => {
                  postMessage({ type: s, err: h });
                });
                break;
              }
              case "end-profiling":
                qr(n), postMessage({ type: s });
                break;
              default:
            }
          } catch (i) {
            postMessage({ type: s, err: i });
          }
        }), kt = vt ? null : (t) => new Worker(t ?? Us, { type: "module", name: mt });
      }), is = {};
      v(is, { default: () => Ds });
      var ys, Cs, Ds, sr = g(() => {
        var e;
        Cs = (ys = import.meta.url, async function(t = {}) {
          function s() {
            return Ot.buffer != Rt.buffer && Ps(), Rt;
          }
          function n() {
            return Ot.buffer != Rt.buffer && Ps(), _s;
          }
          function i() {
            return Ot.buffer != Rt.buffer && Ps(), ot;
          }
          function a() {
            return Ot.buffer != Rt.buffer && Ps(), Et;
          }
          function o() {
            return Ot.buffer != Rt.buffer && Ps(), ps;
          }
          function u() {
            return Ot.buffer != Rt.buffer && Ps(), Ns;
          }
          function p() {
            return Ot.buffer != Rt.buffer && Ps(), xr;
          }
          function h() {
            return Ot.buffer != Rt.buffer && Ps(), Oi;
          }
          var k, C, d = Object.assign({}, t), z = new Promise((l, m) => {
            k = l, C = m;
          }), B = typeof window == "object", V = typeof importScripts == "function", Z = V && self.name == "em-pthread";
          d.mountExternalData = (l, m) => {
            l.startsWith("./") && (l = l.substring(2)), (d.Fb || (d.Fb = /* @__PURE__ */ new Map())).set(l, m);
          }, d.unmountExternalData = () => {
            delete d.Fb;
          };
          var ee = globalThis.SharedArrayBuffer ?? new WebAssembly.Memory({ initial: 0, maximum: 0, shared: !0 }).buffer.constructor;
          let X = () => {
            let l = (E, O, G) => (...we) => {
              let Xe = mn, nt = O == null ? void 0 : O();
              we = E(...we);
              let wt = O == null ? void 0 : O();
              return nt !== wt && (E = wt, G(nt), O = G = null), mn != Xe ? new Promise((Ct, Wt) => {
                _h = { resolve: Ct, reject: Wt };
              }) : we;
            }, m = (E) => async (...O) => {
              var G;
              try {
                if (d.Gb) throw Error("Session already started");
                let we = d.Gb = { hc: O[0], errors: [] }, Xe = await E(...O);
                if (d.Gb !== we) throw Error("Session mismatch");
                (G = d.Hb) == null || G.flush();
                let nt = we.errors;
                if (0 < nt.length) {
                  let wt = await Promise.all(nt);
                  if (wt = wt.filter((Ct) => Ct), 0 < wt.length) throw Error(wt.join(`
`));
                }
                return Xe;
              } finally {
                d.Gb = null;
              }
            };
            d._OrtCreateSession = l(d._OrtCreateSession, () => d._OrtCreateSession, (E) => d._OrtCreateSession = E), d._OrtRun = m(l(d._OrtRun, () => d._OrtRun, (E) => d._OrtRun = E)), d._OrtRunWithBinding = m(l(d._OrtRunWithBinding, () => d._OrtRunWithBinding, (E) => d._OrtRunWithBinding = E)), d._OrtBindInput = l(d._OrtBindInput, () => d._OrtBindInput, (E) => d._OrtBindInput = E), X = void 0;
          };
          d.jsepInit = (l, m) => {
            if (X == null || X(), l === "webgpu") {
              [d.Hb, d.Vb, d.Zb, d.Ob, d.Yb, d.kb, d.$b, d.cc, d.Wb, d.Xb, d.ac] = m;
              let E = d.Hb;
              d.jsepRegisterBuffer = (O, G, we, Xe) => E.registerBuffer(O, G, we, Xe), d.jsepGetBuffer = (O) => E.getBuffer(O), d.jsepCreateDownloader = (O, G, we) => E.createDownloader(O, G, we), d.jsepOnCreateSession = (O) => {
                E.onCreateSession(O);
              }, d.jsepOnReleaseSession = (O) => {
                E.onReleaseSession(O);
              }, d.jsepOnRunStart = (O) => E.onRunStart(O), d.dc = (O, G) => {
                E.upload(O, G);
              };
            } else if (l === "webnn") {
              [d.Hb, d.bc, d.Pb, d.jsepEnsureTensor, d.ec, d.jsepDownloadTensor] = m, d.jsepReleaseTensorId = d.Pb;
              let E = d.Hb;
              d.jsepOnRunStart = (O) => E.onRunStart(O), d.jsepRegisterMLContext = (O, G) => {
                E.registerMLContext(O, G);
              }, d.jsepOnReleaseSession = (O) => {
                E.onReleaseSession(O);
              }, d.jsepCreateMLTensorDownloader = (O, G) => E.createMLTensorDownloader(O, G), d.jsepRegisterMLTensor = (O, G, we) => E.registerMLTensor(O, G, we), d.jsepCreateMLContext = (O) => E.createMLContext(O), d.jsepRegisterMLConstant = (O, G, we, Xe, nt) => E.registerMLConstant(O, G, we, Xe, nt, d.Fb);
            }
          };
          var he, pe, Me = Object.assign({}, d), Oe = "./this.program", Le = (l, m) => {
            throw m;
          }, Ye = "";
          (B || V) && (V ? Ye = self.location.href : typeof document < "u" && document.currentScript && (Ye = document.currentScript.src), ys && (Ye = ys), Ye = Ye.startsWith("blob:") ? "" : Ye.substr(0, Ye.replace(/[?#].*/, "").lastIndexOf("/") + 1), V && (pe = (l) => {
            var m = new XMLHttpRequest();
            return m.open("GET", l, !1), m.responseType = "arraybuffer", m.send(null), new Uint8Array(m.response);
          }), he = (l, m, E) => {
            var O = new XMLHttpRequest();
            O.open("GET", l, !0), O.responseType = "arraybuffer", O.onload = () => {
              O.status == 200 || O.status == 0 && O.response ? m(O.response) : E();
            }, O.onerror = E, O.send(null);
          });
          var at, Pt = console.log.bind(console), Xt = console.error.bind(console), Zt = Pt, bt = Xt;
          if (Object.assign(d, Me), Me = null, Z) {
            let l = function(m) {
              try {
                var E = m.data, O = E.cmd;
                if (O === "load") {
                  let G = [];
                  self.onmessage = (we) => G.push(we), self.startWorker = () => {
                    postMessage({ cmd: "loaded" });
                    for (let we of G) l(we);
                    self.onmessage = l;
                  };
                  for (let we of E.handlers) d[we] && !d[we].proxy || (d[we] = (...Xe) => {
                    postMessage({ Nb: "callHandler", pc: we, args: Xe });
                  }, we == "print" && (Zt = d[we]), we == "printErr" && (bt = d[we]));
                  Ot = E.wasmMemory, Ps(), ss(E.wasmModule);
                } else if (O === "run") {
                  Mh(E.pthread_ptr, 0, 0, 1, 0, 0), mh(E.pthread_ptr), _f(), qh(), St || (Gm(), St = !0);
                  try {
                    gf(E.start_routine, E.arg);
                  } catch (G) {
                    if (G != "unwind") throw G;
                  }
                } else O === "cancel" ? Fa() && Vp(-1) : E.target !== "setimmediate" && (O === "checkMailbox" ? St && Fp() : O && (bt(`worker: received unknown command ${O}`), bt(E)));
              } catch (G) {
                throw Km(), G;
              }
            };
            var ss, St = !1;
            bt = function(...m) {
              m = m.join(" "), console.error(m);
            }, self.alert = function(...m) {
              postMessage({ Nb: "alert", text: m.join(" "), rc: Fa() });
            }, d.instantiateWasm = (m, E) => new Promise((O) => {
              ss = (G) => {
                G = new WebAssembly.Instance(G, Vh()), E(G), O();
              };
            }), self.onunhandledrejection = (m) => {
              throw m.reason || m;
            }, self.onmessage = l;
          }
          d.wasmBinary && (at = d.wasmBinary);
          var Ot, bs, Ht, Rt, _s, ot, Et, ps, Ns, xr, er, cn, Oi, zs = !1;
          function Ps() {
            var l = Ot.buffer;
            d.HEAP8 = Rt = new Int8Array(l), d.HEAP16 = ot = new Int16Array(l), d.HEAPU8 = _s = new Uint8Array(l), d.HEAPU16 = Et = new Uint16Array(l), d.HEAP32 = ps = new Int32Array(l), d.HEAPU32 = Ns = new Uint32Array(l), d.HEAPF32 = xr = new Float32Array(l), d.HEAPF64 = Oi = new Float64Array(l), d.HEAP64 = er = new BigInt64Array(l), d.HEAPU64 = cn = new BigUint64Array(l);
          }
          if (!Z) {
            if (!((Ot = new WebAssembly.Memory({ initial: 256, maximum: 65536, shared: !0 })).buffer instanceof ee)) throw bt("requested a shared WebAssembly.Memory but the returned buffer is not a SharedArrayBuffer, indicating that while the browser has SharedArrayBuffer it does not have WebAssembly threads support - you may need to set a flag"), Error("bad memory");
            Ps();
          }
          var Xn = [], Ac = [], th = [], Ic = 0, Fc = null;
          function Bh() {
            if (--Ic == 0 && Fc) {
              var l = Fc;
              Fc = null, l();
            }
          }
          function Yn(l) {
            throw bt(l = "Aborted(" + l + ")"), zs = !0, Ht = 1, l = new WebAssembly.RuntimeError(l + ". Build with -sASSERTIONS for more info."), C(l), l;
          }
          var sh, Rh = (l) => l.startsWith("data:application/octet-stream;base64,"), Nh = (l) => l.startsWith("file://");
          function jh(l) {
            if (l == sh && at) return new Uint8Array(at);
            if (pe) return pe(l);
            throw "both async and sync fetching of the wasm failed";
          }
          function Uh(l, m, E) {
            return function(O) {
              if (!at && (B || V)) {
                if (typeof fetch == "function" && !Nh(O)) return fetch(O, { credentials: "same-origin" }).then((G) => {
                  if (!G.ok) throw `failed to load wasm binary file at '${O}'`;
                  return G.arrayBuffer();
                }).catch(() => jh(O));
                if (he) return new Promise((G, we) => {
                  he(O, (Xe) => G(new Uint8Array(Xe)), we);
                });
              }
              return Promise.resolve().then(() => jh(O));
            }(l).then((O) => WebAssembly.instantiate(O, m)).then(E, (O) => {
              bt(`failed to asynchronously prepare wasm: ${O}`), Yn(O);
            });
          }
          function Vh() {
            return { a: { O: ff, Aa: mf, b: yf, aa: Jh, B: tm, qa: sm, Y: nm, _: im, ra: om, oa: am, ha: lm, na: um, L: dm, Z: cm, W: pm, pa: hm, X: mm, va: Mf, F: bf, Q: vf, P: Tf, E: Ef, u: Cf, q: kf, G: Sf, A: Lf, R: zf, ua: Bf, ka: Rf, U: Nf, ba: jf, H: Uf, ja: mh, ta: Vf, t: Wf, Ba: Gf, x: qf, n: Qf, l: Yf, c: ph, o: Jf, j: t_, w: s_, p: r_, f: n_, s: i_, m: o_, e: a_, k: l_, i: u_, h: d_, d: c_, ea: p_, fa: h_, ga: m_, ca: km, da: Sm, T: f_, g: __, D: g_, I: w_, M: y_, y: M_, sa: b_, V: v_, v: Am, z: x_, N: T_, S: P_, za: E_, ya: C_, la: Om, ma: Dm, $: ah, C: Lm, K: zm, ia: Bm, J: Rm, a: Ot, xa: oh, wa: Um, r: $_ } };
          }
          var rh = { 913700: (l, m, E, O, G) => {
            if (d === void 0 || !d.Fb) return 1;
            if ((l = Qs(Number(l >>> 0))).startsWith("./") && (l = l.substring(2)), !(l = d.Fb.get(l))) return 2;
            if (m = Number(m >>> 0), E = Number(E >>> 0), O = Number(O >>> 0), m + E > l.byteLength) return 3;
            try {
              let we = l.subarray(m, m + E);
              switch (G) {
                case 0:
                  n().set(we, O >>> 0);
                  break;
                case 1:
                  d.dc(O, we);
                  break;
                default:
                  return 4;
              }
              return 0;
            } catch {
              return 4;
            }
          }, 914415: (l, m, E) => {
            d.ec(l, n().subarray(m >>> 0, m + E >>> 0));
          }, 914478: () => d.bc(), 914519: (l) => {
            d.Pb(l);
          }, 914555: () => {
            d.Wb();
          }, 914586: () => {
            d.Xb();
          }, 914615: () => {
            d.ac();
          }, 914640: (l) => d.Vb(l), 914673: (l) => d.Zb(l), 914705: (l, m, E) => {
            d.Ob(Number(l), Number(m), Number(E), !0);
          }, 914768: (l, m, E) => {
            d.Ob(Number(l), Number(m), Number(E));
          }, 914825: () => typeof wasmOffsetConverter < "u", 914882: (l) => {
            d.kb("Abs", l, void 0);
          }, 914933: (l) => {
            d.kb("Neg", l, void 0);
          }, 914984: (l) => {
            d.kb("Floor", l, void 0);
          }, 915037: (l) => {
            d.kb("Ceil", l, void 0);
          }, 915089: (l) => {
            d.kb("Reciprocal", l, void 0);
          }, 915147: (l) => {
            d.kb("Sqrt", l, void 0);
          }, 915199: (l) => {
            d.kb("Exp", l, void 0);
          }, 915250: (l) => {
            d.kb("Erf", l, void 0);
          }, 915301: (l) => {
            d.kb("Sigmoid", l, void 0);
          }, 915356: (l, m, E) => {
            d.kb("HardSigmoid", l, { alpha: m, beta: E });
          }, 915435: (l) => {
            d.kb("Log", l, void 0);
          }, 915486: (l) => {
            d.kb("Sin", l, void 0);
          }, 915537: (l) => {
            d.kb("Cos", l, void 0);
          }, 915588: (l) => {
            d.kb("Tan", l, void 0);
          }, 915639: (l) => {
            d.kb("Asin", l, void 0);
          }, 915691: (l) => {
            d.kb("Acos", l, void 0);
          }, 915743: (l) => {
            d.kb("Atan", l, void 0);
          }, 915795: (l) => {
            d.kb("Sinh", l, void 0);
          }, 915847: (l) => {
            d.kb("Cosh", l, void 0);
          }, 915899: (l) => {
            d.kb("Asinh", l, void 0);
          }, 915952: (l) => {
            d.kb("Acosh", l, void 0);
          }, 916005: (l) => {
            d.kb("Atanh", l, void 0);
          }, 916058: (l) => {
            d.kb("Tanh", l, void 0);
          }, 916110: (l) => {
            d.kb("Not", l, void 0);
          }, 916161: (l, m, E) => {
            d.kb("Clip", l, { min: m, max: E });
          }, 916230: (l) => {
            d.kb("Clip", l, void 0);
          }, 916282: (l, m) => {
            d.kb("Elu", l, { alpha: m });
          }, 916340: (l) => {
            d.kb("Gelu", l, void 0);
          }, 916392: (l) => {
            d.kb("Relu", l, void 0);
          }, 916444: (l, m) => {
            d.kb("LeakyRelu", l, { alpha: m });
          }, 916508: (l, m) => {
            d.kb("ThresholdedRelu", l, { alpha: m });
          }, 916578: (l, m) => {
            d.kb("Cast", l, { to: m });
          }, 916636: (l) => {
            d.kb("Add", l, void 0);
          }, 916687: (l) => {
            d.kb("Sub", l, void 0);
          }, 916738: (l) => {
            d.kb("Mul", l, void 0);
          }, 916789: (l) => {
            d.kb("Div", l, void 0);
          }, 916840: (l) => {
            d.kb("Pow", l, void 0);
          }, 916891: (l) => {
            d.kb("Equal", l, void 0);
          }, 916944: (l) => {
            d.kb("Greater", l, void 0);
          }, 916999: (l) => {
            d.kb("GreaterOrEqual", l, void 0);
          }, 917061: (l) => {
            d.kb("Less", l, void 0);
          }, 917113: (l) => {
            d.kb("LessOrEqual", l, void 0);
          }, 917172: (l, m, E, O, G) => {
            d.kb("ReduceMean", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 917347: (l, m, E, O, G) => {
            d.kb("ReduceMax", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 917521: (l, m, E, O, G) => {
            d.kb("ReduceMin", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 917695: (l, m, E, O, G) => {
            d.kb("ReduceProd", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 917870: (l, m, E, O, G) => {
            d.kb("ReduceSum", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 918044: (l, m, E, O, G) => {
            d.kb("ReduceL1", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 918217: (l, m, E, O, G) => {
            d.kb("ReduceL2", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 918390: (l, m, E, O, G) => {
            d.kb("ReduceLogSum", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 918567: (l, m, E, O, G) => {
            d.kb("ReduceSumSquare", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 918747: (l, m, E, O, G) => {
            d.kb("ReduceLogSumExp", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 918927: (l) => {
            d.kb("Where", l, void 0);
          }, 918980: (l, m, E) => {
            d.kb("Transpose", l, { perm: m ? Array.from(o().subarray(Number(m) >>> 0, Number(E) >>> 0)) : [] });
          }, 919104: (l, m, E, O) => {
            d.kb("DepthToSpace", l, { blocksize: m, mode: Qs(E), format: O ? "NHWC" : "NCHW" });
          }, 919237: (l, m, E, O) => {
            d.kb("DepthToSpace", l, { blocksize: m, mode: Qs(E), format: O ? "NHWC" : "NCHW" });
          }, 919370: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He, ls) => {
            d.kb("ConvTranspose", l, { format: wt ? "NHWC" : "NCHW", autoPad: m, dilations: [E], group: O, kernelShape: [G], pads: [we, Xe], strides: [nt], wIsConst: () => !!s()[Ct >>> 0], outputPadding: Wt ? Array.from(o().subarray(Number(Wt) >>> 0, Number(hs) >>> 0)) : [], outputShape: vs ? Array.from(o().subarray(Number(vs) >>> 0, Number(He) >>> 0)) : [], activation: Qs(ls) });
          }, 919803: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He) => {
            d.kb("ConvTranspose", l, { format: nt ? "NHWC" : "NCHW", autoPad: m, dilations: Array.from(o().subarray(Number(E) >>> 0, 2 + (Number(E) >>> 0) >>> 0)), group: O, kernelShape: Array.from(o().subarray(Number(G) >>> 0, 2 + (Number(G) >>> 0) >>> 0)), pads: Array.from(o().subarray(Number(we) >>> 0, 4 + (Number(we) >>> 0) >>> 0)), strides: Array.from(o().subarray(Number(Xe) >>> 0, 2 + (Number(Xe) >>> 0) >>> 0)), wIsConst: () => !!s()[wt >>> 0], outputPadding: Ct ? Array.from(o().subarray(Number(Ct) >>> 0, Number(Wt) >>> 0)) : [], outputShape: hs ? Array.from(o().subarray(Number(hs) >>> 0, Number(vs) >>> 0)) : [], activation: Qs(He) });
          }, 920464: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He, ls) => {
            d.kb("ConvTranspose", l, { format: wt ? "NHWC" : "NCHW", autoPad: m, dilations: [E], group: O, kernelShape: [G], pads: [we, Xe], strides: [nt], wIsConst: () => !!s()[Ct >>> 0], outputPadding: Wt ? Array.from(o().subarray(Number(Wt) >>> 0, Number(hs) >>> 0)) : [], outputShape: vs ? Array.from(o().subarray(Number(vs) >>> 0, Number(He) >>> 0)) : [], activation: Qs(ls) });
          }, 920897: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He) => {
            d.kb("ConvTranspose", l, { format: nt ? "NHWC" : "NCHW", autoPad: m, dilations: Array.from(o().subarray(Number(E) >>> 0, 2 + (Number(E) >>> 0) >>> 0)), group: O, kernelShape: Array.from(o().subarray(Number(G) >>> 0, 2 + (Number(G) >>> 0) >>> 0)), pads: Array.from(o().subarray(Number(we) >>> 0, 4 + (Number(we) >>> 0) >>> 0)), strides: Array.from(o().subarray(Number(Xe) >>> 0, 2 + (Number(Xe) >>> 0) >>> 0)), wIsConst: () => !!s()[wt >>> 0], outputPadding: Ct ? Array.from(o().subarray(Number(Ct) >>> 0, Number(Wt) >>> 0)) : [], outputShape: hs ? Array.from(o().subarray(Number(hs) >>> 0, Number(vs) >>> 0)) : [], activation: Qs(He) });
          }, 921558: (l, m) => {
            d.kb("GlobalAveragePool", l, { format: m ? "NHWC" : "NCHW" });
          }, 921649: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He) => {
            d.kb("AveragePool", l, { format: He ? "NHWC" : "NCHW", auto_pad: m, ceil_mode: E, count_include_pad: O, storage_order: G, dilations: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Xe) >>> 0)) : [], kernel_shape: nt ? Array.from(o().subarray(Number(nt) >>> 0, Number(wt) >>> 0)) : [], pads: Ct ? Array.from(o().subarray(Number(Ct) >>> 0, Number(Wt) >>> 0)) : [], strides: hs ? Array.from(o().subarray(Number(hs) >>> 0, Number(vs) >>> 0)) : [] });
          }, 922128: (l, m) => {
            d.kb("GlobalAveragePool", l, { format: m ? "NHWC" : "NCHW" });
          }, 922219: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He) => {
            d.kb("AveragePool", l, { format: He ? "NHWC" : "NCHW", auto_pad: m, ceil_mode: E, count_include_pad: O, storage_order: G, dilations: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Xe) >>> 0)) : [], kernel_shape: nt ? Array.from(o().subarray(Number(nt) >>> 0, Number(wt) >>> 0)) : [], pads: Ct ? Array.from(o().subarray(Number(Ct) >>> 0, Number(Wt) >>> 0)) : [], strides: hs ? Array.from(o().subarray(Number(hs) >>> 0, Number(vs) >>> 0)) : [] });
          }, 922698: (l, m) => {
            d.kb("GlobalMaxPool", l, { format: m ? "NHWC" : "NCHW" });
          }, 922785: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He) => {
            d.kb("MaxPool", l, { format: He ? "NHWC" : "NCHW", auto_pad: m, ceil_mode: E, count_include_pad: O, storage_order: G, dilations: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Xe) >>> 0)) : [], kernel_shape: nt ? Array.from(o().subarray(Number(nt) >>> 0, Number(wt) >>> 0)) : [], pads: Ct ? Array.from(o().subarray(Number(Ct) >>> 0, Number(Wt) >>> 0)) : [], strides: hs ? Array.from(o().subarray(Number(hs) >>> 0, Number(vs) >>> 0)) : [] });
          }, 923260: (l, m) => {
            d.kb("GlobalMaxPool", l, { format: m ? "NHWC" : "NCHW" });
          }, 923347: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He) => {
            d.kb("MaxPool", l, { format: He ? "NHWC" : "NCHW", auto_pad: m, ceil_mode: E, count_include_pad: O, storage_order: G, dilations: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Xe) >>> 0)) : [], kernel_shape: nt ? Array.from(o().subarray(Number(nt) >>> 0, Number(wt) >>> 0)) : [], pads: Ct ? Array.from(o().subarray(Number(Ct) >>> 0, Number(Wt) >>> 0)) : [], strides: hs ? Array.from(o().subarray(Number(hs) >>> 0, Number(vs) >>> 0)) : [] });
          }, 923822: (l, m, E, O, G) => {
            d.kb("Gemm", l, { alpha: m, beta: E, transA: O, transB: G });
          }, 923926: (l) => {
            d.kb("MatMul", l, void 0);
          }, 923980: (l, m, E, O) => {
            d.kb("ArgMax", l, { keepDims: !!m, selectLastIndex: !!E, axis: O });
          }, 924088: (l, m, E, O) => {
            d.kb("ArgMin", l, { keepDims: !!m, selectLastIndex: !!E, axis: O });
          }, 924196: (l, m) => {
            d.kb("Softmax", l, { axis: m });
          }, 924259: (l, m) => {
            d.kb("Concat", l, { axis: m });
          }, 924319: (l, m, E, O, G) => {
            d.kb("Split", l, { axis: m, numOutputs: E, splitSizes: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 924475: (l) => {
            d.kb("Expand", l, void 0);
          }, 924529: (l, m) => {
            d.kb("Gather", l, { axis: Number(m) });
          }, 924600: (l, m) => {
            d.kb("GatherElements", l, { axis: Number(m) });
          }, 924679: (l, m) => {
            d.kb("GatherND", l, { batch_dims: Number(m) });
          }, 924758: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt) => {
            d.kb("Resize", l, { antialias: m, axes: E ? Array.from(o().subarray(Number(E) >>> 0, Number(O) >>> 0)) : [], coordinateTransformMode: Qs(G), cubicCoeffA: we, excludeOutside: Xe, extrapolationValue: nt, keepAspectRatioPolicy: Qs(wt), mode: Qs(Ct), nearestMode: Qs(Wt) });
          }, 925120: (l, m, E, O, G, we, Xe) => {
            d.kb("Slice", l, { starts: m ? Array.from(o().subarray(Number(m) >>> 0, Number(E) >>> 0)) : [], ends: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [], axes: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Xe) >>> 0)) : [] });
          }, 925384: (l) => {
            d.kb("Tile", l, void 0);
          }, 925436: (l, m, E) => {
            d.kb("InstanceNormalization", l, { epsilon: m, format: E ? "NHWC" : "NCHW" });
          }, 925550: (l, m, E) => {
            d.kb("InstanceNormalization", l, { epsilon: m, format: E ? "NHWC" : "NCHW" });
          }, 925664: (l) => {
            d.kb("Range", l, void 0);
          }, 925717: (l, m) => {
            d.kb("Einsum", l, { equation: Qs(m) });
          }, 925798: (l, m, E, O, G) => {
            d.kb("Pad", l, { mode: m, value: E, pads: O ? Array.from(o().subarray(Number(O) >>> 0, Number(G) >>> 0)) : [] });
          }, 925941: (l, m, E, O, G, we) => {
            d.kb("BatchNormalization", l, { epsilon: m, momentum: E, spatial: !!G, trainingMode: !!O, format: we ? "NHWC" : "NCHW" });
          }, 926110: (l, m, E, O, G, we) => {
            d.kb("BatchNormalization", l, { epsilon: m, momentum: E, spatial: !!G, trainingMode: !!O, format: we ? "NHWC" : "NCHW" });
          }, 926279: (l, m, E) => {
            d.kb("CumSum", l, { exclusive: Number(m), reverse: Number(E) });
          }, 926376: (l, m, E) => {
            d.kb("DequantizeLinear", l, { axis: m, blockSize: E });
          }, 926466: (l, m, E, O, G) => {
            d.kb("GridSample", l, { align_corners: m, mode: Qs(E), padding_mode: Qs(O), format: G ? "NHWC" : "NCHW" });
          }, 926636: (l, m, E, O, G) => {
            d.kb("GridSample", l, { align_corners: m, mode: Qs(E), padding_mode: Qs(O), format: G ? "NHWC" : "NCHW" });
          }, 926806: (l, m, E, O, G, we, Xe, nt, wt) => {
            d.kb("Attention", l, { numHeads: m, isUnidirectional: E, maskFilterValue: O, scale: G, doRotary: we, qkvHiddenSizes: Xe ? Array.from(o().subarray(Number(nt) >>> 0, Number(nt) + Xe >>> 0)) : [], pastPresentShareBuffer: !!wt });
          }, 927078: (l) => {
            d.kb("BiasAdd", l, void 0);
          }, 927133: (l) => {
            d.kb("BiasSplitGelu", l, void 0);
          }, 927194: (l) => {
            d.kb("FastGelu", l, void 0);
          }, 927250: (l, m, E, O, G, we, Xe, nt, wt, Ct, Wt, hs, vs, He, ls, js) => {
            d.kb("Conv", l, { format: hs ? "NHWC" : "NCHW", auto_pad: m, dilations: E ? Array.from(o().subarray(Number(E) >>> 0, Number(O) >>> 0)) : [], group: G, kernel_shape: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Xe) >>> 0)) : [], pads: nt ? Array.from(o().subarray(Number(nt) >>> 0, Number(wt) >>> 0)) : [], strides: Ct ? Array.from(o().subarray(Number(Ct) >>> 0, Number(Wt) >>> 0)) : [], w_is_const: () => !!s()[Number(vs) >>> 0], activation: Qs(He), activation_params: ls ? Array.from(p().subarray(Number(ls) >>> 0, Number(js) >>> 0)) : [] });
          }, 927834: (l) => {
            d.kb("Gelu", l, void 0);
          }, 927886: (l, m, E, O, G, we, Xe, nt, wt) => {
            d.kb("GroupQueryAttention", l, { numHeads: m, kvNumHeads: E, scale: O, softcap: G, doRotary: we, rotaryInterleaved: Xe, smoothSoftmax: nt, localWindowSize: wt });
          }, 928103: (l, m, E, O) => {
            d.kb("LayerNormalization", l, { axis: m, epsilon: E, simplified: !!O });
          }, 928214: (l, m, E, O) => {
            d.kb("LayerNormalization", l, { axis: m, epsilon: E, simplified: !!O });
          }, 928325: (l, m, E, O, G, we) => {
            d.kb("MatMulNBits", l, { k: m, n: E, accuracyLevel: O, bits: G, blockSize: we });
          }, 928452: (l, m, E, O, G, we) => {
            d.kb("MultiHeadAttention", l, { numHeads: m, isUnidirectional: E, maskFilterValue: O, scale: G, doRotary: we });
          }, 928611: (l, m) => {
            d.kb("QuickGelu", l, { alpha: m });
          }, 928675: (l, m, E, O, G) => {
            d.kb("RotaryEmbedding", l, { interleaved: !!m, numHeads: E, rotaryEmbeddingDim: O, scale: G });
          }, 928814: (l, m, E) => {
            d.kb("SkipLayerNormalization", l, { epsilon: m, simplified: !!E });
          }, 928916: (l, m, E) => {
            d.kb("SkipLayerNormalization", l, { epsilon: m, simplified: !!E });
          }, 929018: (l, m, E, O) => {
            d.kb("GatherBlockQuantized", l, { gatherAxis: m, quantizeAxis: E, blockSize: O });
          }, 929139: (l) => {
            d.$b(l);
          }, 929173: (l, m) => d.cc(Number(l), Number(m), d.Gb.hc, d.Gb.errors) };
          function mf(l, m, E) {
            return xm(async () => {
              await d.Yb(Number(l), Number(m), Number(E));
            });
          }
          function ff() {
            return typeof wasmOffsetConverter < "u";
          }
          function nh(l) {
            this.name = "ExitStatus", this.message = `Program terminated with exit(${l})`, this.status = l;
          }
          var ih = (l) => {
            l.terminate(), l.onmessage = () => {
            };
          }, Wh = (l) => {
            Jn.length == 0 && (Xh(), Qh(Jn[0]));
            var m = Jn.pop();
            if (!m) return 6;
            Li.push(m), pn[l.Bb] = m, m.Bb = l.Bb;
            var E = { cmd: "run", start_routine: l.ic, arg: l.Rb, pthread_ptr: l.Bb };
            return m.postMessage(E, l.nc), 0;
          }, Di = 0, Bs = (l, m, ...E) => {
            for (var O = 2 * E.length, G = xh(), we = vh(8 * O), Xe = we >>> 3, nt = 0; nt < E.length; nt++) {
              var wt = E[nt];
              typeof wt == "bigint" ? (er[Xe + 2 * nt] = 1n, er[Xe + 2 * nt + 1] = wt) : (er[Xe + 2 * nt] = 0n, h()[Xe + 2 * nt + 1 >>> 0] = wt);
            }
            return l = Hm(l, 0, O, we, m), Wp(G), l;
          };
          function oh(l) {
            if (Z) return Bs(0, 1, l);
            if (Ht = l, !(0 < Di)) {
              for (var m of Li) ih(m);
              for (m of Jn) ih(m);
              Jn = [], Li = [], pn = [], zs = !0;
            }
            Le(l, new nh(l));
          }
          function Gh(l) {
            if (Z) return Bs(1, 0, l);
            ah(l);
          }
          var ah = (l) => {
            if (Ht = l, Z) throw Gh(l), "unwind";
            oh(l);
          }, Jn = [], Li = [], Kh = [], pn = {}, Hh = (l) => {
            var m = l.Bb;
            delete pn[m], Jn.push(l), Li.splice(Li.indexOf(l), 1), l.Bb = 0, bh(m);
          };
          function qh() {
            Kh.forEach((l) => l());
          }
          var Qh = (l) => new Promise((m) => {
            l.onmessage = (G) => {
              var we = (G = G.data).cmd;
              if (G.targetThread && G.targetThread != Fa()) {
                var Xe = pn[G.targetThread];
                Xe ? Xe.postMessage(G, G.transferList) : bt(`Internal error! Worker sent a message "${we}" to target pthread ${G.targetThread}, but that thread no longer exists!`);
              } else we === "checkMailbox" ? Fp() : we === "spawnThread" ? Wh(G) : we === "cleanupThread" ? Hh(pn[G.thread]) : we === "killThread" ? (G = G.thread, we = pn[G], delete pn[G], ih(we), bh(G), Li.splice(Li.indexOf(we), 1), we.Bb = 0) : we === "cancelThread" ? pn[G.thread].postMessage({ cmd: "cancel" }) : we === "loaded" ? (l.loaded = !0, m(l)) : we === "alert" ? alert(`Thread ${G.threadId}: ${G.text}`) : G.target === "setimmediate" ? l.postMessage(G) : we === "callHandler" ? d[G.handler](...G.args) : we && bt(`worker sent an unknown command ${we}`);
            }, l.onerror = (G) => {
              throw bt(`worker sent an error! ${G.filename}:${G.lineno}: ${G.message}`), G;
            };
            var E, O = [];
            for (E of []) d.hasOwnProperty(E) && O.push(E);
            l.postMessage({ cmd: "load", handlers: O, wasmMemory: Ot, wasmModule: bs });
          });
          function Xh() {
            var l = new Worker(import.meta.url.startsWith("file:") ? new URL(
              /* asset import */
              r(
                /*! ort.bundle.min.mjs */
                "./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs?46eb"
              ),
              r.b
            ) : new URL(import.meta.url), { type: "module", workerData: "em-pthread", name: "em-pthread" });
            Jn.push(l);
          }
          var Ip = (l) => {
            for (; 0 < l.length; ) l.shift()(d);
          }, _f = () => {
            var l = Fa(), m = u()[l + 52 >>> 2 >>> 0];
            l = u()[l + 56 >>> 2 >>> 0], Qm(m, m - l), Wp(m);
          }, gf = (l, m) => {
            Di = 0, l = Xm(l, m), 0 < Di ? Ht = l : Vp(l);
          };
          class wf {
            constructor(m) {
              this.Kb = m - 24;
            }
          }
          function yf(l, m, E) {
            var O = new wf(l >>>= 0);
            throw m >>>= 0, E >>>= 0, u()[O.Kb + 16 >>> 2 >>> 0] = 0, u()[O.Kb + 4 >>> 2 >>> 0] = m, u()[O.Kb + 8 >>> 2 >>> 0] = E, l;
          }
          function Yh(l, m, E, O) {
            return Z ? Bs(2, 1, l, m, E, O) : Jh(l, m, E, O);
          }
          function Jh(l, m, E, O) {
            if (l >>>= 0, m >>>= 0, E >>>= 0, O >>>= 0, ee === void 0) return bt("Current environment does not support SharedArrayBuffer, pthreads are not available!"), 6;
            var G = [];
            return Z && G.length === 0 ? Yh(l, m, E, O) : (l = { ic: E, Bb: l, Rb: O, nc: G }, Z ? (l.Nb = "spawnThread", postMessage(l, G), 0) : Wh(l));
          }
          var Zh = typeof TextDecoder < "u" ? new TextDecoder("utf8") : void 0, em = (l, m, E) => {
            var O = (m >>>= 0) + E;
            for (E = m; l[E] && !(E >= O); ) ++E;
            if (16 < E - m && l.buffer && Zh) return Zh.decode(l.buffer instanceof ee ? l.slice(m, E) : l.subarray(m, E));
            for (O = ""; m < E; ) {
              var G = l[m++];
              if (128 & G) {
                var we = 63 & l[m++];
                if ((224 & G) == 192) O += String.fromCharCode((31 & G) << 6 | we);
                else {
                  var Xe = 63 & l[m++];
                  65536 > (G = (240 & G) == 224 ? (15 & G) << 12 | we << 6 | Xe : (7 & G) << 18 | we << 12 | Xe << 6 | 63 & l[m++]) ? O += String.fromCharCode(G) : (G -= 65536, O += String.fromCharCode(55296 | G >> 10, 56320 | 1023 & G));
                }
              } else O += String.fromCharCode(G);
            }
            return O;
          }, Qs = (l, m) => (l >>>= 0) ? em(n(), l, m) : "";
          function tm(l, m, E) {
            return Z ? Bs(3, 1, l, m, E) : 0;
          }
          function sm(l, m) {
            if (Z) return Bs(4, 1, l, m);
          }
          var lh = (l) => {
            for (var m = 0, E = 0; E < l.length; ++E) {
              var O = l.charCodeAt(E);
              127 >= O ? m++ : 2047 >= O ? m += 2 : 55296 <= O && 57343 >= O ? (m += 4, ++E) : m += 3;
            }
            return m;
          }, rm = (l, m, E, O) => {
            if (!(0 < O)) return 0;
            var G = E >>>= 0;
            O = E + O - 1;
            for (var we = 0; we < l.length; ++we) {
              var Xe = l.charCodeAt(we);
              if (55296 <= Xe && 57343 >= Xe && (Xe = 65536 + ((1023 & Xe) << 10) | 1023 & l.charCodeAt(++we)), 127 >= Xe) {
                if (E >= O) break;
                m[E++ >>> 0] = Xe;
              } else {
                if (2047 >= Xe) {
                  if (E + 1 >= O) break;
                  m[E++ >>> 0] = 192 | Xe >> 6;
                } else {
                  if (65535 >= Xe) {
                    if (E + 2 >= O) break;
                    m[E++ >>> 0] = 224 | Xe >> 12;
                  } else {
                    if (E + 3 >= O) break;
                    m[E++ >>> 0] = 240 | Xe >> 18, m[E++ >>> 0] = 128 | Xe >> 12 & 63;
                  }
                  m[E++ >>> 0] = 128 | Xe >> 6 & 63;
                }
                m[E++ >>> 0] = 128 | 63 & Xe;
              }
            }
            return m[E >>> 0] = 0, E - G;
          }, Aa = (l, m, E) => rm(l, n(), m, E);
          function nm(l, m) {
            if (Z) return Bs(5, 1, l, m);
          }
          function im(l, m, E) {
            if (Z) return Bs(6, 1, l, m, E);
          }
          function om(l, m, E) {
            return Z ? Bs(7, 1, l, m, E) : 0;
          }
          function am(l, m) {
            if (Z) return Bs(8, 1, l, m);
          }
          function lm(l, m, E) {
            if (Z) return Bs(9, 1, l, m, E);
          }
          function um(l, m, E, O) {
            if (Z) return Bs(10, 1, l, m, E, O);
          }
          function dm(l, m, E, O) {
            if (Z) return Bs(11, 1, l, m, E, O);
          }
          function cm(l, m, E, O) {
            if (Z) return Bs(12, 1, l, m, E, O);
          }
          function pm(l) {
            if (Z) return Bs(13, 1, l);
          }
          function hm(l, m) {
            if (Z) return Bs(14, 1, l, m);
          }
          function mm(l, m, E) {
            if (Z) return Bs(15, 1, l, m, E);
          }
          var fm, Zn, Mf = () => {
            Yn("");
          }, hn = (l) => {
            for (var m = ""; n()[l >>> 0]; ) m += fm[n()[l++ >>> 0]];
            return m;
          }, uh = {}, dh = {};
          function Cn(l, m, E = {}) {
            if (!("argPackAdvance" in m)) throw new TypeError("registerType registeredInstance requires argPackAdvance");
            return function(O, G, we = {}) {
              var Xe = G.name;
              if (!O) throw new Zn(`type "${Xe}" must have a positive integer typeid pointer`);
              if (dh.hasOwnProperty(O)) {
                if (we.Tb) return;
                throw new Zn(`Cannot register type '${Xe}' twice`);
              }
              dh[O] = G, uh.hasOwnProperty(O) && (G = uh[O], delete uh[O], G.forEach((nt) => nt()));
            }(l, m, E);
          }
          var _m = (l, m, E) => {
            switch (m) {
              case 1:
                return E ? (O) => s()[O >>> 0] : (O) => n()[O >>> 0];
              case 2:
                return E ? (O) => i()[O >>> 1 >>> 0] : (O) => a()[O >>> 1 >>> 0];
              case 4:
                return E ? (O) => o()[O >>> 2 >>> 0] : (O) => u()[O >>> 2 >>> 0];
              case 8:
                return E ? (O) => er[O >>> 3] : (O) => cn[O >>> 3];
              default:
                throw new TypeError(`invalid integer width (${m}): ${l}`);
            }
          };
          function bf(l, m, E) {
            E >>>= 0, Cn(l >>>= 0, { name: m = hn(m >>> 0), fromWireType: (O) => O, toWireType: function(O, G) {
              if (typeof G != "bigint" && typeof G != "number") throw G = G === null ? "null" : (O = typeof G) == "object" || O === "array" || O === "function" ? G.toString() : "" + G, new TypeError(`Cannot convert "${G}" to ${this.name}`);
              return typeof G == "number" && (G = BigInt(G)), G;
            }, argPackAdvance: ei, readValueFromPointer: _m(m, E, m.indexOf("u") == -1), Eb: null });
          }
          var ei = 8;
          function vf(l, m, E, O) {
            Cn(l >>>= 0, { name: m = hn(m >>> 0), fromWireType: function(G) {
              return !!G;
            }, toWireType: function(G, we) {
              return we ? E : O;
            }, argPackAdvance: ei, readValueFromPointer: function(G) {
              return this.fromWireType(n()[G >>> 0]);
            }, Eb: null });
          }
          var ch = [], kn = [];
          function ph(l) {
            9 < (l >>>= 0) && --kn[l + 1] == 0 && (kn[l] = void 0, ch.push(l));
          }
          var Cr = (l) => {
            if (!l) throw new Zn("Cannot use deleted val. handle = " + l);
            return kn[l];
          }, zr = (l) => {
            switch (l) {
              case void 0:
                return 2;
              case null:
                return 4;
              case !0:
                return 6;
              case !1:
                return 8;
              default:
                let m = ch.pop() || kn.length;
                return kn[m] = l, kn[m + 1] = 1, m;
            }
          };
          function hh(l) {
            return this.fromWireType(u()[l >>> 2 >>> 0]);
          }
          var xf = { name: "emscripten::val", fromWireType: (l) => {
            var m = Cr(l);
            return ph(l), m;
          }, toWireType: (l, m) => zr(m), argPackAdvance: ei, readValueFromPointer: hh, Eb: null };
          function Tf(l) {
            return Cn(l >>> 0, xf);
          }
          var Pf = (l, m) => {
            switch (m) {
              case 4:
                return function(E) {
                  return this.fromWireType(p()[E >>> 2 >>> 0]);
                };
              case 8:
                return function(E) {
                  return this.fromWireType(h()[E >>> 3 >>> 0]);
                };
              default:
                throw new TypeError(`invalid float width (${m}): ${l}`);
            }
          };
          function Ef(l, m, E) {
            E >>>= 0, Cn(l >>>= 0, { name: m = hn(m >>> 0), fromWireType: (O) => O, toWireType: (O, G) => G, argPackAdvance: ei, readValueFromPointer: Pf(m, E), Eb: null });
          }
          function Cf(l, m, E, O, G) {
            if (l >>>= 0, E >>>= 0, m = hn(m >>> 0), G === -1 && (G = 4294967295), G = (nt) => nt, O === 0) {
              var we = 32 - 8 * E;
              G = (nt) => nt << we >>> we;
            }
            var Xe = m.includes("unsigned") ? function(nt, wt) {
              return wt >>> 0;
            } : function(nt, wt) {
              return wt;
            };
            Cn(l, { name: m, fromWireType: G, toWireType: Xe, argPackAdvance: ei, readValueFromPointer: _m(m, E, O !== 0), Eb: null });
          }
          function kf(l, m, E) {
            function O(we) {
              var Xe = u()[we >>> 2 >>> 0];
              return we = u()[we + 4 >>> 2 >>> 0], new G(s().buffer, we, Xe);
            }
            var G = [Int8Array, Uint8Array, Int16Array, Uint16Array, Int32Array, Uint32Array, Float32Array, Float64Array, BigInt64Array, BigUint64Array][m];
            Cn(l >>>= 0, { name: E = hn(E >>> 0), fromWireType: O, argPackAdvance: ei, readValueFromPointer: O }, { Tb: !0 });
          }
          function Sf(l, m) {
            l >>>= 0;
            var E = (m = hn(m >>> 0)) === "std::string";
            Cn(l, { name: m, fromWireType: function(O) {
              var G = u()[O >>> 2 >>> 0], we = O + 4;
              if (E) for (var Xe = we, nt = 0; nt <= G; ++nt) {
                var wt = we + nt;
                if (nt == G || n()[wt >>> 0] == 0) {
                  if (Xe = Qs(Xe, wt - Xe), Ct === void 0) var Ct = Xe;
                  else Ct += "\0", Ct += Xe;
                  Xe = wt + 1;
                }
              }
              else {
                for (Ct = Array(G), nt = 0; nt < G; ++nt) Ct[nt] = String.fromCharCode(n()[we + nt >>> 0]);
                Ct = Ct.join("");
              }
              return fn(O), Ct;
            }, toWireType: function(O, G) {
              G instanceof ArrayBuffer && (G = new Uint8Array(G));
              var we = typeof G == "string";
              if (!(we || G instanceof Uint8Array || G instanceof Uint8ClampedArray || G instanceof Int8Array)) throw new Zn("Cannot pass non-string to std::string");
              var Xe = E && we ? lh(G) : G.length, nt = Up(4 + Xe + 1), wt = nt + 4;
              if (u()[nt >>> 2 >>> 0] = Xe, E && we) Aa(G, wt, Xe + 1);
              else if (we) for (we = 0; we < Xe; ++we) {
                var Ct = G.charCodeAt(we);
                if (255 < Ct) throw fn(wt), new Zn("String has UTF-16 code units that do not fit in 8 bits");
                n()[wt + we >>> 0] = Ct;
              }
              else for (we = 0; we < Xe; ++we) n()[wt + we >>> 0] = G[we];
              return O !== null && O.push(fn, nt), nt;
            }, argPackAdvance: ei, readValueFromPointer: hh, Eb(O) {
              fn(O);
            } });
          }
          var gm = typeof TextDecoder < "u" ? new TextDecoder("utf-16le") : void 0, $f = (l, m) => {
            for (var E = l >> 1, O = E + m / 2; !(E >= O) && a()[E >>> 0]; ) ++E;
            if (32 < (E <<= 1) - l && gm) return gm.decode(n().slice(l, E));
            for (E = "", O = 0; !(O >= m / 2); ++O) {
              var G = i()[l + 2 * O >>> 1 >>> 0];
              if (G == 0) break;
              E += String.fromCharCode(G);
            }
            return E;
          }, Af = (l, m, E) => {
            if (E ?? (E = 2147483647), 2 > E) return 0;
            var O = m;
            E = (E -= 2) < 2 * l.length ? E / 2 : l.length;
            for (var G = 0; G < E; ++G) {
              var we = l.charCodeAt(G);
              i()[m >>> 1 >>> 0] = we, m += 2;
            }
            return i()[m >>> 1 >>> 0] = 0, m - O;
          }, If = (l) => 2 * l.length, Ff = (l, m) => {
            for (var E = 0, O = ""; !(E >= m / 4); ) {
              var G = o()[l + 4 * E >>> 2 >>> 0];
              if (G == 0) break;
              ++E, 65536 <= G ? (G -= 65536, O += String.fromCharCode(55296 | G >> 10, 56320 | 1023 & G)) : O += String.fromCharCode(G);
            }
            return O;
          }, Of = (l, m, E) => {
            if (m >>>= 0, E ?? (E = 2147483647), 4 > E) return 0;
            var O = m;
            E = O + E - 4;
            for (var G = 0; G < l.length; ++G) {
              var we = l.charCodeAt(G);
              if (55296 <= we && 57343 >= we && (we = 65536 + ((1023 & we) << 10) | 1023 & l.charCodeAt(++G)), o()[m >>> 2 >>> 0] = we, (m += 4) + 4 > E) break;
            }
            return o()[m >>> 2 >>> 0] = 0, m - O;
          }, Df = (l) => {
            for (var m = 0, E = 0; E < l.length; ++E) {
              var O = l.charCodeAt(E);
              55296 <= O && 57343 >= O && ++E, m += 4;
            }
            return m;
          };
          function Lf(l, m, E) {
            if (l >>>= 0, m >>>= 0, E = hn(E >>>= 0), m === 2) var O = $f, G = Af, we = If, Xe = (nt) => a()[nt >>> 1 >>> 0];
            else m === 4 && (O = Ff, G = Of, we = Df, Xe = (nt) => u()[nt >>> 2 >>> 0]);
            Cn(l, { name: E, fromWireType: (nt) => {
              for (var wt, Ct = u()[nt >>> 2 >>> 0], Wt = nt + 4, hs = 0; hs <= Ct; ++hs) {
                var vs = nt + 4 + hs * m;
                hs != Ct && Xe(vs) != 0 || (Wt = O(Wt, vs - Wt), wt === void 0 ? wt = Wt : (wt += "\0", wt += Wt), Wt = vs + m);
              }
              return fn(nt), wt;
            }, toWireType: (nt, wt) => {
              if (typeof wt != "string") throw new Zn(`Cannot pass non-string to C++ string type ${E}`);
              var Ct = we(wt), Wt = Up(4 + Ct + m);
              return u()[Wt >>> 2 >>> 0] = Ct / m, G(wt, Wt + 4, Ct + m), nt !== null && nt.push(fn, Wt), Wt;
            }, argPackAdvance: ei, readValueFromPointer: hh, Eb(nt) {
              fn(nt);
            } });
          }
          function zf(l, m) {
            Cn(l >>>= 0, { Ub: !0, name: m = hn(m >>> 0), argPackAdvance: 0, fromWireType: () => {
            }, toWireType: () => {
            } });
          }
          var Bf = () => 1;
          function Rf(l) {
            Mh(l >>> 0, !V, 1, !B, 131072, !1), qh();
          }
          var wm = (l) => {
            if (!zs) try {
              if (l(), !(0 < Di)) try {
                Z ? Vp(Ht) : ah(Ht);
              } catch (m) {
                m instanceof nh || m == "unwind" || Le(1, m);
              }
            } catch (m) {
              m instanceof nh || m == "unwind" || Le(1, m);
            }
          };
          function mh(l) {
            l >>>= 0, typeof Atomics.oc == "function" && (Atomics.oc(o(), l >>> 2, l).value.then(Fp), l += 128, Atomics.store(o(), l >>> 2, 1));
          }
          var Fp = () => {
            var l = Fa();
            l && (mh(l), wm(qm));
          };
          function Nf(l, m) {
            (l >>>= 0) == m >>> 0 ? setTimeout(Fp) : Z ? postMessage({ targetThread: l, cmd: "checkMailbox" }) : (l = pn[l]) && l.postMessage({ cmd: "checkMailbox" });
          }
          var fh = [];
          function jf(l, m, E, O, G) {
            for (m >>>= 0, O /= 2, fh.length = O, E = G >>> 0 >>> 3, G = 0; G < O; G++) fh[G] = er[E + 2 * G] ? er[E + 2 * G + 1] : h()[E + 2 * G + 1 >>> 0];
            return (m ? rh[m] : A_[l])(...fh);
          }
          function Uf(l) {
            l >>>= 0, Z ? postMessage({ cmd: "cleanupThread", thread: l }) : Hh(pn[l]);
          }
          function Vf(l) {
          }
          var Op = (l, m) => {
            var E = dh[l];
            if (E === void 0) throw l = Wm(l), E = hn(l), fn(l), new Zn(`${m} has unknown type ${E}`);
            return E;
          }, ym = (l, m, E) => {
            var O = [];
            return l = l.toWireType(O, E), O.length && (u()[m >>> 2 >>> 0] = zr(O)), l;
          };
          function Wf(l, m, E) {
            return m >>>= 0, E >>>= 0, l = Cr(l >>> 0), m = Op(m, "emval::as"), ym(m, E, l);
          }
          function Gf(l, m) {
            return m >>>= 0, l = Cr(l >>> 0), (m = Op(m, "emval::as")).toWireType(null, l);
          }
          var Dp = (l) => {
            try {
              l();
            } catch (m) {
              Yn(m);
            }
          }, ti = 0, mn = null, Mm = 0, Lp = [], bm = {}, vm = {}, Kf = 0, _h = null, Hf = [];
          function xm(l) {
            return function(m) {
              if (!zs) {
                if (ti === 0) {
                  var E = !1, O = !1;
                  m((G = 0) => {
                    if (!zs && (Mm = G, E = !0, O)) {
                      ti = 2, Dp(() => Zm(mn)), typeof Browser < "u" && Browser.Lb.Sb && Browser.Lb.resume(), G = !1;
                      try {
                        var we = function() {
                          var wt = o()[mn + 8 >>> 2 >>> 0];
                          return wt = Ut[vm[wt]], --Di, wt();
                        }();
                      } catch (wt) {
                        we = wt, G = !0;
                      }
                      var Xe = !1;
                      if (!mn) {
                        var nt = _h;
                        nt && (_h = null, (G ? nt.reject : nt.resolve)(we), Xe = !0);
                      }
                      if (G && !Xe) throw we;
                    }
                  }), O = !0, E || (ti = 1, mn = function() {
                    var G = Up(65548), we = G + 12;
                    u()[G >>> 2 >>> 0] = we, u()[G + 4 >>> 2 >>> 0] = we + 65536, we = Lp[0];
                    var Xe = bm[we];
                    return Xe === void 0 && (Xe = Kf++, bm[we] = Xe, vm[Xe] = we), we = Xe, o()[G + 8 >>> 2 >>> 0] = we, G;
                  }(), typeof Browser < "u" && Browser.Lb.Sb && Browser.Lb.pause(), Dp(() => Ym(mn)));
                } else ti === 2 ? (ti = 0, Dp(ef), fn(mn), mn = null, Hf.forEach(wm)) : Yn(`invalid state: ${ti}`);
                return Mm;
              }
            }((m) => {
              l().then(m);
            });
          }
          function qf(l) {
            return l >>>= 0, xm(() => (l = Cr(l)).then(zr));
          }
          var zp = [];
          function Qf(l, m, E, O) {
            return E >>>= 0, O >>>= 0, (l = zp[l >>> 0])(null, m = Cr(m >>> 0), E, O);
          }
          var Xf = {}, Bp = (l) => {
            var m = Xf[l];
            return m === void 0 ? hn(l) : m;
          };
          function Yf(l, m, E, O, G) {
            return E >>>= 0, O >>>= 0, G >>>= 0, (l = zp[l >>> 0])(m = Cr(m >>> 0), m[E = Bp(E)], O, G);
          }
          var Tm = () => typeof globalThis == "object" ? globalThis : Function("return this")();
          function Jf(l) {
            return (l >>>= 0) == 0 ? zr(Tm()) : (l = Bp(l), zr(Tm()[l]));
          }
          var Zf = (l) => {
            var m = zp.length;
            return zp.push(l), m;
          }, e_ = (l, m) => {
            for (var E = Array(l), O = 0; O < l; ++O) E[O] = Op(u()[m + 4 * O >>> 2 >>> 0], "parameter " + O);
            return E;
          }, Pm = (l, m) => Object.defineProperty(m, "name", { value: l });
          function t_(l, m, E) {
            var O = (m = e_(l, m >>> 0)).shift();
            l--;
            var G = `return function (obj, func, destructorsRef, args) {
`, we = 0, Xe = [];
            E === 0 && Xe.push("obj");
            for (var nt = ["retType"], wt = [O], Ct = 0; Ct < l; ++Ct) Xe.push("arg" + Ct), nt.push("argType" + Ct), wt.push(m[Ct]), G += `  var arg${Ct} = argType${Ct}.readValueFromPointer(args${we ? "+" + we : ""});
`, we += m[Ct].argPackAdvance;
            return G += `  var rv = ${E === 1 ? "new func" : "func.call"}(${Xe.join(", ")});
`, O.Ub || (nt.push("emval_returnValue"), wt.push(ym), G += `  return emval_returnValue(retType, destructorsRef, rv);
`), nt.push(G + `};
`), l = function(Wt) {
              var hs = Function;
              if (!(hs instanceof Function)) throw new TypeError(`new_ called with constructor type ${typeof hs} which is not a function`);
              var vs = Pm(hs.name || "unknownFunctionName", function() {
              });
              return vs.prototype = hs.prototype, vs = new vs(), (Wt = hs.apply(vs, Wt)) instanceof Object ? Wt : vs;
            }(nt)(...wt), E = `methodCaller<(${m.map((Wt) => Wt.name).join(", ")}) => ${O.name}>`, Zf(Pm(E, l));
          }
          function s_(l) {
            return l = Bp(l >>> 0), zr(d[l]);
          }
          function r_(l, m) {
            return m >>>= 0, l = Cr(l >>> 0), m = Cr(m), zr(l[m]);
          }
          function n_(l) {
            9 < (l >>>= 0) && (kn[l + 1] += 1);
          }
          function i_() {
            return zr([]);
          }
          function o_(l) {
            l = Cr(l >>> 0);
            for (var m = Array(l.length), E = 0; E < l.length; E++) m[E] = l[E];
            return zr(m);
          }
          function a_(l) {
            return zr(Bp(l >>> 0));
          }
          function l_() {
            return zr({});
          }
          function u_(l) {
            for (var m = Cr(l >>>= 0); m.length; ) {
              var E = m.pop();
              m.pop()(E);
            }
            ph(l);
          }
          function d_(l, m, E) {
            m >>>= 0, E >>>= 0, l = Cr(l >>> 0), m = Cr(m), E = Cr(E), l[m] = E;
          }
          function c_(l, m) {
            return m >>>= 0, l = (l = Op(l >>> 0, "_emval_take_value")).readValueFromPointer(m), zr(l);
          }
          function p_(l, m) {
            l = -9007199254740992 > l || 9007199254740992 < l ? NaN : Number(l), m >>>= 0, l = new Date(1e3 * l), o()[m >>> 2 >>> 0] = l.getUTCSeconds(), o()[m + 4 >>> 2 >>> 0] = l.getUTCMinutes(), o()[m + 8 >>> 2 >>> 0] = l.getUTCHours(), o()[m + 12 >>> 2 >>> 0] = l.getUTCDate(), o()[m + 16 >>> 2 >>> 0] = l.getUTCMonth(), o()[m + 20 >>> 2 >>> 0] = l.getUTCFullYear() - 1900, o()[m + 24 >>> 2 >>> 0] = l.getUTCDay(), l = (l.getTime() - Date.UTC(l.getUTCFullYear(), 0, 1, 0, 0, 0, 0)) / 864e5 | 0, o()[m + 28 >>> 2 >>> 0] = l;
          }
          var Ia = (l) => l % 4 == 0 && (l % 100 != 0 || l % 400 == 0), Em = [0, 31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335], Cm = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334];
          function h_(l, m) {
            l = -9007199254740992 > l || 9007199254740992 < l ? NaN : Number(l), m >>>= 0, l = new Date(1e3 * l), o()[m >>> 2 >>> 0] = l.getSeconds(), o()[m + 4 >>> 2 >>> 0] = l.getMinutes(), o()[m + 8 >>> 2 >>> 0] = l.getHours(), o()[m + 12 >>> 2 >>> 0] = l.getDate(), o()[m + 16 >>> 2 >>> 0] = l.getMonth(), o()[m + 20 >>> 2 >>> 0] = l.getFullYear() - 1900, o()[m + 24 >>> 2 >>> 0] = l.getDay();
            var E = (Ia(l.getFullYear()) ? Em : Cm)[l.getMonth()] + l.getDate() - 1 | 0;
            o()[m + 28 >>> 2 >>> 0] = E, o()[m + 36 >>> 2 >>> 0] = -60 * l.getTimezoneOffset(), E = new Date(l.getFullYear(), 6, 1).getTimezoneOffset();
            var O = new Date(l.getFullYear(), 0, 1).getTimezoneOffset();
            l = 0 | (E != O && l.getTimezoneOffset() == Math.min(O, E)), o()[m + 32 >>> 2 >>> 0] = l;
          }
          function m_(l) {
            l >>>= 0;
            var m = new Date(o()[l + 20 >>> 2 >>> 0] + 1900, o()[l + 16 >>> 2 >>> 0], o()[l + 12 >>> 2 >>> 0], o()[l + 8 >>> 2 >>> 0], o()[l + 4 >>> 2 >>> 0], o()[l >>> 2 >>> 0], 0), E = o()[l + 32 >>> 2 >>> 0], O = m.getTimezoneOffset(), G = new Date(m.getFullYear(), 6, 1).getTimezoneOffset(), we = new Date(m.getFullYear(), 0, 1).getTimezoneOffset(), Xe = Math.min(we, G);
            return 0 > E ? o()[l + 32 >>> 2 >>> 0] = +(G != we && Xe == O) : 0 < E != (Xe == O) && (G = Math.max(we, G), m.setTime(m.getTime() + 6e4 * ((0 < E ? Xe : G) - O))), o()[l + 24 >>> 2 >>> 0] = m.getDay(), E = (Ia(m.getFullYear()) ? Em : Cm)[m.getMonth()] + m.getDate() - 1 | 0, o()[l + 28 >>> 2 >>> 0] = E, o()[l >>> 2 >>> 0] = m.getSeconds(), o()[l + 4 >>> 2 >>> 0] = m.getMinutes(), o()[l + 8 >>> 2 >>> 0] = m.getHours(), o()[l + 12 >>> 2 >>> 0] = m.getDate(), o()[l + 16 >>> 2 >>> 0] = m.getMonth(), o()[l + 20 >>> 2 >>> 0] = m.getYear(), l = m.getTime(), BigInt(isNaN(l) ? -1 : l / 1e3);
          }
          function km(l, m, E, O, G, we, Xe) {
            return Z ? Bs(16, 1, l, m, E, O, G, we, Xe) : -52;
          }
          function Sm(l, m, E, O, G, we) {
            if (Z) return Bs(17, 1, l, m, E, O, G, we);
          }
          function f_(l, m, E, O) {
            l >>>= 0, m >>>= 0, E >>>= 0, O >>>= 0;
            var G = (/* @__PURE__ */ new Date()).getFullYear(), we = new Date(G, 0, 1), Xe = new Date(G, 6, 1);
            G = we.getTimezoneOffset();
            var nt = Xe.getTimezoneOffset(), wt = Math.max(G, nt);
            u()[l >>> 2 >>> 0] = 60 * wt, o()[m >>> 2 >>> 0] = +(G != nt), we = (l = (Ct) => Ct.toLocaleTimeString(void 0, { hour12: !1, timeZoneName: "short" }).split(" ")[1])(we), Xe = l(Xe), nt < G ? (Aa(we, E, 17), Aa(Xe, O, 17)) : (Aa(we, O, 17), Aa(Xe, E, 17));
          }
          var gh = [], $m = (l, m) => {
            gh.length = 0;
            for (var E; E = n()[l++ >>> 0]; ) {
              var O = E != 105;
              m += (O &= E != 112) && m % 8 ? 4 : 0, gh.push(E == 112 ? u()[m >>> 2 >>> 0] : E == 106 ? er[m >>> 3] : E == 105 ? o()[m >>> 2 >>> 0] : h()[m >>> 3 >>> 0]), m += O ? 8 : 4;
            }
            return gh;
          };
          function __(l, m, E) {
            return l >>>= 0, m = $m(m >>> 0, E >>> 0), rh[l](...m);
          }
          function g_(l, m, E) {
            return l >>>= 0, m = $m(m >>> 0, E >>> 0), rh[l](...m);
          }
          var w_ = () => {
          }, y_ = () => Date.now();
          function M_(l, m) {
            return bt(Qs(l >>> 0, m >>> 0));
          }
          var Am, b_ = () => {
            throw Di += 1, "unwind";
          };
          function v_() {
            return 4294901760;
          }
          Am = () => performance.timeOrigin + performance.now();
          var x_ = () => navigator.hardwareConcurrency;
          function T_() {
            return Yn("Cannot use emscripten_pc_get_function without -sUSE_OFFSET_CONVERTER"), 0;
          }
          function P_(l) {
            l >>>= 0;
            var m = n().length;
            if (l <= m || 4294901760 < l) return !1;
            for (var E = 1; 4 >= E; E *= 2) {
              var O = m * (1 + 0.2 / E);
              O = Math.min(O, l + 100663296);
              var G = Math;
              O = Math.max(l, O);
              e: {
                G = (G.min.call(G, 4294901760, O + (65536 - O % 65536) % 65536) - Ot.buffer.byteLength + 65535) / 65536;
                try {
                  Ot.grow(G), Ps();
                  var we = 1;
                  break e;
                } catch {
                }
                we = void 0;
              }
              if (we) return !0;
            }
            return !1;
          }
          var Rp = () => (Yn("Cannot use convertFrameToPC (needed by __builtin_return_address) without -sUSE_OFFSET_CONVERTER"), 0), Oc = {}, Im = (l) => {
            l.forEach((m) => {
              Rp();
            });
          };
          function E_() {
            var l = Error().stack.toString().split(`
`);
            return l[0] == "Error" && l.shift(), Im(l), Oc.Qb = Rp(), Oc.fc = l, Oc.Qb;
          }
          function C_(l, m, E) {
            if (l >>>= 0, m >>>= 0, Oc.Qb == l) var O = Oc.fc;
            else (O = Error().stack.toString().split(`
`))[0] == "Error" && O.shift(), Im(O);
            for (var G = 3; O[G] && Rp() != l; ) ++G;
            for (l = 0; l < E && O[l + G]; ++l) o()[m + 4 * l >>> 2 >>> 0] = Rp();
            return l;
          }
          var wh, yh = {}, Fm = () => {
            if (!wh) {
              var l, m = { USER: "web_user", LOGNAME: "web_user", PATH: "/", PWD: "/", HOME: "/home/web_user", LANG: (typeof navigator == "object" && navigator.languages && navigator.languages[0] || "C").replace("-", "_") + ".UTF-8", _: Oe };
              for (l in yh) yh[l] === void 0 ? delete m[l] : m[l] = yh[l];
              var E = [];
              for (l in m) E.push(`${l}=${m[l]}`);
              wh = E;
            }
            return wh;
          };
          function Om(l, m) {
            if (Z) return Bs(18, 1, l, m);
            l >>>= 0, m >>>= 0;
            var E = 0;
            return Fm().forEach((O, G) => {
              var we = m + E;
              for (G = u()[l + 4 * G >>> 2 >>> 0] = we, we = 0; we < O.length; ++we) s()[G++ >>> 0] = O.charCodeAt(we);
              s()[G >>> 0] = 0, E += O.length + 1;
            }), 0;
          }
          function Dm(l, m) {
            if (Z) return Bs(19, 1, l, m);
            l >>>= 0, m >>>= 0;
            var E = Fm();
            u()[l >>> 2 >>> 0] = E.length;
            var O = 0;
            return E.forEach((G) => O += G.length + 1), u()[m >>> 2 >>> 0] = O, 0;
          }
          function Lm(l) {
            return Z ? Bs(20, 1, l) : 52;
          }
          function zm(l, m, E, O) {
            return Z ? Bs(21, 1, l, m, E, O) : 52;
          }
          function Bm(l, m, E, O) {
            return Z ? Bs(22, 1, l, m, E, O) : 70;
          }
          var k_ = [null, [], []];
          function Rm(l, m, E, O) {
            if (Z) return Bs(23, 1, l, m, E, O);
            m >>>= 0, E >>>= 0, O >>>= 0;
            for (var G = 0, we = 0; we < E; we++) {
              var Xe = u()[m >>> 2 >>> 0], nt = u()[m + 4 >>> 2 >>> 0];
              m += 8;
              for (var wt = 0; wt < nt; wt++) {
                var Ct = n()[Xe + wt >>> 0], Wt = k_[l];
                Ct === 0 || Ct === 10 ? ((l === 1 ? Zt : bt)(em(Wt, 0)), Wt.length = 0) : Wt.push(Ct);
              }
              G += nt;
            }
            return u()[O >>> 2 >>> 0] = G, 0;
          }
          var Nm = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], jm = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], S_ = (l, m) => {
            s().set(l, m >>> 0);
          };
          function Um(l, m, E, O) {
            function G(He, ls, js) {
              for (He = typeof He == "number" ? He.toString() : He || ""; He.length < ls; ) He = js[0] + He;
              return He;
            }
            function we(He, ls) {
              return G(He, ls, "0");
            }
            function Xe(He, ls) {
              function js(sf) {
                return 0 > sf ? -1 : 0 < sf ? 1 : 0;
              }
              var zi;
              return (zi = js(He.getFullYear() - ls.getFullYear())) === 0 && (zi = js(He.getMonth() - ls.getMonth())) === 0 && (zi = js(He.getDate() - ls.getDate())), zi;
            }
            function nt(He) {
              switch (He.getDay()) {
                case 0:
                  return new Date(He.getFullYear() - 1, 11, 29);
                case 1:
                  return He;
                case 2:
                  return new Date(He.getFullYear(), 0, 3);
                case 3:
                  return new Date(He.getFullYear(), 0, 2);
                case 4:
                  return new Date(He.getFullYear(), 0, 1);
                case 5:
                  return new Date(He.getFullYear() - 1, 11, 31);
                case 6:
                  return new Date(He.getFullYear() - 1, 11, 30);
              }
            }
            function wt(He) {
              var ls = He.Cb;
              for (He = new Date(new Date(He.Db + 1900, 0, 1).getTime()); 0 < ls; ) {
                var js = He.getMonth(), zi = (Ia(He.getFullYear()) ? Nm : jm)[js];
                if (!(ls > zi - He.getDate())) {
                  He.setDate(He.getDate() + ls);
                  break;
                }
                ls -= zi - He.getDate() + 1, He.setDate(1), 11 > js ? He.setMonth(js + 1) : (He.setMonth(0), He.setFullYear(He.getFullYear() + 1));
              }
              return js = new Date(He.getFullYear() + 1, 0, 4), ls = nt(new Date(He.getFullYear(), 0, 4)), js = nt(js), 0 >= Xe(ls, He) ? 0 >= Xe(js, He) ? He.getFullYear() + 1 : He.getFullYear() : He.getFullYear() - 1;
            }
            l >>>= 0, m >>>= 0, E >>>= 0, O >>>= 0;
            var Ct = u()[O + 40 >>> 2 >>> 0];
            for (var Wt in O = { lc: o()[O >>> 2 >>> 0], kc: o()[O + 4 >>> 2 >>> 0], Ib: o()[O + 8 >>> 2 >>> 0], Mb: o()[O + 12 >>> 2 >>> 0], Jb: o()[O + 16 >>> 2 >>> 0], Db: o()[O + 20 >>> 2 >>> 0], vb: o()[O + 24 >>> 2 >>> 0], Cb: o()[O + 28 >>> 2 >>> 0], sc: o()[O + 32 >>> 2 >>> 0], jc: o()[O + 36 >>> 2 >>> 0], mc: Ct ? Qs(Ct) : "" }, E = Qs(E), Ct = { "%c": "%a %b %d %H:%M:%S %Y", "%D": "%m/%d/%y", "%F": "%Y-%m-%d", "%h": "%b", "%r": "%I:%M:%S %p", "%R": "%H:%M", "%T": "%H:%M:%S", "%x": "%m/%d/%y", "%X": "%H:%M:%S", "%Ec": "%c", "%EC": "%C", "%Ex": "%m/%d/%y", "%EX": "%H:%M:%S", "%Ey": "%y", "%EY": "%Y", "%Od": "%d", "%Oe": "%e", "%OH": "%H", "%OI": "%I", "%Om": "%m", "%OM": "%M", "%OS": "%S", "%Ou": "%u", "%OU": "%U", "%OV": "%V", "%Ow": "%w", "%OW": "%W", "%Oy": "%y" }) E = E.replace(new RegExp(Wt, "g"), Ct[Wt]);
            var hs = "Sunday Monday Tuesday Wednesday Thursday Friday Saturday".split(" "), vs = "January February March April May June July August September October November December".split(" ");
            for (Wt in Ct = { "%a": (He) => hs[He.vb].substring(0, 3), "%A": (He) => hs[He.vb], "%b": (He) => vs[He.Jb].substring(0, 3), "%B": (He) => vs[He.Jb], "%C": (He) => we((He.Db + 1900) / 100 | 0, 2), "%d": (He) => we(He.Mb, 2), "%e": (He) => G(He.Mb, 2, " "), "%g": (He) => wt(He).toString().substring(2), "%G": wt, "%H": (He) => we(He.Ib, 2), "%I": (He) => ((He = He.Ib) == 0 ? He = 12 : 12 < He && (He -= 12), we(He, 2)), "%j": (He) => {
              for (var ls = 0, js = 0; js <= He.Jb - 1; ls += (Ia(He.Db + 1900) ? Nm : jm)[js++]) ;
              return we(He.Mb + ls, 3);
            }, "%m": (He) => we(He.Jb + 1, 2), "%M": (He) => we(He.kc, 2), "%n": () => `
`, "%p": (He) => 0 <= He.Ib && 12 > He.Ib ? "AM" : "PM", "%S": (He) => we(He.lc, 2), "%t": () => "	", "%u": (He) => He.vb || 7, "%U": (He) => we(Math.floor((He.Cb + 7 - He.vb) / 7), 2), "%V": (He) => {
              var ls = Math.floor((He.Cb + 7 - (He.vb + 6) % 7) / 7);
              if (2 >= (He.vb + 371 - He.Cb - 2) % 7 && ls++, ls) ls == 53 && ((js = (He.vb + 371 - He.Cb) % 7) == 4 || js == 3 && Ia(He.Db) || (ls = 1));
              else {
                ls = 52;
                var js = (He.vb + 7 - He.Cb - 1) % 7;
                (js == 4 || js == 5 && Ia(He.Db % 400 - 1)) && ls++;
              }
              return we(ls, 2);
            }, "%w": (He) => He.vb, "%W": (He) => we(Math.floor((He.Cb + 7 - (He.vb + 6) % 7) / 7), 2), "%y": (He) => (He.Db + 1900).toString().substring(2), "%Y": (He) => He.Db + 1900, "%z": (He) => {
              var ls = 0 <= (He = He.jc);
              return He = Math.abs(He) / 60, (ls ? "+" : "-") + ("0000" + (He / 60 * 100 + He % 60)).slice(-4);
            }, "%Z": (He) => He.mc, "%%": () => "%" }, E = E.replace(/%%/g, "\0\0"), Ct) E.includes(Wt) && (E = E.replace(new RegExp(Wt, "g"), Ct[Wt](O)));
            return Wt = function(He) {
              var ls = Array(lh(He) + 1);
              return rm(He, ls, 0, ls.length), ls;
            }(E = E.replace(/\0\0/g, "%")), Wt.length > m ? 0 : (S_(Wt, l), Wt.length - 1);
          }
          function $_(l, m, E, O) {
            return Um(l >>> 0, m >>> 0, E >>> 0, O >>> 0);
          }
          Z || function() {
            for (var l = d.numThreads - 1; l--; ) Xh();
            Xn.unshift(() => {
              Ic++, function(m) {
                Z ? m() : Promise.all(Jn.map(Qh)).then(m);
              }(() => Bh());
            });
          }();
          for (var Vm = Array(256), Np = 0; 256 > Np; ++Np) Vm[Np] = String.fromCharCode(Np);
          fm = Vm, Zn = d.BindingError = class extends Error {
            constructor(l) {
              super(l), this.name = "BindingError";
            }
          }, d.InternalError = class extends Error {
            constructor(l) {
              super(l), this.name = "InternalError";
            }
          }, kn.push(0, 1, void 0, 1, null, 1, !0, 1, !1, 1), d.count_emval_handles = () => kn.length / 2 - 5 - ch.length;
          var A_ = [oh, Gh, Yh, tm, sm, nm, im, om, am, lm, um, dm, cm, pm, hm, mm, km, Sm, Om, Dm, Lm, zm, Bm, Rm], Ut = function() {
            function l(E, O) {
              return Ut = E.exports, Ut = function() {
                var G = Ut, we = {};
                for (let [Xe, nt] of Object.entries(G)) we[Xe] = typeof nt == "function" ? (...wt) => {
                  Lp.push(Xe);
                  try {
                    return nt(...wt);
                  } finally {
                    zs || (Lp.pop(), mn && ti === 1 && Lp.length === 0 && (ti = 0, Di += 1, Dp(Jm), typeof Fibers < "u" && Fibers.tc()));
                  }
                } : nt;
                return we;
              }(), Ut = function() {
                var G = Ut, we = (nt) => (wt) => nt(wt) >>> 0, Xe = (nt) => () => nt() >>> 0;
                return (G = Object.assign({}, G)).Da = we(G.Da), G.gb = Xe(G.gb), G.ib = we(G.ib), G.emscripten_main_runtime_thread_id = Xe(G.emscripten_main_runtime_thread_id), G.tb = we(G.tb), G.ub = Xe(G.ub), G;
              }(), Kh.push(Ut.jb), Ac.unshift(Ut.Ca), bs = O, Bh(), Ut;
            }
            var m = Vh();
            if (Ic++, d.instantiateWasm) try {
              return d.instantiateWasm(m, l);
            } catch (E) {
              bt(`Module.instantiateWasm callback failed with error: ${E}`), C(E);
            }
            return sh || (sh = d.locateFile ? Rh("ort-wasm-simd-threaded.jsep.wasm") ? "ort-wasm-simd-threaded.jsep.wasm" : d.locateFile ? d.locateFile("ort-wasm-simd-threaded.jsep.wasm", Ye) : Ye + "ort-wasm-simd-threaded.jsep.wasm" : new URL(
              /* asset import */
              r(
                /*! ort-wasm-simd-threaded.jsep.wasm */
                "./node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm"
              ),
              r.b
            ).href), function(E, O) {
              var G = sh;
              return at || typeof WebAssembly.instantiateStreaming != "function" || Rh(G) || Nh(G) || typeof fetch != "function" ? Uh(G, E, O) : fetch(G, { credentials: "same-origin" }).then((we) => WebAssembly.instantiateStreaming(we, E).then(O, function(Xe) {
                return bt(`wasm streaming compile failed: ${Xe}`), bt("falling back to ArrayBuffer instantiation"), Uh(G, E, O);
              }));
            }(m, function(E) {
              l(E.instance, E.module);
            }).catch(C), {};
          }(), Wm = (l) => (Wm = Ut.Da)(l), Gm = () => (Gm = Ut.Ea)();
          d._OrtInit = (l, m) => (d._OrtInit = Ut.Fa)(l, m), d._OrtGetLastError = (l, m) => (d._OrtGetLastError = Ut.Ga)(l, m), d._OrtCreateSessionOptions = (l, m, E, O, G, we, Xe, nt, wt, Ct) => (d._OrtCreateSessionOptions = Ut.Ha)(l, m, E, O, G, we, Xe, nt, wt, Ct), d._OrtAppendExecutionProvider = (l, m) => (d._OrtAppendExecutionProvider = Ut.Ia)(l, m), d._OrtAddFreeDimensionOverride = (l, m, E) => (d._OrtAddFreeDimensionOverride = Ut.Ja)(l, m, E), d._OrtAddSessionConfigEntry = (l, m, E) => (d._OrtAddSessionConfigEntry = Ut.Ka)(l, m, E), d._OrtReleaseSessionOptions = (l) => (d._OrtReleaseSessionOptions = Ut.La)(l), d._OrtCreateSession = (l, m, E) => (d._OrtCreateSession = Ut.Ma)(l, m, E), d._OrtReleaseSession = (l) => (d._OrtReleaseSession = Ut.Na)(l), d._OrtGetInputOutputCount = (l, m, E) => (d._OrtGetInputOutputCount = Ut.Oa)(l, m, E), d._OrtGetInputName = (l, m) => (d._OrtGetInputName = Ut.Pa)(l, m), d._OrtGetOutputName = (l, m) => (d._OrtGetOutputName = Ut.Qa)(l, m), d._OrtFree = (l) => (d._OrtFree = Ut.Ra)(l), d._OrtCreateTensor = (l, m, E, O, G, we) => (d._OrtCreateTensor = Ut.Sa)(l, m, E, O, G, we), d._OrtGetTensorData = (l, m, E, O, G) => (d._OrtGetTensorData = Ut.Ta)(l, m, E, O, G), d._OrtReleaseTensor = (l) => (d._OrtReleaseTensor = Ut.Ua)(l), d._OrtCreateRunOptions = (l, m, E, O) => (d._OrtCreateRunOptions = Ut.Va)(l, m, E, O), d._OrtAddRunConfigEntry = (l, m, E) => (d._OrtAddRunConfigEntry = Ut.Wa)(l, m, E), d._OrtReleaseRunOptions = (l) => (d._OrtReleaseRunOptions = Ut.Xa)(l), d._OrtCreateBinding = (l) => (d._OrtCreateBinding = Ut.Ya)(l), d._OrtBindInput = (l, m, E) => (d._OrtBindInput = Ut.Za)(l, m, E), d._OrtBindOutput = (l, m, E, O) => (d._OrtBindOutput = Ut._a)(l, m, E, O), d._OrtClearBoundOutputs = (l) => (d._OrtClearBoundOutputs = Ut.$a)(l), d._OrtReleaseBinding = (l) => (d._OrtReleaseBinding = Ut.ab)(l), d._OrtRunWithBinding = (l, m, E, O, G) => (d._OrtRunWithBinding = Ut.bb)(l, m, E, O, G), d._OrtRun = (l, m, E, O, G, we, Xe, nt) => (d._OrtRun = Ut.cb)(l, m, E, O, G, we, Xe, nt), d._OrtEndProfiling = (l) => (d._OrtEndProfiling = Ut.db)(l), d._JsepOutput = (l, m, E) => (d._JsepOutput = Ut.eb)(l, m, E), d._JsepGetNodeName = (l) => (d._JsepGetNodeName = Ut.fb)(l);
          var jp, Fa = () => (Fa = Ut.gb)(), fn = d._free = (l) => (fn = d._free = Ut.hb)(l), Up = d._malloc = (l) => (Up = d._malloc = Ut.ib)(l), Mh = (l, m, E, O, G, we) => (Mh = Ut.lb)(l, m, E, O, G, we), Km = () => (Km = Ut.mb)(), Hm = (l, m, E, O, G) => (Hm = Ut.nb)(l, m, E, O, G), bh = (l) => (bh = Ut.ob)(l), Vp = (l) => (Vp = Ut.pb)(l), qm = () => (qm = Ut.qb)(), Qm = (l, m) => (Qm = Ut.rb)(l, m), Wp = (l) => (Wp = Ut.sb)(l), vh = (l) => (vh = Ut.tb)(l), xh = () => (xh = Ut.ub)(), Xm = d.dynCall_ii = (l, m) => (Xm = d.dynCall_ii = Ut.wb)(l, m), Ym = (l) => (Ym = Ut.xb)(l), Jm = () => (Jm = Ut.yb)(), Zm = (l) => (Zm = Ut.zb)(l), ef = () => (ef = Ut.Ab)();
          function tf() {
            0 < Ic || (Z ? (k(d), Z || Ip(Ac), startWorker(d)) : (Ip(Xn), 0 < Ic || jp || (jp = !0, d.calledRun = !0, zs || (Z || Ip(Ac), k(d), Z || Ip(th)))));
          }
          return d.___start_em_js = 929301, d.___stop_em_js = 929547, d.stackSave = () => xh(), d.stackRestore = (l) => Wp(l), d.stackAlloc = (l) => vh(l), d.setValue = function(l, m, E = "i8") {
            switch (E.endsWith("*") && (E = "*"), E) {
              case "i1":
              case "i8":
                s()[l >>> 0] = m;
                break;
              case "i16":
                i()[l >>> 1 >>> 0] = m;
                break;
              case "i32":
                o()[l >>> 2 >>> 0] = m;
                break;
              case "i64":
                er[l >>> 3] = BigInt(m);
                break;
              case "float":
                p()[l >>> 2 >>> 0] = m;
                break;
              case "double":
                h()[l >>> 3 >>> 0] = m;
                break;
              case "*":
                u()[l >>> 2 >>> 0] = m;
                break;
              default:
                Yn(`invalid type for setValue: ${E}`);
            }
          }, d.getValue = function(l, m = "i8") {
            switch (m.endsWith("*") && (m = "*"), m) {
              case "i1":
              case "i8":
                return s()[l >>> 0];
              case "i16":
                return i()[l >>> 1 >>> 0];
              case "i32":
                return o()[l >>> 2 >>> 0];
              case "i64":
                return er[l >>> 3];
              case "float":
                return p()[l >>> 2 >>> 0];
              case "double":
                return h()[l >>> 3 >>> 0];
              case "*":
                return u()[l >>> 2 >>> 0];
              default:
                Yn(`invalid type for getValue: ${m}`);
            }
          }, d.UTF8ToString = Qs, d.stringToUTF8 = Aa, d.lengthBytesUTF8 = lh, Fc = function l() {
            jp || tf(), jp || (Fc = l);
          }, tf(), d.PTR_SIZE = 4, z;
        }), Ds = Cs, ((e = globalThis.self) == null ? void 0 : e.name) === "em-pthread" && Cs();
      }), kr, Qr, Us, Tr, Nt, Xr, Sr, $r, Yr, dr, Jr, Ar, Br, Rr = g(() => {
        Je(), kr = typeof location > "u" ? void 0 : location.origin, Qr = () => {
          var e;
          return (e = import.meta.url) != null && e.startsWith("file:") ? new URL(new URL(
            /* asset import */
            r(
              /*! ort.bundle.min.mjs */
              "./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs?46eb"
            ),
            r.b
          ).href, kr).href : import.meta.url;
        }, Us = Qr(), Tr = () => {
          if (Us && !Us.startsWith("blob:")) return Us.substring(0, Us.lastIndexOf("/") + 1);
        }, Nt = (e, t) => {
          try {
            let s = t ?? Us;
            return (s ? new URL(e, s) : new URL(e)).origin === kr;
          } catch {
            return !1;
          }
        }, Xr = (e, t) => {
          let s = t ?? Us;
          try {
            return (s ? new URL(e, s) : new URL(e)).href;
          } catch {
            return;
          }
        }, Sr = (e, t) => `${t ?? "./"}${e}`, $r = async (e) => {
          let t = await (await fetch(e, { credentials: "same-origin" })).blob();
          return URL.createObjectURL(t);
        }, Yr = async (e) => (await import(
          /*webpackIgnore:true*/
          e
        )).default, dr = (At(), y(ut)).default, Jr = async () => {
          if (!Us) throw new Error("Failed to load proxy worker: cannot determine the script source URL.");
          if (Nt(Us)) return [void 0, dr()];
          let e = await $r(Us);
          return [e, dr(e)];
        }, Ar = (sr(), y(is)).default, Br = async (e, t, s) => {
          if (!e && !t && Ar && Us && Nt(Us)) return [void 0, Ar];
          {
            let n = "ort-wasm-simd-threaded.jsep.mjs", i = e ?? Xr(n, t), a = s && i && !Nt(i, t), o = a ? await $r(i) : i ?? Sr(n, t);
            return [a ? o : void 0, await Yr(o)];
          }
        };
      }), ar, it, Tt, Dt, Vs, Nr, Ir, Ms, lr = g(() => {
        Rr(), it = !1, Tt = !1, Dt = !1, Vs = () => {
          if (typeof SharedArrayBuffer > "u") return !1;
          try {
            return typeof MessageChannel < "u" && new MessageChannel().port1.postMessage(new SharedArrayBuffer(1)), WebAssembly.validate(new Uint8Array([0, 97, 115, 109, 1, 0, 0, 0, 1, 4, 1, 96, 0, 0, 3, 2, 1, 0, 5, 4, 1, 3, 1, 1, 10, 11, 1, 9, 0, 65, 0, 254, 16, 2, 0, 26, 11]));
          } catch {
            return !1;
          }
        }, Nr = () => {
          try {
            return WebAssembly.validate(new Uint8Array([0, 97, 115, 109, 1, 0, 0, 0, 1, 4, 1, 96, 0, 0, 3, 2, 1, 0, 10, 30, 1, 28, 0, 65, 0, 253, 15, 253, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 253, 186, 1, 26, 11]));
          } catch {
            return !1;
          }
        }, Ir = async (e) => {
          if (it) return Promise.resolve();
          if (Tt) throw new Error("multiple calls to 'initializeWebAssembly()' detected.");
          if (Dt) throw new Error("previous call to 'initializeWebAssembly()' failed.");
          Tt = !0;
          let t = e.initTimeout, s = e.numThreads;
          if (!Nr()) throw new Error("WebAssembly SIMD is not supported in the current environment.");
          let n = Vs();
          s > 1 && !n && (typeof self < "u" && !self.crossOriginIsolated && console.warn("env.wasm.numThreads is set to " + s + ", but this will not work unless you enable crossOriginIsolated mode. See https://web.dev/cross-origin-isolation-guide/ for more info."), console.warn("WebAssembly multi-threading is not supported in the current environment. Falling back to single-threading."), e.numThreads = s = 1);
          let i = e.wasmPaths, a = typeof i == "string" ? i : void 0, o = i == null ? void 0 : i.mjs, u = (o == null ? void 0 : o.href) ?? o, p = i == null ? void 0 : i.wasm, h = (p == null ? void 0 : p.href) ?? p, k = e.wasmBinary, [C, d] = await Br(u, a, s > 1), z = !1, B = [];
          if (t > 0 && B.push(new Promise((V) => {
            setTimeout(() => {
              z = !0, V();
            }, t);
          })), B.push(new Promise((V, Z) => {
            let ee = { numThreads: s };
            if (k) ee.wasmBinary = k;
            else if (h || a) ee.locateFile = (X) => h ?? a + X;
            else if (u && u.indexOf("blob:") !== 0) ee.locateFile = (X) => new URL(X, u).href;
            else if (C) {
              let X = Tr();
              X && (ee.locateFile = (he) => X + he);
            }
            d(ee).then((X) => {
              Tt = !1, it = !0, ar = X, V(), C && URL.revokeObjectURL(C);
            }, (X) => {
              Tt = !1, Dt = !0, Z(X);
            });
          })), await Promise.race(B), z) throw new Error(`WebAssembly backend initializing failed due to timeout: ${t}ms`);
        }, Ms = () => {
          if (it && ar) return ar;
          throw new Error("WebAssembly is not initialized yet.");
        };
      }), Fs, Pr, es, _n = g(() => {
        lr(), Fs = (e, t) => {
          let s = Ms(), n = s.lengthBytesUTF8(e) + 1, i = s._malloc(n);
          return s.stringToUTF8(e, i, n), t.push(i), i;
        }, Pr = (e, t, s, n) => {
          if (typeof e == "object" && e !== null) {
            if (s.has(e)) throw new Error("Circular reference in options");
            s.add(e);
          }
          Object.entries(e).forEach(([i, a]) => {
            let o = t ? t + i : i;
            if (typeof a == "object") Pr(a, o + ".", s, n);
            else if (typeof a == "string" || typeof a == "number") n(o, a.toString());
            else if (typeof a == "boolean") n(o, a ? "1" : "0");
            else throw new Error(`Can't handle extra config type: ${typeof a}`);
          });
        }, es = (e) => {
          let t = Ms(), s = t.stackSave();
          try {
            let n = t.PTR_SIZE, i = t.stackAlloc(2 * n);
            t._OrtGetLastError(i, i + n);
            let a = Number(t.getValue(i, n === 4 ? "i32" : "i64")), o = t.getValue(i + n, "*"), u = o ? t.UTF8ToString(o) : "";
            throw new Error(`${e} ERROR_CODE: ${a}, ERROR_MESSAGE: ${u}`);
          } finally {
            t.stackRestore(s);
          }
        };
      }), jr, si = g(() => {
        lr(), _n(), jr = (e) => {
          let t = Ms(), s = 0, n = [], i = e || {};
          try {
            if ((e == null ? void 0 : e.logSeverityLevel) === void 0) i.logSeverityLevel = 2;
            else if (typeof e.logSeverityLevel != "number" || !Number.isInteger(e.logSeverityLevel) || e.logSeverityLevel < 0 || e.logSeverityLevel > 4) throw new Error(`log serverity level is not valid: ${e.logSeverityLevel}`);
            if ((e == null ? void 0 : e.logVerbosityLevel) === void 0) i.logVerbosityLevel = 0;
            else if (typeof e.logVerbosityLevel != "number" || !Number.isInteger(e.logVerbosityLevel)) throw new Error(`log verbosity level is not valid: ${e.logVerbosityLevel}`);
            (e == null ? void 0 : e.terminate) === void 0 && (i.terminate = !1);
            let a = 0;
            return (e == null ? void 0 : e.tag) !== void 0 && (a = Fs(e.tag, n)), s = t._OrtCreateRunOptions(i.logSeverityLevel, i.logVerbosityLevel, !!i.terminate, a), s === 0 && es("Can't create run options."), (e == null ? void 0 : e.extra) !== void 0 && Pr(e.extra, "", /* @__PURE__ */ new WeakSet(), (o, u) => {
              let p = Fs(o, n), h = Fs(u, n);
              t._OrtAddRunConfigEntry(s, p, h) !== 0 && es(`Can't set a run config entry: ${o} - ${u}.`);
            }), [s, n];
          } catch (a) {
            throw s !== 0 && t._OrtReleaseRunOptions(s), n.forEach((o) => t._free(o)), a;
          }
        };
      }), Sn, $n, An, Ur, In, ri = g(() => {
        lr(), _n(), Sn = (e) => {
          switch (e) {
            case "disabled":
              return 0;
            case "basic":
              return 1;
            case "extended":
              return 2;
            case "all":
              return 99;
            default:
              throw new Error(`unsupported graph optimization level: ${e}`);
          }
        }, $n = (e) => {
          switch (e) {
            case "sequential":
              return 0;
            case "parallel":
              return 1;
            default:
              throw new Error(`unsupported execution mode: ${e}`);
          }
        }, An = (e) => {
          e.extra || (e.extra = {}), e.extra.session || (e.extra.session = {});
          let t = e.extra.session;
          t.use_ort_model_bytes_directly || (t.use_ort_model_bytes_directly = "1"), e.executionProviders && e.executionProviders.some((s) => (typeof s == "string" ? s : s.name) === "webgpu") && (e.enableMemPattern = !1);
        }, Ur = (e, t, s) => {
          for (let n of t) {
            let i = typeof n == "string" ? n : n.name;
            switch (i) {
              case "webnn":
                if (i = "WEBNN", typeof n != "string") {
                  let o = n == null ? void 0 : n.deviceType;
                  if (o) {
                    let u = Fs("deviceType", s), p = Fs(o, s);
                    Ms()._OrtAddSessionConfigEntry(e, u, p) !== 0 && es(`Can't set a session config entry: 'deviceType' - ${o}.`);
                  }
                }
                break;
              case "webgpu":
                if (i = "JS", typeof n != "string") {
                  let o = n;
                  if (o != null && o.preferredLayout) {
                    if (o.preferredLayout !== "NCHW" && o.preferredLayout !== "NHWC") throw new Error(`preferredLayout must be either 'NCHW' or 'NHWC': ${o.preferredLayout}`);
                    let u = Fs("preferredLayout", s), p = Fs(o.preferredLayout, s);
                    Ms()._OrtAddSessionConfigEntry(e, u, p) !== 0 && es(`Can't set a session config entry: 'preferredLayout' - ${o.preferredLayout}.`);
                  }
                }
                break;
              case "wasm":
              case "cpu":
                continue;
              default:
                throw new Error(`not supported execution provider: ${i}`);
            }
            let a = Fs(i, s);
            Ms()._OrtAppendExecutionProvider(e, a) !== 0 && es(`Can't append execution provider: ${i}.`);
          }
        }, In = (e) => {
          let t = Ms(), s = 0, n = [], i = e || {};
          An(i);
          try {
            let a = Sn(i.graphOptimizationLevel ?? "all"), o = $n(i.executionMode ?? "sequential"), u = typeof i.logId == "string" ? Fs(i.logId, n) : 0, p = i.logSeverityLevel ?? 2;
            if (!Number.isInteger(p) || p < 0 || p > 4) throw new Error(`log serverity level is not valid: ${p}`);
            let h = i.logVerbosityLevel ?? 0;
            if (!Number.isInteger(h) || h < 0 || h > 4) throw new Error(`log verbosity level is not valid: ${h}`);
            let k = typeof i.optimizedModelFilePath == "string" ? Fs(i.optimizedModelFilePath, n) : 0;
            if (s = t._OrtCreateSessionOptions(a, !!i.enableCpuMemArena, !!i.enableMemPattern, o, !!i.enableProfiling, 0, u, p, h, k), s === 0 && es("Can't create session options."), i.executionProviders && Ur(s, i.executionProviders, n), i.enableGraphCapture !== void 0) {
              if (typeof i.enableGraphCapture != "boolean") throw new Error(`enableGraphCapture must be a boolean value: ${i.enableGraphCapture}`);
              let C = Fs("enableGraphCapture", n), d = Fs(i.enableGraphCapture.toString(), n);
              t._OrtAddSessionConfigEntry(s, C, d) !== 0 && es(`Can't set a session config entry: 'enableGraphCapture' - ${i.enableGraphCapture}.`);
            }
            if (i.freeDimensionOverrides) for (let [C, d] of Object.entries(i.freeDimensionOverrides)) {
              if (typeof C != "string") throw new Error(`free dimension override name must be a string: ${C}`);
              if (typeof d != "number" || !Number.isInteger(d) || d < 0) throw new Error(`free dimension override value must be a non-negative integer: ${d}`);
              let z = Fs(C, n);
              t._OrtAddFreeDimensionOverride(s, z, d) !== 0 && es(`Can't set a free dimension override: ${C} - ${d}.`);
            }
            return i.extra !== void 0 && Pr(i.extra, "", /* @__PURE__ */ new WeakSet(), (C, d) => {
              let z = Fs(C, n), B = Fs(d, n);
              t._OrtAddSessionConfigEntry(s, z, B) !== 0 && es(`Can't set a session config entry: ${C} - ${d}.`);
            }), [s, n];
          } catch (a) {
            throw s !== 0 && t._OrtReleaseSessionOptions(s) !== 0 && es("Can't release session options."), n.forEach((o) => t._free(o)), a;
          }
        };
      }), Vr, mr, ur, gn, Zr, wn, yn, Mn, zt = g(() => {
        Vr = (e) => {
          switch (e) {
            case "int8":
              return 3;
            case "uint8":
              return 2;
            case "bool":
              return 9;
            case "int16":
              return 5;
            case "uint16":
              return 4;
            case "int32":
              return 6;
            case "uint32":
              return 12;
            case "float16":
              return 10;
            case "float32":
              return 1;
            case "float64":
              return 11;
            case "string":
              return 8;
            case "int64":
              return 7;
            case "uint64":
              return 13;
            case "int4":
              return 22;
            case "uint4":
              return 21;
            default:
              throw new Error(`unsupported data type: ${e}`);
          }
        }, mr = (e) => {
          switch (e) {
            case 3:
              return "int8";
            case 2:
              return "uint8";
            case 9:
              return "bool";
            case 5:
              return "int16";
            case 4:
              return "uint16";
            case 6:
              return "int32";
            case 12:
              return "uint32";
            case 10:
              return "float16";
            case 1:
              return "float32";
            case 11:
              return "float64";
            case 8:
              return "string";
            case 7:
              return "int64";
            case 13:
              return "uint64";
            case 22:
              return "int4";
            case 21:
              return "uint4";
            default:
              throw new Error(`unsupported data type: ${e}`);
          }
        }, ur = (e, t) => {
          let s = [-1, 4, 1, 1, 2, 2, 4, 8, -1, 1, 2, 8, 4, 8, -1, -1, -1, -1, -1, -1, -1, 0.5, 0.5][e], n = typeof t == "number" ? t : t.reduce((i, a) => i * a, 1);
          return s > 0 ? Math.ceil(n * s) : void 0;
        }, gn = (e) => {
          switch (e) {
            case "float16":
              return typeof Float16Array < "u" && Float16Array.from ? Float16Array : Uint16Array;
            case "float32":
              return Float32Array;
            case "uint8":
              return Uint8Array;
            case "int8":
              return Int8Array;
            case "uint16":
              return Uint16Array;
            case "int16":
              return Int16Array;
            case "int32":
              return Int32Array;
            case "bool":
              return Uint8Array;
            case "float64":
              return Float64Array;
            case "uint32":
              return Uint32Array;
            case "int64":
              return BigInt64Array;
            case "uint64":
              return BigUint64Array;
            default:
              throw new Error(`unsupported type: ${e}`);
          }
        }, Zr = (e) => {
          switch (e) {
            case "verbose":
              return 0;
            case "info":
              return 1;
            case "warning":
              return 2;
            case "error":
              return 3;
            case "fatal":
              return 4;
            default:
              throw new Error(`unsupported logging level: ${e}`);
          }
        }, wn = (e) => e === "float32" || e === "float16" || e === "int32" || e === "int64" || e === "uint32" || e === "uint8" || e === "bool" || e === "uint4" || e === "int4", yn = (e) => e === "float32" || e === "float16" || e === "int32" || e === "int64" || e === "uint32" || e === "uint64" || e === "int8" || e === "uint8" || e === "bool" || e === "uint4" || e === "int4", Mn = (e) => {
          switch (e) {
            case "none":
              return 0;
            case "cpu":
              return 1;
            case "cpu-pinned":
              return 2;
            case "texture":
              return 3;
            case "gpu-buffer":
              return 4;
            case "ml-tensor":
              return 5;
            default:
              throw new Error(`unsupported data location: ${e}`);
          }
        };
      }), bn, Fn = g(() => {
        Je(), bn = async (e) => {
          if (typeof e == "string") {
            let t = await fetch(e);
            if (!t.ok) throw new Error(`failed to load external data file: ${e}`);
            let s = t.headers.get("Content-Length"), n = s ? parseInt(s, 10) : 0;
            if (n < 1073741824) return new Uint8Array(await t.arrayBuffer());
            {
              if (!t.body) throw new Error(`failed to load external data file: ${e}, no response body.`);
              let i = t.body.getReader(), a;
              try {
                a = new ArrayBuffer(n);
              } catch (u) {
                if (u instanceof RangeError) {
                  let p = Math.ceil(n / 65536);
                  a = new WebAssembly.Memory({ initial: p, maximum: p }).buffer;
                } else throw u;
              }
              let o = 0;
              for (; ; ) {
                let { done: u, value: p } = await i.read();
                if (u) break;
                let h = p.byteLength;
                new Uint8Array(a, o, h).set(p), o += h;
              }
              return new Uint8Array(a, 0, n);
            }
          } else return e instanceof Blob ? new Uint8Array(await e.arrayBuffer()) : e instanceof Uint8Array ? e : new Uint8Array(e);
        };
      }), On, Dn, Wr, Ln, vn, zn, as, Pe = g(() => {
        zt(), On = ["V", "I", "W", "E", "F"], Dn = (e, t) => {
          console.log(`[${On[e]},${(/* @__PURE__ */ new Date()).toISOString()}]${t}`);
        }, vn = (e, t) => {
          Wr = e, Ln = t;
        }, zn = (e, t) => {
          let s = Zr(e), n = Zr(Wr);
          s >= n && Dn(s, typeof t == "function" ? t() : t);
        }, as = (...e) => {
          Ln && zn(...e);
        };
      }), P, Q = g(() => {
        zt(), P = (e, t) => new (gn(t))(e);
      }), ue = g(() => {
      }), ve, Se, Qe, pt, gt, ft, xt, Kt, ms, us = g(() => {
        Pe(), ue(), ve = /* @__PURE__ */ new Map([[64, 250], [128, 200], [256, 200], [512, 200], [2048, 230], [4096, 200], [8192, 50], [16384, 50], [32768, 50], [65536, 50], [131072, 50], [262144, 50], [524288, 50], [1048576, 50], [2097152, 30], [4194304, 20], [8388608, 10], [12582912, 10], [16777216, 10], [26214400, 15], [33554432, 22], [44236800, 2], [58982400, 6], [67108864, 6], [134217728, 6], [167772160, 6]]), Se = [], Qe = (e) => Math.ceil(Number(e) / 16) * 16, pt = (e) => {
          for (let t = 0; t < Se.length; t++) {
            let s = Se[t];
            if (e <= s) return s;
          }
          return Math.ceil(e / 16) * 16;
        }, gt = 1, ft = () => gt++, xt = async (e, t, s, n) => {
          let i = Qe(s), a = e.device.createBuffer({ size: i, usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ });
          try {
            let o = e.getCommandEncoder();
            e.endComputePass(), o.copyBufferToBuffer(t, 0, a, 0, i), e.flush(), await a.mapAsync(GPUMapMode.READ);
            let u = a.getMappedRange();
            if (n) {
              let p = n();
              return p.set(new Uint8Array(u, 0, s)), p;
            } else return new Uint8Array(u.slice(0, s));
          } finally {
            a.destroy();
          }
        }, Kt = class {
          constructor(e) {
            this.backend = e, this.storageCache = /* @__PURE__ */ new Map(), this.freeBuffers = /* @__PURE__ */ new Map(), this.freeUniformBuffers = /* @__PURE__ */ new Map(), this.buffersPending = [], this.capturedPendingBuffers = /* @__PURE__ */ new Map();
            for (let [t] of ve) Se.push(t), this.freeBuffers.set(t, []), this.freeUniformBuffers.set(t, []);
            this.sessionCount = 0;
          }
          upload(e, t) {
            let s = t.buffer, n = t.byteOffset, i = t.byteLength, a = Qe(i), o = this.storageCache.get(e);
            if (!o) throw new Error("gpu data for uploading does not exist");
            if (Number(o.originalSize) !== i) throw new Error(`inconsistent data size. gpu data size=${o.originalSize}, data size=${i}`);
            let u = this.backend.device.createBuffer({ mappedAtCreation: !0, size: a, usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC }), p = u.getMappedRange();
            new Uint8Array(p).set(new Uint8Array(s, n, i)), u.unmap();
            let h = this.backend.device.createCommandEncoder();
            h.copyBufferToBuffer(u, 0, o.gpuData.buffer, 0, a), this.backend.device.queue.submit([h.finish()]), u.destroy(), as("verbose", () => `[WebGPU] GpuDataManager.upload(id=${e})`);
          }
          memcpy(e, t) {
            let s = this.storageCache.get(e);
            if (!s) throw new Error("source gpu data for memcpy does not exist");
            let n = this.storageCache.get(t);
            if (!n) throw new Error("destination gpu data for memcpy does not exist");
            if (s.originalSize !== n.originalSize) throw new Error("inconsistent source and destination gpu data size");
            let i = Qe(s.originalSize), a = this.backend.getCommandEncoder();
            this.backend.endComputePass(), a.copyBufferToBuffer(s.gpuData.buffer, 0, n.gpuData.buffer, 0, i);
          }
          registerExternalBuffer(e, t, s) {
            let n;
            if (s) {
              if (n = s[0], e === s[1]) return as("verbose", () => `[WebGPU] GpuDataManager.registerExternalBuffer(size=${t}) => id=${n}, buffer is the same, skip.`), n;
              if (this.backend.capturedCommandList.has(this.backend.currentSessionId)) throw new Error(`Registering a different external buffer under graph capture mode is not supported yet.
             Please use the previous external buffer!`);
            } else n = ft();
            return this.storageCache.set(n, { gpuData: { id: n, type: 0, buffer: e }, originalSize: t }), as("verbose", () => `[WebGPU] GpuDataManager.registerExternalBuffer(size=${t}) => id=${n}, registered.`), n;
          }
          unregisterExternalBuffer(e) {
            e !== void 0 && (this.storageCache.delete(e), as("verbose", () => `[WebGPU] GpuDataManager.unregisterExternalBuffer() => id=${e}`));
          }
          create(e, t = GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST) {
            let s = pt(e), n, i = (t & GPUBufferUsage.STORAGE) === GPUBufferUsage.STORAGE, a = (t & GPUBufferUsage.UNIFORM) === GPUBufferUsage.UNIFORM;
            if (i || a) {
              let u = (i ? this.freeBuffers : this.freeUniformBuffers).get(s);
              u ? u.length > 0 ? n = u.pop() : n = this.backend.device.createBuffer({ size: s, usage: t }) : n = this.backend.device.createBuffer({ size: s, usage: t });
            } else n = this.backend.device.createBuffer({ size: s, usage: t });
            let o = { id: ft(), type: 0, buffer: n };
            return this.storageCache.set(o.id, { gpuData: o, originalSize: Number(e) }), as("verbose", () => `[WebGPU] GpuDataManager.create(size=${e}) => id=${o.id}`), o;
          }
          get(e) {
            var t;
            return (t = this.storageCache.get(e)) == null ? void 0 : t.gpuData;
          }
          release(e) {
            let t = typeof e == "bigint" ? Number(e) : e, s = this.storageCache.get(t);
            if (!s) {
              if (this.storageCache.size === 0) return 0;
              throw new Error("releasing data does not exist");
            }
            return as("verbose", () => `[WebGPU] GpuDataManager.release(id=${t}), gpuDataId=${s.gpuData.id}`), this.storageCache.delete(t), this.buffersPending.push(s.gpuData.buffer), s.originalSize;
          }
          async download(e, t) {
            let s = this.storageCache.get(Number(e));
            if (!s) throw new Error("data does not exist");
            await xt(this.backend, s.gpuData.buffer, s.originalSize, t);
          }
          refreshPendingBuffers() {
            if (this.buffersPending.length !== 0) if (this.backend.sessionStatus === "default") {
              for (let e of this.buffersPending) {
                let t = ve.get(e.size);
                if ((e.usage & GPUBufferUsage.STORAGE) === GPUBufferUsage.STORAGE) {
                  let s = this.freeBuffers.get(e.size) || [];
                  t === void 0 || s.length >= t ? e.destroy() : s.push(e);
                } else if ((e.usage & GPUBufferUsage.UNIFORM) === GPUBufferUsage.UNIFORM) {
                  let s = this.freeUniformBuffers.get(e.size) || [];
                  t === void 0 || s.length >= t ? e.destroy() : s.push(e);
                } else e.destroy();
              }
              this.buffersPending = [];
            } else {
              let e = this.capturedPendingBuffers.get(this.backend.currentSessionId);
              e || (e = [], this.capturedPendingBuffers.set(this.backend.currentSessionId, e));
              for (let t of this.buffersPending) e.push(t);
              this.buffersPending = [];
            }
          }
          dispose() {
            this.freeBuffers.forEach((e) => {
              e.forEach((t) => {
                t.destroy();
              });
            }), this.freeUniformBuffers.forEach((e) => {
              e.forEach((t) => {
                t.destroy();
              });
            }), this.storageCache.forEach((e) => {
              e.gpuData.buffer.destroy();
            }), this.capturedPendingBuffers.forEach((e) => {
              e.forEach((t) => {
                t.destroy();
              });
            }), this.storageCache = /* @__PURE__ */ new Map(), this.freeBuffers = /* @__PURE__ */ new Map(), this.freeUniformBuffers = /* @__PURE__ */ new Map(), this.capturedPendingBuffers = /* @__PURE__ */ new Map();
          }
          onCreateSession() {
            this.sessionCount += 1;
          }
          onReleaseSession(e) {
            let t = this.capturedPendingBuffers.get(e);
            t && (t.forEach((s) => {
              s.destroy();
            }), this.capturedPendingBuffers.delete(e)), this.sessionCount -= 1, this.sessionCount === 0 && (as("warning", () => "[WebGPU] Clearing webgpu buffer cache"), this.storageCache.forEach((s) => {
              s.gpuData.buffer.destroy();
            }), this.storageCache = /* @__PURE__ */ new Map());
          }
        }, ms = (...e) => new Kt(...e);
      }), Os, Bt, rs = g(() => {
        Os = class {
          constructor(e) {
            Object.assign(this, e);
          }
          get cacheKey() {
            return this.key || (this.key = Object.getOwnPropertyNames(this).sort().map((e) => `${this[e]}`).join(";")), this.key;
          }
        }, Bt = (e) => new Os(e);
      }), rr, Ws, ze, Js, Fr, ks, Xs, Ft = g(() => {
        rr = class {
          static calcMatMulShape(e, t) {
            return e[1] !== t[0] ? void 0 : [e[0], t[1]];
          }
        }, Ws = class {
          static calcShape(e, t, s = !1) {
            let n = e.length, i = t.length;
            if (n === 0) return t;
            if (i === 0) return e;
            let a = Math.max(e.length, t.length), o = new Array(a);
            if (s) {
              if (n < 2 || i < 2) return;
              let u = rr.calcMatMulShape([e[n - 2], e[n - 1]], [t[i - 2], t[i - 1]]);
              if (u === void 0) return;
              [o[a - 2], o[a - 1]] = u;
            }
            for (let u = s ? 3 : 1; u <= a; u++) {
              let p = n - u < 0 ? 1 : e[n - u], h = i - u < 0 ? 1 : t[i - u];
              if (p !== h && p > 1 && h > 1) return;
              let k = Math.max(p, h);
              if (p && h) o[a - u] = Math.max(p, h);
              else {
                if (k > 1) return;
                o[a - u] = 0;
              }
            }
            return o;
          }
          static isValidBroadcast(e, t) {
            let s = e.length, n = t.length;
            if (s > n) return !1;
            for (let i = 1; i <= s; i++) if (e[s - i] !== 1 && e[s - i] !== t[n - i]) return !1;
            return !0;
          }
        }, ze = class Hp {
          static size(t) {
            return Hp.getSizeFromDimensionRange(t, 0, t.length);
          }
          static convertShape(t, s = 4) {
            let n = t.length;
            if (n === 0) return [];
            let i = new Array(n), a = n - 1;
            for (; a >= 0; ) {
              if (t[a] % s === 0) {
                i[a] = t[a] / s;
                break;
              }
              if (s % t[a] !== 0) throw new Error("cannot convert shape");
              i[a] = 1, s /= t[a], a--;
            }
            for (a--; a >= 0; a--) i[a] = t[a];
            return i;
          }
          static sizeFromDimension(t, s) {
            if (s < 0 || s > t.length) throw new Error(`invalid dimension of ${s} for sizeFromDimension as Tensor has ${t.length} dimensions.`);
            return Hp.getSizeFromDimensionRange(t, s, t.length);
          }
          static sizeToDimension(t, s) {
            if (s < 0 || s > t.length) throw new Error(`invalid dimension of ${s} for sizeToDimension as Tensor has ${t.length} dimensions.`);
            return Hp.getSizeFromDimensionRange(t, 0, s);
          }
          static getSizeFromDimensionRange(t, s, n) {
            let i = 1;
            for (let a = s; a < n; a++) {
              if (t[a] < 0) throw new Error("cannot get valid size from specified dimension range. Most likely the range contains negative values in them.");
              i *= Number(t[a]);
            }
            return i;
          }
          static computeStrides(t) {
            let s = t.length;
            if (s === 0) return [];
            if (s === 1) return [1];
            let n = new Array(s);
            n[s - 1] = 1, n[s - 2] = t[s - 1];
            for (let i = s - 3; i >= 0; --i) n[i] = n[i + 1] * t[i + 1];
            return n;
          }
          static normalizeAxis(t, s) {
            if (t < -s && t >= s) throw new Error("unsupported axis for this operation.");
            return t < 0 ? t + s : t;
          }
          static normalizeAxes(t, s) {
            return t.map((n) => this.normalizeAxis(n, s ?? t.length));
          }
          static sortBasedOnPerm(t, s) {
            return s ? s.map((n) => t[n]) : t.slice().reverse();
          }
          static padShape(t, s) {
            let n = t.length;
            return t.map((i, a) => i + s[a] + s[a + n]);
          }
          static areEqual(t, s) {
            return t.length !== s.length ? !1 : t.every((n, i) => n === s[i]);
          }
        }, Js = class zc {
          static adjustPoolAttributes(t, s, n, i, a, o) {
            if (!t && n.length !== s.length - 2) throw new Error("length of specified kernel shapes should be 2 less than length of input dimensions");
            if (t) for (let u = 0; u < s.length - 2; u++) u >= n.length ? n.push(s[u + 2]) : n[u] = s[u + 2];
            for (let u = 0; u < n.length; u++) if (u < i.length) {
              if (i[u] < 0) throw new Error("strides should be greater than or equal to 1");
            } else i.push(1);
            for (let u = 0; u < n.length; u++) if (u < a.length) {
              if (a[u] < 0) throw new Error("dilations should be greater than or equal to 1");
            } else a.push(1);
            for (let u = 0; u < n.length * 2; u++) if (u < o.length) {
              if (o[u] < 0) throw new Error("pad should be greater than or equal to 1");
            } else o.push(0);
            for (let u = 0; u < n.length; u++) {
              if (n[u] <= 0) throw new Error("kernel shapes need to be greater than 0");
              if (o[u] >= n[u] || o[u + n.length] >= n[u]) throw new Error("pads should be smaller than kernel");
            }
          }
          static adjustPadsBasedOnAutoPad(t, s, n, i, a, o, u) {
            if (u) {
              if (a.length !== 2 * (t.length - 2)) throw new Error("length of pads should be twice the length of data dimensions");
              if (s.length !== t.length - 2) throw new Error("length of strides should be the length of data dimensions");
              if (i.length !== t.length - 2) throw new Error("length of kernel shapes should be the length of data dimensions");
              for (let p = 0; p < t.length - 2; p++) zc.adjustPadAndReturnShape(t[p + (o ? 1 : 2)], s[p], n[p], i[p], a, p, p + t.length - 2, u);
            }
          }
          static computePoolOutputShape(t, s, n, i, a, o, u) {
            if (s.length <= 0) throw new Error("input shape must be of size greater than 0");
            let p = [s[0], s[1]];
            return zc.computeShapeHelper(t, s, p, n, i, a, o, u), p;
          }
          static computeConvOutputShape(t, s, n, i, a, o, u) {
            if (t.length <= 0 || s.length <= 0) throw new Error("invalid input tensor dims or invalid filter tensor dims");
            let p = [t[0], s[0]];
            return zc.computeShapeHelper(!1, t, p, n, i, a, o, u), p;
          }
          static computeShapeHelper(t, s, n, i, a, o, u, p) {
            if (t) for (let h = 0; h < s.length - 2; h++) n.push(1);
            else for (let h = 0; h < s.length - 2; h++) n.push(zc.adjustPadAndReturnShape(s[h + 2], i[h], a[h], o[h], u, h, h + s.length - 2, p));
          }
          static adjustPadAndReturnShape(t, s, n, i, a, o, u, p) {
            let h = n * (i - 1) + 1;
            if (p && p !== "NOTSET") switch (p) {
              case "VALID":
                return a[o] = 0, a[u] = 0, Math.floor((t - h) / s + 1);
              case "SAME_LOWER":
              case "SAME_UPPER":
                if (n !== 1) throw new Error("Dilation not supported for SAME_UPPER or SAME_LOWER");
                {
                  let k = ((t + s - 1) / s - 1) * s + i - t;
                  return a[o] = Math.floor(p === "SAME_LOWER" ? (k + 1) / 2 : k / 2), a[u] = k - a[o], Math.floor((t + k - i) / s + 1);
                }
              default:
                throw new Error("Unsupported AutoPad type");
            }
            else return Math.floor((t + a[o] + a[u] - h) / s + 1);
          }
        }, Fr = class {
          static getShapeOfGemmResult(e, t, s, n, i) {
            if (e.length !== 2 || s.length !== 2) throw new Error("shape need to be of size 2");
            let a, o, u;
            t ? (a = e[1], o = e[0]) : (a = e[0], o = e[1]);
            let p = -1;
            if (n ? (u = s[0], p = 1) : (u = s[1], p = 0), s[p] !== o) throw new Error("dimension mismatch");
            if (a <= 0 || u <= 0 || o <= 0) throw new Error("invalid shape specified");
            if (i && !Ws.isValidBroadcast(i, [a, u])) throw new Error("gemm: invalid bias shape for broadcast");
            return [a, u, o];
          }
        }, ks = -34028234663852886e22, Xs = 34028234663852886e22;
      }), ir, fr, fs, Ss, yt, qt, Ls, $s, Gs, $t, en, qe, It, Oa, Ri, Da, La, Yt = g(() => {
        zt(), Ft(), ir = 64, fr = (e, t) => {
          if (t === 3) throw new Error("vec3 has same alignment as vec4, use vec4 instead");
          switch (Number(e)) {
            case 10:
              return t > 1 ? `vec${t}<f16>` : "f16";
            case 1:
              return t > 1 ? `vec${t}<f32>` : "f32";
            case 6:
              return t > 1 ? `vec${t}<i32>` : "i32";
            case 12:
              return t > 1 ? `vec${t}<u32>` : "u32";
            case 7:
              if (t > 1) throw new Error("currently not supported vecX of uint64 yet");
              return ["vec2<u32>", "i32"];
            case 13:
              if (t > 1) throw new Error("currently not supported vecX of uint64 yet");
              return ["vec2<u32>", "u32"];
            case 9:
              if (t !== 4) throw new Error("bool must be vec4");
              return ["u32", "vec4<bool>"];
            case 22:
              return "i32";
            case 21:
              return "u32";
            default:
              throw new Error(`Unknown data type: ${e}`);
          }
        }, fs = (e, t = 1) => {
          let s = fr(e, t);
          return typeof s == "string" ? s : s[0];
        }, Ss = (e, t = 1) => {
          let s = fr(e, t);
          return typeof s == "string" ? s : s[1];
        }, yt = (...e) => {
          let t = [];
          return e.forEach((s) => {
            s.length !== 0 && t.push({ type: 12, data: s }, { type: 12, data: ze.computeStrides(s) });
          }), t;
        }, qt = (e) => e % 4 === 0 ? 4 : e % 2 === 0 ? 2 : 1, Ls = (e = "f32", t, s = "0") => !t || t === 1 ? `${e}(${s})` : `vec${t}<${e}>(${s})`, $s = (e, t, s) => e === "f32" ? s : t === 1 ? `f32(${s})` : `vec${t}<f32>(${s})`, Gs = (e, t) => t === 4 ? `(${e}.x + ${e}.y + ${e}.z + ${e}.w)` : t === 2 ? `(${e}.x + ${e}.y)` : t === 3 ? `(${e}.x + ${e}.y + ${e}.z)` : e, $t = (e, t, s, n) => e.startsWith("uniforms.") && s > 4 ? typeof t == "string" ? n === "f16" ? `${e}[(${t}) / 8][(${t}) % 8 / 4][(${t}) % 8 % 4]` : `${e}[(${t}) / 4][(${t}) % 4]` : n === "f16" ? `${e}[${Math.floor(t / 8)}][${Math.floor(t % 8 / 4)}][${t % 8 % 4}]` : `${e}[${Math.floor(t / 4)}][${t % 4}]` : s > 1 ? `${e}[${t}]` : e, en = (e, t, s, n, i) => {
          let a = typeof s == "number", o = a ? s : s.length, u = [...new Array(o).keys()], p = o < 2 ? "u32" : o <= 4 ? `vec${o}<u32>` : `array<u32, ${o}>`, h = fr(t, i), k = typeof h == "string" ? h : h[1], C = typeof h == "string" ? h : h[0], d = { indices: p, value: k, storage: C, tensor: t }, z = (ot) => typeof ot == "string" ? ot : `${ot}u`, B = { offsetToIndices: !1, indicesToOffset: !1, broadcastedIndicesToOffset: !1, set: !1, setByIndices: !1, get: !1, getByIndices: !1 }, V = a ? "uniforms." : "", Z = `${V}${e}_shape`, ee = `${V}${e}_strides`, X = "";
          for (let ot = 0; ot < o - 1; ot++) X += `
    let dim${ot} = current / ${$t(ee, ot, o)};
    let rest${ot} = current % ${$t(ee, ot, o)};
    indices[${ot}] = dim${ot};
    current = rest${ot};
    `;
          X += `indices[${o - 1}] = current;`;
          let he = o < 2 ? "" : `
  fn o2i_${e}(offset: u32) -> ${d.indices} {
    var indices: ${d.indices};
    var current = offset;
    ${X}
    return indices;
  }`, pe = (ot) => (B.offsetToIndices = !0, o < 2 ? ot : `o2i_${e}(${ot})`), Me = [];
          if (o >= 2) for (let ot = o - 1; ot >= 0; ot--) Me.push(`${$t(ee, ot, o)} * (indices[${ot}])`);
          let Oe = o < 2 ? "" : `
  fn i2o_${e}(indices: ${d.indices}) -> u32 {
    return ${Me.join("+")};
  }`, Le = (ot) => (B.indicesToOffset = !0, o < 2 ? ot : `i2o_${e}(${ot})`), Ye = (...ot) => o === 0 ? "0u" : `${d.indices}(${ot.map(z).join(",")})`, at = (ot, Et) => o < 2 ? `${ot}` : `${$t(ot, Et, o)}`, Pt = (ot, Et, ps) => o < 2 ? `${ot}=${ps};` : `${$t(ot, Et, o)}=${ps};`, Xt = {}, Zt = (ot, Et) => {
            B.broadcastedIndicesToOffset = !0;
            let ps = `${Et.name}broadcastedIndicesTo${e}Offset`;
            if (ps in Xt) return `${ps}(${ot})`;
            let Ns = [];
            for (let xr = o - 1; xr >= 0; xr--) {
              let er = Et.indicesGet("outputIndices", xr + Et.rank - o);
              Ns.push(`${at(ee, xr)} * (${er} % ${at(Z, xr)})`);
            }
            return Xt[ps] = `fn ${ps}(outputIndices: ${Et.type.indices}) -> u32 {
             return ${Ns.length > 0 ? Ns.join("+") : "0u"};
           }`, `${ps}(${ot})`;
          }, bt = (ot, Et) => (() => {
            if (d.storage === d.value) return `${e}[${ot}]=${Et};`;
            if (d.storage === "vec2<u32>" && d.value === "i32") return `${e}[${ot}]=vec2<u32>(u32(${Et}), select(0u, 0xFFFFFFFFu, ${Et} < 0));`;
            if (d.storage === "vec2<u32>" && d.value === "u32") return `${e}[${ot}]=vec2<u32>(u32(${Et}), 0u);`;
            if (d.storage === "u32" && d.value === "vec4<bool>") return `${e}[${ot}]=dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(${Et}));`;
            throw new Error(`not supported combination of storage type ${d.storage} and value type ${d.value} yet`);
          })(), ss = (ot) => (() => {
            if (d.storage === d.value) return `${e}[${ot}]`;
            if (d.storage === "vec2<u32>" && d.value === "i32") return `i32(${e}[${ot}].x)`;
            if (d.storage === "vec2<u32>" && d.value === "u32") return `u32(${e}[${ot}].x)`;
            if (d.storage === "u32" && d.value === "vec4<bool>") return `vec4<bool>(bool(${e}[${ot}] & 0xFFu), bool(${e}[${ot}] & 0xFF00u), bool(${e}[${ot}] & 0xFF0000u), bool(${e}[${ot}] & 0xFF000000u))`;
            throw new Error(`not supported combination of storage type ${d.storage} and value type ${d.value} yet`);
          })(), St = o < 2 ? "" : `
  fn get_${e}ByIndices(indices: ${d.indices}) -> ${k} {
    return ${ss(`i2o_${e}(indices)`)};
  }`, Ot = o < 2 ? "" : (() => {
            let ot = u.map((ps) => `d${ps}: u32`).join(", "), Et = u.map((ps) => `d${ps}`).join(", ");
            return `
  fn get_${e}(${ot}) -> ${k} {
    return get_${e}ByIndices(${Ye(Et)});
  }`;
          })(), bs = (...ot) => {
            if (ot.length !== o) throw new Error(`indices length must be ${o}`);
            let Et = ot.map(z).join(",");
            return o === 0 ? ss("0u") : o === 1 ? ss(Et[0]) : (B.get = !0, B.getByIndices = !0, B.indicesToOffset = !0, `get_${e}(${Et})`);
          }, Ht = (ot) => o < 2 ? ss(ot) : (B.getByIndices = !0, B.indicesToOffset = !0, `get_${e}ByIndices(${ot})`), Rt = o < 2 ? "" : `
  fn set_${e}ByIndices(indices: ${d.indices}, value: ${k}) {
    ${bt(`i2o_${e}(indices)`, "value")}
  }`, _s = o < 2 ? "" : (() => {
            let ot = u.map((ps) => `d${ps}: u32`).join(", "), Et = u.map((ps) => `d${ps}`).join(", ");
            return `
  fn set_${e}(${ot}, value: ${k}) {
    set_${e}ByIndices(${Ye(Et)}, value);
  }`;
          })();
          return { impl: () => {
            let ot = [], Et = !1;
            return B.offsetToIndices && (ot.push(he), Et = !0), B.indicesToOffset && (ot.push(Oe), Et = !0), B.broadcastedIndicesToOffset && (Object.values(Xt).forEach((ps) => ot.push(ps)), Et = !0), B.set && (ot.push(_s), Et = !0), B.setByIndices && (ot.push(Rt), Et = !0), B.get && (ot.push(Ot), Et = !0), B.getByIndices && (ot.push(St), Et = !0), !a && Et && ot.unshift(`const ${Z} = ${d.indices}(${s.join(",")});`, `const ${ee} = ${d.indices}(${ze.computeStrides(s).join(",")});`), ot.join(`
`);
          }, type: d, offsetToIndices: pe, indicesToOffset: Le, broadcastedIndicesToOffset: Zt, indices: Ye, indicesGet: at, indicesSet: Pt, set: (...ot) => {
            if (ot.length !== o + 1) throw new Error(`indices length must be ${o}`);
            let Et = ot[o];
            if (typeof Et != "string") throw new Error("value must be string");
            let ps = ot.slice(0, o).map(z).join(",");
            return o === 0 ? bt("0u", Et) : o === 1 ? bt(ps[0], Et) : (B.set = !0, B.setByIndices = !0, B.indicesToOffset = !0, `set_${e}(${ps}, ${Et})`);
          }, setByOffset: bt, setByIndices: (ot, Et) => o < 2 ? bt(ot, Et) : (B.setByIndices = !0, B.indicesToOffset = !0, `set_${e}ByIndices(${ot}, ${Et});`), get: bs, getByOffset: ss, getByIndices: Ht, usage: n, name: e, strides: ee, shape: Z, rank: o };
        }, qe = (e, t, s, n = 1) => en(e, t, s, "input", n), It = (e, t, s, n = 1) => en(e, t, s, "output", n), Oa = (e, t, s) => en(e, t, s, "atomicOutput", 1), Ri = (e, t, s, n = 1) => en(e, t, s, "internal", n), Da = class {
          constructor(e, t) {
            this.normalizedDispatchGroup = e, this.limits = t, this.internalVariables = [], this.variables = [], this.uniforms = [], this.variableIndex = 0;
          }
          guardAgainstOutOfBoundsWorkgroupSizes(e) {
            return `if (global_idx >= ${typeof e == "number" ? `${e}u` : e}) { return; }`;
          }
          mainStart(e = ir) {
            let t = typeof e == "number" ? e : e[0], s = typeof e == "number" ? 1 : e[1], n = typeof e == "number" ? 1 : e[2];
            if (t > this.limits.maxComputeWorkgroupSizeX || s > this.limits.maxComputeWorkgroupSizeY || n > this.limits.maxComputeWorkgroupSizeZ) throw new Error(`workgroup size [${t}, ${s}, ${n}] exceeds the maximum workgroup size [${this.limits.maxComputeWorkgroupSizeX}, ${this.limits.maxComputeWorkgroupSizeY}, ${this.limits.maxComputeWorkgroupSizeZ}].`);
            if (t * s * n > this.limits.maxComputeInvocationsPerWorkgroup) throw new Error(`workgroup size [${t}, ${s}, ${n}] exceeds the maximum workgroup invocations ${this.limits.maxComputeInvocationsPerWorkgroup}.`);
            let i = this.normalizedDispatchGroup[1] === 1 && this.normalizedDispatchGroup[2] === 1, a = i ? `@builtin(global_invocation_id) global_id : vec3<u32>,
    @builtin(workgroup_id) workgroup_id : vec3<u32>,
    @builtin(local_invocation_index) local_idx : u32,
    @builtin(local_invocation_id) local_id : vec3<u32>` : `@builtin(global_invocation_id) global_id : vec3<u32>,
                                             @builtin(local_invocation_id) local_id : vec3<u32>,
    @builtin(local_invocation_index) local_idx : u32,
    @builtin(workgroup_id) workgroup_id : vec3<u32>,
    @builtin(num_workgroups) num_workgroups : vec3<u32>`, o = i ? `let global_idx = global_id.x;
         let workgroup_index = workgroup_id.x;` : `let workgroup_index = workgroup_id.z * num_workgroups[0] * num_workgroups[1] +
             workgroup_id.y * num_workgroups[0] + workgroup_id.x;
         let global_idx = workgroup_index * ${t * s * n}u + local_idx;`;
            return `@compute @workgroup_size(${t}, ${s}, ${n})
  fn main(${a}) {
    ${o}
  `;
          }
          appendVariableUniforms(e) {
            e.rank !== 0 && (e.shape.startsWith("uniforms.") && this.uniforms.push({ name: e.shape.replace("uniforms.", ""), type: "u32", length: e.rank }), e.strides.startsWith("uniforms.") && this.uniforms.push({ name: e.strides.replace("uniforms.", ""), type: "u32", length: e.rank }));
          }
          declareVariable(e, t) {
            if (e.usage === "internal") throw new Error("cannot use internal variable with declareVariable(). use registerInternalVariables() instead.");
            this.variables.push(e), this.appendVariableUniforms(e);
            let s = e.usage === "input" ? "read" : "read_write", n = e.usage === "atomicOutput" ? "atomic<i32>" : e.type.storage;
            return `@group(0) @binding(${t}) var<storage, ${s}> ${e.name}: array<${n}>;`;
          }
          declareVariables(...e) {
            return e.map((t) => this.declareVariable(t, this.variableIndex++)).join(`
`);
          }
          registerInternalVariable(e) {
            if (e.usage !== "internal") throw new Error("cannot use input or output variable with registerInternalVariable(). use declareVariables() instead.");
            this.internalVariables.push(e), this.appendVariableUniforms(e);
          }
          registerInternalVariables(...e) {
            return e.forEach((t) => this.registerInternalVariable(t)), this;
          }
          registerUniform(e, t, s = 1) {
            return this.uniforms.push({ name: e, type: t, length: s }), this;
          }
          registerUniforms(e) {
            return this.uniforms = this.uniforms.concat(e), this;
          }
          uniformDeclaration() {
            if (this.uniforms.length === 0) return "";
            let e = [];
            for (let { name: t, type: s, length: n } of this.uniforms) if (n && n > 4) s === "f16" ? e.push(`@align(16) ${t}:array<mat2x4<${s}>, ${Math.ceil(n / 8)}>`) : e.push(`${t}:array<vec4<${s}>, ${Math.ceil(n / 4)}>`);
            else {
              let i = n == null || n === 1 ? s : `vec${n}<${s}>`;
              e.push(`${t}:${i}`);
            }
            return `
      struct Uniforms { ${e.join(", ")} };
      @group(0) @binding(${this.variableIndex}) var<uniform> uniforms: Uniforms;`;
          }
          get additionalImplementations() {
            return this.uniformDeclaration() + this.variables.map((e) => e.impl()).join(`
`) + this.internalVariables.map((e) => e.impl()).join(`
`);
          }
          get variablesInfo() {
            if (this.uniforms.length === 0) return;
            let e = (t) => [12, 10, 1, 6][["u32", "f16", "f32", "i32"].indexOf(t)];
            return this.uniforms.map((t) => [e(t.type), t.length ?? 1]);
          }
        }, La = (e, t) => new Da(e, t);
      }), za, Ni, ji, Ba, Ra, Ui, cr, Na, Vi, Gr = g(() => {
        zt(), Ft(), rs(), Yt(), za = (e, t) => {
          if (!e || e.length !== 1) throw new Error("Transpose requires 1 input.");
          if (t.length !== 0 && t.length !== e[0].dims.length) throw new Error(`perm size ${t.length} does not match input rank ${e[0].dims.length}`);
        }, Ni = (e, t) => t.length !== 0 ? t : [...new Array(e).keys()].reverse(), ji = (e, t) => ze.sortBasedOnPerm(e, Ni(e.length, t)), Ba = (e, t, s, n) => {
          let i = `fn perm(i: ${n.type.indices}) -> ${s.type.indices} {
    var a: ${s.type.indices};`;
          for (let a = 0; a < t; ++a) i += `a[${e[a]}]=i[${a}];`;
          return i += "return a;}";
        }, Ra = (e, t) => {
          let s = [], n = [];
          for (let i = 0; i < e.length; ++i) e[i] !== 1 && s.push(e[i]), e[t[i]] !== 1 && n.push(t[i]);
          return { newShape: s, newPerm: n };
        }, Ui = (e, t) => {
          let s = 0;
          for (let n = 0; n < e.length; ++n) if (t[e[n]] !== 1) {
            if (e[n] < s) return !1;
            s = e[n];
          }
          return !0;
        }, cr = (e, t) => {
          let s = e.dataType, n = e.dims.length, i = Ni(n, t), a = ji(e.dims, i), o = e.dims, u = a, p = n < 2 || Ui(i, e.dims), h;
          if (p) return h = (B) => {
            let V = qe("input", s, o, 4), Z = It("output", s, u, 4);
            return `
  ${B.registerUniform("output_size", "u32").declareVariables(V, Z)}
  ${B.mainStart()}
    ${B.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    output[global_idx] = input[global_idx];
  }`;
          }, { name: "TransposeCopy", shaderCache: { inputDependencies: ["type"] }, getRunData: () => {
            let B = ze.size(a);
            return { outputs: [{ dims: a, dataType: e.dataType }], dispatchGroup: { x: Math.ceil(B / 64 / 4) }, programUniforms: [{ type: 12, data: Math.ceil(B / 4) }] };
          }, getShaderSource: h };
          let { newShape: k, newPerm: C } = Ra(e.dims, i), d = ze.areEqual(C, [2, 3, 1]), z = ze.areEqual(C, [3, 1, 2]);
          if (k.length === 2 || d || z) {
            o = d ? [k[0], k[1] * k[2]] : z ? [k[0] * k[1], k[2]] : k, u = [o[1], o[0]];
            let B = 16;
            return h = (V) => {
              let Z = qe("a", s, o.length), ee = It("output", s, u.length);
              return `
  ${V.registerUniform("output_size", "u32").declareVariables(Z, ee)}
  var<workgroup> tile : array<array<${ee.type.value}, ${B + 1}>, ${B}>;
  ${V.mainStart([B, B, 1])}
    let stride = (uniforms.output_shape[1] - 1) / ${B} + 1;
    let workgroup_id_x = workgroup_index % stride;
    let workgroup_id_y = workgroup_index / stride;
    let input_col = workgroup_id_y * ${B}u + local_id.x;
    let input_row = workgroup_id_x * ${B}u + local_id.y;
    if (input_row < uniforms.a_shape[0] && input_col < uniforms.a_shape[1]) {
      tile[local_id.y][local_id.x] = ${Z.getByIndices(`${Z.type.indices}(input_row, input_col)`)};
    }
    workgroupBarrier();

    let output_col = workgroup_id_x * ${B}u + local_id.x;
    let output_row = workgroup_id_y * ${B}u + local_id.y;
    if (output_row < uniforms.output_shape[0] && output_col < uniforms.output_shape[1]) {
      ${ee.setByIndices(`${ee.type.indices}(output_row, output_col)`, "tile[local_id.x][local_id.y]")}
    }
  }`;
            }, { name: "TransposeShared", shaderCache: { inputDependencies: ["type"] }, getRunData: () => {
              let V = ze.size(a);
              return { outputs: [{ dims: a, dataType: e.dataType }], dispatchGroup: { x: Math.ceil(u[1] / B), y: Math.ceil(u[0] / B) }, programUniforms: [{ type: 12, data: V }, ...yt(o, u)] };
            }, getShaderSource: h };
          }
          return h = (B) => {
            let V = qe("a", s, o.length), Z = It("output", s, u.length);
            return `
  ${B.registerUniform("output_size", "u32").declareVariables(V, Z)}

  ${Ba(i, n, V, Z)}

  ${B.mainStart()}
    ${B.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let indices = ${Z.offsetToIndices("global_idx")};
    let aIndices = perm(indices);

    ${Z.setByOffset("global_idx", V.getByIndices("aIndices"))}
  }`;
          }, { name: "Transpose", shaderCache: { hint: `${t}`, inputDependencies: ["rank"] }, getRunData: () => {
            let B = ze.size(a);
            return { outputs: [{ dims: a, dataType: e.dataType }], dispatchGroup: { x: Math.ceil(B / 64) }, programUniforms: [{ type: 12, data: B }, ...yt(o, u)] };
          }, getShaderSource: h };
        }, Na = (e, t) => {
          za(e.inputs, t.perm), e.compute(cr(e.inputs[0], t.perm));
        }, Vi = (e) => Bt({ perm: e.perm });
      }), ni, ja, Ua, Va, Wa, Ga, Ka, Ha, Wi, qa, pr, tn, Qa, Nc, Xa, jc, Ya, Gi, Ja, Za, el, Uc = g(() => {
        zt(), Ft(), Yt(), ai(), Gr(), ni = { max: "select(bestValue, candidate, candidate > bestValue)", min: "select(bestValue, candidate, candidate < bestValue)", mean: "bestValue + candidate", sum: "bestValue + candidate", prod: "bestValue * candidate", sumSquare: "bestValue + candidate * candidate", logSumExp: "bestValue + exp(candidate)", l1: "bestValue + abs(candidate)", l2: "bestValue + candidate * candidate", logSum: "bestValue + candidate" }, ja = { max: "select(bestValue, candidate, candidate > bestValue)", min: "select(bestValue, candidate, candidate < bestValue)", mean: "bestValue + candidate", sum: "bestValue + candidate", prod: "bestValue * candidate", sumSquare: "bestValue + candidate", logSumExp: "bestValue + candidate", l1: "bestValue + candidate", l2: "bestValue + candidate", logSum: "bestValue + candidate" }, Ua = { max: "_A[offset]", min: "_A[offset]", mean: "0", sum: "0", prod: "1", sumSquare: "0", logSumExp: "0", l1: "0", l2: "0", logSum: "0" }, Va = { max: "bestValue", min: "bestValue", sum: "bestValue", prod: "bestValue", sumSquare: "bestValue", logSumExp: "log(bestValue)", l1: "bestValue", l2: "sqrt(bestValue)", logSum: "log(bestValue)" }, Wa = (e, t) => {
          let s = [];
          for (let n = t - e; n < t; ++n) s.push(n);
          return s;
        }, Ga = (e, t) => {
          let s = [], n = e.length;
          for (let a = 0; a < n; a++) t.indexOf(a) === -1 && s.push(e[a]);
          let i = t.map((a) => e[a]);
          return [s, i];
        }, Ka = (e, t) => {
          let s = e.length + t.length, n = [], i = 0;
          for (let a = 0; a < s; a++) t.indexOf(a) === -1 ? n.push(e[i++]) : n.push(1);
          return n;
        }, Ha = (e, t) => {
          for (let s = 0; s < e.length; ++s) if (e[e.length - s - 1] !== t - 1 - s) return !1;
          return !0;
        }, Wi = (e, t) => {
          let s = [];
          if (!Ha(e, t)) {
            for (let n = 0; n < t; ++n) e.indexOf(n) === -1 && s.push(n);
            e.forEach((n) => s.push(n));
          }
          return s;
        }, qa = (e, t, s, n, i, a, o) => {
          let u = s[0].dims, p = ze.size(a), h = ze.size(o), k = qe("_A", s[0].dataType, u), C = It("output", i, a), d = 64;
          p === 1 && (d = 256);
          let z = `
          var<workgroup> aBestValues : array<f32, ${d}>;
       `, B = (V) => `
        ${V.registerUniform("reduceSize", "u32").declareVariables(k, C)}
        ${z}
        fn DIV_CEIL(a : u32, b : u32) -> u32 {
          return ((a - 1u) / b + 1u);
         }
         ${V.mainStart(d)}

          let outputIndex = global_idx / ${d};
          let offset = outputIndex * uniforms.reduceSize;

          var bestValue = f32(${Ua[n]});
          let Length = uniforms.reduceSize;
          for (var k = local_idx; k < Length; k = k + ${d}) {
           let candidate = f32(${k.getByOffset("offset + k")});
           bestValue = ${ni[n]};
          }
          aBestValues[local_idx] = bestValue;
          workgroupBarrier();

         var reduceSize = min(Length, ${d}u);
         for (var currentSize = reduceSize / 2u; reduceSize > 1u;
             currentSize = reduceSize / 2u) {
           let interval = DIV_CEIL(reduceSize, 2u);
           if (local_idx < currentSize) {
            let candidate = aBestValues[local_idx + interval];
            bestValue = ${ja[n]};
            aBestValues[local_idx] = bestValue;
           }
           reduceSize = interval;
           workgroupBarrier();
         }

         if (local_idx == 0u) {
          ${C.setByOffset("outputIndex", `${n === "mean" ? `${C.type.storage}(bestValue / f32(uniforms.reduceSize))` : `${C.type.storage}(${Va[n]})`}`)};
         }
        }`;
          return { name: e, shaderCache: { hint: `${t};${d}`, inputDependencies: ["type"] }, getShaderSource: B, getRunData: () => ({ outputs: [{ dims: a, dataType: i }], dispatchGroup: { x: p }, programUniforms: [{ type: 12, data: h }] }) };
        }, pr = (e, t, s, n) => {
          let i = e.inputs.length === 1 ? s : Ki(e.inputs, s), a = i.axes;
          a.length === 0 && !i.noopWithEmptyAxes && (a = e.inputs[0].dims.map((z, B) => B));
          let o = ze.normalizeAxes(a, e.inputs[0].dims.length), u = o, p = e.inputs[0], h = Wi(u, e.inputs[0].dims.length);
          h.length > 0 && (p = e.compute(cr(e.inputs[0], h), { inputs: [0], outputs: [-1] })[0], u = Wa(u.length, p.dims.length));
          let [k, C] = Ga(p.dims, u), d = k;
          i.keepDims && (d = Ka(k, o)), e.compute(qa(t, i.cacheKey, [p], n, e.inputs[0].dataType, d, C), { inputs: [p] });
        }, tn = (e, t) => {
          pr(e, "ReduceMeanShared", t, "mean");
        }, Qa = (e, t) => {
          pr(e, "ReduceL1Shared", t, "l1");
        }, Nc = (e, t) => {
          pr(e, "ReduceL2Shared", t, "l2");
        }, Xa = (e, t) => {
          pr(e, "ReduceLogSumExpShared", t, "logSumExp");
        }, jc = (e, t) => {
          pr(e, "ReduceMaxShared", t, "max");
        }, Ya = (e, t) => {
          pr(e, "ReduceMinShared", t, "min");
        }, Gi = (e, t) => {
          pr(e, "ReduceProdShared", t, "prod");
        }, Ja = (e, t) => {
          pr(e, "ReduceSumShared", t, "sum");
        }, Za = (e, t) => {
          pr(e, "ReduceSumSquareShared", t, "sumSquare");
        }, el = (e, t) => {
          pr(e, "ReduceLogSumShared", t, "logSum");
        };
      }), _r, ii, oi, Ki, gr, Hi, tl, sl, qi, rl, nl, Qi, il, ol, Xi, wr, al, Yi, ll, ul, Ji, dl, cl, Zi, pl, hl, ai = g(() => {
        zt(), Ft(), rs(), Yt(), Uc(), _r = (e) => {
          if (!e || e.length === 0 || e.length > 2) throw new Error("Reduce op requires 1 or 2 inputs.");
          if (e.length === 2 && e[1].dims.length !== 1) throw new Error("Invalid axes input dims.");
        }, ii = (e) => ["", "", `var value = ${e.getByIndices("input_indices")};`, ""], oi = (e, t, s, n, i, a, o = !1, u = !1) => {
          let p = [], h = s[0].dims, k = h.length, C = ze.normalizeAxes(i, k), d = !u && C.length === 0;
          h.forEach((V, Z) => {
            d || C.indexOf(Z) >= 0 ? o && p.push(1) : p.push(V);
          });
          let z = p.length, B = ze.size(p);
          return { name: e, shaderCache: t, getShaderSource: (V) => {
            let Z = [], ee = qe("_A", s[0].dataType, k), X = It("output", a, z), he = n(ee, X, C), pe = he[2];
            for (let Me = 0, Oe = 0; Me < k; Me++) d || C.indexOf(Me) >= 0 ? (o && Oe++, pe = `for(var j${Me}: u32 = 0; j${Me} < ${h[Me]}; j${Me}++) {
                  ${he[2].includes("last_index") ? `let last_index = j${Me};` : ""}
                  ${ee.indicesSet("input_indices", Me, `j${Me}`)}
                  ${pe}
                }`) : (Z.push(`${ee.indicesSet("input_indices", Me, X.indicesGet("output_indices", Oe))};`), Oe++);
            return `

        ${V.registerUniform("output_size", "u32").declareVariables(ee, X)}

        ${V.mainStart()}
          ${V.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          var input_indices: ${ee.type.indices};
          let output_indices = ${X.offsetToIndices("global_idx")};

          ${Z.join(`
`)}
          ${he[0]}       // init ops for reduce max/min
          ${he[1]}
          ${pe}
          ${he[3]}
          ${he.length === 4 ? X.setByOffset("global_idx", "value") : he.slice(4).join(`
`)}
        }`;
          }, getRunData: () => ({ outputs: [{ dims: p, dataType: a }], dispatchGroup: { x: Math.ceil(B / 64) }, programUniforms: [{ type: 12, data: B }, ...yt(h, p)] }) };
        }, Ki = (e, t) => {
          let s = [];
          return e[1].dims[0] > 0 && e[1].getBigInt64Array().forEach((n) => s.push(Number(n))), Bt({ axes: s, keepDims: t.keepDims, noopWithEmptyAxes: t.noopWithEmptyAxes });
        }, gr = (e, t, s, n) => {
          let i = e.inputs, a = i.length === 1 ? s : Ki(i, s);
          e.compute(oi(t, { hint: a.cacheKey, inputDependencies: ["rank"] }, [i[0]], a.noopWithEmptyAxes && a.axes.length === 0 ? ii : n, a.axes, i[0].dataType, a.keepDims, a.noopWithEmptyAxes), { inputs: [0] });
        }, Hi = (e, t) => {
          _r(e.inputs), gr(e, "ReduceLogSum", t, (s, n) => [`var value = ${n.type.storage}(0);`, "", `value += ${s.getByIndices("input_indices")};`, "value = log(value);"]);
        }, tl = (e, t) => {
          _r(e.inputs), gr(e, "ReduceL1", t, (s, n) => [`var value = ${n.type.storage}(0);`, "", `value += abs(${s.getByIndices("input_indices")});`, ""]);
        }, sl = (e, t) => {
          _r(e.inputs), gr(e, "ReduceL2", t, (s, n) => [`var t = ${n.type.value}(0); var value = ${n.type.value}(0);`, "", `t = ${s.getByIndices("input_indices")}; value += (t * t);`, "value = sqrt(value);"]);
        }, qi = (e, t) => {
          _r(e.inputs), gr(e, "ReduceLogSumExp", t, (s, n) => [`var value = ${n.type.storage}(0);`, "", `value += exp(${s.getByIndices("input_indices")});`, "value = log(value);"]);
        }, rl = (e, t) => {
          _r(e.inputs), gr(e, "ReduceMax", t, (s, n, i) => {
            let a = [];
            for (let o = 0; o < s.rank; o++) (i.indexOf(o) >= 0 || i.length === 0) && a.push(s.indicesSet("input_indices", o, 0));
            return [`${a.join(`
`)}`, `var value = ${s.getByIndices("input_indices")};`, `value = max(value, ${s.getByIndices("input_indices")});`, ""];
          });
        }, nl = (e, t) => {
          _r(e.inputs), gr(e, "ReduceMean", t, (s, n, i) => {
            let a = 1;
            for (let o = 0; o < s.rank; o++) (i.indexOf(o) >= 0 || i.length === 0) && (a *= e.inputs[0].dims[o]);
            return ["var sum = f32(0);", "", `sum += f32(${s.getByIndices("input_indices")});`, `let value = ${n.type.value}(sum / ${a});`];
          });
        }, Qi = (e, t) => {
          _r(e.inputs), gr(e, "ReduceMin", t, (s, n, i) => {
            let a = [];
            for (let o = 0; o < s.rank; o++) (i.indexOf(o) >= 0 || i.length === 0) && a.push(`input_indices[${o}] = 0;`);
            return [`${a.join(`
`)}`, `var value = ${s.getByIndices("input_indices")};`, `value = min(value, ${s.getByIndices("input_indices")});`, ""];
          });
        }, il = (e, t) => {
          _r(e.inputs), gr(e, "ReduceProd", t, (s, n) => [`var value = ${n.type.storage}(1);`, "", `value *= ${s.getByIndices("input_indices")};`, ""]);
        }, ol = (e, t) => {
          _r(e.inputs), gr(e, "ReduceSum", t, (s, n) => [`var value = ${n.type.storage}(0);`, "", `value += ${s.getByIndices("input_indices")};`, ""]);
        }, Xi = (e, t) => {
          _r(e.inputs), gr(e, "ReduceSumSquare", t, (s, n) => [`var t = ${n.type.value}(0); var value = ${n.type.value}(0);`, "", `t = ${s.getByIndices("input_indices")}; value += t * t;`, ""]);
        }, wr = (e, t, s) => {
          if (t.length === 0) return s;
          let n = 1, i = 1;
          for (let a = 0; a < t.length; a++) t.indexOf(a) === -1 ? n *= e[a] : i *= e[a];
          return i < 32 && n > 1024;
        }, al = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? nl(e, t) : tn(e, t);
        }, Yi = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? tl(e, t) : Qa(e, t);
        }, ll = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? sl(e, t) : Nc(e, t);
        }, ul = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? qi(e, t) : Xa(e, t);
        }, Ji = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? rl(e, t) : jc(e, t);
        }, dl = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Qi(e, t) : Ya(e, t);
        }, cl = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? il(e, t) : Gi(e, t);
        }, Zi = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? ol(e, t) : Ja(e, t);
        }, pl = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Xi(e, t) : Za(e, t);
        }, hl = (e, t) => {
          wr(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Hi(e, t) : el(e, t);
        };
      }), eo, ml, to, so, Vc = g(() => {
        zt(), rs(), ai(), eo = (e) => {
          if (!e || e.length === 0 || e.length > 2) throw new Error("ArgMinMaxOp op requires 1 or 2 inputs.");
          if (e[0].dataType !== 1) throw new Error("Invalid input type.");
        }, ml = (e, t) => {
          eo(e.inputs);
          let s = (n, i, a) => {
            let o = [];
            for (let u = 0; u < n.rank; u++) (a.indexOf(u) >= 0 || a.length === 0) && o.push(`input_indices[${u}] = 0;`);
            return [`${o.join(`
`)}`, `var value = ${n.getByIndices("input_indices")};
var best_index : i32 = 0;`, `if (${n.getByIndices("input_indices")} ${t.selectLastIndex > 0 ? "<=" : "<"} value) {
         value = ${n.getByIndices("input_indices")};
         best_index = i32(last_index);
       }`, "", i.setByOffset("global_idx", "best_index")];
          };
          e.compute(oi("ArgMin", { hint: t.cacheKey, inputDependencies: ["rank"] }, [e.inputs[0]], s, [t.axis], 7, t.keepDims), { inputs: [0] });
        }, to = (e, t) => {
          eo(e.inputs);
          let s = (n, i, a) => {
            let o = [];
            for (let u = 0; u < n.rank; u++) (a.indexOf(u) >= 0 || a.length === 0) && o.push(`input_indices[${u}] = 0;`);
            return [`${o.join(`
`)}`, `var value = ${n.getByIndices("input_indices")};
var best_index : i32 = 0;`, `if (${n.getByIndices("input_indices")} ${t.selectLastIndex > 0 ? ">=" : ">"} value) {
         value = ${n.getByIndices("input_indices")};
         best_index = i32(last_index);
       }`, "", i.setByOffset("global_idx", "best_index")];
          };
          e.compute(oi("argMax", { hint: t.cacheKey, inputDependencies: ["rank"] }, [e.inputs[0]], s, [t.axis], 7, t.keepDims), { inputs: [0] });
        }, so = (e) => Bt(e);
      }), ro, li, fl, no, _l, Bn, io, gl, oo = g(() => {
        zt(), Ft(), ue(), Yt(), ro = (e, t) => {
          let s = e[0], n = e[1], i = e[2], a = e[3], o = e[4], u = e[5];
          if (o && u) throw new Error("Attention cannot have both past and attention_bias");
          if (s.dims.length !== 3) throw new Error('Input "input" must have 3 dimensions');
          let p = s.dims[0], h = s.dims[1], k = s.dims[2];
          if (i.dims.length !== 1) throw new Error('Input "bias" is expected to have 1 dimensions');
          if (n.dims.length !== 2) throw new Error('Input "weights" is expected to have 2 dimensions');
          if (n.dims[0] !== k) throw new Error("Input 1 dimension 0 should have same length as dimension 2 of input 0");
          if (i.dims[0] !== n.dims[1]) throw new Error('Input "bias" dimension 0 should have same length as dimension 1 of input "weights"');
          let C = i.dims[0] / 3, d = C, z = d;
          if (t.qkvHiddenSizes.length > 0) {
            if (t.qkvHiddenSizes.length !== 3) throw new Error("qkv_hidden_sizes attribute should have 3 elements");
            for (let he of t.qkvHiddenSizes) if (he % t.numHeads !== 0) throw new Error("qkv_hidden_sizes should be divisible by num_heads");
            C = t.qkvHiddenSizes[0], d = t.qkvHiddenSizes[1], z = t.qkvHiddenSizes[2];
          }
          let B = h;
          if (C !== d) throw new Error("qkv_hidden_sizes first element should be same as the second");
          if (i.dims[0] !== C + d + z) throw new Error('Input "bias" dimension 0 should have same length as sum of Q/K/V hidden sizes');
          let V = 0;
          if (o) {
            if (d !== z) throw new Error('Input "past" expect k_hidden_size == v_hidden_size');
            if (o.dims.length !== 5) throw new Error('Input "past" must have 5 dimensions');
            if (o.dims[0] !== 2) throw new Error('Input "past" first dimension must be 2');
            if (o.dims[1] !== p) throw new Error('Input "past" second dimension must be batch_size');
            if (o.dims[2] !== t.numHeads) throw new Error('Input "past" third dimension must be num_heads');
            if (o.dims[4] !== d / t.numHeads) throw new Error('Input "past" fifth dimension must be k_hidden_size / num_heads');
            t.pastPresentShareBuffer || (V = o.dims[3]);
          }
          let Z = B + V, ee = -1, X = 0;
          if (a) throw new Error("Mask not supported");
          if (o) throw new Error("past is not supported");
          if (u) {
            if (u.dims.length !== 4) throw new Error('Input "attention_bias" must have 4 dimensions');
            if (u.dims[0] !== p || u.dims[1] !== t.numHeads || u.dims[2] !== h || u.dims[3] !== Z) throw new Error('Expect "attention_bias" shape (batch_size, num_heads, sequence_length, total_sequence_length)');
          }
          return { batchSize: p, sequenceLength: h, pastSequenceLength: V, kvSequenceLength: B, totalSequenceLength: Z, maxSequenceLength: ee, inputHiddenSize: k, hiddenSize: C, vHiddenSize: z, headSize: Math.floor(C / t.numHeads), vHeadSize: Math.floor(z / t.numHeads), numHeads: t.numHeads, isUnidirectional: !1, pastPresentShareBuffer: !1, maskFilterValue: t.maskFilterValue, maskType: X, scale: t.scale, broadcastResPosBias: !1, passPastInKv: !1, qkvFormat: 1 };
        }, li = (e, t, s) => t && e ? `
      let total_sequence_length_input = u32(${t.getByOffset("0")});
      let present_sequence_length = max(total_sequence_length_input, uniforms.past_sequence_length);
      let is_subsequent_prompt: bool = sequence_length > 1 && sequence_length != total_sequence_length_input;
      let is_first_prompt: bool = is_subsequent_prompt == false && sequence_length == total_sequence_length_input;
      total_sequence_length = u32(${e == null ? void 0 : e.getByOffset("batchIdx")}) + 1;
      var past_sequence_length: u32 = 0;
      if (is_first_prompt == false) {
        past_sequence_length = total_sequence_length - sequence_length;
      }
       ` : `
    ${s ? "let past_sequence_length = uniforms.past_sequence_length" : ""};
    let present_sequence_length = total_sequence_length;
    `, fl = (e, t, s, n, i, a, o, u) => {
          let p = qt(o ? 1 : a), h = 64, k = a / p;
          k < h && (h = 32);
          let C = Math.ceil(a / p / h), d = [{ type: 12, data: t }, { type: 12, data: s }, { type: 12, data: n }, { type: 12, data: i }, { type: 12, data: k }, { type: 12, data: C }], z = fs(e.dataType, p), B = Ss(1, p), V = ["type"];
          o && V.push("type"), u && V.push("type");
          let Z = (ee) => {
            let X = It("x", e.dataType, e.dims, p), he = [X], pe = o ? qe("seq_lens", o.dataType, o.dims) : void 0;
            pe && he.push(pe);
            let Me = u ? qe("total_sequence_length_input", u.dataType, u.dims) : void 0;
            Me && he.push(Me);
            let Oe = Ss(e.dataType), Le = [{ name: "batch_size", type: "u32" }, { name: "num_heads", type: "u32" }, { name: "past_sequence_length", type: "u32" }, { name: "sequence_length", type: "u32" }, { name: "total_sequence_length", type: "u32" }, { name: "elements_per_thread", type: "u32" }];
            return `
  var<workgroup> thread_max: array<f32, ${h}>;
  var<workgroup> thread_sum: array<f32, ${h}>;
  ${ee.registerUniforms(Le).declareVariables(...he)}
  ${ee.mainStart([h, 1, 1])}
    let batchIdx = workgroup_id.z / uniforms.num_heads;
    let headIdx = workgroup_id.z % uniforms.num_heads;
    let sequence_length = uniforms.sequence_length;
    var total_sequence_length = uniforms.total_sequence_length;
    ${li(pe, Me, !1)}
    let local_offset = local_idx * uniforms.elements_per_thread;
    let offset = (global_idx / ${h}) * uniforms.total_sequence_length + local_offset;
    let seq_causal_length = ${o ? "u32(past_sequence_length + workgroup_id.y + 1)" : "total_sequence_length"};
    var thread_max_vector = ${B}(-3.402823e+38f);
    for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
      thread_max_vector = max(${B}(x[offset + i]), thread_max_vector);
    }
    thread_max[local_idx] = ${(() => {
              switch (p) {
                case 1:
                  return "thread_max_vector";
                case 2:
                  return "max(thread_max_vector.x, thread_max_vector.y)";
                case 4:
                  return "max(max(thread_max_vector.x, thread_max_vector.y), max(thread_max_vector.z, thread_max_vector.w))";
                default:
                  throw new Error(`Unsupported components: ${p}`);
              }
            })()};
    workgroupBarrier();

    var max_value =  f32(-3.402823e+38f);
    for (var i = 0u; i < ${h}; i++) {
      max_value = max(thread_max[i], max_value);
    }

    var sum_vector = ${B}(0);
    for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
      sum_vector += exp(${B}(x[offset + i]) - max_value);
    }
    thread_sum[local_idx] = ${(() => {
              switch (p) {
                case 1:
                  return "sum_vector";
                case 2:
                  return "sum_vector.x + sum_vector.y";
                case 4:
                  return "sum_vector.x + sum_vector.y + sum_vector.z + sum_vector.w";
                default:
                  throw new Error(`Unsupported components: ${p}`);
              }
            })()};
    workgroupBarrier();

    var sum: f32 = 0;
    for (var i = 0u; i < ${h}; i++) {
      sum += thread_sum[i];
    }

    if (sum == 0) {
      for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
        x[offset + i] = ${X.type.value}(${Oe}(1.0) / ${Oe}(seq_causal_length));
      }
    } else {
      for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
        var f32input = ${B}(x[offset + i]);
        x[offset + i] = ${X.type.value}(exp(f32input - max_value) / sum);
      }
    }
      ${o ? `
        for (var total_seq_id: u32 = seq_causal_length; total_seq_id + local_offset < uniforms.total_sequence_length; total_seq_id++) {
          x[offset + total_seq_id] = ${X.type.value}(${Oe}(0));
        }` : ""};
  }`;
          };
          return { name: "AttentionProbsSoftmax", shaderCache: { hint: `${h};${z};${p}`, inputDependencies: V }, getShaderSource: Z, getRunData: () => ({ outputs: [], dispatchGroup: { x: Math.ceil(a / h), y: i, z: t * s }, programUniforms: d }) };
        }, no = (e, t, s, n, i, a, o, u, p) => {
          let h = o + a.kvSequenceLength, k = [a.batchSize, a.numHeads, a.sequenceLength, h], C = e > 1 && n, d = a.kvNumHeads ? a.kvNumHeads : a.numHeads, z = C ? [a.batchSize, d, h, a.headSize] : void 0, B = a.nReps ? a.nReps : 1, V = a.scale === 0 ? 1 / Math.sqrt(a.headSize) : a.scale, Z = qt(a.headSize), ee = a.headSize / Z, X = 12, he = { x: Math.ceil(h / X), y: Math.ceil(a.sequenceLength / X), z: a.batchSize * a.numHeads }, pe = [{ type: 12, data: a.sequenceLength }, { type: 12, data: ee }, { type: 12, data: h }, { type: 12, data: a.numHeads }, { type: 12, data: a.headSize }, { type: 1, data: V }, { type: 12, data: o }, { type: 12, data: a.kvSequenceLength }, { type: 12, data: B }], Me = C && n && ze.size(n.dims) > 0, Oe = ["type", "type"];
          Me && Oe.push("type"), i && Oe.push("type"), u && Oe.push("type"), p && Oe.push("type");
          let Le = [{ dims: k, dataType: t.dataType, gpuDataType: 0 }];
          C && Le.push({ dims: z, dataType: t.dataType, gpuDataType: 0 });
          let Ye = (at) => {
            let Pt = qe("q", t.dataType, t.dims, Z), Xt = qe("key", s.dataType, s.dims, Z), Zt = [Pt, Xt];
            if (Me) {
              let Rt = qe("past_key", n.dataType, n.dims, Z);
              Zt.push(Rt);
            }
            i && Zt.push(qe("attention_bias", i.dataType, i.dims));
            let bt = u ? qe("seq_lens", u.dataType, u.dims) : void 0;
            bt && Zt.push(bt);
            let ss = p ? qe("total_sequence_length_input", p.dataType, p.dims) : void 0;
            ss && Zt.push(ss);
            let St = It("output", t.dataType, k), Ot = [St];
            C && Ot.push(It("present_key", t.dataType, z, Z));
            let bs = Ss(1, Z), Ht = [{ name: "M", type: "u32" }, { name: "K", type: "u32" }, { name: "N", type: "u32" }, { name: "num_heads", type: "u32" }, { name: "head_size", type: "u32" }, { name: "alpha", type: "f32" }, { name: "past_sequence_length", type: "u32" }, { name: "kv_sequence_length", type: "u32" }, { name: "n_reps", type: "u32" }];
            return `
  const TILE_SIZE = ${X}u;

  var<workgroup> tileQ: array<${Pt.type.storage}, ${X * X}>;
  var<workgroup> tileK: array<${Pt.type.storage}, ${X * X}>;
  ${at.registerUniforms(Ht).declareVariables(...Zt, ...Ot)}
  ${at.mainStart([X, X, 1])}
    // x holds the N and y holds the M
    let headIdx = workgroup_id.z % uniforms.num_heads;
    let kvHeadIdx = ${B === 1 ? "headIdx" : "headIdx / uniforms.n_reps"};
    let kv_num_heads = ${B === 1 ? "uniforms.num_heads" : "uniforms.num_heads / uniforms.n_reps"};
    let batchIdx = workgroup_id.z / uniforms.num_heads;
    let m = workgroup_id.y * TILE_SIZE;
    let n = workgroup_id.x * TILE_SIZE;
    let sequence_length = uniforms.M;
    var total_sequence_length = uniforms.N;
    ${li(bt, ss, !0)}
    let absKvHeadIdx = batchIdx * kv_num_heads + kvHeadIdx;
    let qOffset = workgroup_id.z * uniforms.M * uniforms.K + m * uniforms.K;
    ${Me && C ? "let pastKeyOffset = absKvHeadIdx * uniforms.past_sequence_length * uniforms.K;" : ""};
    let kOffset = absKvHeadIdx * uniforms.kv_sequence_length * uniforms.K;
    ${C ? "let presentKeyOffset = absKvHeadIdx * uniforms.N * uniforms.K;" : ""}
    var value = ${bs}(0);
    for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (global_id.y < uniforms.M && w + local_id.x < uniforms.K) {
        tileQ[TILE_SIZE * local_id.y + local_id.x] = q[qOffset + local_id.y * uniforms.K + w + local_id.x];
      }
      if (n + local_id.y < uniforms.N && w + local_id.x < uniforms.K) {
        var idx = TILE_SIZE * local_id.y + local_id.x;
      ${Me && C ? `
              if (n + local_id.y < past_sequence_length) {
                tileK[idx] = past_key[pastKeyOffset + (n + local_id.y) * uniforms.K + w + local_id.x];
              } else if (n + local_id.y - past_sequence_length < uniforms.kv_sequence_length) {
                tileK[idx] = key[kOffset + (n + local_id.y - past_sequence_length) * uniforms.K + w + local_id.x];
              }` : `
          if (n + local_id.y < uniforms.kv_sequence_length) {
            tileK[idx] = key[kOffset + (n + local_id.y) * uniforms.K + w + local_id.x];
          }`}
      ${C ? `if (n + local_id.y < present_sequence_length) {
        present_key[presentKeyOffset + (n + local_id.y) * uniforms.K + w + local_id.x] = tileK[idx];
      }` : ""}
      }
      workgroupBarrier();

      for (var k: u32 = 0u; k < TILE_SIZE && w+k < uniforms.K; k++) {
          value += ${bs}(tileQ[TILE_SIZE * local_id.y + k] * tileK[TILE_SIZE * local_id.x + k]);
      }

      workgroupBarrier();
    }

    if (global_id.y < uniforms.M && global_id.x < total_sequence_length) {
      let headOffset = workgroup_id.z * uniforms.M * uniforms.N;
      let outputIdx = headOffset + global_id.y * uniforms.N + global_id.x;
      var sum: f32 = ${(() => {
              switch (Z) {
                case 1:
                  return "value";
                case 2:
                  return "value.x + value.y";
                case 4:
                  return "value.x + value.y + value.z + value.w";
                default:
                  throw new Error(`Unsupported components: ${Z}`);
              }
            })()};
        output[outputIdx] = ${St.type.value} (sum * uniforms.alpha) + ${i ? "attention_bias[outputIdx]" : "0.0"};
    }
  }`;
          };
          return { name: "AttentionProbs", shaderCache: { hint: `${Z};${i !== void 0};${n !== void 0};${e}`, inputDependencies: Oe }, getRunData: () => ({ outputs: Le, dispatchGroup: he, programUniforms: pe }), getShaderSource: Ye };
        }, _l = (e, t, s, n, i, a, o = void 0, u = void 0) => {
          let p = a + i.kvSequenceLength, h = i.nReps ? i.nReps : 1, k = i.vHiddenSize * h, C = e > 1 && n, d = i.kvNumHeads ? i.kvNumHeads : i.numHeads, z = C ? [i.batchSize, d, p, i.headSize] : void 0, B = [i.batchSize, i.sequenceLength, k], V = 12, Z = { x: Math.ceil(i.vHeadSize / V), y: Math.ceil(i.sequenceLength / V), z: i.batchSize * i.numHeads }, ee = [{ type: 12, data: i.sequenceLength }, { type: 12, data: p }, { type: 12, data: i.vHeadSize }, { type: 12, data: i.numHeads }, { type: 12, data: i.headSize }, { type: 12, data: k }, { type: 12, data: a }, { type: 12, data: i.kvSequenceLength }, { type: 12, data: h }], X = C && n && ze.size(n.dims) > 0, he = ["type", "type"];
          X && he.push("type"), o && he.push("type"), u && he.push("type");
          let pe = [{ dims: B, dataType: t.dataType, gpuDataType: 0 }];
          C && pe.push({ dims: z, dataType: t.dataType, gpuDataType: 0 });
          let Me = (Oe) => {
            let Le = qe("probs", t.dataType, t.dims), Ye = qe("v", s.dataType, s.dims), at = [Le, Ye];
            X && at.push(qe("past_value", n.dataType, n.dims));
            let Pt = o ? qe("seq_lens", o.dataType, o.dims) : void 0;
            o && at.push(Pt);
            let Xt = u ? qe("total_sequence_length_input", u.dataType, u.dims) : void 0;
            u && at.push(Xt);
            let Zt = [It("output", t.dataType, B)];
            C && Zt.push(It("present_value", t.dataType, z));
            let bt = [{ name: "M", type: "u32" }, { name: "K", type: "u32" }, { name: "N", type: "u32" }, { name: "num_heads", type: "u32" }, { name: "head_size", type: "u32" }, { name: "v_hidden_size", type: "u32" }, { name: "past_sequence_length", type: "u32" }, { name: "kv_sequence_length", type: "u32" }, { name: "n_reps", type: "u32" }];
            return `
  const TILE_SIZE = ${V}u;
  var<workgroup> tileQ: array<${Le.type.value}, ${V * V}>;
  var<workgroup> tileV: array<${Le.type.value}, ${V * V}>;
  ${Oe.registerUniforms(bt).declareVariables(...at, ...Zt)}
  ${Oe.mainStart([V, V, 1])}
   let headIdx = workgroup_id.z % uniforms.num_heads;
   let batchIdx = workgroup_id.z / uniforms.num_heads;
   let kvHeadIdx = ${h === 1 ? "headIdx" : "headIdx / uniforms.n_reps"};
   let kv_num_heads = ${h === 1 ? "uniforms.num_heads" : "uniforms.num_heads / uniforms.n_reps"};
   let m = global_id.y;
   let n = global_id.x;
   let sequence_length = uniforms.M;
   var total_sequence_length = uniforms.K;
   ${li(Pt, Xt, !0)}
   let offsetA = workgroup_id.z * uniforms.M * uniforms.K + m * uniforms.K;
   let absKvHeadIdx = batchIdx * kv_num_heads + kvHeadIdx; // kvHeadIdx is relative to the batch
   ${X && C ? "let pastValueOffset = absKvHeadIdx * uniforms.N * uniforms.past_sequence_length + n;" : ""};
   let vOffset = absKvHeadIdx * uniforms.N * uniforms.kv_sequence_length + n;
   ${C ? "let presentValueOffset = absKvHeadIdx * uniforms.N * uniforms.K + n;" : ""}
   var value = ${Le.type.storage}(0);
   for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (m < uniforms.M && w + local_id.x < uniforms.K) {
        tileQ[TILE_SIZE * local_id.y + local_id.x] = probs[offsetA + w + local_id.x];
      }
      if (n < uniforms.N && w + local_id.y < uniforms.K) {
        var idx = TILE_SIZE * local_id.y + local_id.x;
        ${X && C ? `
        if (w + local_id.y < past_sequence_length) {
          tileV[idx] = past_value[pastValueOffset + (w + local_id.y) * uniforms.N];
        } else if (w + local_id.y - past_sequence_length < uniforms.kv_sequence_length) {
          tileV[idx] = v[vOffset + (w + local_id.y - past_sequence_length) * uniforms.N];
        }
      ` : `
            if (w + local_id.y < uniforms.kv_sequence_length) {
              tileV[idx] = v[vOffset + (w + local_id.y) * uniforms.N];
            }`}
        ${C ? `
            if (w + local_id.y < present_sequence_length) {
          present_value[presentValueOffset + (w + local_id.y) * uniforms.N] = tileV[idx];
        }` : ""}
      }
     workgroupBarrier();
     for (var k: u32 = 0u; k < TILE_SIZE && w+k < total_sequence_length; k++) {
       value += tileQ[TILE_SIZE * local_id.y + k] * tileV[TILE_SIZE * k + local_id.x];
     }
     workgroupBarrier();
   }

   // we need to transpose output from BNSH_v to BSND_v
   if (m < uniforms.M && n < uniforms.N) {
     let outputIdx = batchIdx * uniforms.M * uniforms.v_hidden_size + m * uniforms.v_hidden_size
       + headIdx * uniforms.N + n;
     output[outputIdx] = value;
   }
  }`;
          };
          return { name: "AttentionScore", shaderCache: { hint: `${n !== void 0};${e}`, inputDependencies: he }, getRunData: () => ({ outputs: pe, dispatchGroup: Z, programUniforms: ee }), getShaderSource: Me };
        }, Bn = (e, t, s, n, i, a, o, u, p, h, k = void 0, C = void 0) => {
          let d = Math.min(e.outputCount, 1 + (o ? 1 : 0) + (u ? 1 : 0)), z = d > 1 ? h.pastSequenceLength : 0, B = z + h.kvSequenceLength, V = p && ze.size(p.dims) > 0 ? p : void 0, Z = [t, s];
          d > 1 && o && ze.size(o.dims) > 0 && Z.push(o), V && Z.push(V), k && Z.push(k), C && Z.push(C);
          let ee = e.compute(no(d, t, s, o, V, h, z, k, C), { inputs: Z, outputs: d > 1 ? [-1, 1] : [-1] })[0];
          e.compute(fl(ee, h.batchSize, h.numHeads, z, h.sequenceLength, B, k, C), { inputs: k && C ? [ee, k, C] : [ee], outputs: [] });
          let X = [ee, n];
          d > 1 && u && ze.size(u.dims) > 0 && X.push(u), k && X.push(k), C && X.push(C), e.compute(_l(d, ee, n, u, h, z, k, C), { inputs: X, outputs: d > 1 ? [0, 2] : [0] });
        }, io = (e, t) => {
          let s = [t.batchSize, t.numHeads, t.sequenceLength, t.headSize], n = t.sequenceLength, i = t.inputHiddenSize, a = t.headSize, o = 12, u = { x: Math.ceil(t.headSize / o), y: Math.ceil(t.sequenceLength / o), z: t.batchSize * t.numHeads }, p = [e.inputs[0], e.inputs[1], e.inputs[2]], h = [{ type: 12, data: n }, { type: 12, data: i }, { type: 12, data: a }, { type: 12, data: t.numHeads }, { type: 12, data: t.headSize }, { type: 12, data: t.hiddenSize }, { type: 12, data: t.hiddenSize + t.hiddenSize + t.vHiddenSize }], k = (C) => {
            let d = It("output_q", p[0].dataType, s), z = It("output_k", p[0].dataType, s), B = It("output_v", p[0].dataType, s), V = qe("input", p[0].dataType, p[0].dims), Z = qe("weight", p[1].dataType, p[1].dims), ee = qe("bias", p[2].dataType, p[2].dims), X = V.type.storage, he = [{ name: "M", type: "u32" }, { name: "K", type: "u32" }, { name: "N", type: "u32" }, { name: "num_heads", type: "u32" }, { name: "head_size", type: "u32" }, { name: "hidden_size", type: "u32" }, { name: "ldb", type: "u32" }];
            return `
  const TILE_SIZE = ${o}u;
  var<workgroup> tileInput: array<${X}, ${o * o}>;
  var<workgroup> tileWeightQ: array<${X}, ${o * o}>;
  var<workgroup> tileWeightK: array<${X}, ${o * o}>;
  var<workgroup> tileWeightV: array<${X}, ${o * o}>;
  ${C.registerUniforms(he).declareVariables(V, Z, ee, d, z, B)}
  ${C.mainStart([o, o, 1])}
    let batchIndex = workgroup_id.z / uniforms.num_heads;
    let headNumber = workgroup_id.z % uniforms.num_heads;
    let m = global_id.y;
    let n = global_id.x;

    let inputOffset = batchIndex * (uniforms.M * uniforms.K) + m * uniforms.K;
    let biasOffsetQ = headNumber * uniforms.head_size;
    let biasOffsetK = uniforms.hidden_size + biasOffsetQ;
    let biasOffsetV = uniforms.hidden_size + biasOffsetK;

    var valueQ = ${X}(0);
    var valueK = ${X}(0);
    var valueV = ${X}(0);
    for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (m < uniforms.M && w + local_id.x < uniforms.K) {
        tileInput[TILE_SIZE * local_id.y + local_id.x] = input[inputOffset + w + local_id.x];
      }
      if (n < uniforms.N && w + local_id.y < uniforms.K) {
        let offset = n + (w + local_id.y) * uniforms.ldb;
        tileWeightQ[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetQ + offset];
        tileWeightK[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetK + offset];
        tileWeightV[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetV + offset];
      }
      workgroupBarrier();
      for (var k: u32 = 0u; k<TILE_SIZE && w+k < uniforms.K; k++) {
        let inputTileOffset = TILE_SIZE * local_id.y + k;
        let weightTileOffset = TILE_SIZE * k + local_id.x;
        valueQ += tileInput[inputTileOffset] * tileWeightQ[weightTileOffset];
        valueK += tileInput[inputTileOffset] * tileWeightK[weightTileOffset];
        valueV += tileInput[inputTileOffset] * tileWeightV[weightTileOffset];
      }

      workgroupBarrier();
    }

    let headOffset = (m * uniforms.N + n) % uniforms.head_size;
    valueQ += bias[headOffset + biasOffsetQ];
    valueK += bias[headOffset + biasOffsetK];
    valueV += bias[headOffset + biasOffsetV];

    let offset = workgroup_id.z * uniforms.M * uniforms.N;
    if (m < uniforms.M && n < uniforms.N) {
      let outputIdx = offset + m * uniforms.N + n;
      output_q[outputIdx] = valueQ;
      output_k[outputIdx] = valueK;
      output_v[outputIdx] = valueV;
    }
  }`;
          };
          return e.compute({ name: "AttentionPrepare", shaderCache: { inputDependencies: ["type", "type", "type"] }, getRunData: () => ({ outputs: [{ dims: s, dataType: e.inputs[0].dataType, gpuDataType: 0 }, { dims: s, dataType: e.inputs[0].dataType, gpuDataType: 0 }, { dims: s, dataType: e.inputs[0].dataType, gpuDataType: 0 }], dispatchGroup: u, programUniforms: h }), getShaderSource: k }, { inputs: p, outputs: [-1, -1, -1] });
        }, gl = (e, t) => {
          let s = ro(e.inputs, t), [n, i, a] = io(e, s);
          return Bn(e, n, i, a, e.inputs[4], void 0, void 0, void 0, e.inputs[5], s);
        };
      }), ao, wl, yl, lo, Wc = g(() => {
        We(), zt(), Ft(), rs(), Yt(), ao = (e, t) => {
          if (!e || e.length !== 5) throw new Error("BatchNormalization requires 5 inputs");
          let s = (n, i, a) => {
            let o = i.length;
            if (o !== n.length) throw new Error(`${a}: num dimensions != ${o}`);
            i.forEach((u, p) => {
              if (u !== n[p]) throw new Error(`${a}: dim[${p}] do not match`);
            });
          };
          if (e[0].dims.length > 1) {
            let n = t.format === "NHWC" ? t.spatial ? e[0].dims.slice(-1) : e[0].dims.slice(-1).concat(e[0].dims.slice(1, e[0].dims.length - 1)) : e[0].dims.slice(1, t.spatial ? 2 : void 0);
            s(e[1].dims, n, "Invalid input scale"), s(e[2].dims, n, "Invalid input B"), s(e[3].dims, n, "Invalid input mean"), s(e[4].dims, n, "Invalid input var");
          } else s(e[1].dims, [1], "Invalid input scale"), s(e[2].dims, [1], "Invalid input B"), s(e[3].dims, [1], "Invalid input mean"), s(e[4].dims, [1], "Invalid input var");
        }, wl = (e, t) => {
          let { epsilon: s, spatial: n, format: i } = t, a = e[0].dims, o = n ? qt(a[a.length - 1]) : 1, u = i === "NHWC" && a.length > 1 ? o : 1, p = ze.size(a) / o, h = n, k = h ? a.length : a, C = qe("x", e[0].dataType, e[0].dims, o), d = qe("scale", e[1].dataType, e[1].dims, u), z = qe("bias", e[2].dataType, e[2].dims, u), B = qe("inputMean", e[3].dataType, e[3].dims, u), V = qe("inputVar", e[4].dataType, e[4].dims, u), Z = It("y", e[0].dataType, k, o), ee = () => {
            let he = "";
            if (n) he = `let cOffset = ${a.length === 1 ? "0u" : i === "NHWC" ? `outputIndices[${a.length - 1}] / ${o}` : "outputIndices[1]"};`;
            else if (i === "NCHW") he = `
            ${Z.indicesSet("outputIndices", "0", "0")}
            let cOffset = ${Z.indicesToOffset("outputIndices")};`;
            else {
              he = `var cIndices = ${d.type.indices}(0);
                       cIndices[0] = outputIndices[${a.length - 1}];`;
              for (let pe = 1; pe < d.rank; pe++) he += `cIndices[${pe}] = outputIndices[${pe}];`;
              he += `let cOffset = ${d.indicesToOffset("cIndices")};`;
            }
            return he;
          }, X = (he) => `
  const epsilon = ${s};
  ${he.registerUniform("outputSize", "u32").declareVariables(C, d, z, B, V, Z)}
  ${he.mainStart()}
  ${he.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
    var outputIndices = ${Z.offsetToIndices(`global_idx * ${o}`)};
    ${ee()}
    let scale = ${d.getByOffset("cOffset")};
    let bias = ${z.getByOffset("cOffset")};
    let inputMean = ${B.getByOffset("cOffset")};
    let inputVar = ${V.getByOffset("cOffset")};
    let x = ${C.getByOffset("global_idx")};
    let value = (x - inputMean) * inverseSqrt(inputVar + epsilon) * scale + bias;
    ${Z.setByOffset("global_idx", "value")}
  }`;
          return { name: "BatchNormalization", shaderCache: { hint: `${t.epsilon}_${t.format}_${n}_${o}`, inputDependencies: h ? ["rank", "type", "type", "type", "type"] : void 0 }, getShaderSource: X, getRunData: () => ({ outputs: [{ dims: e[0].dims, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(p / 64) }, programUniforms: h ? [{ type: 12, data: p }, ...yt(a)] : [{ type: 12, data: p }] }) };
        }, yl = (e) => Bt(e), lo = (e, t) => {
          let { inputs: s, outputCount: n } = e, i = yl({ ...t, outputCount: n });
          if (F.webgpu.validateInputContent && ao(s, i), t.trainingMode) throw new Error("BatchNormalization trainingMode is not supported yet.");
          e.compute(wl(s, i));
        };
      }), Ml, uo, bl, Gc = g(() => {
        Ft(), Yt(), Ml = (e) => {
          if (e[0].dims.length !== 3) throw new Error("input should have 3 dimensions");
          if (![320, 640, 1280].includes(e[0].dims[2])) throw new Error("number of channels should be 320, 640 or 1280");
          if (e[1].dims.length !== 1) throw new Error("bias is expected to have 1 dimensions");
          if (e[0].dims[2] !== e[1].dims[0]) throw new Error("last dimension of input and bias are not the same");
        }, uo = (e) => {
          let t = e[0].dims, s = e[0].dims[2], n = ze.size(t) / 4, i = e[0].dataType, a = qe("input", i, t, 4), o = qe("bias", i, [s], 4), u = qe("residual", i, t, 4), p = It("output", i, t, 4);
          return { name: "BiasAdd", getRunData: () => ({ outputs: [{ dims: t, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(n / 64) } }), getShaderSource: (h) => `
  const channels = ${s}u / 4;
  ${h.declareVariables(a, o, u, p)}

  ${h.mainStart()}
    ${h.guardAgainstOutOfBoundsWorkgroupSizes(n)}
    let value = ${a.getByOffset("global_idx")}
      + ${o.getByOffset("global_idx % channels")} + ${u.getByOffset("global_idx")};
    ${p.setByOffset("global_idx", "value")}
  }` };
        }, bl = (e) => {
          Ml(e.inputs), e.compute(uo(e.inputs));
        };
      }), co, ds, vl, po, xl, Tl, ho, Pl, El, mo, Cl, kl, fo, Sl, $l, _o, Rn, Al, ui, Il, go, Fl, Ol, wo, Dl, Ll, yo, zl, Bl, Mo, Rl, Nl, bo, jl, Ul, di, Vl, vo, ci, Wl, Gl, Kl, Hl, xo, ql, To = g(() => {
        zt(), Ft(), rs(), Yt(), co = (e, t, s, n, i, a, o) => {
          let u = Math.ceil(t / 4), p = "";
          typeof i == "string" ? p = `${i}(a)` : p = i("a");
          let h = qe("inputData", s, [u], 4), k = It("outputData", n, [u], 4), C = [{ name: "vec_size", type: "u32" }];
          return o && C.push(...o), `
      ${e.registerUniforms(C).declareVariables(h, k)}

  ${a ?? ""}

  ${e.mainStart()}
    ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}

    let a = ${h.getByOffset("global_idx")};
    ${k.setByOffset("global_idx", p)}
  }`;
        }, ds = (e, t, s, n, i, a = e.dataType, o, u) => {
          let p = [{ type: 12, data: Math.ceil(ze.size(e.dims) / 4) }];
          return o && p.push(...o), { name: t, shaderCache: { hint: i, inputDependencies: ["type"] }, getShaderSource: (h) => co(h, ze.size(e.dims), e.dataType, a, s, n, u), getRunData: (h) => ({ outputs: [{ dims: e.dims, dataType: a }], dispatchGroup: { x: Math.ceil(ze.size(h[0].dims) / 64 / 4) }, programUniforms: p }) };
        }, vl = (e) => {
          e.compute(ds(e.inputs[0], "Abs", "abs"));
        }, po = (e) => {
          e.compute(ds(e.inputs[0], "Acos", "acos"));
        }, xl = (e) => {
          e.compute(ds(e.inputs[0], "Acosh", "acosh"));
        }, Tl = (e) => {
          e.compute(ds(e.inputs[0], "Asin", "asin"));
        }, ho = (e) => {
          e.compute(ds(e.inputs[0], "Asinh", "asinh"));
        }, Pl = (e) => {
          e.compute(ds(e.inputs[0], "Atan", "atan"));
        }, El = (e) => {
          e.compute(ds(e.inputs[0], "Atanh", "atanh"));
        }, mo = (e) => Bt(e), Cl = (e, t) => {
          let s;
          switch (t.to) {
            case 10:
              s = "vec4<f16>";
              break;
            case 1:
              s = "vec4<f32>";
              break;
            case 12:
              s = "vec4<u32>";
              break;
            case 6:
              s = "vec4<i32>";
              break;
            case 9:
              s = "vec4<bool>";
              break;
            default:
              throw new RangeError(`not supported type (specified in attribute 'to' from 'Cast' operator): ${t.to}`);
          }
          e.compute(ds(e.inputs[0], "Cast", s, void 0, t.cacheKey, t.to));
        }, kl = (e) => {
          let t, s, n = e.length >= 2 && e[1].data !== 0, i = e.length >= 3 && e[2].data !== 0;
          switch (e[0].dataType) {
            case 1:
              t = n ? e[1].getFloat32Array()[0] : -34028234663852886e22, s = i ? e[2].getFloat32Array()[0] : 34028234663852886e22;
              break;
            case 10:
              t = n ? e[1].getUint16Array()[0] : 64511, s = i ? e[2].getUint16Array()[0] : 31743;
              break;
            default:
              throw new Error("Unsupport data type");
          }
          return Bt({ min: t, max: s });
        }, fo = (e, t) => {
          let s = t || kl(e.inputs), n = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "Clip", (i) => `clamp(${i}, vec4<${n}>(uniforms.min), vec4<${n}>(uniforms.max))`, void 0, s.cacheKey, void 0, [{ type: e.inputs[0].dataType, data: s.min }, { type: e.inputs[0].dataType, data: s.max }], [{ name: "min", type: n }, { name: "max", type: n }]), { inputs: [0] });
        }, Sl = (e) => {
          e.compute(ds(e.inputs[0], "Ceil", "ceil"));
        }, $l = (e) => {
          e.compute(ds(e.inputs[0], "Cos", "cos"));
        }, _o = (e) => {
          e.compute(ds(e.inputs[0], "Cosh", "cosh"));
        }, Rn = (e) => Bt(e), Al = (e, t) => {
          let s = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "Elu", (n) => `elu_vf32(${n})`, `
  const elu_alpha_ = ${s}(${t.alpha});

  fn elu_f32(a: ${s}) -> ${s} {
  return select((exp(a) - 1.0) * elu_alpha_, a, a >= 0.0);
  }

  fn elu_vf32(v: vec4<${s}>) -> vec4<${s}> {
  return vec4(elu_f32(v.x), elu_f32(v.y), elu_f32(v.z), elu_f32(v.w));
  }`, t.cacheKey));
        }, ui = (e = "f32") => `
const r0: ${e} = 0.3275911;
const r1: ${e} = 0.254829592;
const r2: ${e} = -0.284496736;
const r3: ${e} = 1.421413741;
const r4: ${e} = -1.453152027;
const r5: ${e} = 1.061405429;

fn erf_vf32(v: vec4<${e}>) -> vec4<${e}> {
  let absv = abs(v);
  let x = 1.0 / (1.0 + r0 * absv);
  return sign(v) * (1.0 - ((((r5 * x + r4) * x + r3) * x + r2) * x + r1) * x * exp(-absv * absv));
}`, Il = (e) => {
          let t = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "Erf", (s) => `erf_vf32(${s})`, ui(t)));
        }, go = (e) => {
          e.compute(ds(e.inputs[0], "Exp", "exp"));
        }, Fl = (e) => {
          e.compute(ds(e.inputs[0], "Floor", "floor"));
        }, Ol = (e) => {
          let t = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "Gelu", (s) => `0.5 * ${s} * (1.0 + erf_vf32(${s} * 0.7071067811865475))`, ui(t)));
        }, wo = (e, t) => {
          let s = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "LeakyRelu", (n) => `select(leaky_relu_alpha_ * ${n}, ${n}, ${n} >= vec4<${s}>(0.0))`, `const leaky_relu_alpha_ = ${s}(${t.alpha});`, t.cacheKey));
        }, Dl = (e) => {
          e.compute(ds(e.inputs[0], "Not", (t) => `!${t}`));
        }, Ll = (e) => {
          e.compute(ds(e.inputs[0], "Neg", (t) => `-${t}`));
        }, yo = (e) => {
          e.compute(ds(e.inputs[0], "Reciprocal", (t) => `1.0/${t}`));
        }, zl = (e) => {
          let t = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "Relu", (s) => `select(vec4<${t}>(0.0), ${s}, ${s} > vec4<${t}>(0.0))`));
        }, Bl = (e) => {
          e.compute(ds(e.inputs[0], "Sigmoid", (t) => `(1.0 / (1.0 + exp(-${t})))`));
        }, Mo = (e) => Bt(e), Rl = (e, t) => {
          let s = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "HardSigmoid", (n) => `max(vec4<${s}>(0.0), min(vec4<${s}>(1.0), ${t.alpha} * ${n} + vec4<${s}>(${t.beta})))`, void 0, t.cacheKey));
        }, Nl = (e) => {
          e.compute(ds(e.inputs[0], "Sin", "sin"));
        }, bo = (e) => {
          e.compute(ds(e.inputs[0], "Sinh", "sinh"));
        }, jl = (e) => {
          e.compute(ds(e.inputs[0], "Sqrt", "sqrt"));
        }, Ul = (e) => {
          e.compute(ds(e.inputs[0], "Tan", "tan"));
        }, di = (e) => `sign(${e}) * (1 - exp(-2 * abs(${e}))) / (1 + exp(-2 * abs(${e})))`, Vl = (e) => {
          e.compute(ds(e.inputs[0], "Tanh", di));
        }, vo = (e = "f32") => `
const fast_gelu_a: ${e} = 0.5;
const fast_gelu_b: ${e} = 0.7978845608028654;
const fast_gelu_c: ${e} = 0.035677408136300125;

fn tanh_v(v: vec4<${e}>) -> vec4<${e}> {
  return ${di("v")};
}
`, ci = (e) => `(fast_gelu_a + fast_gelu_a * tanh_v(${e} * (fast_gelu_c * ${e} * ${e} + fast_gelu_b))) * ${e}`, Wl = (e) => {
          let t = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "FastGelu", ci, vo(t), void 0, e.inputs[0].dataType));
        }, Gl = (e, t) => {
          let s = Ss(e.inputs[0].dataType);
          return e.compute(ds(e.inputs[0], "ThresholdedRelu", (n) => `select(vec4<${s}>(0.0), ${n}, ${n} > thresholded_relu_alpha_)`, `const thresholded_relu_alpha_ = vec4<${s}>(${t.alpha});`, t.cacheKey)), 0;
        }, Kl = (e) => {
          e.compute(ds(e.inputs[0], "Log", "log"));
        }, Hl = (e, t) => `
const alpha = vec4<${e}>(${t});
const one = ${e}(1.0);
const zero = ${e}(0.0);

fn quick_gelu_impl(x: vec4<${e}>) -> vec4<${e}> {
  let v = x *alpha;
  var x1 : vec4<${e}>;
  for (var i = 0; i < 4; i = i + 1) {
    if (v[i] >= zero) {
      x1[i] = one / (one + exp(-v[i]));
    } else {
      x1[i] = one - one / (one + exp(v[i]));
    }
  }
  return x * x1;
}
`, xo = (e) => `quick_gelu_impl(${e})`, ql = (e, t) => {
          let s = Ss(e.inputs[0].dataType);
          e.compute(ds(e.inputs[0], "QuickGelu", xo, Hl(s, t.alpha), t.cacheKey, e.inputs[0].dataType));
        };
      }), Ql, Xl, Po, Kc = g(() => {
        Ft(), Yt(), To(), Ql = (e) => {
          if (e[0].dims.length !== 3) throw new Error("input should have 3 dimensions");
          if (![2560, 5120, 10240].includes(e[0].dims[2])) throw new Error("hidden state should be 2560, 5120 or 10240");
          if (e[1].dims.length !== 1) throw new Error("bias is expected to have 1 dimensions");
          if (e[0].dims[2] !== e[1].dims[0]) throw new Error("last dimension of input and bias are not the same");
        }, Xl = (e) => {
          let t = e[0].dims.slice();
          t[2] = t[2] / 2;
          let s = qe("input", e[0].dataType, e[0].dims, 4), n = qe("bias", e[0].dataType, [e[0].dims[2]], 4), i = It("output", e[0].dataType, t, 4), a = ze.size(t) / 4, o = fs(e[0].dataType);
          return { name: "BiasSplitGelu", getRunData: () => ({ outputs: [{ dims: t, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(a / 64) } }), getShaderSource: (u) => `
  const M_SQRT2 = sqrt(2.0);
  const halfChannels = ${e[0].dims[2] / 4 / 2}u;

  ${u.declareVariables(s, n, i)}

  ${ui(o)}

  ${u.mainStart()}
    ${u.guardAgainstOutOfBoundsWorkgroupSizes(a)}
    let biasIdx = global_idx % halfChannels;
    let batchIndex = global_idx / halfChannels;
    let inputOffset = biasIdx + batchIndex * halfChannels * 2;
    let valueLeft = input[inputOffset] + bias[biasIdx];
    let valueRight = input[inputOffset + halfChannels] + bias[biasIdx + halfChannels];
    let geluRight = valueRight * 0.5 * (erf_vf32(valueRight / M_SQRT2) + 1);

    ${i.setByOffset("global_idx", "valueLeft * geluRight")}
  }` };
        }, Po = (e) => {
          Ql(e.inputs), e.compute(Xl(e.inputs));
        };
      }), Yl, Jl, yr, Eo, Zl, eu, tu, su, Co, ru, nu, ko, iu, Hc = g(() => {
        zt(), Ft(), Yt(), Yl = (e, t, s, n, i, a, o, u, p, h, k, C) => {
          let d, z;
          typeof u == "string" ? d = z = (X, he) => `${u}((${X}),(${he}))` : typeof u == "function" ? d = z = u : (d = u.scalar, z = u.vector);
          let B = It("outputData", k, n.length, 4), V = qe("aData", p, t.length, 4), Z = qe("bData", h, s.length, 4), ee;
          if (i) if (a) {
            let X = ze.size(t) === 1, he = ze.size(s) === 1, pe = t.length > 0 && t[t.length - 1] % 4 === 0, Me = s.length > 0 && s[s.length - 1] % 4 === 0;
            X || he ? ee = B.setByOffset("global_idx", z(X ? `${V.type.value}(${V.getByOffset("0")}.x)` : V.getByOffset("global_idx"), he ? `${Z.type.value}(${Z.getByOffset("0")}.x)` : Z.getByOffset("global_idx"))) : ee = `
            let outputIndices = ${B.offsetToIndices("global_idx * 4u")};
            let offsetA = ${V.broadcastedIndicesToOffset("outputIndices", B)};
            let offsetB = ${Z.broadcastedIndicesToOffset("outputIndices", B)};
            ${B.setByOffset("global_idx", z(o || pe ? V.getByOffset("offsetA / 4u") : `${V.type.value}(${V.getByOffset("offsetA / 4u")}[offsetA % 4u])`, o || Me ? Z.getByOffset("offsetB / 4u") : `${Z.type.value}(${Z.getByOffset("offsetB / 4u")}[offsetB % 4u])`))}
          `;
          } else ee = B.setByOffset("global_idx", z(V.getByOffset("global_idx"), Z.getByOffset("global_idx")));
          else {
            if (!a) throw new Error("no necessary to use scalar implementation for element-wise binary op implementation.");
            let X = (he, pe, Me = "") => {
              let Oe = `aData[indexA${pe}][componentA${pe}]`, Le = `bData[indexB${pe}][componentB${pe}]`;
              return `
            let outputIndices${pe} = ${B.offsetToIndices(`global_idx * 4u + ${pe}u`)};
            let offsetA${pe} = ${V.broadcastedIndicesToOffset(`outputIndices${pe}`, B)};
            let offsetB${pe} = ${Z.broadcastedIndicesToOffset(`outputIndices${pe}`, B)};
            let indexA${pe} = offsetA${pe} / 4u;
            let indexB${pe} = offsetB${pe} / 4u;
            let componentA${pe} = offsetA${pe} % 4u;
            let componentB${pe} = offsetB${pe} % 4u;
            ${he}[${pe}] = ${Me}(${d(Oe, Le)});
          `;
            };
            k === 9 ? ee = `
            var data = vec4<u32>(0);
            ${X("data", 0, "u32")}
            ${X("data", 1, "u32")}
            ${X("data", 2, "u32")}
            ${X("data", 3, "u32")}
            outputData[global_idx] = dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(data));` : ee = `
            ${X("outputData[global_idx]", 0)}
            ${X("outputData[global_idx]", 1)}
            ${X("outputData[global_idx]", 2)}
            ${X("outputData[global_idx]", 3)}
          `;
          }
          return `
        ${e.registerUniform("vec_size", "u32").declareVariables(V, Z, B)}

        ${C ?? ""}

        ${e.mainStart()}
        ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
        ${ee}
      }`;
        }, Jl = (e, t, s, n, i, a, o = s.dataType) => {
          let u = s.dims.map((V) => Number(V) ?? 1), p = n.dims.map((V) => Number(V) ?? 1), h = !ze.areEqual(u, p), k = u, C = ze.size(u), d = !1, z = !1, B = [h];
          if (h) {
            let V = Ws.calcShape(u, p, !1);
            if (!V) throw new Error("Can't perform binary op on the given tensors");
            k = V.slice(), C = ze.size(k);
            let Z = ze.size(u) === 1, ee = ze.size(p) === 1, X = u.length > 0 && u[u.length - 1] % 4 === 0, he = p.length > 0 && p[p.length - 1] % 4 === 0;
            B.push(Z), B.push(ee), B.push(X), B.push(he);
            let pe = 1;
            for (let Me = 1; Me < k.length; Me++) {
              let Oe = u[u.length - Me], Le = p[p.length - Me];
              if (Oe === Le) pe *= Oe;
              else break;
            }
            pe % 4 === 0 ? (z = !0, d = !0) : (Z || ee || X || he) && (d = !0);
          } else d = !0;
          return B.push(d), { name: e, shaderCache: { hint: t + B.map((V) => V.toString()).join("_"), inputDependencies: ["rank", "rank"] }, getShaderSource: (V) => Yl(V, u, p, k, d, h, z, i, s.dataType, n.dataType, o, a), getRunData: () => ({ outputs: [{ dims: k, dataType: o }], dispatchGroup: { x: Math.ceil(C / 64 / 4) }, programUniforms: [{ type: 12, data: Math.ceil(ze.size(k) / 4) }, ...yt(u, p, k)] }) };
        }, yr = (e, t, s, n, i, a) => {
          e.compute(Jl(t, i ?? "", e.inputs[0], e.inputs[1], s, n, a));
        }, Eo = (e) => {
          yr(e, "Add", (t, s) => `${t}+${s}`);
        }, Zl = (e) => {
          yr(e, "Div", (t, s) => `${t}/${s}`);
        }, eu = (e) => {
          yr(e, "Equal", { scalar: (t, s) => `u32(${t}==${s})`, vector: (t, s) => `vec4<u32>(${t}==${s})` }, void 0, void 0, 9);
        }, tu = (e) => {
          yr(e, "Mul", (t, s) => `${t}*${s}`);
        }, su = (e) => {
          let t = qe("input", e.inputs[0].dataType, e.inputs[0].dims).type.value;
          yr(e, "Pow", { scalar: (s, n) => `pow_custom(${s},${n})`, vector: (s, n) => `pow_vector_custom(${s},${n})` }, `
    fn pow_custom(a : ${t}, b : ${t}) -> ${t} {
      if (b == ${t}(0.0)) {
        return ${t}(1.0);
      } else if (a < ${t}(0.0) && f32(b) != floor(f32(b))) {
        return ${t}(pow(f32(a), f32(b))); // NaN
      }
      return select(sign(a), ${t}(1.0), round(f32(abs(b) % ${t}(2.0))) != 1.0) * ${t}(${t === "i32" ? "round" : ""}(pow(f32(abs(a)), f32(b))));
    }
    fn pow_vector_custom(a : vec4<${t}>, b : vec4<${t}>) -> vec4<${t}> {
      // TODO: implement vectorized pow
      return vec4<${t}>(pow_custom(a.x, b.x), pow_custom(a.y, b.y), pow_custom(a.z, b.z), pow_custom(a.w, b.w));
    }
      `);
        }, Co = (e) => {
          yr(e, "Sub", (t, s) => `${t}-${s}`);
        }, ru = (e) => {
          yr(e, "Greater", { scalar: (t, s) => `u32(${t}>${s})`, vector: (t, s) => `vec4<u32>(${t}>${s})` }, void 0, void 0, 9);
        }, nu = (e) => {
          yr(e, "Less", { scalar: (t, s) => `u32(${t}<${s})`, vector: (t, s) => `vec4<u32>(${t}<${s})` }, void 0, void 0, 9);
        }, ko = (e) => {
          yr(e, "GreaterOrEqual", { scalar: (t, s) => `u32(${t}>=${s})`, vector: (t, s) => `vec4<u32>(${t}>=${s})` }, void 0, void 0, 9);
        }, iu = (e) => {
          yr(e, "LessOrEqual", { scalar: (t, s) => `u32(${t}<=${s})`, vector: (t, s) => `vec4<u32>(${t}<=${s})` }, void 0, void 0, 9);
        };
      }), So, ou, au, $o, lu, uu, du = g(() => {
        zt(), Ft(), rs(), Yt(), So = (e, t) => {
          if (!e || e.length < 1) throw new Error("too few inputs");
          let s = 0, n = e[s], i = n.dataType, a = n.dims.length;
          e.forEach((o, u) => {
            if (u !== s) {
              if (o.dataType !== i) throw new Error("input tensors should be one type");
              if (o.dims.length !== a) throw new Error("input tensors should have the same shape");
              o.dims.forEach((p, h) => {
                if (h !== t && p !== n.dims[h]) throw new Error("non concat dimensions must match");
              });
            }
          });
        }, ou = (e, t) => `
  fn calculateInputIndex(index: u32) -> u32 {
    let sizeInConcatAxis = array<u32, ${e}u>(${t});
    for (var i: u32 = 0u; i < ${e}; i += 1u ) {
      if (index < sizeInConcatAxis[i]) {
        return i;
      }
    }
    return ${e}u;
  }`, au = (e, t) => {
          let s = e.length, n = [];
          for (let i = 0; i < s; ++i) {
            let a = t.setByOffset("global_idx", e[i].getByIndices("indices"));
            s === 1 ? n.push(a) : i === 0 ? n.push(`if (inputIndex == ${i}u) { ${a} }`) : i === s - 1 ? n.push(`else { ${a} }`) : n.push(`else if (inputIndex == ${i}) { ${a} }`);
          }
          return n.join(`
`);
        }, $o = (e, t, s, n) => {
          let i = ze.size(s), a = new Array(e.length), o = new Array(e.length), u = 0, p = [], h = [], k = [{ type: 12, data: i }];
          for (let V = 0; V < e.length; ++V) u += e[V].dims[t], a[V] = u, h.push(e[V].dims.length), o[V] = qe(`input${V}`, n, h[V]), p.push("rank"), k.push({ type: 12, data: a[V] });
          for (let V = 0; V < e.length; ++V) k.push(...yt(e[V].dims));
          k.push(...yt(s));
          let C = It("output", n, s.length), d = C.indicesGet("indices", t), z = Array.from(Array(a.length).keys()).map((V) => `uniforms.sizeInConcatAxis${V}`).join(","), B = (V) => `

  ${(() => {
            V.registerUniform("outputSize", "u32");
            for (let Z = 0; Z < e.length; Z++) V.registerUniform(`sizeInConcatAxis${Z}`, "u32");
            return V.declareVariables(...o, C);
          })()}

  ${ou(a.length, z)}

  ${V.mainStart()}
    ${V.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

    var indices = ${C.offsetToIndices("global_idx")};

    let inputIndex = calculateInputIndex(${d});
    if (inputIndex != 0u) {
      let sizeInConcatAxis = array<u32, ${a.length}u>(${z});
      ${d} -= sizeInConcatAxis[inputIndex - 1u];
    }

    ${au(o, C)}
  }`;
          return { name: "Concat", shaderCache: { hint: `${t}`, inputDependencies: p }, getRunData: () => ({ outputs: [{ dims: s, dataType: n }], dispatchGroup: { x: Math.ceil(i / 64) }, programUniforms: k }), getShaderSource: B };
        }, lu = (e, t) => {
          let s = e.inputs, n = s[0].dims, i = ze.normalizeAxis(t.axis, n.length);
          So(s, i);
          let a = n.slice();
          a[i] = s.reduce((u, p) => u + (p.dims.length > i ? p.dims[i] : 0), 0);
          let o = s.filter((u) => ze.size(u.dims) > 0);
          e.compute($o(o, i, a, s[0].dataType), { inputs: o });
        }, uu = (e) => Bt({ axis: e.axis });
      }), sn, rn, Or, Ao, nn = g(() => {
        zt(), Ft(), sn = (e, t, s = "f32") => {
          switch (e.activation) {
            case "Relu":
              return `value = max(value, ${t}(0.0));`;
            case "Sigmoid":
              return `value = (${t}(1.0) / (${t}(1.0) + exp(-value)));`;
            case "Clip":
              return `value = clamp(value, ${t}(${s}(uniforms.clip_min)), ${t}(${s}(uniforms.clip_max)));`;
            case "HardSigmoid":
              return `value = max(${t}(0.0), min(${t}(1.0), ${s}(uniforms.alpha) * value + ${s}(uniforms.beta)));`;
            case "LeakyRelu":
              return `value = select(${s}(uniforms.alpha) * value, value, value >= ${t}(0.0));`;
            case "Tanh":
              return `let e2x = exp(-2.0 * abs(value));
              value = sign(value) * (1.0 - e2x) / (1.0 + e2x);
        `;
            case "":
              return "";
            default:
              throw new Error(`Unsupported activation ${e.activation}`);
          }
        }, rn = (e, t) => {
          e.activation === "Clip" ? t.push({ type: 1, data: e.clipMax }, { type: 1, data: e.clipMin }) : e.activation === "HardSigmoid" ? t.push({ type: 1, data: e.alpha }, { type: 1, data: e.beta }) : e.activation === "LeakyRelu" && t.push({ type: 1, data: e.alpha });
        }, Or = (e, t) => {
          e.activation === "Clip" ? t.push({ name: "clip_max", type: "f32" }, { name: "clip_min", type: "f32" }) : e.activation === "HardSigmoid" ? t.push({ name: "alpha", type: "f32" }, { name: "beta", type: "f32" }) : e.activation === "LeakyRelu" && t.push({ name: "alpha", type: "f32" });
        }, Ao = (e) => {
          let t = (e == null ? void 0 : e.activation) || "";
          if (t === "HardSigmoid") {
            let [s, n] = (e == null ? void 0 : e.activation_params) || [0.2, 0.5];
            return { activation: t, alpha: s, beta: n };
          } else if (t === "Clip") {
            let [s, n] = (e == null ? void 0 : e.activation_params) || [ks, Xs];
            return { activation: t, clipMax: n, clipMin: s };
          } else if (t === "LeakyRelu") {
            let [s] = (e == null ? void 0 : e.activation_params) || [0.01];
            return { activation: t, alpha: s };
          }
          return { activation: t };
        };
      }), Ks, Io, Fo = g(() => {
        Ks = (e, t) => {
          switch (e) {
            case 1:
              return t;
            case 2:
              return `vec2<${t}>`;
            case 3:
              return `vec3<${t}>`;
            case 4:
              return `vec4<${t}>`;
            default:
              throw new Error(`${e}-component is not supported.`);
          }
        }, Io = (e) => `
      ${e ? "value = value + getBiasByOutputCoords(coords);" : ""}
      `;
      }), Oo, qc = g(() => {
        Oo = (e) => `
fn getIndexFromCoords4D(coords : vec4<i32>, shape : vec4<i32>) -> i32 {
  return dot(coords, vec4<i32>(
      shape.y * shape.z * shape.w, shape.z * shape.w, shape.w, 1));
}
fn getOutputIndexFromCoords(coords : vec4<i32>) -> i32 {
  return dot(coords, vec4<i32>(
    i32(${e}.x), i32(${e}.y), i32(${e}.z), 1));
}
`;
      }), Nn, Do, pi = g(() => {
        zt(), Ft(), Yt(), nn(), Nn = (e, t, s, n, i) => {
          let a = n - s;
          return `
      ${Array.from({ length: s }).map((o, u) => `
      if (${$t(t.shape, u, t.rank)} != 1) {
        ${t.indicesSet(e, u, $t(i, u + a, n))}
      } else {
        ${t.indicesSet(e, u, 0)}
      }`).join("")}
`;
        }, Do = (e, t, s, n, i = !1, a) => {
          let o = e[0].dims, u = e[1].dims, p = o[o.length - 2], h = u[u.length - 1], k = o[o.length - 1], C = qt(h), d = qt(k), z = qt(p), B = ze.size(s) / C / z, V = e.length > 2, Z = n ? n.slice(0, -2) : s.slice(0, -2), ee = [ze.size(Z), p, h], X = [{ type: 12, data: B }, { type: 12, data: p }, { type: 12, data: h }, { type: 12, data: k }];
          rn(t, X), X.push(...yt(Z, o, u)), V && X.push(...yt(e[2].dims)), X.push(...yt(ee));
          let he = (pe) => {
            let Me = Ri("batch_dims", e[0].dataType, Z.length), Oe = qe("a", e[0].dataType, o.length, d), Le = qe("b", e[1].dataType, u.length, C), Ye = It("output", e[0].dataType, ee.length, C), at = fs(Ye.type.tensor), Pt = sn(t, Ye.type.value, at), Xt = [Oe, Le], Zt = "";
            if (V) {
              let St = i ? C : 1;
              Xt.push(qe("bias", e[2].dataType, e[2].dims.length, St)), Zt = `${i ? `value += bias[col / ${St}];` : `value += ${Ye.type.value}(bias[row + i]);`}`;
            }
            let bt = [{ name: "output_size", type: "u32" }, { name: "M", type: "u32" }, { name: "N", type: "u32" }, { name: "K", type: "u32" }];
            Or(t, bt);
            let ss = () => {
              let St = `var a_data: ${Oe.type.value};`;
              for (let Ot = 0; Ot < d; Ot++) St += `
              let b_data${Ot} = b[(b_offset + (k + ${Ot}) * uniforms.N + col) / ${C}];`;
              for (let Ot = 0; Ot < z; Ot++) {
                St += `a_data = a[(a_offset + (row + ${Ot}) * uniforms.K + k) / ${d}];`;
                for (let bs = 0; bs < d; bs++) St += `
            values[${Ot}] = fma(${Le.type.value}(a_data${d === 1 ? "" : `[${bs}]`}), b_data${bs}, values[${Ot}]);
`;
              }
              return St;
            };
            return `
  ${pe.registerUniforms(bt).registerInternalVariables(Me).declareVariables(...Xt, Ye)}
  ${pe.mainStart()}
    ${pe.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let col = (global_idx % (uniforms.N / ${C})) * ${C};
    var index1 = global_idx / (uniforms.N / ${C});
    let stride1 = uniforms.M / ${z};
    let row = (index1 % stride1) * ${z};
    let batch = index1 / stride1;

    ${s.length === 2 ? "" : `let batch_indices = ${Me.offsetToIndices("batch")};`}

    var a_indices: ${Oe.type.indices};
    ${Nn("a_indices", Oe, Oe.rank - 2, Me.rank, "batch_indices")}
    ${Oe.indicesSet("a_indices", Oe.rank - 2, 0)}
    ${Oe.indicesSet("a_indices", Oe.rank - 1, 0)}
    let a_offset = ${Oe.indicesToOffset("a_indices")};

    var b_indices: ${Le.type.indices};
    ${Nn("b_indices", Le, Le.rank - 2, Me.rank, "batch_indices")}
    ${Le.indicesSet("b_indices", Le.rank - 2, 0)}
    ${Le.indicesSet("b_indices", Le.rank - 1, 0)}
    let b_offset = ${Le.indicesToOffset("b_indices")};
    var values: array<${Ye.type.value}, ${z}>;
    for (var k: u32 = 0u; k < uniforms.K; k = k + ${d}) {
      ${ss()}
    }
    for (var i = 0u; i < ${z}u; i++) {
      var value = values[i];
      ${Zt}
      ${Pt}
      let cur_indices = ${Ye.type.indices}(batch, row + i, col);
      let offset = ${Ye.indicesToOffset("cur_indices")};
      ${Ye.setByOffset(`offset / ${C}`, "value")};
    }
  }
  `;
          };
          return { name: "MatMulNaive", shaderCache: { hint: `${t.activation};${C};${d};${z};${i}`, inputDependencies: V ? ["rank", "rank", "rank"] : ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: a ? a(s) : s, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(B / 64) }, programUniforms: X }), getShaderSource: he };
        };
      }), cu, pu, Lo, hi, hu, zo, Bo, mi, Ro = g(() => {
        zt(), Ft(), Yt(), nn(), pi(), Fo(), cu = (e, t) => e ? `
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          kStart + inputRow,
          globalRowStart / innerElementSize + inputCol${t ? ", batchIndices" : ""});
        ` : `
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          globalRow + innerRow,
          kStart / innerElementSize + inputCol${t ? ", batchIndices" : ""});
        `, pu = (e, t) => e ? `
        let ACached0 = mm_Asub[k * innerElementSize][localRow];
        let ACached1 = mm_Asub[k * innerElementSize + 1][localRow];
        let ACached2 = mm_Asub[k * innerElementSize + 2][localRow];
        ${t === 3 ? "" : "let ACached3 = mm_Asub[k * innerElementSize + 3][localRow];"}
        for (var i = 0; i < rowPerThread; i = i + 1) {
          acc[i] = BCached0 * ACached0[i] + acc[i];
          acc[i] = BCached1 * ACached1[i] + acc[i];
          acc[i] = BCached2 * ACached2[i] + acc[i];
          ${t === 3 ? "" : "acc[i] = BCached3 * ACached3[i] + acc[i];"}
        }` : `
        for (var i = 0; i < rowPerThread; i = i + 1) {
          let ACached = mm_Asub[tileRow + i][k];
          acc[i] = BCached0 * ACached.x + acc[i];
          acc[i] = BCached1 * ACached.y + acc[i];
          acc[i] = BCached2 * ACached.z + acc[i];
          ${t === 3 ? "" : "acc[i] = BCached3 * ACached.w + acc[i];"}
        }`, Lo = (e, t, s = "f32", n, i = !1, a = 32, o = !1, u = 32) => {
          let p = t[1] * e[1], h = t[0] * e[0], k = i ? p : a, C = i ? a : p, d = k / t[0], z = a / t[1];
          if (!((i && d === 4 && e[1] === 4 || !i && (d === 3 || d === 4)) && k % t[0] === 0 && a % t[1] === 0 && e[0] === 4)) throw new Error(`If transposeA ${i} is true, innerElementSize ${d} and workPerThread[1] ${e[1]} must be 4.
      Otherwise, innerElementSize ${d} must be 3 or 4.
  tileAWidth ${k} must be divisible by workgroupSize[0]${t[0]}. tileInner ${a} must be divisible by workgroupSize[1] ${t[1]}. colPerThread ${e[0]} must be 4.`);
          return `
var<workgroup> mm_Asub: array<array<vec${d}<${s}>, ${k / d}>, ${C}>;
var<workgroup> mm_Bsub: array<array<vec4<${s}>, ${h / e[0]}>, ${a}>;

const rowPerThread = ${e[1]};
const colPerThread = ${e[0]};
const innerElementSize = ${d};
const tileInner = ${a};

@compute @workgroup_size(${t[0]}, ${t[1]}, ${t[2]})
fn main(@builtin(local_invocation_id) localId : vec3<u32>,
        @builtin(global_invocation_id) globalId : vec3<u32>,
        @builtin(workgroup_id) workgroupId : vec3<u32>) {
  let localRow = i32(localId.y);
  let tileRow = localRow * rowPerThread;
  let tileCol = i32(localId.x);

  let globalRow =i32(globalId.y) * rowPerThread;
  let globalCol = i32(globalId.x);
  let batch = ${o ? "0" : "i32(globalId.z)"};
  ${n ? `let batchIndices = ${n.offsetToIndices("u32(batch)")};` : ""}
  let globalRowStart = i32(workgroupId.y) * ${p};

  let num_tiles = ${o ? `${Math.ceil(u / a)}` : "(uniforms.dim_inner - 1) / tileInner + 1"};
  var kStart = ${o ? `i32(globalId.z) * ${u}` : "0"};

  var acc: array<vec4<${s}>, rowPerThread>;

  // Loop over shared dimension.
  let tileRowB = localRow * ${z};
  for (var t = 0; t < num_tiles; t = t + 1) {
      // Load one tile of A into local memory.
      for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
          let inputRow = tileRow + innerRow;
          let inputCol = tileCol;
          ${cu(i, n)}
      }

      // Load one tile of B into local memory.
      for (var innerRow = 0; innerRow < ${z}; innerRow = innerRow + 1) {
          let inputRow = tileRowB + innerRow;
          let inputCol = tileCol;
          mm_Bsub[inputRow][inputCol] = mm_readB(batch, kStart + inputRow, globalCol${n ? ", batchIndices" : ""});
      }
      kStart = kStart + tileInner;
      workgroupBarrier();

      // Compute acc values for a single thread.
      for (var k = 0; k < tileInner / innerElementSize; k = k + 1) {
          let BCached0 = mm_Bsub[k * innerElementSize][tileCol];
          let BCached1 = mm_Bsub[k * innerElementSize + 1][tileCol];
          let BCached2 = mm_Bsub[k * innerElementSize + 2][tileCol];
          ${d === 3 ? "" : "let BCached3 = mm_Bsub[k * innerElementSize + 3][tileCol];"}

          ${pu(i, d)}
      }

      workgroupBarrier();
  }

  for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      mm_write(batch, globalRow + innerRow, globalCol, acc[innerRow]);
  }
}`;
        }, hi = (e, t) => e ? `
            mm_Asub[inputRow][inputCol] = mm_readA(batch,
              kStart + inputRow,
              globalRowStart + inputCol${t ? ", batchIndices" : ""});
            ` : `
            mm_Asub[inputRow][inputCol] = mm_readA(batch,
              globalRowStart + inputRow,
              kStart + inputCol${t ? ", batchIndices" : ""});
            `, hu = (e) => e ? "let ACached = mm_Asub[k][tileRow + innerRow];" : "let ACached = mm_Asub[tileRow + innerRow][k];", zo = (e, t, s = "f32", n, i = !1, a = 32, o = !1, u = 32, p = !1) => {
          let h = e[1] * t[1], k = e[0] * t[0], C = i ? h : a, d = i ? a : h;
          if (!(d % t[1] === 0 && C % t[0] === 0 && a % t[1] === 0)) throw new Error(`tileAHight ${d} must be divisible by workgroupSize[1]${t[1]}, tileAWidth ${C} must be divisible by workgroupSize[0]${t[0]}, tileInner ${a} must be divisible by workgroupSize[1]${t[1]}`);
          let z = d / t[1], B = C / t[0], V = a / t[1], Z = p ? `
    let localRow = i32(localId.y);
    let localCol = i32(localId.x);
    let globalRowStart = i32(workgroupId.y) * ${h};
    let globalColStart = i32(workgroupId.x) * ${k};

    // Loop over shared dimension.
    for (var t = 0; t < num_tiles; t = t + 1) {
      // Load one tile of A into local memory.
      for (var inputRow = localRow; inputRow < ${d}; inputRow = inputRow + ${t[1]}) {
        for (var inputCol = localCol; inputCol < ${C}; inputCol = inputCol + ${t[0]}) {
          ${hi(i, n)}
        }
      }
      // Load one tile of B into local memory.
      for (var inputRow = localRow; inputRow < ${a}; inputRow = inputRow + ${t[1]}) {
            for (var inputCol = localCol; inputCol < ${k}; inputCol = inputCol + ${t[0]}) {
          mm_Bsub[inputRow][inputCol] = mm_readB(batch,
            kStart + inputRow,
            globalColStart + inputCol${n ? ", batchIndices" : ""});
        }
      }
      kStart = kStart + tileInner;
      workgroupBarrier();

      // Compute acc values for a single thread.
      var BCached : array<${s}, colPerThread>;
      for (var k = 0; k < tileInner; k = k + 1) {
        for (var inner = 0; inner < colPerThread; inner = inner + 1) {
          BCached[inner] = mm_Bsub[k][localCol + inner * ${t[0]}];
        }
        for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
          let ACached = ${i ? `mm_Asub[k][localRow + innerRow * ${t[1]}];` : `mm_Asub[localRow + innerRow * ${t[1]}][k];`}
          for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
            acc[innerRow][innerCol] = acc[innerRow][innerCol] +
                ACached * BCached[innerCol];
          }
        }
      }
      workgroupBarrier();
    }
    for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      let gRow = globalRowStart + localRow + innerRow * ${t[1]};
      for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
        let gCol = globalColStart + localCol + innerCol * ${t[0]};
        mm_write(batch, gRow, gCol, acc[innerRow][innerCol]);
      }
    }
    ` : `
let tileRow = i32(localId.y) * rowPerThread;
let tileCol = i32(localId.x) * colPerThread;

let globalRow = i32(globalId.y) * rowPerThread;
let globalCol = i32(globalId.x) * colPerThread;
let globalRowStart = i32(workgroupId.y) * ${h};

let tileRowA = i32(localId.y) * ${z};
let tileColA = i32(localId.x) * ${B};
let tileRowB = i32(localId.y) * ${V};
// Loop over shared dimension.
for (var t = 0; t < num_tiles; t = t + 1) {
  // Load one tile of A into local memory.
  for (var innerRow = 0; innerRow < ${z}; innerRow = innerRow + 1) {
    for (var innerCol = 0; innerCol < ${B}; innerCol = innerCol + 1) {
      let inputRow = tileRowA + innerRow;
      let inputCol = tileColA + innerCol;
      ${hi(i, n)}
    }
  }

  // Load one tile of B into local memory.
  for (var innerRow = 0; innerRow < ${V}; innerRow = innerRow + 1) {
    for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
      let inputRow = tileRowB + innerRow;
      let inputCol = tileCol + innerCol;
      mm_Bsub[inputRow][inputCol] = mm_readB(batch,
        kStart + inputRow,
        globalCol + innerCol${n ? ", batchIndices" : ""});
    }
  }
  kStart = kStart + tileInner;
  workgroupBarrier();

  // Compute acc values for a single thread.
  var BCached : array<${s}, colPerThread>;
  for (var k = 0; k < tileInner; k = k + 1) {
    for (var inner = 0; inner < colPerThread; inner = inner + 1) {
      BCached[inner] = mm_Bsub[k][tileCol + inner];
    }

    for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      ${hu(i)}
      for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
        acc[innerRow][innerCol] = acc[innerRow][innerCol] + ACached * BCached[innerCol];
      }
    }
  }

  workgroupBarrier();
}

for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
  for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
    mm_write(batch, globalRow + innerRow, globalCol + innerCol,
        acc[innerRow][innerCol]);
  }
}
`;
          return `
  var<workgroup> mm_Asub : array<array<${s}, ${C}>, ${d}>;
  var<workgroup> mm_Bsub : array<array<${s}, ${k}>, ${a}>;
  const rowPerThread = ${e[1]};
  const colPerThread = ${e[0]};
  const tileInner = ${a};

@compute @workgroup_size(${t[0]}, ${t[1]}, ${t[2]})
fn main(@builtin(local_invocation_id) localId : vec3<u32>,
        @builtin(global_invocation_id) globalId : vec3<u32>,
        @builtin(workgroup_id) workgroupId : vec3<u32>) {
    let batch = ${o ? "0" : "i32(globalId.z)"};
    ${n ? `let batchIndices = ${n.offsetToIndices("u32(batch)")};` : ""}
    let num_tiles = ${o ? `${Math.ceil(u / a)}` : "(uniforms.dim_inner - 1) / tileInner + 1"};
    var kStart = ${o ? `i32(globalId.z) * ${u}` : "0"};

    var acc : array<array<${s}, colPerThread>, rowPerThread>;
    ${Z}
  }
`;
        }, Bo = (e, t, s, n, i = !1) => {
          let [a, o, u, p] = n, h = fs(n[0].type.tensor);
          return `
    fn mm_readA(batch: i32, row: i32, colIn: i32, batchIndices: ${a.type.indices}) -> ${Ks(e, h)} {
      var value = ${Ks(e, h)}(0.0);
      let col = colIn * ${e};
      if(row < uniforms.dim_a_outer && col < uniforms.dim_inner)
      {
        var aIndices: ${o.type.indices};
        ${Nn("aIndices", o, o.rank - 2, a.rank, "batchIndices")}
        ${o.indicesSet("aIndices", o.rank - 2, "u32(row)")}
        ${o.indicesSet("aIndices", o.rank - 1, "u32(colIn)")}
        value = ${o.getByIndices("aIndices")};
      }
      return value;
    }

    fn mm_readB(batch: i32, row: i32, colIn: i32, batchIndices: ${a.type.indices}) -> ${Ks(e, h)} {
      var value = ${Ks(e, h)}(0.0);
      let col = colIn * ${e};
      if(row < uniforms.dim_inner && col < uniforms.dim_b_outer)
      {
        var bIndices: ${u.type.indices};
        ${Nn("bIndices", u, u.rank - 2, a.rank, "batchIndices")}
        ${u.indicesSet("bIndices", u.rank - 2, "u32(row)")}
        ${u.indicesSet("bIndices", u.rank - 1, "u32(colIn)")}
        value = ${u.getByIndices("bIndices")};
      }
      return value;
    }

    fn mm_write(batch: i32, row: i32, colIn: i32, valueIn: ${Ks(e, h)}) {
      let col = colIn * ${e};
      if (row < uniforms.dim_a_outer && col < uniforms.dim_b_outer) {
        var value = valueIn;
        let coords = vec3<i32>(batch, row, colIn);
        ${t ? `value = value + ${i ? "bias[colIn]" : `${Ks(e, h)}(bias[row])`};` : ""}
        ${s}
        ${p.setByIndices("vec3<u32>(coords)", "value")}
      }
    }
    `;
        }, mi = (e, t, s, n, i = !1, a) => {
          let o = e[0].dims, u = e[1].dims, p = o.slice(0, -2), h = u.slice(0, -2), k = n ? n.slice(0, -2) : s.slice(0, -2), C = ze.size(k), d = o[o.length - 2], z = o[o.length - 1], B = u[u.length - 1], V = z % 4 === 0 && B % 4 === 0, Z = d <= 8 ? [4, 1, 1] : [4, 4, 1], ee = [8, 8, 1], X = [Math.ceil(B / ee[0] / Z[0]), Math.ceil(d / ee[1] / Z[1]), Math.ceil(C / ee[2] / Z[2])], he = V ? 4 : 1, pe = [...p, d, z / he], Me = pe.length, Oe = [...h, z, B / he], Le = Oe.length, Ye = [C, d, B / he], at = [{ type: 6, data: d }, { type: 6, data: B }, { type: 6, data: z }];
          rn(t, at), at.push(...yt(k, pe, Oe));
          let Pt = ["rank", "rank"], Xt = e.length > 2;
          Xt && (at.push(...yt(e[2].dims)), Pt.push("rank")), at.push(...yt(Ye));
          let Zt = (bt) => {
            let ss = k.length, St = Ri("batchDims", e[0].dataType, ss, 1), Ot = fs(e[0].dataType), bs = qe("a", e[0].dataType, Me, he), Ht = qe("b", e[1].dataType, Le, he), Rt = It("result", e[0].dataType, Ye.length, he), _s = [bs, Ht];
            if (Xt) {
              let xr = i ? he : 1;
              _s.push(qe("bias", e[2].dataType, e[2].dims.length, xr));
            }
            let ot = [{ name: "dim_a_outer", type: "i32" }, { name: "dim_b_outer", type: "i32" }, { name: "dim_inner", type: "i32" }];
            Or(t, ot);
            let Et = fs(Rt.type.tensor), ps = sn(t, Rt.type.value, Et), Ns = Bo(he, Xt, ps, [St, bs, Ht, Rt], i);
            return `
  ${bt.registerUniforms(ot).registerInternalVariables(St).declareVariables(..._s, Rt)}
  ${Ns}
  ${V ? Lo(Z, ee, Ot, St) : zo(Z, ee, Ot, St)}
                   `;
          };
          return { name: "MatMul", shaderCache: { hint: `${Z};${t.activation};${V};${i}`, inputDependencies: Pt }, getRunData: () => ({ outputs: [{ dims: a ? a(s) : s, dataType: e[0].dataType }], dispatchGroup: { x: X[0], y: X[1], z: X[2] }, programUniforms: at }), getShaderSource: Zt };
        };
      }), No, mu, Qc = g(() => {
        zt(), Pe(), Yt(), nn(), Fo(), qc(), Ro(), No = (e, t, s, n, i = !1, a, o = 4, u = 4, p = 4, h = "f32") => {
          let k = (at) => {
            switch (at) {
              case 1:
                return "resData = x[xIndex];";
              case 3:
                return `resData = vec3<${h}>(x[xIndex], x[xIndex + 1], x[xIndex + 2]);`;
              case 4:
                return "resData = x[xIndex / 4];";
              default:
                throw new Error(`innerElementSize ${at} is not supported.`);
            }
          }, C = (at) => {
            switch (at) {
              case 1:
                return "return w[row * i32(uniforms.w_shape[3]) + colIn];";
              case 4:
                return "return w[row * i32(uniforms.w_shape[3]) / 4 + colIn];";
              default:
                throw new Error(`innerElementSize ${at} is not supported.`);
            }
          }, d = e ? `
    let coord = vec4<i32>(batch, xRow, xCol, xCh);
    ` : `
    let coord = vec4<i32>(batch, xCh, xRow, xCol);
    `, z = e ? `
    let coords = vec4<i32>(
      batch,
      row / outWidth,
      row % outWidth,
      col);
    ` : `
    let coords = vec4<i32>(
      batch,
      row,
      col / outWidth,
      col % outWidth);
    `, B = e ? "i32(uniforms.x_shape[1])" : "i32(uniforms.x_shape[2])", V = e ? "i32(uniforms.x_shape[2])" : "i32(uniforms.x_shape[3])", Z = e ? "row" : "col", ee = e ? "col" : "row", X = `
    let inChannels = i32(uniforms.w_shape[2]);
    let outWidth = ${e ? "i32(uniforms.result_shape[2])" : "i32(uniforms.result_shape[3])"};
    let outRow = ${Z} / outWidth;
    let outCol = ${Z} % outWidth;

    let WRow = ${ee} / (i32(uniforms.w_shape[1]) * inChannels);
    let WCol = ${ee} / inChannels % i32(uniforms.w_shape[1]);
    let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * WRow - uniforms.pad[0];
    let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * WCol - uniforms.pad[1];
    let xCh = ${ee} % inChannels;
    var resData = ${Ks(o, h)}(0.0);
    // The bounds checking is always needed since we use it to pad zero for
    // the 'same' padding type.
    if (xRow >= 0 && xRow < ${B} && xCol >= 0 && xCol < ${V}) {
      ${d}
      let xIndex = getIndexFromCoords4D(coord, vec4<i32>(uniforms.x_shape));
      ${k(o)}
    }
    return resData;`, he = e ? t && n ? `
    let col = colIn * ${o};
    ${X}` : `
    let col = colIn * ${o};
    if (row < uniforms.dim_a_outer && col < uniforms.dim_inner) {
      ${X}
    }
    return ${Ks(o, h)}(0.0);` : n && s ? `
    let col = colIn * ${o};
    ${X}` : `
    let col = colIn * ${o};
    if (row < uniforms.dim_inner && col < uniforms.dim_b_outer) {
      ${X}
    }
    return ${Ks(o, h)}(0.0);`, pe = e ? n && s ? C(u) : `
    let col = colIn * ${u};
    if (row < uniforms.dim_inner && col < uniforms.dim_b_outer) {
      ${C(u)}
    }
    return ${Ks(u, h)}(0.0);` : `
    let col = colIn * ${u};
    if (row < uniforms.dim_inner && col < uniforms.dim_a_outer) {
      ${C(u)}
    }
    return ${Ks(u, h)}(0.0);`, Me = Ks(p, h), Oe = Ks(e ? o : u, h), Le = Ks(e ? u : o, h), Ye = sn(a, Me, h);
          return `
    fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${Oe} {
      ${e ? he : pe}
    }

    fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${Le} {
      ${e ? pe : he}
    }

    fn mm_write(batch: i32, row : i32, colIn : i32, valueIn : ${Me}) {
      let col = colIn * ${p};
      if (row < uniforms.dim_a_outer && col < uniforms.dim_b_outer)
      {
      var value = valueIn;
      let outWidth = ${e ? "i32(uniforms.result_shape[2])" : "i32(uniforms.result_shape[3])"};
      ${z}
      ${Io(i)}
      ${Ye}
      setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
      }
    }`;
        }, mu = (e, t, s, n, i, a, o, u, p) => {
          let h = t.format === "NHWC", k = h ? e[0].dims[3] : e[0].dims[1], C = s[0], d = h ? s[2] : s[3], z = h ? s[1] : s[2], B = h ? s[3] : s[1], V = h && (k % 4 === 0 || k % 3 === 0) && B % 4 === 0, Z = h ? B : d * z, ee = h ? d * z : B, X = [8, 8, 1], he = n <= 8 ? [4, 1, 1] : [4, 4, 1], pe = [Math.ceil(Z / X[0] / he[0]), Math.ceil(ee / X[1] / he[1]), Math.ceil(C / X[2] / he[2])];
          as("verbose", () => `[conv2d_mm_webgpu] dispatch = ${pe}`);
          let Me = V ? h && k % 4 !== 0 ? 3 : 4 : 1, Oe = X[1] * he[1], Le = X[0] * he[0], Ye = Math.max(X[0] * Me, X[1]), at = n % Oe === 0, Pt = i % Le === 0, Xt = a % Ye === 0, Zt = V ? [Me, 4, 4] : [1, 1, 1], bt = [{ type: 6, data: n }, { type: 6, data: i }, { type: 6, data: a }, { type: 6, data: [t.pads[0], t.pads[1]] }, { type: 6, data: t.strides }, { type: 6, data: t.dilations }];
          rn(t, bt), bt.push(...yt(e[0].dims, e[1].dims));
          let ss = ["rank", "rank"];
          o && (bt.push(...yt(e[2].dims)), ss.push("rank")), bt.push(...yt(s));
          let St = (Ot) => {
            let bs = [{ name: "dim_a_outer", type: "i32" }, { name: "dim_b_outer", type: "i32" }, { name: "dim_inner", type: "i32" }, { name: "pad", type: "i32", length: 2 }, { name: "stride", type: "i32", length: 2 }, { name: "dilation", type: "i32", length: 2 }];
            Or(t, bs);
            let Ht = V ? 4 : 1, Rt = fs(e[0].dataType), _s = `
      fn setOutputAtIndex(flatIndex : i32, value : ${V ? `vec4<${Rt}>` : Rt}) {
        result[flatIndex] = ${V ? `vec4<${Rt}>` : Rt}(value);
      }
      fn setOutputAtCoords(d0 : i32, d1 : i32, d2 : i32, d3 : i32, value : ${V ? `vec4<${Rt}>` : Rt}) {
        let flatIndex = getOutputIndexFromCoords(vec4<i32>(d0, d1, d2, d3));
        setOutputAtIndex(flatIndex ${V ? "/ 4" : ""}, value);
      }`, ot = qe("x", e[0].dataType, e[0].dims.length, Me === 3 ? 1 : Me), Et = qe("w", e[1].dataType, e[1].dims.length, Ht), ps = [ot, Et], Ns = It("result", e[0].dataType, s.length, Ht);
            if (o) {
              let xr = qe("bias", e[2].dataType, e[2].dims.length, Ht);
              ps.push(xr), _s += `
        fn getBiasByOutputCoords(coords : vec4<i32>) -> ${V ? `vec4<${Rt}>` : Rt} {
          return bias[coords.${h ? "w" : "y"}${V ? "/ 4" : ""}];
        }`;
            }
            return `
        ${Oo("uniforms.result_strides")}
        //struct Uniforms { xShape : vec4<i32>, wShape : vec4<i32>, outShape : vec4<i32>,
        //  outShapeStrides: vec3<i32>, filterDims : vec2<i32>, pad : vec2<i32>, stride : vec2<i32>,
        //  dilation : vec2<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32 };
        ${Ot.registerUniforms(bs).declareVariables(...ps, Ns)}
        ${_s}
        ${No(h, at, Pt, Xt, o, t, Zt[0], Zt[1], Zt[2], Rt)}
        ${V ? Lo(he, X, Rt, void 0, !h, Ye) : zo(he, X, Rt, void 0, !h, Ye, !1, void 0, u)}`;
          };
          return { name: "Conv2DMatMul", shaderCache: { hint: `${t.cacheKey};${Me};${V};${at};${Pt};${Xt};${Oe};${Le};${Ye}`, inputDependencies: ss }, getRunData: () => ({ outputs: [{ dims: p ? p(s) : s, dataType: e[0].dataType }], dispatchGroup: { x: pe[0], y: pe[1], z: pe[2] }, programUniforms: bt }), getShaderSource: St };
        };
      }), jo, Uo, jn, Vo, Wo, fu, Go, _u, Xc = g(() => {
        zt(), Pe(), Ft(), Yt(), nn(), Fo(), jo = (e) => {
          let t = 1;
          for (let s = 0; s < e.length; s++) t *= e[s];
          return t;
        }, Uo = (e) => typeof e == "number" ? [e, e, e] : e, jn = (e, t) => t <= 1 ? e : e + (e - 1) * (t - 1), Vo = (e, t, s, n = 1) => {
          let i = jn(t, n);
          return Math.floor((e[0] * (s - 1) - s + i) / 2);
        }, Wo = (e, t, s, n, i) => {
          i == null && (i = Vo(e, t[0], n[0]));
          let a = [0, 0, 0, s];
          for (let o = 0; o < 3; o++) e[o] + 2 * i >= t[o] && (a[o] = Math.trunc((e[o] - t[o] + 2 * i) / n[o] + 1));
          return a;
        }, fu = (e, t, s, n, i, a, o, u, p, h) => {
          let k, C, d, z;
          if (e === "VALID" && (e = 0), typeof e == "number") {
            k = { top: e, bottom: e, left: e, right: e, front: e, back: e };
            let B = Wo([t, s, n, 1], [u, p, h], 1, [i, a, o], e);
            C = B[0], d = B[1], z = B[2];
          } else if (Array.isArray(e)) {
            if (!e.every((V, Z, ee) => V === ee[0])) throw Error(`Unsupported padding parameter: ${e}`);
            k = { top: e[0], bottom: e[1], left: e[2], right: e[3], front: e[4], back: e[5] };
            let B = Wo([t, s, n, 1], [u, p, h], 1, [i, a, o], e[0]);
            C = B[0], d = B[1], z = B[2];
          } else if (e === "SAME_UPPER") {
            C = Math.ceil(t / i), d = Math.ceil(s / a), z = Math.ceil(n / o);
            let B = (C - 1) * i + u - t, V = (d - 1) * a + p - s, Z = (z - 1) * o + h - n, ee = Math.floor(B / 2), X = B - ee, he = Math.floor(V / 2), pe = V - he, Me = Math.floor(Z / 2), Oe = Z - Me;
            k = { top: he, bottom: pe, left: Me, right: Oe, front: ee, back: X };
          } else throw Error(`Unknown padding parameter: ${e}`);
          return { padInfo: k, outDepth: C, outHeight: d, outWidth: z };
        }, Go = (e, t, s, n, i, a = !1, o = "channelsLast") => {
          let u, p, h, k, C;
          if (o === "channelsLast") [u, p, h, k, C] = e;
          else if (o === "channelsFirst") [u, C, p, h, k] = e;
          else throw new Error(`Unknown dataFormat ${o}`);
          let [d, , z, B, V] = t, [Z, ee, X] = Uo(s), [he, pe, Me] = Uo(n), Oe = jn(z, he), Le = jn(B, pe), Ye = jn(V, Me), { padInfo: at, outDepth: Pt, outHeight: Xt, outWidth: Zt } = fu(i, p, h, k, Z, ee, X, Oe, Le, Ye), bt = a ? d * C : d, ss = [0, 0, 0, 0, 0];
          return o === "channelsFirst" ? ss = [u, bt, Pt, Xt, Zt] : o === "channelsLast" && (ss = [u, Pt, Xt, Zt, bt]), { batchSize: u, dataFormat: o, inDepth: p, inHeight: h, inWidth: k, inChannels: C, outDepth: Pt, outHeight: Xt, outWidth: Zt, outChannels: bt, padInfo: at, strideDepth: Z, strideHeight: ee, strideWidth: X, filterDepth: z, filterHeight: B, filterWidth: V, effectiveFilterDepth: Oe, effectiveFilterHeight: Le, effectiveFilterWidth: Ye, dilationDepth: he, dilationHeight: pe, dilationWidth: Me, inShape: e, outShape: ss, filterShape: t };
        }, _u = (e, t, s, n, i, a) => {
          let o = a === "channelsLast";
          o ? e[0].dims[3] : e[0].dims[1];
          let u = [64, 1, 1], p = { x: s.map((Z, ee) => ee) }, h = [Math.ceil(jo(p.x.map((Z) => s[Z])) / u[0]), 1, 1];
          as("verbose", () => `[conv3d_naive_webgpu] dispatch = ${h}`);
          let k = 1, C = ze.size(s), d = [{ type: 12, data: C }, { type: 12, data: n }, { type: 12, data: i }, { type: 12, data: t.strides }, { type: 12, data: t.dilations }];
          rn(t, d), d.push(...yt(e[0].dims, e[1].dims));
          let z = ["rank", "rank"], B = e.length === 3;
          B && (d.push(...yt(e[2].dims)), z.push("rank")), d.push(...yt(s));
          let V = (Z) => {
            let ee = [{ name: "output_size", type: "u32" }, { name: "filter_dims", type: "u32", length: n.length }, { name: "pads", type: "u32", length: i.length }, { name: "strides", type: "u32", length: t.strides.length }, { name: "dilations", type: "u32", length: t.dilations.length }];
            Or(t, ee);
            let X = 1, he = fs(e[0].dataType), pe = qe("x", e[0].dataType, e[0].dims.length, k), Me = qe("W", e[1].dataType, e[1].dims.length, X), Oe = [pe, Me], Le = It("result", e[0].dataType, s.length, X), Ye = "";
            if (B) {
              let Xt = qe("bias", e[2].dataType, e[2].dims.length, X);
              Oe.push(Xt), Ye += `
        fn getBiasByOutputCoords(coords : array<u32, 5>) -> ${he} {
          return bias[${o ? $t("coords", 4, 5) : $t("coords", 1, 5)}];
        }`;
            }
            let at = Ks(k, he), Pt = sn(t, at, he);
            return `
            ${Ye}
            fn getX(d0 : u32, d1 : u32, d2 : u32, d3 : u32, d4 : u32) -> f32 {
              let aIndices = array<u32, 5>(d0, d1, d2, d3, d4);
              return ${pe.getByIndices("aIndices")};
            }
            fn getW(d0 : u32, d1 : u32, d2 : u32, d3 : u32, d4 : u32) -> f32 {
              let aIndices = array<u32, 5>(d0, d1, d2, d3, d4);
              return ${Me.getByIndices("aIndices")};
            }
          ${Z.registerUniforms(ee).declareVariables(...Oe, Le)}
          ${Z.mainStart()}
          ${Z.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
              let coords = ${Le.offsetToIndices("global_idx")};
              let batch = ${$t("coords", 0, pe.rank)};
              let d2 = ${o ? $t("coords", pe.rank - 1, pe.rank) : $t("coords", 1, pe.rank)};
              let xFRCCorner = vec3<u32>(${o ? $t("coords", 1, pe.rank) : $t("coords", 2, pe.rank)},
              ${o ? $t("coords", 2, pe.rank) : $t("coords", 3, pe.rank)},
              ${o ? $t("coords", 3, pe.rank) : $t("coords", 4, pe.rank)}) * uniforms.strides - uniforms.pads;
              let xFCorner = xFRCCorner.x;
              let xRCorner = xFRCCorner.y;
              let xCCorner = xFRCCorner.z;
              let xShapeY = ${o ? $t("uniforms.x_shape", 1, pe.rank) : $t("uniforms.x_shape", 2, pe.rank)};
              let xShapeZ = ${o ? $t("uniforms.x_shape", 2, pe.rank) : $t("uniforms.x_shape", 3, pe.rank)};
              let xShapeW = ${o ? $t("uniforms.x_shape", 3, pe.rank) : $t("uniforms.x_shape", 4, pe.rank)};
              let xShapeU = ${o ? $t("uniforms.x_shape", 4, pe.rank) : $t("uniforms.x_shape", 1, pe.rank)};
              let inputDepthNearestVec4 = (xShapeU / 4) * 4;
              let inputDepthVec4Remainder = xShapeU % 4;

              var value = 0.0;
              for (var wF = 0u; wF < uniforms.filter_dims[0]; wF++) {
                let xF = xFCorner + wF * uniforms.dilations[0];
                if (xF < 0 || xF >= xShapeY) {
                  continue;
                }

                for (var wR = 0u; wR < uniforms.filter_dims[1]; wR++) {
                  let xR = xRCorner + wR * uniforms.dilations[1];
                  if (xR < 0 || xR >= xShapeZ) {
                    continue;
                  }

                  for (var wC = 0u; wC < uniforms.filter_dims[2]; wC++) {
                    let xC = xCCorner + wC * uniforms.dilations[2];
                    if (xC < 0 || xC >= xShapeW) {
                      continue;
                    }

                    for (var d1 = 0u; d1 < inputDepthNearestVec4; d1 += 4) {
                      ${o ? `let xValues = vec4<f32>(
                               getX(batch, xF, xR, xC, d1),
                               getX(batch, xF, xR, xC, d1 + 1),
                               getX(batch, xF, xR, xC, d1 + 2),
                               getX(batch, xF, xR, xC, d1 + 3));
                            ` : `let xValues = vec4<f32>(
                               getX(batch, d1, xF, xR, xC),
                               getX(batch, d1 + 1, xF, xR, xC),
                               getX(batch, d1 + 2, xF, xR, xC),
                               getX(batch, d1 + 3, xF, xR, xC));
                            `}
                            let wValues = vec4<f32>(
                              getW(d2, d1, wF, wR, wC),
                              getW(d2, d1 + 1, wF, wR, wC),
                              getW(d2, d1 + 2, wF, wR, wC),
                              getW(d2, d1 + 3, wF, wR, wC));
                      value += dot(xValues, wValues);
                    }
                    if (inputDepthVec4Remainder == 1) {
                        ${o ? `value += getX(batch, xF, xR, xC, inputDepthNearestVec4)
                          * getW(d2, inputDepthNearestVec4, wF, wR, wC);` : `value += getX(batch, inputDepthNearestVec4, xF, xR, xC)
                          * getW(d2, inputDepthNearestVec4, wF, wR, wC);`}
                    } else if (inputDepthVec4Remainder == 2) {
                      ${o ? `let xValues = vec2<f32>(
                        getX(batch, xF, xR, xC, inputDepthNearestVec4),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 1));
                      ` : `let xValues = vec2<f32>(
                        getX(batch, inputDepthNearestVec4, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 1, xF, xR, xC));
                    `}
                    let wValues = vec2<f32>(
                      getW(d2, inputDepthNearestVec4, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 1, wF, wR, wC));
                      value += dot(xValues, wValues);
                    } else if (inputDepthVec4Remainder == 3) {
                      ${o ? `let xValues = vec3<f32>(
                        getX(batch, xF, xR, xC, inputDepthNearestVec4),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 1),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 2));
                      ` : `let xValues = vec3<f32>(
                        getX(batch, inputDepthNearestVec4, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 1, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 2, xF, xR, xC));
                    `}
                    let wValues = vec3<f32>(
                      getW(d2, inputDepthNearestVec4, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 1, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 2, wF, wR, wC));
                      value += dot(xValues, wValues);
                    }
                  }
                }
              }
              ${B ? "value = value + getBiasByOutputCoords(coords)" : ""};
              ${Pt}
              result[global_idx] = f32(value);
          }`;
          };
          return { name: "Conv3DNaive", shaderCache: { hint: `${t.cacheKey};${o};${k};${B}`, inputDependencies: z }, getRunData: () => ({ outputs: [{ dims: s, dataType: e[0].dataType }], dispatchGroup: { x: h[0], y: h[1], z: h[2] }, programUniforms: d }), getShaderSource: V };
        };
      }), gu, wu, Ko = g(() => {
        zt(), Ft(), Yt(), nn(), gu = (e, t, s, n) => {
          let i = e.length > 2, a = i ? "value += b[output_channel];" : "", o = e[0].dims, u = e[1].dims, p = t.format === "NHWC", h = p ? s[3] : s[1], k = h / t.group, C = p && k >= 4 ? qt(h) : 1, d = ze.size(s) / C, z = [{ type: 12, data: d }, { type: 12, data: t.dilations }, { type: 12, data: [t.strides[0], t.strides[1]] }, { type: 12, data: [t.pads[0], t.pads[1]] }, { type: 12, data: k }];
          rn(t, z), z.push(...yt(o, [u[0], u[1], u[2], u[3] / C]));
          let B = i ? ["rank", "rank", "rank"] : ["rank", "rank"];
          z.push(...yt([s[0], s[1], s[2], s[3] / C]));
          let V = (Z) => {
            let ee = It("output", e[0].dataType, s.length, C), X = fs(ee.type.tensor), he = sn(t, ee.type.value, X), pe = qe("x", e[0].dataType, o.length), Me = qe("w", e[1].dataType, u.length, C), Oe = [pe, Me];
            i && Oe.push(qe("b", e[2].dataType, e[2].dims, C));
            let Le = [{ name: "output_size", type: "u32" }, { name: "dilations", type: "u32", length: t.dilations.length }, { name: "strides", type: "u32", length: 2 }, { name: "pads", type: "u32", length: 2 }, { name: "output_channels_per_group", type: "u32" }];
            Or(t, Le);
            let Ye = p ? `
      for (var wHeight: u32 = 0u; wHeight < uniforms.w_shape[0]; wHeight++) {
        let xHeight = xRCCorner.x + wHeight * uniforms.dilations[0];

        if (xHeight < 0u || xHeight >= uniforms.x_shape[1]) {
          continue;
        }

        for (var wWidth: u32 = 0u; wWidth < uniforms.w_shape[1]; wWidth++) {
          let xWidth = xRCCorner.y + wWidth * uniforms.dilations[1];
          if (xWidth < 0u || xWidth >= uniforms.x_shape[2]) {
            continue;
          }

          for (var wInChannel: u32 = 0u; wInChannel < uniforms.w_shape[2]; wInChannel++) {
            let input_channel = in_channel_offset + wInChannel;
            let xVal = ${pe.get("batch", "xHeight", "xWidth", "input_channel")};
            let wVal = ${Me.get("wHeight", "wWidth", "wInChannel", "output_channel")};
            value += xVal * wVal;
          }
        }
      }
      ` : `
      for (var wInChannel: u32 = 0u; wInChannel < uniforms.w_shape[1]; wInChannel++) {
        let input_channel = in_channel_offset + wInChannel;
        for (var wHeight: u32 = 0u; wHeight < uniforms.w_shape[2]; wHeight++) {
          let xHeight = xRCCorner.x + wHeight * uniforms.dilations[0];

          if (xHeight < 0u || xHeight >= uniforms.x_shape[2]) {
            continue;
          }

          for (var wWidth: u32 = 0u; wWidth < uniforms.w_shape[3]; wWidth++) {
            let xWidth = xRCCorner.y + wWidth * uniforms.dilations[1];
            if (xWidth < 0u || xWidth >= uniforms.x_shape[3]) {
              continue;
            }

            let xVal = ${pe.get("batch", "input_channel", "xHeight", "xWidth")};
            let wVal = ${Me.get("output_channel", "wInChannel", "wHeight", "wWidth")};
            value += xVal * wVal;
          }
        }
      }
      `;
            return `
  ${Z.registerUniforms(Le).declareVariables(...Oe, ee)}

  ${Z.mainStart()}
    ${Z.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let outputIndices = ${ee.offsetToIndices("global_idx")};
    let batch: u32 = outputIndices[0];
    let output_channel: u32 = outputIndices[${p ? 3 : 1}];
    let xRCCorner: vec2<u32> = vec2<u32>(outputIndices[${p ? 1 : 2}], outputIndices[${p ? 2 : 3}]) * uniforms.strides - uniforms.pads;
    let group_id: u32 = output_channel * ${C} / uniforms.output_channels_per_group;
    var in_channel_offset = group_id * uniforms.w_shape[${p ? 2 : 1}];

    var value: ${ee.type.value} = ${ee.type.value}(0);
    ${Ye}
    ${a}
    ${he}
    ${ee.setByOffset("global_idx", "value")}
  }`;
          };
          return { name: "GroupedConv", shaderCache: { hint: `${t.cacheKey}_${C}`, inputDependencies: B }, getRunData: () => ({ outputs: [{ dims: n ? n(s) : s, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(d / 64) }, programUniforms: z }), getShaderSource: V };
        }, wu = (e, t, s, n) => {
          let i = e.length > 2, a = qt(s[3]), o = qt(s[2]), u = ze.size(s) / a / o, p = [e[0].dims[0], e[0].dims[1], e[0].dims[2], e[0].dims[3] / a], h = [e[1].dims[0], e[1].dims[1], e[1].dims[2], e[1].dims[3] / a], k = [s[0], s[1], s[2], s[3] / a], C = [{ type: 12, data: u }, { type: 6, data: [t.strides[0], t.strides[1]] }, { type: 6, data: [t.pads[0], t.pads[1]] }];
          rn(t, C), C.push(...yt(p, h, k));
          let d = (o - 1) * t.strides[1] + h[1], z = (B) => {
            let V = It("output", e[0].dataType, k.length, a), Z = fs(V.type.tensor), ee = sn(t, V.type.value, Z), X = qe("x", e[0].dataType, p.length, a), he = qe("w", e[1].dataType, h.length, a), pe = [X, he];
            i && pe.push(qe("b", e[2].dataType, e[2].dims, a));
            let Me = i ? "value += b[output_channel];" : "", Oe = [{ name: "output_size", type: "u32" }, { name: "strides", type: "i32", length: 2 }, { name: "pads", type: "i32", length: 2 }];
            return Or(t, Oe), `
  ${B.registerUniforms(Oe).declareVariables(...pe, V)}
  ${B.mainStart()}
    ${B.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let width0 = uniforms.output_shape[3];
    let output_channel = global_idx % width0;
    var index1 = global_idx / width0;
    let width1 = uniforms.output_shape[2] / ${o}u;
    let col = (index1 % width1) * ${o}u;
    index1 = index1 / width1;
    let row = index1 % uniforms.output_shape[1];
    let batch = index1 / uniforms.output_shape[1];

    let x_corner = vec2<i32>(i32(row), i32(col)) * uniforms.strides - uniforms.pads;

    var x_vals: array<${X.type.value}, ${d}>;
    var values: array<${V.type.value}, ${o}>;
    let input_channel = output_channel;
    // Use constant instead of uniform can give better performance for w's height/width.
    for (var w_height: u32 = 0u; w_height < ${h[0]}; w_height++) {
      let x_height = x_corner.x + i32(w_height);
      if (x_height >= 0 && u32(x_height) < uniforms.x_shape[1]) {
        for (var i = 0; i < ${d}; i++) {
          let x_width = x_corner.y + i;
          if (x_width >= 0 && u32(x_width) < uniforms.x_shape[2]) {
            x_vals[i] = ${X.get("batch", "u32(x_height)", "u32(x_width)", "input_channel")};
          } else {
            x_vals[i] = ${X.type.value}(0);
          }
        }
        for (var w_width: u32 = 0u; w_width < ${h[1]}; w_width++) {
          let w_val = ${he.get("w_height", "w_width", "0", "output_channel")};
          for (var i = 0u; i < ${o}u; i++) {
            values[i] = fma(x_vals[i * u32(uniforms.strides[1]) + w_width], w_val, values[i]);
          }
        }
      }
    }

    for (var i = 0u; i < ${o}u; i++) {
      var value = values[i];
      ${Me}
      ${ee}
      ${V.set("batch", "row", "col + i", "output_channel", "value")};
    }
  }`;
          };
          return { name: "GroupedConv-Vectorize", shaderCache: { hint: `${t.cacheKey};${a};${o};${d};${h[0]};${h[1]}`, inputDependencies: i ? ["rank", "rank", "type"] : ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: n ? n(s) : s, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(u / 64) }, programUniforms: C }), getShaderSource: z };
        };
      }), yu, fi, Mu, _i, Ho, gi, bu, vu, wi, Yc = g(() => {
        Ft(), Qc(), Xc(), Ro(), Ko(), nn(), pi(), Gr(), yu = (e, t, s, n, i, a) => {
          let o = e[0], u = e.slice(a ? 1 : 2, a ? 3 : 4), p = u.length, h = t[0], k = t.slice(2).map((d, z) => d + (d - 1) * (s[z] - 1)), C = u.map((d, z) => d + n[z] + n[z + p]).map((d, z) => Math.floor((d - k[z] + i[z]) / i[z]));
          return C.splice(0, 0, o), C.splice(a ? 3 : 1, 0, h), C;
        }, fi = [2, 3, 1, 0], Mu = (e, t) => {
          if (!e || e.length !== 2 && e.length !== 3) throw new Error("Conv requires 2 or 3 inputs");
          if (e[0].dims.length > 5) throw new Error("greater than 5D is not supported");
          if (e[0].dims.length !== e[1].dims.length) throw new Error("filter does not have same dimension as input");
          let s = e[0].dims[t.format === "NHWC" ? e[0].dims.length - 1 : 1], n = e[1].dims[1] * t.group;
          if (s !== n) throw new Error("FILTER_IN_CHANNEL should be equal to DATA_CHANNEL");
          if (e.length === 3 && (e[2].dims.length !== 1 || e[1].dims[0] !== e[2].dims[0])) throw new Error("invalid bias");
          let i = e[0].dims.length - 2;
          if (t.dilations.length !== i) throw new Error(`dilations should be ${i}D`);
          if (t.strides.length !== i) throw new Error(`strides should be ${i}D`);
          if (t.pads.length !== i * 2) throw new Error(`pads should be ${i * 2}D`);
          if (t.kernelShape.length !== 0 && t.kernelShape.length !== e[1].dims.length - 2) throw new Error("invalid kernel shape");
        }, _i = (e, t) => {
          let s = e.kernelShape.slice();
          s.length < t[1].dims.length - 2 && s.push(...Array(t[1].dims.length - 2 - s.length).fill(0));
          for (let a = 2; a < t[1].dims.length; ++a) s[a - 2] === 0 && (s[a - 2] = t[1].dims[a]);
          let n = e.pads.slice();
          Js.adjustPadsBasedOnAutoPad(t[0].dims, e.strides, e.dilations, s, n, e.format === "NHWC", e.autoPad);
          let i = Object.assign({}, e);
          return Object.assign(i, { kernelShape: s, pads: n }), i;
        }, Ho = (e) => {
          let t = Ao(e), s = e.format, n = ["NOTSET", "VALID", "SAME_UPPER", "SAME_LOWER"][e.auto_pad], i = e.dilations, a = e.group, o = e.kernel_shape, u = e.pads, p = e.strides, h = e.w_is_const();
          return { autoPad: n, format: s, dilations: i, group: a, kernelShape: o, pads: u, strides: p, wIsConst: h, ...t, cacheKey: `${e.format};${t.activation};` };
        }, gi = (e, t, s, n) => {
          let i = s.format === "NHWC", a = yu(t[0].dims, t[1].dims, s.dilations, s.pads, s.strides, i);
          if (s.group !== 1) {
            let Oe = [t[0]];
            if (i) {
              let Le = e.kernelCustomData.wT ?? e.compute(cr(t[1], fi), { inputs: [1], outputs: [s.wIsConst ? -2 : -1] })[0];
              s.wIsConst && !e.kernelCustomData.wT && (e.kernelCustomData.wT = Le), Oe.push(Le);
            } else Oe.push(t[1]);
            t.length === 3 && Oe.push(t[2]), !e.adapterInfo.isArchitecture("ampere") && i && t[1].dims[0] === s.group && t[1].dims[1] === 1 && s.dilations[0] === 1 && s.dilations[1] === 1 ? e.compute(wu(Oe, s, a, n), { inputs: Oe }) : e.compute(gu(Oe, s, a, n), { inputs: Oe });
            return;
          }
          let o = t.length === 3, u = t[0].dims[i ? 1 : 2], p = t[0].dims[i ? 2 : 3], h = t[0].dims[i ? 3 : 1], k = t[1].dims[2], C = t[1].dims[3], d = a[i ? 1 : 2], z = a[i ? 2 : 3], B = a[i ? 3 : 1], V = i && k === u && C === p && s.pads[0] === 0 && s.pads[1] === 0;
          if (V || k === 1 && C === 1 && s.dilations[0] === 1 && s.dilations[1] === 1 && s.strides[0] === 1 && s.strides[1] === 1 && s.pads[0] === 0 && s.pads[1] === 0) {
            let Oe = a[0], Le, Ye, at, Pt = [];
            if (i) {
              let bt = e.kernelCustomData.wT ?? e.compute(cr(t[1], fi), { inputs: [1], outputs: [s.wIsConst ? -2 : -1] })[0];
              if (s.wIsConst && !e.kernelCustomData.wT && (e.kernelCustomData.wT = bt), V) {
                let ss = u * p * h;
                Le = t[0].reshape([1, Oe, ss]), Ye = bt.reshape([1, ss, B]), at = [1, Oe, B];
              } else Le = t[0].reshape([Oe, u * p, h]), Ye = bt.reshape([1, h, B]), at = [Oe, d * z, B];
              Pt.push(Le), Pt.push(Ye);
            } else Le = t[0].reshape([Oe, h, u * p]), Ye = t[1].reshape([1, B, h]), at = [Oe, B, d * z], Pt.push(Ye), Pt.push(Le);
            o && Pt.push(t[2]);
            let Xt = at[2], Zt = Pt[0].dims[Pt[0].dims.length - 1];
            Xt < 8 && Zt < 8 ? e.compute(Do(Pt, s, a, at, i, n), { inputs: Pt }) : e.compute(mi(Pt, s, a, at, i, n), { inputs: Pt });
            return;
          }
          let Z = !0, ee = e.kernelCustomData.wT ?? e.compute(cr(t[1], fi), { inputs: [1], outputs: [s.wIsConst ? -2 : -1] })[0];
          s.wIsConst && !e.kernelCustomData.wT && (e.kernelCustomData.wT = ee);
          let X = [t[0], ee];
          o && X.push(t[2]);
          let he = i ? d * z : B, pe = i ? B : d * z, Me = k * C * h;
          e.compute(mu(X, s, a, he, pe, Me, o, Z, n), { inputs: X });
        }, bu = (e, t) => {
          let s = t.format === "NHWC", n = [e.inputs[0].reshape(s ? [e.inputs[0].dims[0], 1, e.inputs[0].dims[1], e.inputs[0].dims[2]] : [e.inputs[0].dims[0], e.inputs[0].dims[1], 1, e.inputs[0].dims[2]]), e.inputs[1].reshape([e.inputs[1].dims[0], e.inputs[1].dims[1], 1, e.inputs[1].dims[2]])];
          e.inputs.length === 3 && n.push(e.inputs[2]);
          let i = [0, t.pads[0], 0, t.pads[1]], a = [1].concat(t.strides), o = [1].concat(t.dilations), u = [1].concat(t.kernelShape), p = _i({ ...t, pads: i, strides: a, dilations: o, kernelShape: u }, n);
          gi(e, n, p, (h) => s ? [h[0], h[2], h[3]] : [h[0], h[1], h[3]]);
        }, vu = (e, t, s) => {
          let n = s.format === "NHWC" ? "channelsLast" : "channelsFirst", i = _i(s, t), a = s.autoPad === "NOTSET" ? s.pads : s.autoPad, o = Go(t[0].dims, t[1].dims, s.strides, s.dilations, a, !1, n);
          e.compute(_u(t, i, o.outShape, [o.filterDepth, o.filterHeight, o.filterWidth], [o.padInfo.front, o.padInfo.top, o.padInfo.left], n));
        }, wi = (e, t) => {
          if (Mu(e.inputs, t), e.inputs[0].dims.length === 3) bu(e, t);
          else if (e.inputs[0].dims.length === 5) vu(e, e.inputs, t);
          else {
            let s = _i(t, e.inputs);
            gi(e, e.inputs, s);
          }
        };
      }), xu, Jc = g(() => {
        zt(), Pe(), Ft(), Yt(), xu = (e, t, s) => {
          let n = e.length > 2, i = t.outputShape, a = t.format === "NHWC", o = t.group, u = e[1].dims, p = u[2] / o, h = u[3], k = a ? qt(p) : 1, C = a ? qt(h) : 1, d = a ? h === 1 ? k : C : 1, z = ze.size(i) / C, B = [Math.ceil(z / 64), 1, 1];
          as("verbose", () => `[conv2d_backprop_webgpu] dispatch = ${B}`);
          let V = ["rank", "rank"], Z = [t.strides[0], t.strides[1]], ee = [t.kernelShape[a ? 1 : 2], t.kernelShape[a ? 2 : 3]], X = [t.dilations[0], t.dilations[1]], he = [ee[0] + (t.dilations[0] <= 1 ? 0 : (t.kernelShape[a ? 1 : 2] - 1) * (t.dilations[0] - 1)), ee[1] + (t.dilations[1] <= 1 ? 0 : (t.kernelShape[a ? 2 : 3] - 1) * (t.dilations[1] - 1))], pe = [he[0] - 1 - Math.floor((t.pads[0] + t.pads[2]) / 2), he[1] - 1 - Math.floor((t.pads[1] + t.pads[3]) / 2)], Me = [{ type: 12, data: z }, { type: 12, data: Z }, { type: 12, data: ee }, { type: 12, data: X }, { type: 12, data: he }, { type: 6, data: pe }, { type: 12, data: p }, { type: 12, data: h }, ...yt(e[0].dims, e[1].dims)];
          n && (Me.push(...yt(e[2].dims)), V.push("rank")), Me.push(...yt(i));
          let Oe = (Le) => {
            let Ye = [{ name: "output_size", type: "u32" }, { name: "strides", type: "u32", length: Z.length }, { name: "filter_dims", type: "u32", length: ee.length }, { name: "dilations", type: "u32", length: ee.length }, { name: "effective_filter_dims", type: "u32", length: he.length }, { name: "pads", type: "i32", length: pe.length }, { name: "input_channels_per_group", type: "u32" }, { name: "output_channels_per_group", type: "u32" }], at = fs(e[0].dataType), Pt = a ? 1 : 2, Xt = a ? 2 : 3, Zt = a ? 3 : 1, bt = qe("W", e[1].dataType, e[1].dims.length, d), ss = qe("Dy", e[0].dataType, e[0].dims.length, k), St = [ss, bt];
            n && St.push(qe("bias", e[2].dataType, [i[Zt]].length, C));
            let Ot = It("result", e[0].dataType, i.length, C), bs = () => {
              let Rt = "";
              if (k === 1) Rt += `
        let w_offset = ${bt.indicesToOffset(`${bt.type.indices}(u32(wRPerm), u32(wCPerm), inputChannel, wOutChannel)`)};
        let wValue = ${bt.getByOffset(`w_offset / ${d}`)};
        dotProd = dotProd + xValue * wValue;`;
              else if (h === 1) Rt += `
          let wValue = ${bt.getByOffset(`${bt.indicesToOffset(`${bt.type.indices}(u32(wRPerm), u32(wCPerm), inputChannel, wOutChannel)`)} / ${d}`)};
          dotProd = dotProd + dot(xValue, wValue);`;
              else for (let _s = 0; _s < k; _s++) Rt += `
            let wValue${_s} = ${bt.getByOffset(`${bt.indicesToOffset(`${bt.type.indices}(u32(wRPerm), u32(wCPerm), inputChannel + ${_s}, wOutChannel)`)} / ${d}`)};
            dotProd = dotProd + xValue[${_s}] * wValue${_s};`;
              return Rt;
            }, Ht = `
            let outputIndices = ${Ot.offsetToIndices(`global_idx * ${C}`)};
            let batch = ${Ot.indicesGet("outputIndices", 0)};
            let d1 = ${Ot.indicesGet("outputIndices", Zt)};
            let r = ${Ot.indicesGet("outputIndices", Pt)};
            let c = ${Ot.indicesGet("outputIndices", Xt)};
            let dyCorner = vec2<i32>(i32(r), i32(c)) - uniforms.pads;
            let dyRCorner = dyCorner.x;
            let dyCCorner = dyCorner.y;
            let groupId = d1 / uniforms.output_channels_per_group;
            let wOutChannel = d1 - groupId * uniforms.output_channels_per_group;
            // Convolve dy(?, ?, d2) with w(:, :, d1, d2) to compute dx(xR, xC, d1).
            // ? = to be determined. : = across all values in that axis.
            var dotProd = ${Ot.type.value}(0.0);
            for (var wR: u32 = 0; wR < uniforms.effective_filter_dims.x; wR = wR + 1) {
              if (wR % uniforms.dilations.x != 0) {
                continue;
              }
              let dyR = (${at}(dyRCorner) + ${at}(wR)) / ${at}(uniforms.strides[0]);
              let wRPerm = uniforms.filter_dims.x - 1 - wR / uniforms.dilations.x;
              if (dyR < 0.0 || dyR >= ${at}(uniforms.Dy_shape[${Pt}]) || fract(dyR) > 0.0 ||
                  wRPerm < 0) {
                continue;
              }
              wR = wR + uniforms.strides[0] - 1;
              let idyR: u32 = u32(dyR);

              for (var wC: u32 = 0; wC < uniforms.effective_filter_dims.y; wC = wC + 1) {
                if (wC % uniforms.dilations.y != 0) {
                  continue;
                }
                let dyC = (${at}(dyCCorner) + ${at}(wC)) / ${at}(uniforms.strides.y);
                let wCPerm = uniforms.filter_dims.y - 1 - wC / uniforms.dilations.y;
                if (dyC < 0.0 || dyC >= ${at}(uniforms.Dy_shape[${Xt}]) ||
                    fract(dyC) > 0.0 || wCPerm < 0) {
                  continue;
                }
                wC = wC + uniforms.strides.y - 1;
                let idyC: u32 = u32(dyC);
                var inputChannel = groupId * uniforms.input_channels_per_group;
                for (var d2: u32 = 0; d2 < uniforms.input_channels_per_group; d2 = d2 + ${k}) {
                  let xValue = ${a ? ss.getByOffset(`${ss.indicesToOffset(`${ss.type.indices}(batch, idyR, idyC, inputChannel)`)} / ${k}`) : ss.get("batch", "inputChannel", "idyR", "idyC")};
                  ${bs()}
                  inputChannel = inputChannel + ${k};
                }
              }
            }
            let value = dotProd${n ? ` + bias[d1 / ${C}]` : ""};
            ${Ot.setByOffset("global_idx", "value")};
          `;
            return `
    ${Le.registerUniforms(Ye).declareVariables(...St, Ot)}
      ${Le.mainStart()}
      ${Le.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")};
    ${Ht}}`;
          };
          return { name: "ConvTranspose2D", shaderCache: { hint: `${t.cacheKey};${k}${d}${C}${h === 1}`, inputDependencies: V }, getRunData: () => ({ dispatchGroup: { x: B[0], y: B[1], z: B[2] }, outputs: [{ dims: s ? s(i) : i, dataType: e[0].dataType }], programUniforms: Me }), getShaderSource: Oe };
        };
      }), Tu, qo, Pu, Qo, Xo, Eu, Yo, Jo, Cu, Zc = g(() => {
        Jc(), nn(), Gr(), Tu = (e, t, s, n, i, a) => (e - 1) * t + s + (n - 1) * i + 1 - a, qo = (e, t, s, n, i) => {
          let a = Math.floor(e / 2);
          t === "SAME_UPPER" ? (s[n] = a, s[i] = e - a) : t === "SAME_LOWER" && (s[n] = e - a, s[i] = a);
        }, Pu = (e, t, s, n, i, a, o, u, p, h) => {
          let k = e.length - 2, C = h.length === 0;
          p.length < k && p.push(...Array(k - p.length).fill(0));
          let d = e[0], z = t[u ? 3 : 1] * i;
          for (let B = 0, V = e.length - k - (u ? 1 : 0); B < k; ++B, ++V) {
            let Z = e[V], ee = C ? Z * o[B] : h[B], X = Tu(Z, o[B], a[B], t[V], s[B], ee);
            qo(X, n, a, B, B + k), C && h.push(o[B] * (Z - 1) + p[B] + (t[V] - 1) * s[B] + 1 - a[B] - a[B + k]);
          }
          h.splice(0, 0, d), h.splice(u ? 3 : 1, 0, z);
        }, Qo = (e, t) => {
          let s = e.kernelShape.slice();
          if (e.kernelShape.length === 0 || e.kernelShape.reduce((C, d) => C * d, 1) === 0) {
            s.length = 0;
            for (let C = 2; C < t[1].dims.length; ++C) s.push(t[1].dims[C]);
          }
          let n = e.format === "NHWC";
          s.splice(0, 0, t[1].dims[0]), s.splice(n ? 3 : 1, 0, t[1].dims[1]);
          let i = e.pads.slice(), a = e.outputShape.slice(), o = e.outputPadding.slice(), u = t[0].dims, p = e.dilations.slice();
          if (p.reduce((C, d) => C + d, 0) === 0) {
            let C = t[0].dims.length - 2;
            p = new Array(C).fill(1);
          }
          let h = e.strides.slice();
          if (h.reduce((C, d) => C + d, 0) === 0) {
            let C = t[0].dims.length - 2;
            h = new Array(C).fill(1);
          }
          Pu(u, s, p, e.autoPad, e.group, i, h, n, o, a);
          let k = Object.assign({}, e);
          return Object.assign(k, { kernelShape: s, pads: i, outputPadding: o, outputShape: a, dilations: p, strides: h }), k;
        }, Xo = (e) => {
          let t = Ao(e), s = e.format, n = ["NOTSET", "VALID", "SAME_UPPER", "SAME_LOWER"][typeof e.autoPad > "u" ? 0 : e.autoPad], i = e.dilations, a = e.group, o = e.kernelShape, u = e.pads, p = e.strides, h = e.wIsConst(), k = e.outputPadding, C = e.outputShape;
          return { autoPad: n, format: s, dilations: i, group: a, kernelShape: o, outputPadding: k, outputShape: C, pads: u, strides: p, wIsConst: h, ...t, cacheKey: `${e.format};${t.activation};` };
        }, Eu = (e, t) => {
          if (!e || e.length !== 2 && e.length !== 3) throw new Error("Conv requires 2 or 3 inputs");
          if (e[0].dims.length !== 4 && e[0].dims.length !== 3) throw new Error("currently only support 2-dimensional conv");
          if (e[0].dims.length !== e[1].dims.length) throw new Error("filter does not have same dimension as input");
          let s = e[0].dims[t.format === "NHWC" ? e[0].dims.length - 1 : 1], n = e[1].dims[0];
          if (s !== n) throw new Error("FILTER_IN_CHANNEL should be equal to DATA_CHANNEL");
          let i = e[1].dims[1] * t.group;
          if (e.length === 3 && (e[2].dims.length !== 1 || e[2].dims[0] !== i)) throw new Error("invalid bias");
          let a = e[0].dims.length - 2;
          if (t.dilations.reduce((o, u) => o + u, 0) > 0 && t.dilations.length !== a) throw new Error(`dilations should be ${a}D`);
          if (t.strides.reduce((o, u) => o + u, 0) > 0 && t.strides.length !== a) throw new Error(`strides should be ${a}D`);
          if (t.pads.reduce((o, u) => o + u, 0) > 0 && t.pads.length !== a * 2) throw new Error(`pads should be ${a * 2}D`);
          if (t.outputPadding.length !== a && t.outputPadding.length !== 0) throw new Error(`output_padding should be ${a}D`);
          if (t.kernelShape.reduce((o, u) => o + u, 0) > 0 && t.kernelShape.length !== 0 && t.kernelShape.length !== e[1].dims.length - 2) throw new Error("invalid kernel shape");
          if (t.outputShape.length !== 0 && t.outputShape.length !== e[0].dims.length - 2) throw new Error("invalid output shape");
        }, Yo = (e, t, s, n) => {
          let i = e.kernelCustomData.wT ?? e.compute(cr(t[1], [2, 3, 0, 1]), { inputs: [1], outputs: [s.wIsConst ? -2 : -1] })[0];
          s.wIsConst && !e.kernelCustomData.wT && (e.kernelCustomData.wT = i);
          let a = [t[0], i];
          t.length === 3 && a.push(t[2]), e.compute(xu(a, s, n), { inputs: a });
        }, Jo = (e, t) => {
          let s = t.format === "NHWC", n = [e.inputs[0].reshape(s ? [e.inputs[0].dims[0], 1, e.inputs[0].dims[1], e.inputs[0].dims[2]] : [e.inputs[0].dims[0], e.inputs[0].dims[1], 1, e.inputs[0].dims[2]]), e.inputs[1].reshape([e.inputs[1].dims[0], e.inputs[1].dims[1], 1, e.inputs[1].dims[2]])];
          e.inputs.length === 3 && n.push(e.inputs[2]);
          let i = t.kernelShape;
          (i.length === 0 || i[0] === 0) && (i = [e.inputs[1].dims[2]]);
          let a = t.dilations;
          (a.length === 0 || a[0] === 0) && (a = [1]);
          let o = t.strides;
          (o.length === 0 || o[0] === 0) && (o = [1]);
          let u = t.pads;
          u.length === 0 && (u = [0, 0]), u = [0, u[0], 0, u[1]], o = [1].concat(o), a = [1].concat(a), i = [1].concat(i);
          let p = Qo({ ...t, pads: u, strides: o, dilations: a, kernelShape: i }, n);
          Yo(e, n, p, (h) => s ? [h[0], h[2], h[3]] : [h[0], h[1], h[3]]);
        }, Cu = (e, t) => {
          if (Eu(e.inputs, t), e.inputs[0].dims.length === 3) Jo(e, t);
          else {
            let s = Qo(t, e.inputs);
            Yo(e, e.inputs, s);
          }
        };
      }), Zo, ku, Su, ep = g(() => {
        zt(), Ft(), rs(), Yt(), Zo = (e, t, s, n) => {
          let i = ze.size(t), a = t.length, o = qe("input", e, a), u = It("output", e, a), p = s.dataType === 6 ? s.getInt32Array()[0] : Number(s.getBigInt64Array()[0]), h = ze.normalizeAxis(p, a), k = (C) => {
            let d = ` i32(${o.indicesGet("inputIndices", "uniforms.axis")}) `, z = $t("uniforms.input_shape", "uniforms.axis", a), B = n.reverse ? d + (n.exclusive ? " + 1" : "") : "0", V = n.reverse ? z : d + (n.exclusive ? "" : " + 1");
            return `
                ${C.registerUniform("outputSize", "u32").registerUniform("axis", "u32").declareVariables(o, u)}
                ${C.mainStart()}
                  ${C.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
                  var inputIndices = ${u.offsetToIndices("global_idx")};
                  var sum = ${u.type.value}(0);
                  let first : i32 = ${B};
                  let last : i32 = ${V};
                  for (var i : i32 = first; i < last; i++) {
                    ${o.indicesSet("inputIndices", "uniforms.axis", "u32(i)")};
                    sum = sum + ${o.getByIndices("inputIndices")};
                  }
                  ${u.setByOffset("global_idx", "sum")};
                }`;
          };
          return { name: "CumSum", shaderCache: { hint: n.cacheKey, inputDependencies: ["rank"] }, getRunData: () => ({ outputs: [{ dims: t, dataType: e }], dispatchGroup: { x: Math.ceil(i / 64) }, programUniforms: [{ type: 12, data: i }, { type: 12, data: h }, ...yt(t, t)] }), getShaderSource: k };
        }, ku = (e, t) => {
          let s = e.inputs[0].dims, n = e.inputs[0].dataType, i = e.inputs[1];
          e.compute(Zo(n, s, i, t), { inputs: [0] });
        }, Su = (e) => {
          let t = e.exclusive === 1, s = e.reverse === 1;
          return Bt({ exclusive: t, reverse: s });
        };
      }), $u, ea, Au, Iu, Fu, tp = g(() => {
        zt(), Ft(), rs(), Yt(), $u = (e) => {
          if (!e || e.length !== 1) throw new Error("DepthToSpace requires 1 input.");
          if (e[0].dims.length !== 4) throw new Error("DepthToSpace requires 4D input.");
        }, ea = (e, t, s, n) => {
          let i = [];
          i.push(`fn perm(i: ${n.type.indices}) -> ${s.type.indices} {
    var a: ${s.type.indices};`);
          for (let a = 0; a < t; ++a) i.push(s.indicesSet("a", e[a], `i[${a}]`));
          return i.push("return a;}"), i.join(`
`);
        }, Au = (e, t) => {
          let s, n, i, a, o, u, p = t.format === "NHWC", h = t.blocksize, k = t.mode === "DCR";
          p ? ([s, n, i, a] = e.dims, o = k ? [s, n, i, h, h, a / h ** 2] : [s, n, i, a / h ** 2, h, h], u = k ? [0, 1, 3, 2, 4, 5] : [0, 1, 4, 2, 5, 3]) : ([s, n, i, a] = [e.dims[0], e.dims[2], e.dims[3], e.dims[1]], o = k ? [s, h, h, a / h ** 2, n, i] : [s, a / h ** 2, h, h, n, i], u = k ? [0, 3, 4, 1, 5, 2] : [0, 1, 4, 2, 5, 3]);
          let C = e.reshape(o), d = C.dims.length, z = e.dataType, B = qe("a", z, d), V = It("output", z, d), Z = (ee) => `
  ${ee.registerUniform("output_size", "u32").declareVariables(B, V)}

  ${ea(u, d, B, V)}

  ${ee.mainStart()}
    ${ee.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let indices = ${V.offsetToIndices("global_idx")};
    let aIndices = perm(indices);

    ${V.setByOffset("global_idx", B.getByIndices("aIndices"))}
  }`;
          return { name: "DepthToSpace", shaderCache: { hint: `${e.dims};${t.blocksize};${t.mode}`, inputDependencies: ["rank"] }, getRunData: (ee) => {
            let X = p ? [s, n * h, i * h, a / h ** 2] : [s, a / h ** 2, n * h, i * h], he = ze.size(X), pe = C.dims, Me = ze.sortBasedOnPerm(pe, u);
            return { outputs: [{ dims: X, dataType: ee[0].dataType }], dispatchGroup: { x: Math.ceil(he / 64) }, programUniforms: [{ type: 12, data: he }, ...yt(pe, Me)] };
          }, getShaderSource: Z };
        }, Iu = (e, t) => {
          $u(e.inputs), e.compute(Au(e.inputs[0], t));
        }, Fu = (e) => Bt({ blocksize: e.blocksize, mode: e.mode, format: e.format });
      }), yi, Un, Mi, Ou, Du, ta, Lu, sa, Kr, zu, Bu, sp = g(() => {
        zt(), Ft(), rs(), Yt(), yi = "[a-zA-Z]|\\.\\.\\.", Un = "(" + yi + ")+", Mi = "^" + Un + "$", Ou = "(" + Un + ",)*" + Un, Du = "^" + Ou + "$", ta = class {
          constructor(e = -1) {
            this.symbolToIndices = /* @__PURE__ */ new Map(), this.inputIndex = e;
          }
          addSymbol(e, t) {
            let s = this.symbolToIndices.get(e);
            s === void 0 ? s = [t] : s.push(t), this.symbolToIndices.set(e, s);
          }
        }, Lu = class {
          constructor(e, t) {
            var i;
            this.equation = t, this.hasEllipsis = !1, this.symbolToInfo = /* @__PURE__ */ new Map(), this.lhs = new Array(), this.outputDims = [];
            let [s, n] = t.includes("->") ? t.split("->", 2) : [t, ""];
            if (!s.match(RegExp(Du))) throw new Error("Invalid LHS term");
            if (s.split(",").forEach((a, o) => {
              let u = e[o].dims.slice();
              if (!a.match(RegExp(Mi))) throw new Error("Invalid LHS term");
              let p = this.processTerm(a, !0, u, o);
              this.lhs.push(p);
            }), n === "") n += [...this.symbolToInfo.entries()].filter(([a, o]) => o.count === 1 || a === "...").map(([a]) => a).join("");
            else if (!n.match(RegExp(Un))) throw new Error("Invalid RHS");
            (i = n.match(RegExp(yi, "g"))) == null || i.forEach((a) => {
              if (a === "...") this.outputDims = this.outputDims.concat(this.ellipsisDims);
              else {
                let o = this.symbolToInfo.get(a);
                if (o === void 0) throw new Error("Invalid RHS symbol");
                this.outputDims.push(o.dimValue);
              }
            }), this.rhs = this.processTerm(n, !1, this.outputDims);
          }
          addSymbol(e, t, s) {
            let n = this.symbolToInfo.get(e);
            if (n !== void 0) {
              if (n.dimValue !== t && n.count !== 1) throw new Error("Dimension mismatch");
              n.count++, n.inputIndices.push(s);
            } else n = { count: 1, dimValue: t, inputIndices: [s] };
            this.symbolToInfo.set(e, n);
          }
          processTerm(e, t, s, n = -1) {
            let i = s.length, a = !1, o = [], u = 0;
            if (!e.match(RegExp(Mi)) && !t && e !== "") throw new Error("Invalid LHS term");
            let p = e.match(RegExp(yi, "g")), h = new ta(n);
            return p == null || p.forEach((k, C) => {
              if (k === "...") {
                if (a) throw new Error("Only one ellipsis is allowed per input term");
                a = !0;
                let d = i - p.length + 1;
                if (d < 0) throw new Error("Ellipsis out of bounds");
                if (o = s.slice(u, u + d), this.hasEllipsis) {
                  if (this.ellipsisDims.length !== o.length || this.ellipsisDims.toString() !== o.toString()) throw new Error("Ellipsis dimensions mismatch");
                } else if (t) this.hasEllipsis = !0, this.ellipsisDims = o;
                else throw new Error("Ellipsis must be specified in the LHS");
                for (let z = 0; z < o.length; z++) {
                  let B = String.fromCharCode(48 + z);
                  h.addSymbol(B, C + z), this.addSymbol(B, s[u++], n);
                }
              } else h.addSymbol(k, C + (this.hasEllipsis ? this.ellipsisDims.length - 1 : 0)), this.addSymbol(k, s[u++], n);
            }), h;
          }
        }, sa = (e) => e + "_max", Kr = (e, t, s, n) => {
          let i = e.map((h) => h.length).map((h, k) => qe(`input${k}`, t, h)), a = ze.size(n), o = It("output", t, n.length), u = [...s.symbolToInfo.keys()].filter((h) => !s.rhs.symbolToIndices.has(h)), p = (h) => {
            let k = [], C = "var prod = 1.0;", d = "var sum = 0.0;", z = "sum += prod;", B = [], V = [], Z = [], ee = [], X = s.symbolToInfo.size === s.rhs.symbolToIndices.size;
            s.symbolToInfo.forEach((pe, Me) => {
              var Oe;
              if (s.rhs.symbolToIndices.has(Me)) {
                let Le = (Oe = s.rhs.symbolToIndices.get(Me)) == null ? void 0 : Oe[0];
                Le !== void 0 && s.lhs.forEach((Ye, at) => {
                  if (pe.inputIndices.includes(at)) {
                    let Pt = Ye.symbolToIndices.get(Me);
                    if (Pt === void 0) throw new Error("Invalid symbol error");
                    Pt.forEach((Xt) => {
                      k.push(`${i[at].indicesSet(`input${at}Indices`, Xt, o.indicesGet("outputIndices", Le))}`);
                    });
                  }
                });
              } else s.lhs.forEach((Le, Ye) => {
                if (pe.inputIndices.includes(Ye)) {
                  let at = Le.symbolToIndices.get(Me);
                  if (at === void 0) throw new Error("Invalid symbol error");
                  at.forEach((Pt) => {
                    B.push(`${i[Ye].indicesSet(`input${Ye}Indices`, Pt, `${Me}`)}`);
                  }), ee.push(`prod *= ${i[Ye].getByIndices(`input${Ye}Indices`)};`);
                }
              }), V.push(`for(var ${Me}: u32 = 0; ${Me} < uniforms.${sa(Me)}; ${Me}++) {`), Z.push("}");
            });
            let he = X ? [...k, `let sum = ${i.map((pe, Me) => pe.getByIndices(`input${Me}Indices`)).join(" * ")};`] : [...k, d, ...V, ...B, C, ...ee, z, ...Z];
            return `
            ${h.registerUniforms(u.map((pe) => ({ name: `${sa(pe)}`, type: "u32" }))).registerUniform("outputSize", "u32").declareVariables(...i, o)}

            ${h.mainStart()}
            ${h.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
            var outputIndices = ${o.offsetToIndices("global_idx")};
            ${i.map((pe, Me) => `var input${Me}Indices: ${i[Me].type.indices};`).join(`
`)}
            ${he.join(`
`)};
            ${o.setByOffset("global_idx", "sum")};
          }`;
          };
          return { name: "Einsum", shaderCache: { hint: s.equation, inputDependencies: e.map(() => "rank") }, getRunData: () => {
            let h = u.filter((C) => s.symbolToInfo.has(C)).map((C) => {
              var d;
              return { type: 12, data: ((d = s.symbolToInfo.get(C)) == null ? void 0 : d.dimValue) || 0 };
            });
            h.push({ type: 12, data: a });
            let k = e.map((C, d) => [...yt(C)]).reduce((C, d) => C.concat(d), h);
            return k.push(...yt(n)), { outputs: [{ dims: n, dataType: t }], dispatchGroup: { x: Math.ceil(a / 64) }, programUniforms: k };
          }, getShaderSource: p };
        }, zu = (e, t) => {
          let s = new Lu(e.inputs, t.equation), n = s.outputDims, i = e.inputs.map((a, o) => a.dims);
          e.compute(Kr(i, e.inputs[0].dataType, s, n));
        }, Bu = (e) => {
          let t = e.equation.replace(/\s+/g, "");
          return Bt({ equation: t });
        };
      }), Ru, bi, Nu, ju, Uu, rp = g(() => {
        zt(), Ft(), Yt(), Ru = (e) => {
          if (!e || e.length !== 2) throw new Error("Expand requires 2 input.");
          let t = e[0].dims, s = Array.from(e[1].getBigInt64Array(), Number), n = s.length < t.length ? 0 : s.length - t.length, i = t.length < s.length ? 0 : t.length - s.length;
          for (; n < s.length && i < t.length; ++n, ++i) if (s[n] !== t[i] && s[n] !== 1 && t[i] !== 1) throw new Error("Expand requires shape to be broadcastable to input");
        }, bi = (e, t) => {
          let s = e.length - t.length, n = [];
          for (let i = 0; i < s; ++i) n.push(e[i]);
          for (let i = 0; i < t.length; ++i) n.push(t[i] === 1 ? e[i + s] : t[i]);
          return n;
        }, Nu = (e, t) => e.length > t.length ? bi(e, t) : bi(t, e), ju = (e) => {
          let t = e[0].dims, s = Array.from(e[1].getBigInt64Array(), Number), n = Nu(t, s), i = e[0].dataType, a = i === 9 || ze.size(t) === 1, o = i === 9 || t.length > 0 && t[t.length - 1] % 4 === 0 ? 4 : 1, u = a || n.length > 0 && n[n.length - 1] % 4 === 0 ? 4 : 1, p = Math.ceil(ze.size(n) / u), h = (C) => {
            let d = qe("input", i, t.length, o), z = It("output", i, n.length, u), B;
            if (i === 9) {
              let V = (Z, ee, X = "") => `
          let outputIndices${ee} = ${z.offsetToIndices(`outputOffset + ${ee}u`)};
          let offset${ee} = ${d.broadcastedIndicesToOffset(`outputIndices${ee}`, z)};
          let index${ee} = offset${ee} / 4u;
          let component${ee} = offset${ee} % 4u;
          ${Z}[${ee}] = ${X}(${d.getByOffset(`index${ee}`)}[component${ee}]);
        `;
              B = `
        let outputOffset = global_idx * ${u};
        var data = vec4<u32>(0);
        ${V("data", 0, "u32")}
        ${V("data", 1, "u32")}
        ${V("data", 2, "u32")}
        ${V("data", 3, "u32")}
        ${z.setByOffset("global_idx", "data")}
      }`;
            } else B = `
        let outputIndices = ${z.offsetToIndices(`global_idx * ${u}`)};
        let inputOffset = ${d.broadcastedIndicesToOffset("outputIndices", z)};
        let data = ${z.type.value}(${d.getByOffset(`inputOffset / ${o}`)});
        ${z.setByOffset("global_idx", "data")}
      }`;
            return `
    ${C.registerUniform("vec_size", "u32").declareVariables(d, z)}
    ${C.mainStart()}
    ${C.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
    ${B}`;
          }, k = [{ type: 12, data: p }, ...yt(t, n)];
          return { name: "Expand", shaderCache: { hint: `${n.length};${o}${u}`, inputDependencies: ["rank"] }, getShaderSource: h, getRunData: () => ({ outputs: [{ dims: n, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(p / 64) }, programUniforms: k }) };
        }, Uu = (e) => {
          Ru(e.inputs), e.compute(ju(e.inputs), { inputs: [0] });
        };
      }), vi, Vu, np = g(() => {
        zt(), Ft(), Yt(), To(), vi = (e) => {
          let t = e[0].dataType, s = ze.size(e[0].dims), n = ze.size(e[1].dims), i = n % 4 === 0, a = (o) => {
            let u = qe("x", t, [1], 4), p = qe("bias", t, [1], 4), h = It("y", t, [1], 4), k = [{ name: "output_vec_size", type: "u32" }, { name: "bias_size", type: "u32" }], C = (z) => `
      let bias${z}_offset: u32 = (global_idx * 4 + ${z}) % uniforms.bias_size;
      let bias${z} = ${p.getByOffset(`bias${z}_offset / 4`)}[bias${z}_offset % 4];`, d = i ? `
      let bias = ${p.getByOffset("global_idx % (uniforms.bias_size / 4)")};` : `${C(0)}${C(1)}${C(2)}${C(3)}
      let bias = ${u.type.value}(bias0, bias1, bias2, bias3);`;
            return `${o.registerUniforms(k).declareVariables(u, p, h)}

    ${vo(Ss(t))}

    ${o.mainStart(ir)}
      ${o.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_vec_size")}

      let x = ${u.getByOffset("global_idx")};
      ${d}
      let x_in = x + bias;
      ${h.setByOffset("global_idx", ci("x_in"))}
    }`;
          };
          return { name: "FastGeluWithBias", shaderCache: { hint: `${i}`, inputDependencies: ["type", "type"] }, getShaderSource: a, getRunData: (o) => ({ outputs: [{ dims: o[0].dims, dataType: o[0].dataType }], programUniforms: [{ type: 12, data: Math.ceil(s / 4) }, { type: 12, data: n }], dispatchGroup: { x: Math.ceil(s / ir / 4) } }) };
        }, Vu = (e) => {
          e.inputs.length < 2 || ze.size(e.inputs[1].dims) === 0 ? Wl(e) : e.compute(vi(e.inputs));
        };
      }), Wu, Vn, Gu, Ku, ip = g(() => {
        zt(), Ft(), rs(), Yt(), Wu = (e) => {
          if (!e || e.length !== 2) throw new Error("Gather requires 2 inputs.");
        }, Vn = (e, t) => {
          let s = e[0].dims, n = e[1].dims, i = s.length, a = ze.normalizeAxis(t.axis, i), o = s.slice(0);
          o.splice(a, 1, ...n);
          let u = s[a], p = e[0].dataType === 9 ? 4 : 1, h = Math.ceil(ze.size(o) / p), k = [{ type: 12, data: h }, { type: 6, data: u }, { type: 12, data: a }, ...yt(e[0].dims, e[1].dims, o)], C = (d) => {
            let z = qe("data", e[0].dataType, e[0].dims.length, p), B = qe("inputIndices", e[1].dataType, e[1].dims.length), V = It("output", e[0].dataType, o.length, p), Z = (X) => {
              let he = n.length, pe = `var indicesIndices${X}  = ${B.type.indices}(0);`;
              for (let Me = 0; Me < he; Me++) pe += `${he > 1 ? `indicesIndices${X}[${Me}]` : `indicesIndices${X}`} = ${o.length > 1 ? `outputIndices${X}[uniforms.axis + ${Me}]` : `outputIndices${X}`};`;
              pe += `
          var idx${X} = ${B.getByIndices(`indicesIndices${X}`)};
          if (idx${X} < 0) {
            idx${X} = idx${X} + uniforms.axisDimLimit;
          }
          var dataIndices${X} : ${z.type.indices};
        `;
              for (let Me = 0, Oe = 0; Me < i; Me++) Me === a ? (pe += `${i > 1 ? `dataIndices${X}[${Me}]` : `dataIndices${X}`} = u32(idx${X});`, Oe += he) : (pe += `${i > 1 ? `dataIndices${X}[${Me}]` : `dataIndices${X}`} = ${o.length > 1 ? `outputIndices${X}[${Oe}]` : `outputIndices${X}`};`, Oe++);
              return pe;
            }, ee;
            if (e[0].dataType === 9) {
              let X = (he, pe, Me = "") => `
          let outputIndices${pe} = ${V.offsetToIndices(`outputOffset + ${pe}u`)};
          ${Z(pe)};
          let offset${pe} = ${z.indicesToOffset(`dataIndices${pe}`)};
          let index${pe} = offset${pe} / 4u;
          let component${pe} = offset${pe} % 4u;
          ${he}[${pe}] = ${Me}(${z.getByOffset(`index${pe}`)}[component${pe}]);
        `;
              ee = `
        let outputOffset = global_idx * ${p};
        var value = vec4<u32>(0);
        ${X("value", 0, "u32")}
        ${X("value", 1, "u32")}
        ${X("value", 2, "u32")}
        ${X("value", 3, "u32")}
        ${V.setByOffset("global_idx", "value")}
      `;
            } else ee = `
      let outputIndices = ${V.offsetToIndices("global_idx")};
      ${Z("")};
      let value = ${z.getByIndices("dataIndices")};
      ${V.setByOffset("global_idx", "value")};
      `;
            return `
      ${d.registerUniform("outputSize", "u32").registerUniform("axisDimLimit", "i32").registerUniform("axis", "u32").declareVariables(z, B, V)}
      ${d.mainStart()}
        ${d.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
        ${ee}
      }`;
          };
          return { name: "Gather", shaderCache: { hint: t.cacheKey, inputDependencies: ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: o, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(h / 64) }, programUniforms: k }), getShaderSource: C };
        }, Gu = (e) => Bt({ axis: e.axis }), Ku = (e, t) => {
          let s = e.inputs;
          Wu(s), e.compute(Vn(e.inputs, t));
        };
      }), Hu, xi, qu, op = g(() => {
        zt(), Ft(), Yt(), Hu = (e, t, s, n, i, a, o, u, p) => {
          let h = [{ type: 12, data: a }, { type: 12, data: n }, { type: 12, data: i }, { type: 12, data: s }, { type: 12, data: o }, { type: 12, data: u }, { type: 12, data: p }], k = [a];
          h.push(...yt(t.dims, k));
          let C = (d) => {
            let z = qe("indices_data", t.dataType, t.dims.length), B = It("input_slice_offsets_data", 12, 1, 1), V = [z, B], Z = [{ name: "output_size", type: "u32" }, { name: "batch_dims", type: "u32" }, { name: "input_dims", type: "u32", length: i.length }, { name: "sizes_from_slice_dims_data", type: "u32", length: s.length }, { name: "num_slices_per_batch", type: "u32" }, { name: "input_batch_stride", type: "u32" }, { name: "num_slice_dims", type: "u32" }];
            return `
  ${d.registerUniforms(Z).declareVariables(...V)}
  ${d.mainStart()}
    ${d.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let batch_idx = global_idx / uniforms.num_slices_per_batch;
    let base_offset = batch_idx * uniforms.input_batch_stride;

    let slice_indices_base_offset = global_idx * uniforms.num_slice_dims;
    var relative_slice_offset = 0;
    for (var dim_idx = 0u; dim_idx < uniforms.num_slice_dims; dim_idx ++) {
      var index = i32(indices_data[dim_idx + slice_indices_base_offset].x);
      let input_dim_idx = uniforms.batch_dims + dim_idx;
      if (index < 0) {
        ${i.length === 1 ? "index += i32(uniforms.input_dims);" : "index += i32(uniforms.input_dims[input_dim_idx]);"}
      }
      ${s.length === 1 ? "relative_slice_offset += index * i32(uniforms.sizes_from_slice_dims_data);" : "relative_slice_offset += index * i32(uniforms.sizes_from_slice_dims_data[dim_idx]);"}
    }

    input_slice_offsets_data[global_idx] =  base_offset + u32(relative_slice_offset);
  }`;
          };
          return e.compute({ name: "computeSliceOffsets", shaderCache: { hint: `${i.length}_${s.length}`, inputDependencies: ["rank"] }, getRunData: () => ({ outputs: [{ dims: k, dataType: e.inputs[1].dataType }], dispatchGroup: { x: Math.ceil(a / 64) }, programUniforms: h }), getShaderSource: C }, { inputs: [t], outputs: [-1] })[0];
        }, xi = (e, t) => {
          let s = e.inputs, n = s[0].dims, i = s[0].dataType, a = s[1].dims, o = a[a.length - 1], u = ze.sizeToDimension(a, a.length - 1), p = ze.sizeFromDimension(n, t.batchDims + o), h = ze.sizeToDimension(n, t.batchDims), k = ze.sizeFromDimension(n, t.batchDims), C = u / h, d = new Array(o), z = p;
          for (let pe = 0; pe < o; ++pe) d[o - 1 - pe] = z, z *= n[t.batchDims + o - 1 - pe];
          let B = Hu(e, s[1], d, t.batchDims, n, u, C, k, o), V = t.batchDims + o;
          if (V > n.length) throw new Error("last dimension of indices must not be larger than rank of input tensor");
          let Z = a.slice(0, -1).concat(n.slice(V)), ee = ze.size(Z), X = [{ type: 12, data: ee }, { type: 12, data: p }, ...yt(s[0].dims, B.dims, Z)], he = (pe) => {
            let Me = qe("data", s[0].dataType, s[0].dims.length), Oe = qe("slice_offsets", 12, B.dims.length), Le = It("output", s[0].dataType, Z.length);
            return `
          ${pe.registerUniform("output_size", "u32").registerUniform("slice_size", "u32").declareVariables(Me, Oe, Le)}
            ${pe.mainStart()}
            ${pe.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          let slice_offset = slice_offsets[global_idx / uniforms.slice_size];
          output[global_idx] = data[u32(slice_offset) + global_idx % uniforms.slice_size];
        }`;
          };
          e.compute({ name: "GatherND", shaderCache: { hint: t.cacheKey, inputDependencies: ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: Z, dataType: i }], dispatchGroup: { x: Math.ceil(ee / 64) }, programUniforms: X }), getShaderSource: he }, { inputs: [s[0], B] });
        }, qu = (e) => ({ batchDims: e.batch_dims, cacheKey: "" });
      }), Qu, ap, Xu, Yu, lp = g(() => {
        zt(), Ft(), rs(), Yt(), Qu = (e, t) => {
          if (e.length < 3 || e.length > 4) throw new Error("GatherBlockQuantized requires 3 or 4 inputs.");
          let s = ze.normalizeAxis(t.quantizeAxis, e[0].dims.length), n = t.blockSize, i = e[0], a = e[2], o = e.length === 4 ? e[3] : void 0;
          if (a.dims.length !== i.dims.length || !i.dims.map((u, p) => p === s ? Math.ceil(u / n) === a.dims[p] : u === a.dims[p]).reduce((u, p) => u && p, !0)) throw new Error("Scales must have the same rank as the input tensor and the dims should match except on gatherAxis.");
          if (o) {
            if (o.dataType !== i.dataType) throw new Error("Zero point must have the same data type as the input tensor.");
            if (o.dims.length !== a.dims.length || !o.dims.map((u, p) => u === a.dims[p]).reduce((u, p) => u && p, !0)) throw new Error("Zero point must have the same rank as the input tensor and the dims should match except on quantizeAxis.");
          }
        }, ap = (e, t) => {
          let s = e[0].dims, n = e[1].dims, i = s.length, a = ze.normalizeAxis(t.gatherAxis, i), o = ze.normalizeAxis(t.quantizeAxis, i), u = s.slice(0);
          u.splice(a, 1, ...n);
          let p = ze.size(u), h = e[2].dataType, k = e[0].dataType === 22, C = [{ type: 12, data: p }, { type: 12, data: o }, { type: 12, data: a }, { type: 12, data: t.blockSize }, ...yt(...e.map((z, B) => z.dims), u)], d = (z) => {
            let B = qe("data", e[0].dataType, e[0].dims.length), V = qe("inputIndices", e[1].dataType, e[1].dims.length), Z = qe("scales", e[2].dataType, e[2].dims.length), ee = e.length > 3 ? qe("zeroPoint", e[3].dataType, e[3].dims.length) : void 0, X = It("output", h, u.length), he = [B, V, Z];
            ee && he.push(ee);
            let pe = [{ name: "output_size", type: "u32" }, { name: "quantize_axis", type: "u32" }, { name: "gather_axis", type: "u32" }, { name: "block_size", type: "u32" }];
            return `
        ${z.registerUniforms(pe).declareVariables(...he, X)}
        ${z.mainStart()}
        let output_indices = ${X.offsetToIndices("global_idx")};
        var indices_indices = ${V.type.indices}(0);
        ${n.length > 1 ? `
          for (var i: u32 = 0; i < ${n.length}; i++) {
            let index = ${X.indicesGet("output_indices", "uniforms.gather_axis + i")};
            ${V.indicesSet("indices_indices", "i", "index")};
          }` : `indices_indices = ${X.indicesGet("output_indices", "uniforms.gather_axis")};`};
        var data_indices = ${B.type.indices}(0);
        for (var i: u32 = 0; i < uniforms.gather_axis; i++) {
          let index = ${X.indicesGet("output_indices", "i")};
          ${B.indicesSet("data_indices", "i", "index")};
        }
        var index_from_indices = ${V.getByIndices("indices_indices")};
        if (index_from_indices < 0) {
          index_from_indices += ${s[a]};
        }
        ${B.indicesSet("data_indices", "uniforms.gather_axis", "u32(index_from_indices)")};
        for (var i = uniforms.gather_axis + 1; i < ${u.length}; i++) {
          let index = ${X.indicesGet("output_indices", `i + ${n.length} - 1`)};
          ${B.indicesSet("data_indices", "i", "index")};
        }
        let data_offset = ${B.indicesToOffset("data_indices")};
        let data_index = data_offset % 8;
        // Convert 4-bit packed data to 8-bit packed data.
        let packed_4bit_quantized_data = ${B.getByOffset("data_offset / 8")};
        let packed_8bit_quantized_data = (packed_4bit_quantized_data >> (4 * (data_index % 2))) & 0x0f0f0f0f;
        let quantized_data_vec = ${k ? "unpack4xI8" : "unpack4xU8"}(u32(packed_8bit_quantized_data));
        let quantized_data = quantized_data_vec[data_index / 2];
        var scale_indices = data_indices;
        let quantize_axis_index = ${Z.indicesGet("data_indices", "uniforms.quantize_axis")} / uniforms.block_size;
        ${Z.indicesSet("scale_indices", "uniforms.quantize_axis", "quantize_axis_index")};
        var scale = ${Z.getByIndices("scale_indices")};
        ${ee ? `
              let zero_point_indices = scale_indices;
              let zero_point_offset = ${ee.indicesToOffset("zero_point_indices")};
              let zero_point_index = zero_point_offset % 8;
              let packed_4bit_zero_points = ${ee.getByOffset("zero_point_offset / 8")};
              let packed_8bit_zero_points = (packed_4bit_zero_points >> (4 * (zero_point_index % 2))) & 0x0f0f0f0f;
              let zero_point_vec = ${k ? "unpack4xI8" : "unpack4xU8"}(u32(packed_8bit_zero_points));
              let zero_point = zero_point_vec[zero_point_index / 2];` : "var zero_point = 0"};
        let dequantized_data = ${Ss(h)}(quantized_data - zero_point) * scale;
        ${X.setByOffset("global_idx", "dequantized_data")};
    }`;
          };
          return { name: "GatherBlockQuantized", shaderCache: { hint: `${t.cacheKey};${e.filter((z, B) => B !== 1).map((z) => z.dims.join("_")).join(";")}`, inputDependencies: Array.from({ length: e.length }, (z, B) => "rank") }, getRunData: () => ({ outputs: [{ dims: u, dataType: h }], dispatchGroup: { x: Math.ceil(p / 64) }, programUniforms: C }), getShaderSource: d };
        }, Xu = (e, t) => {
          let s = e.inputs;
          Qu(s, t), e.compute(ap(e.inputs, t));
        }, Yu = (e) => Bt({ blockSize: e.blockSize, gatherAxis: e.gatherAxis, quantizeAxis: e.quantizeAxis });
      }), xn, Ju, Zu, ed, up = g(() => {
        zt(), Ft(), rs(), Yt(), xn = (e) => {
          if (!e || e.length !== 2) throw new Error("GatherElements requires 2 inputs.");
          if (e[0].dims.length < 1) throw new Error("GatherElements requires that the data input be rank >= 1.");
          if (e[0].dims.length !== e[1].dims.length) throw new Error(`GatherElements requires that the data input and
                     indices input tensors be of same rank.`);
        }, Ju = (e, t) => {
          let s = e[0].dims, n = e[0].dataType, i = s.length, a = e[1].dims, o = e[1].dataType, u = ze.normalizeAxis(t.axis, i), p = s[u], h = a.slice(0), k = ze.size(h), C = qe("input", n, i), d = qe("indicesInput", o, a.length), z = It("output", n, h.length), B = [{ type: 12, data: k }, { type: 6, data: p }, { type: 12, data: u }];
          return B.push(...yt(s, a, h)), { name: "GatherElements", shaderCache: { inputDependencies: ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: h, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(k / 64) }, programUniforms: B }), getShaderSource: (V) => `
      ${V.registerUniform("outputSize", "u32").registerUniform("axisDimLimit", "i32").registerUniform("axis", "u32").declareVariables(C, d, z)}
      ${V.mainStart()}
      ${V.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

      let outputIndices = ${z.offsetToIndices("global_idx")};

      var idx = ${d.getByOffset("global_idx")};
      if (idx < 0) {
        idx = idx + uniforms.axisDimLimit;
      }
      var inputIndices = ${C.type.indices}(outputIndices);
      ${C.indicesSet("inputIndices", "uniforms.axis", "u32(idx)")};
      let value = ${C.getByIndices("inputIndices")};

      ${z.setByOffset("global_idx", "value")};
  }` };
        }, Zu = (e) => Bt({ axis: e.axis }), ed = (e, t) => {
          let s = e.inputs;
          xn(s), e.compute(Ju(e.inputs, t));
        };
      }), td, sd, rd, Ti, qp = g(() => {
        zt(), Ft(), Yt(), td = (e) => {
          if (!e) throw new Error("Input is missing");
          if (e.length < 2 || e.length > 3) throw new Error("Invaid input number.");
          if (e.length === 3 && e[2].dims.length > 2) throw new Error("Invalid input shape of C");
          if (e[0].dataType !== e[1].dataType || e.length === 3 && e[0].dataType !== e[2].dataType) throw new Error("Input types are mismatched");
        }, sd = (e, t) => {
          let s = e[0].dims.slice(), n = e[1].dims.slice(), [i, a, o] = Fr.getShapeOfGemmResult(s, t.transA, n, t.transB, e.length === 3 ? e[2].dims : void 0), u = [i, a];
          if (!u) throw new Error("Can't use gemm on the given tensors");
          let p = 16, h = Math.ceil(a / p), k = Math.ceil(i / p), C = !0, d = ze.size(u), z = [{ type: 12, data: C ? h : d }, { type: 12, data: i }, { type: 12, data: a }, { type: 12, data: o }, { type: 1, data: t.alpha }, { type: 1, data: t.beta }], B = ["type", "type"];
          e.length === 3 && (z.push(...yt(e[2].dims)), B.push("rank")), z.push(...yt(u));
          let V = (ee) => {
            let X = "";
            t.transA && t.transB ? X = "value += a[k * uniforms.M + m] * b[n * uniforms.K + k];" : t.transA && !t.transB ? X = "value += a[k * uniforms.M + m] * b[k * uniforms.N + n];" : !t.transA && t.transB ? X = "value += a[m * uniforms.K + k] * b[n * uniforms.K + k];" : !t.transA && !t.transB && (X = "value += a[m * uniforms.K + k] * b[k * uniforms.N + n];");
            let he = t.alpha === 1 ? "" : "value *= uniforms.alpha;", pe = qe("a", e[0].dataType, e[0].dims), Me = qe("b", e[1].dataType, e[1].dims), Oe = pe.type.value, Le = null, Ye = [pe, Me];
            e.length === 3 && (Le = qe("c", e[2].dataType, e[2].dims.length), Ye.push(Le));
            let at = It("output", e[0].dataType, u.length);
            Ye.push(at);
            let Pt = [{ name: "output_size", type: "u32" }, { name: "M", type: "u32" }, { name: "N", type: "u32" }, { name: "K", type: "u32" }, { name: "alpha", type: "f32" }, { name: "beta", type: "f32" }];
            return `
  ${ee.registerUniforms(Pt).declareVariables(...Ye)}

  ${ee.mainStart()}
    ${ee.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let m = global_idx / uniforms.N;
    let n = global_idx % uniforms.N;

    var value = ${Oe}(0);
    for (var k: u32 = 0u; k < uniforms.K; k++) {
      ${X}
    }

    ${he}
    ${Le != null ? `let cOffset = ${Le.broadcastedIndicesToOffset("vec2(m, n)", at)}; value += ${Oe}(uniforms.beta) * ${Le.getByOffset("cOffset")};` : ""}
    output[global_idx] = value;
  }`;
          }, Z = (ee) => {
            let X = qe("a", e[0].dataType, e[0].dims), he = qe("b", e[1].dataType, e[1].dims), pe = null, Me = [X, he];
            e.length === 3 && (pe = qe("c", e[2].dataType, e[2].dims.length), Me.push(pe));
            let Oe = It("output", e[0].dataType, u.length);
            Me.push(Oe);
            let Le = [{ name: "num_tile_n", type: "u32" }, { name: "M", type: "u32" }, { name: "N", type: "u32" }, { name: "K", type: "u32" }, { name: "alpha", type: "f32" }, { name: "beta", type: "f32" }], Ye = "", at = "";
            t.transA && t.transB ? (at = `
      var col = tile_row_start + local_id.x;
      var row = k_start + local_id.y;
      if (col < uniforms.M && row < uniforms.K) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.M + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${X.type.value}(0);
      }

      col = k_start + local_id.x;
      row = tile_col_start + local_id.y;
      if (col < uniforms.K && row < uniforms.N) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.K + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${he.type.value}(0);
      }
      `, Ye = "value += tile_a[k][local_id.y] * tile_b[local_id.x][k];") : t.transA && !t.transB ? (at = `
      var col = tile_row_start + local_id.x;
      var row = k_start + local_id.y;
      if (col < uniforms.M && row < uniforms.K) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.M + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${X.type.value}(0);
      }

      col = tile_col_start + local_id.x;
      row = k_start + local_id.y;
      if (col < uniforms.N && row < uniforms.K) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.N + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${he.type.value}(0);
      }
      `, Ye = "value += tile_a[k][local_id.y] * tile_b[k][local_id.x];") : !t.transA && t.transB ? (at = `
      var col = k_start + local_id.x;
      var row = tile_row_start + local_id.y;
      if (col < uniforms.K && row < uniforms.M) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.K + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${X.type.value}(0);
      }

      col = k_start + local_id.x;
      row = tile_col_start + local_id.y;
      if (col < uniforms.K && row < uniforms.N) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.K + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${he.type.value}(0);
      }
      `, Ye = "value += tile_a[local_id.y][k] * tile_b[local_id.x][k];") : !t.transA && !t.transB && (at = `
      var col = k_start + local_id.x;
      var row = tile_row_start + local_id.y;
      if (col < uniforms.K && row < uniforms.M) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.K + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${X.type.value}(0);
      }

      col = tile_col_start + local_id.x;
      row = k_start + local_id.y;
      if (col < uniforms.N && row < uniforms.K) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.N + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${he.type.value}(0);
      }
      `, Ye = "value += tile_a[local_id.y][k] * tile_b[k][local_id.x];");
            let Pt = t.alpha === 1 ? "" : "value *= uniforms.alpha;";
            return `
  ${ee.registerUniforms(Le).declareVariables(...Me)}
  var<workgroup> tile_a: array<array<${X.type.storage}, ${p}>, ${p}>;
  var<workgroup> tile_b: array<array<${he.type.storage}, ${p}>, ${p}>;
  ${ee.mainStart([p, p, 1])}
    let tile_col_start = (workgroup_index % uniforms.num_tile_n) * ${p};
    let tile_row_start = (workgroup_index / uniforms.num_tile_n) * ${p};
    let num_tiles = (uniforms.K - 1) / ${p} + 1;
    var k_start = 0u;
    var value = ${Oe.type.value}(0);
    for (var t: u32 = 0u; t < num_tiles; t++) {
      ${at}
      k_start = k_start + ${p};
      workgroupBarrier();

      for (var k: u32 = 0u; k < ${p}; k++) {
        ${Ye}
      }
      workgroupBarrier();
    }

    ${Pt}
    let m = tile_row_start + local_id.y;
    let n = tile_col_start + local_id.x;
    ${pe != null ? `let cOffset = ${pe.broadcastedIndicesToOffset("vec2(m, n)", Oe)}; value += ${Oe.type.value}(uniforms.beta) * ${pe.getByOffset("cOffset")};` : ""}
    if (m < uniforms.M && n < uniforms.N) {
      output[m * uniforms.N + n] = value;
    }
  }`;
          };
          return C ? { name: "GemmShared", shaderCache: { hint: `${t.cacheKey}`, inputDependencies: B }, getRunData: () => ({ outputs: [{ dims: u, dataType: e[0].dataType }], dispatchGroup: { x: h * k }, programUniforms: z }), getShaderSource: Z } : { name: "Gemm", shaderCache: { hint: `${t.cacheKey}`, inputDependencies: B }, getRunData: () => ({ outputs: [{ dims: u, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(d / 64) }, programUniforms: z }), getShaderSource: V };
        }, rd = (e) => {
          let t = e.transA, s = e.transB, n = e.alpha, i = e.beta;
          return { transA: t, transB: s, alpha: n, beta: i, cacheKey: `${e.transA};${e.transB};${e.alpha === 1}` };
        }, Ti = (e, t) => {
          td(e.inputs), e.compute(sd(e.inputs, t));
        };
      }), Er, Dr, on, an, nd, ra, id, od, na, ad, ld, ia, ud, dd, oa = g(() => {
        zt(), Ft(), rs(), Yt(), [Er, Dr, on, an] = [0, 1, 2, 3], nd = (e) => {
          if (e[0].dims.length !== 4) throw new Error("only 4-D tensor is supported.");
          if (e[0].dims.length !== e[1].dims.length) throw new Error("input dimensions must be equal to grid dimensions");
          if (e[0].dims.length - 2 !== e[1].dims[e[1].dims.length - 1]) throw new Error(`last dimension of grid must be equal to ${e[0].dims.length - 2}`);
          if (e[0].dims[0] !== e[1].dims[0]) throw new Error("grid batch size must match input batch size");
        }, ra = `
  fn gs_get_cubic_coeffs(x: f32) -> vec4<f32> {
    let cubic_alpha = -0.75f;
    let x_abs = abs(x);
    var coeffs: vec4<f32>;
    coeffs[0] = (((cubic_alpha * (x_abs + 1) - 5 * cubic_alpha) * (x_abs + 1) + 8 * cubic_alpha) * (x_abs + 1) - 4 * cubic_alpha);
    coeffs[1] = (((cubic_alpha + 2) * x_abs - (cubic_alpha + 3)) * x_abs * x_abs + 1);
    coeffs[2] = (((cubic_alpha + 2) * (1 - x_abs) - (cubic_alpha + 3)) * (1 - x_abs) * (1 - x_abs) + 1);
    coeffs[3] = (((cubic_alpha * (2 - x_abs) - 5 * cubic_alpha) * (2 - x_abs) + 8 * cubic_alpha) * (2 - x_abs) - 4 * cubic_alpha);
    return coeffs;
  }
`, id = (e) => `
  fn gs_bicubic_interpolate(p: mat4x4<${e}>, x: f32, y: f32) -> ${e} {
    var v: vec4<f32>;
    var coeffs = gs_get_cubic_coeffs(x);
    for (var i = 0; i < 4; i++) {
      v[i] = coeffs[0] * p[i][0] + coeffs[1] * p[i][1] + coeffs[2] * p[i][2] + coeffs[3] * p[i][3];
    }
    coeffs = gs_get_cubic_coeffs(y);
    let pixel = ${e}(coeffs[0] * v[0] + coeffs[1] * v[1] + coeffs[2] * v[2] + coeffs[3] * v[3]);
    return pixel;
  }
`, od = (e) => `
  fn gs_denormalize(n: f32, length: i32) -> f32 {
    ${e.alignCorners === 0 ? `
    // alignCorners: false => [-1, 1] to [-0.5, length - 0.5]
    return ((n + 1.0) * f32(length) - 1.0) / 2.0;
    ` : `
    // alignCorners: true => [-1, 1] to [0, length - 1]
    return (n + 1.0) / 2.0 * (f32(length - 1));
    `}
  }
`, na = (e) => `
  ${e.paddingMode === "reflection" ? `
      fn gs_reflect(x: i32, x_min: f32, x_max: f32) -> u32 {
        var dx = 0.0;
        var fx = f32(x);
        let range = x_max - x_min;
        if (fx < x_min) {
          dx = x_min - fx;
          let n = u32(dx / range);
          let r = dx - f32(n) * range;
          if (n % 2 == 0) {
            fx = x_min + r;
          } else {
            fx = x_max - r;
          }
        } else if (fx > x_max) {
          dx = fx - x_max;
          let n = u32(dx / range);
          let r = dx - f32(n) * range;
          if (n % 2 == 0) {
            fx = x_max - r;
          } else {
            fx = x_min + r;
          }
        }
        return u32(fx);
      }` : ""}
`, ad = (e, t, s) => `
  fn pixel_at_grid(r: i32, c: i32, H: i32, W: i32, batch: u32, channel: u32, border: vec4<f32>) -> ${t} {
     var pixel = ${t}(0);
     var indices = vec4<u32>(0);
     indices[${Er}] = batch;
     indices[${Dr}] = channel;` + (() => {
          switch (s.paddingMode) {
            case "zeros":
              return `
          if (r >= 0 && r < H && c >=0 && c < W) {
            indices[${on}] = u32(r);
            indices[${an}] = u32(c);
          }
        `;
            case "border":
              return `
          indices[${on}] = u32(clamp(r, 0, H - 1));
          indices[${an}] = u32(clamp(c, 0, W - 1));
        `;
            case "reflection":
              return `
          indices[${on}] = gs_reflect(r, border[1], border[3]);
          indices[${an}] = gs_reflect(c, border[0], border[2]);
        `;
            default:
              throw new Error(`padding mode ${s.paddingMode} is not supported`);
          }
        })() + `
    return ${e.getByIndices("indices")};
  }
`, ld = (e, t, s) => (() => {
          switch (s.mode) {
            case "nearest":
              return `
          let result = pixel_at_grid(i32(round(y)), i32(round(x)), H_in, W_in, indices[${Er}], indices[${Dr}], border);
        `;
            case "bilinear":
              return `
          let x1 = i32(floor(x));
          let y1 = i32(floor(y));
          let x2 = x1 + 1;
          let y2 = y1 + 1;

          let p11 = pixel_at_grid(y1, x1, H_in, W_in, indices[${Er}], indices[${Dr}], border);
          let p12 = pixel_at_grid(y1, x2, H_in, W_in, indices[${Er}], indices[${Dr}], border);
          let p21 = pixel_at_grid(y2, x1, H_in, W_in, indices[${Er}], indices[${Dr}], border);
          let p22 = pixel_at_grid(y2, x2, H_in, W_in, indices[${Er}], indices[${Dr}], border);

          let dx2 = ${t}(f32(x2) - x);
          let dx1 = ${t}(x - f32(x1));
          let dy2 = ${t}(f32(y2) - y);
          let dy1 = ${t}(y - f32(y1));
          let result = dy2 * (dx2 * p11 + dx1 * p12) + dy1 * (dx2 * p21 + dx1 * p22);
        `;
            case "bicubic":
              return `
          let x0 = i32(floor(x)) - 1;
          let y0 = i32(floor(y)) - 1;
          var p: mat4x4<${t}>;
          for (var h = 0; h < 4; h++) {
            for (var w = 0; w < 4; w++) {
              p[h][w] = pixel_at_grid(h + y0, w + x0, H_in, W_in, indices[${Er}], indices[${Dr}], border);
            }
          }

          let dx = x - f32(x0 + 1);
          let dy = y - f32(y0 + 1);
          let result = gs_bicubic_interpolate(p, dx, dy);
        `;
            default:
              throw new Error(`mode ${s.mode} is not supported`);
          }
        })() + `${e.setByOffset("global_idx", "result")}`, ia = (e, t) => {
          let s = qe("x", e[0].dataType, e[0].dims.length), n = [e[1].dims[0], e[1].dims[1], e[1].dims[2]], i = qe("grid", e[1].dataType, n.length, 2), a = [e[0].dims[0], e[0].dims[1], e[1].dims[1], e[1].dims[2]];
          t.format === "NHWC" && (a = [e[0].dims[0], e[1].dims[1], e[1].dims[2], e[0].dims[3]], [Er, Dr, on, an] = [0, 3, 1, 2]);
          let o = It("output", e[0].dataType, a.length), u = s.type.value, p = ze.size(a), h = [{ type: 12, data: p }, ...yt(e[0].dims, n, a)], k = (C) => `
  ${C.registerUniform("output_size", "u32").declareVariables(s, i, o)}
  ${ra}
  ${id(u)}
  ${od(t)}
  ${na(t)}
  ${ad(s, u, t)}

  ${C.mainStart()}
    ${C.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let H_in = i32(uniforms.x_shape[${on}]);
      let W_in = i32(uniforms.x_shape[${an}]);

      ${t.alignCorners === 0 ? `
      let x_min = -0.5;
      let x_max = f32(W_in) - 0.5;
      let y_min = -0.5;
      let y_max = f32(H_in) - 0.5;
      ` : `
      let x_min = 0.0;
      let x_max = f32(W_in) - 1.0;
      let y_min = 0.0;
      let y_max = f32(H_in) - 1.0;
      `};
      let border = vec4<f32>(x_min, y_min, x_max, y_max);

      let indices = ${o.offsetToIndices("global_idx")};
      var grid_indices = vec3<u32>(indices[${Er}], indices[${on}], indices[${an}]);
      let nxy = ${i.getByIndices("grid_indices")};
      var x = gs_denormalize(f32(nxy[0]), W_in);
      var y = gs_denormalize(f32(nxy[1]), H_in);

      ${ld(o, u, t)}
  }`;
          return { name: "GridSample", shaderCache: { hint: `${t.cacheKey}`, inputDependencies: ["type", "type"] }, getRunData: (C) => {
            let d = ze.size(a);
            return { outputs: [{ dims: a, dataType: C[0].dataType }], dispatchGroup: { x: Math.ceil(d / 64) }, programUniforms: h };
          }, getShaderSource: k };
        }, ud = (e, t) => {
          nd(e.inputs), e.compute(ia(e.inputs, t));
        }, dd = (e) => Bt({ alignCorners: e.align_corners, mode: e.mode, paddingMode: e.padding_mode, format: e.format });
      }), nr, cd, pd, aa, la, ln, dp, hd = g(() => {
        zt(), Ft(), rs(), ue(), oo(), Yt(), Gr(), nr = (e, t) => e.length > t && e[t].dims.length > 0 ? e[t] : void 0, cd = (e, t) => {
          let s = e[0], n = nr(e, 1), i = nr(e, 2), a = nr(e, 3), o = nr(e, 4), u = nr(e, 5), p = nr(e, 6), h = nr(e, 7);
          if (s.dims.length !== 3 && s.dims.length !== 5) throw new Error("Input query is expected to have 3 or 5 dimensions");
          let k = s.dims[0], C = s.dims[1], d = s.dims.length === 3 ? s.dims[2] : t.numHeads * s.dims[4], z = C, B = 0, V = 0, Z = Math.floor(d / t.numHeads);
          if (p && h && ze.size(p.dims) && ze.size(h.dims)) {
            if (p.dims.length !== 4) throw new Error('Input "past_key" is expected to have 4 dimensions');
            if (p.dims[0] !== k || p.dims[1] !== t.numHeads || p.dims[3] !== Z) throw new Error('Input "past_key" shape (batch_size, num_heads, past_sequence_length, head_size)');
            if (h.dims[0] !== k || h.dims[1] !== t.numHeads || h.dims[3] !== Z) throw new Error('Input "past_value" shape (batch_size, num_heads, past_sequence_length, head_size)');
            if (p.dims[2] !== h.dims[2]) throw new Error('Input "past_key" and "past_value" shall have same dim 2 (past_sequence_length)');
            if (h.dims.length !== 4) throw new Error('Input "past_value" is expected to have 4 dimensions');
            B = p.dims[2], V = p.dims[2];
          } else if (p && ze.size(p.dims) || h && ze.size(h.dims)) throw new Error('Input "past_key" and "past_value" shall be both present or both absent');
          let ee;
          if (n && ze.size(n.dims) > 0) {
            if (s.dims.length !== 3) throw new Error('Input "query" is expected to have 3 dimensions when key is given');
            if (n.dims.length < 3 || n.dims.length > 5) throw new Error('Input "key" is expected to have 3, 4, or 5 dimensions');
            if (s.dims[0] !== n.dims[0]) throw new Error('Input "query" and "key" shall have same dim 0 (batch size)');
            if (n.dims.length === 3) {
              if (n.dims[2] !== s.dims[2]) throw new Error('Input "query" and "key" shall have same dim 2 (hidden_size)');
              ee = 2, z = n.dims[1];
            } else if (n.dims.length === 5) {
              if (n.dims[2] !== t.numHeads || n.dims[3] !== 2 || n.dims[4] !== Z) throw new Error('Expect "key" shape (batch_size, kv_sequence_length, num_heads, 2, head_size) for packed kv');
              if (i) throw new Error('Expect "value" be none when "key" has packed kv format.');
              ee = 5, z = n.dims[1];
            } else {
              if (n.dims[1] !== t.numHeads || n.dims[3] !== Z) throw new Error('Expect "key" shape (batch_size, num_heads, kv_sequence_length, head_size) for past_key');
              ee = 0, z = n.dims[2];
            }
          } else {
            if (s.dims.length !== 5) throw new Error('Input "query" is expected to have 5 dimensions when key is empty');
            if (s.dims[2] !== t.numHeads || s.dims[3] !== 3) throw new Error('Expect "query" shape (batch_size, kv_sequence_length, num_heads, 3, head_size) for packed kv');
            ee = 3;
          }
          if (a && ze.size(a.dims) > 0) {
            if (a.dims.length !== 1) throw new Error('Input "bias" is expected to have 1 dimension');
            if (n && n.dims.length === 5 && n.dims[3] === 2) throw new Error("bias is not allowed for packed kv.");
          }
          let X = B + z, he = 0;
          if (o && ze.size(o.dims) > 0) {
            he = 8;
            let Le = o.dims;
            throw Le.length === 1 ? Le[0] === k ? he = 1 : Le[0] === 3 * k + 2 && (he = 3) : Le.length === 2 && Le[0] === k && Le[1] === X && (he = 5), he === 8 ? new Error('Input "key_padding_mask" shape shall be (batch_size) or (batch_size, total_sequence_length)') : new Error("Mask not supported");
          }
          let pe = !1, Me = d;
          if (i && ze.size(i.dims) > 0) {
            if (i.dims.length !== 3 && i.dims.length !== 4) throw new Error('Input "value" is expected to have 3 or 4 dimensions');
            if (s.dims[0] !== i.dims[0]) throw new Error('Input "query" and "value" shall have same dim 0 (batch_size)');
            if (i.dims.length === 3) {
              if (z !== i.dims[1]) throw new Error('Input "key" and "value" shall have the same dim 1 (kv_sequence_length)');
              Me = i.dims[2];
            } else {
              if (z !== i.dims[2]) throw new Error('Input "key" and "value" shall have the same dim 2 (kv_sequence_length)');
              Me = i.dims[1] * i.dims[3], pe = !0;
            }
          }
          let Oe = !1;
          if (o && ze.size(o.dims) > 0) throw new Error("Key padding mask is not supported");
          if (u && ze.size(u.dims) > 0) {
            if (u.dims.length !== 4) throw new Error('Input "attention_bias" is expected to have 4 dimensions');
            if (u.dims[0] !== k || u.dims[1] !== t.numHeads || u.dims[2] !== C || u.dims[3] !== X) throw new Error('Expect "attention_bias" shape (batch_size, num_heads, sequence_length, total_sequence_length)');
          }
          return { batchSize: k, sequenceLength: C, pastSequenceLength: B, kvSequenceLength: z, totalSequenceLength: X, maxSequenceLength: V, inputHiddenSize: 0, hiddenSize: d, vHiddenSize: Me, headSize: Z, vHeadSize: Math.floor(Me / t.numHeads), numHeads: t.numHeads, isUnidirectional: !1, pastPresentShareBuffer: !1, maskFilterValue: t.maskFilterValue, maskType: he, scale: t.scale, broadcastResPosBias: Oe, passPastInKv: pe, qkvFormat: ee };
        }, pd = (e) => Bt({ ...e }), aa = Bt({ perm: [0, 2, 1, 3] }), la = (e, t, s, n, i, a, o) => {
          let u = [n, i, a], p = ze.size(u), h = [{ type: 12, data: p }, { type: 12, data: o }, { type: 12, data: a }], k = (C) => {
            let d = It("qkv_with_bias", t.dataType, u), z = qe("qkv", t.dataType, u), B = qe("bias", s.dataType, u), V = [{ name: "output_size", type: "u32" }, { name: "bias_offset", type: "u32" }, { name: "hidden_size", type: "u32" }];
            return `
  ${C.registerUniforms(V).declareVariables(z, B, d)}
  ${C.mainStart()}
    ${C.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let bias_offset_idx = (global_idx % uniforms.hidden_size) + uniforms.bias_offset;

    qkv_with_bias[global_idx] = qkv[global_idx] + bias[bias_offset_idx];
  }`;
          };
          return e.compute({ name: "MultiHeadAttentionAddBias", shaderCache: { inputDependencies: ["type", "type"] }, getRunData: () => ({ outputs: [{ dims: u, dataType: t.dataType, gpuDataType: 0 }], dispatchGroup: { x: Math.ceil(p / 64) }, programUniforms: h }), getShaderSource: k }, { inputs: [t, s], outputs: [-1] })[0];
        }, ln = (e, t, s, n, i, a, o, u) => {
          let p = a;
          if (o && ze.size(o.dims) > 0) {
            if (n === 1) throw new Error("AddBiasReshape is not implemented. Please export your model with packed QKV or KV");
            return p = la(e, a, o, t, n, s * i, u), p = p.reshape([t, n, s, i]), s === 1 || n === 1 ? p : e.compute(cr(p, aa.perm), { inputs: [p], outputs: [-1] })[0];
          } else return a.dims.length === 3 && (p = a.reshape([t, n, s, i])), s === 1 || n === 1 ? p : e.compute(cr(p, aa.perm), { inputs: [p], outputs: [-1] })[0];
        }, dp = (e, t) => {
          let s = cd(e.inputs, t), n = e.inputs[0], i = nr(e.inputs, 1), a = nr(e.inputs, 2), o = nr(e.inputs, 3), u = nr(e.inputs, 4), p = nr(e.inputs, 5), h = nr(e.inputs, 6), k = nr(e.inputs, 7);
          if (n.dims.length === 5) throw new Error("Packed QKV is not implemented");
          if ((i == null ? void 0 : i.dims.length) === 5) throw new Error("Packed KV is not implemented");
          let C = i && a && i.dims.length === 4 && a.dims.length === 4, d = ln(e, s.batchSize, s.numHeads, s.sequenceLength, s.headSize, n, o, 0);
          if (C) return Bn(e, d, i, a, u, void 0, h, k, p, s);
          if (!i || !a) throw new Error("key and value must be provided");
          let z = ln(e, s.batchSize, s.numHeads, s.kvSequenceLength, s.headSize, i, o, s.hiddenSize), B = ln(e, s.batchSize, s.numHeads, s.kvSequenceLength, s.vHeadSize, a, o, 2 * s.hiddenSize);
          Bn(e, d, z, B, u, void 0, h, k, p, s);
        };
      }), md, ua, fd, _d, Pi, gd, wd, da = g(() => {
        zt(), Ft(), rs(), Yt(), md = (e) => {
          if (!e || e.length < 1) throw new Error("too few inputs");
        }, ua = (e, t) => {
          let s = [], n = t.numOutputs;
          return e[1].dims[0] > 0 && (e[1].getBigInt64Array().forEach((i) => s.push(Number(i))), n = s.length), Bt({ numOutputs: n, axis: t.axis, splitSizes: s });
        }, fd = (e) => `
fn calculateOutputIndex(index: u32) -> u32 {
    for (var i: u32 = 0u; i < ${e}u; i += 1u ) {
    if (index < ${$t("uniforms.size_in_split_axis", "i", e)}) {
        return i;
    }
    }
    return ${e}u;
}`, _d = (e) => {
          let t = e.length, s = [];
          for (let n = 0; n < t; ++n) {
            let i = e[n].setByIndices("indices", "input[global_idx]");
            t === 1 ? s.push(i) : n === 0 ? s.push(`if (output_number == ${n}u) { ${i} }`) : n === t - 1 ? s.push(`else { ${i} }`) : s.push(`else if (output_number == ${n}) { ${i} }`);
          }
          return `
      fn writeBufferData(output_number: u32, indices: ${e[0].type.indices}, global_idx: u32) {
        ${s.join(`
`)}
      }`;
        }, Pi = (e, t) => {
          let s = e[0].dims, n = ze.size(s), i = e[0].dataType, a = ze.normalizeAxis(t.axis, s.length), o = new Array(t.numOutputs), u = qe("input", i, s.length), p = new Array(t.numOutputs), h = [], k = [], C = 0, d = [{ type: 12, data: n }];
          for (let B = 0; B < t.numOutputs; B++) {
            C += t.splitSizes[B], p[B] = C;
            let V = s.slice();
            V[a] = t.splitSizes[B], k.push(V), o[B] = It(`output${B}`, i, V.length), h.push({ dims: k[B], dataType: e[0].dataType });
          }
          d.push({ type: 12, data: p }, ...yt(s, ...k));
          let z = (B) => `
  ${B.registerUniform("input_size", "u32").registerUniform("size_in_split_axis", "u32", p.length).declareVariables(u, ...o)}
  ${fd(p.length)}
  ${_d(o)}

  ${B.mainStart()}
    ${B.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.input_size")}

    var indices = ${u.offsetToIndices("global_idx")};
    var index = ${u.indicesGet("indices", a)};
    let output_number = calculateOutputIndex(index);
    if (output_number != 0) {
      index -= ${$t("uniforms.size_in_split_axis", "output_number - 1u", p.length)};
      ${u.indicesSet("indices", a, "index")};
    }
    writeBufferData(output_number, indices, global_idx);
  }`;
          return { name: "Split", shaderCache: { hint: t.cacheKey, inputDependencies: ["rank"] }, getShaderSource: z, getRunData: () => ({ outputs: h, dispatchGroup: { x: Math.ceil(n / 64) }, programUniforms: d }) };
        }, gd = (e, t) => {
          md(e.inputs);
          let s = e.inputs.length === 1 ? t : ua(e.inputs, t);
          e.compute(Pi(e.inputs, s), { inputs: [0] });
        }, wd = (e) => {
          let t = e.axis, s = e.splitSizes, n = e.numOutputs < 0 ? s.length : e.numOutputs;
          if (n !== s.length) throw new Error("numOutputs and splitSizes lengh must be equal");
          return Bt({ axis: t, numOutputs: n, splitSizes: s });
        };
      }), cp, pp, Ei, ca, hp = g(() => {
        rs(), oo(), hd(), da(), Gr(), cp = (e, t) => {
          if (t.doRotary) throw new Error("GroupQuerryAttention do_rotary attribute is not supported");
          if (t.doRotary && e.length <= 7) throw new Error("cos_cache and sin_cache inputs are required if do_rotary is specified");
          let s = e[0], n = e[1], i = e[2], a = e[3], o = e[4];
          if (t.localWindowSize !== -1) throw new Error("Local attention is not supported");
          if (t.softcap !== 0) throw new Error("Softcap is not supported");
          if (t.rotaryInterleaved !== 0) throw new Error("Rotary interleaved is not supported");
          if (t.smoothSoftmax) throw new Error("Smooth softmax is not supported");
          if (s.dims.length !== 3 && s.dims.length !== 5) throw new Error("Input query is expected to have 3 or 5 dimensions");
          let u = !1, p = s.dims[0], h = s.dims[1], k = s.dims.length === 3 ? u ? s.dims[2] / 3 : s.dims[2] : t.numHeads * s.dims[4], C = h, d = 0, z = !n || n.dims.length === 0, B = Math.floor(z ? k / (t.numHeads + 2 * t.kvNumHeads) : k / t.numHeads);
          z && (k = B * t.numHeads);
          let V = a && a.dims.length !== 0, Z = o && o.dims.length !== 0;
          if (V && a.dims.length === 4 && a.dims[0] === p && a.dims[1] !== t.kvNumHeads && a.dims[2] === t.kvNumHeads && a.dims[3] === B) throw new Error("BSNH pastKey/pastValue is not supported");
          if (V && Z) {
            if (a.dims.length !== 4) throw new Error('Input "past_key" is expected to have 4 dimensions');
            if (o.dims.length !== 4) throw new Error('Input "past_value" is expected to have 4 dimensions');
            d = a.dims[2];
          } else if (V || Z) throw new Error('Input "past_key" and "past_value" shall be both present or both absent');
          let ee = 1;
          if (n && n.dims.length > 0) {
            if (s.dims.length !== 3) throw new Error('Input "query" is expected to have 3 dimensions when key is given');
            if (n.dims.length < 3 || n.dims.length > 5) throw new Error('Input "key" is expected to have 3, 4, or 5 dimensions');
            if (s.dims[0] !== n.dims[0]) throw new Error('Input "query" and "key" shall have same dim 0 (batch size)');
            if (n.dims.length === 3) {
              if (s.dims[2] % n.dims[2] !== 0) throw new Error('Dimension 2 of "query" should be a multiple of "key"');
              C = n.dims[1];
            } else if (n.dims.length === 5) {
              if (n.dims[2] !== t.numHeads || n.dims[3] !== 2 || n.dims[4] !== B) throw new Error('Expect "key" shape (batch_size, kv_sequence_length, num_heads, 2, head_size) for packed kv');
              if (i) throw new Error('Expect "value" be none when "key" has packed kv format.');
              C = n.dims[1];
            } else {
              if (n.dims[1] !== t.numHeads || n.dims[3] !== B) throw new Error('Expect "key" shape (batch_size, num_heads, kv_sequence_length, head_size) for past_key');
              C = n.dims[2];
            }
          } else {
            if (s.dims.length !== 3 && s.dims.length !== 5) throw new Error('Input "query" is expected to have 3 or 5 dimensions when key is empty');
            if (s.dims.length === 5 && (s.dims[2] !== t.numHeads || s.dims[3] !== 3)) throw new Error('Expect "query" shape (batch_size, kv_sequence_length, num_heads, 3, head_size) for packed kv');
            ee = 3;
          }
          let X = 0, he = !1, pe = t.kvNumHeads ? B * t.kvNumHeads : k;
          if (i && i.dims.length > 0) {
            if (i.dims.length !== 3 && i.dims.length !== 4) throw new Error('Input "value" is expected to have 3 or 4 dimensions');
            if (s.dims[0] !== i.dims[0]) throw new Error('Input "query" and "value" shall have same dim 0 (batch_size)');
            if (i.dims.length === 3) {
              if (C !== i.dims[1]) throw new Error('Input "key" and "value" shall have the same dim 1 (kv_sequence_length)');
              pe = i.dims[2];
            } else {
              if (C !== i.dims[2]) throw new Error('Input "past_key" and "past_value" shall have the same dim 2 (kv_sequence_length)');
              pe = i.dims[1] * i.dims[3], he = !0;
            }
          }
          let Me = e.length > 4 ? e[5] : void 0;
          if (Me && Me.dims.length !== 1 && Me.dims[0] !== p) throw new Error('Input "seqlens" is expected to have 1 dimension and the same dim 0 as batch_size');
          return { batchSize: p, sequenceLength: h, pastSequenceLength: d, kvSequenceLength: C, totalSequenceLength: -1, maxSequenceLength: -1, inputHiddenSize: 0, hiddenSize: k, vHiddenSize: pe, headSize: B, vHeadSize: Math.floor(pe / t.kvNumHeads), numHeads: t.numHeads, kvNumHeads: t.kvNumHeads, nReps: t.numHeads / t.kvNumHeads, pastPresentShareBuffer: !1, maskType: X, scale: t.scale, broadcastResPosBias: !1, passPastInKv: he, qkvFormat: ee };
        }, pp = Bt({ perm: [0, 2, 1, 3] }), Ei = (e, t, s) => {
          let n = t, i = s.kvNumHeads;
          return t.dims.length === 3 && s.kvSequenceLength !== 0 && (n = t.reshape([s.batchSize, s.kvSequenceLength, i, s.headSize]), n = e.compute(cr(n, pp.perm), { inputs: [n], outputs: [-1] })[0]), n;
        }, ca = (e, t) => {
          var Z;
          let s = cp(e.inputs, t);
          if (e.inputs[0].dims.length === 5) throw new Error("Packed QKV is not implemented");
          if (((Z = e.inputs[1]) == null ? void 0 : Z.dims.length) === 5) throw new Error("Packed KV is not implemented");
          let n = e.inputs[0], i = e.inputs[1] && e.inputs[1].dims.length > 0 ? e.inputs[1] : void 0, a = e.inputs[2] && e.inputs[2].dims.length > 0 ? e.inputs[2] : void 0, o = e.inputs[3] && e.inputs[3].dims.length !== 0 ? e.inputs[3] : void 0, u = e.inputs[4] && e.inputs[4].dims.length !== 0 ? e.inputs[4] : void 0, p = e.inputs.length > 4 ? e.inputs[5] : void 0, h = e.inputs.length > 5 ? e.inputs[6] : void 0, k = s.kvNumHeads ? s.kvNumHeads : s.numHeads, C = Bt({ axis: 2, numOutputs: 3, splitSizes: [s.numHeads * s.headSize, k * s.headSize, k * s.headSize] }), [d, z, B] = !i && !a ? e.compute(Pi([n], C), { inputs: [n], outputs: [-1, -1, -1] }) : [n, i, a], V = ln(e, s.batchSize, s.numHeads, s.sequenceLength, s.headSize, d, void 0, 0);
          Bn(e, V, Ei(e, z, s), Ei(e, B, s), void 0, void 0, o, u, void 0, s, p, h);
        };
      }), pa, ha, yd, Md, bd = g(() => {
        zt(), Ft(), Gr(), Yt(), pa = (e, t, s, n, i, a, o, u) => {
          let p = qt(a), h = p === 1 ? "f32" : `vec${p}f`, k = p === 1 ? "vec2f" : `mat2x${p}f`, C = i * o, d = 64;
          C === 1 && (d = 256);
          let z = [i, o, a / p], B = [i, o, 2], V = ["rank", "type", "type"], Z = [];
          Z.push(...yt(z, B));
          let ee = (X) => {
            let he = qe("x", t.dataType, 3, p), pe = qe("scale", s.dataType, s.dims), Me = qe("bias", n.dataType, n.dims), Oe = It("output", 1, 3, 2), Le = [he, pe, Me, Oe];
            return `
  var<workgroup> workgroup_shared : array<${k}, ${d}>;
  const workgroup_size = ${d}u;
  ${X.declareVariables(...Le)}
  ${X.mainStart(d)}
    let batch = workgroup_index / uniforms.x_shape[1];
    let channel = workgroup_index % uniforms.x_shape[1];
    let hight = uniforms.x_shape[2];
    // initialize workgroup memory
    var sum = ${h}(0);
    var squared_sum = ${h}(0);
    for (var h = local_idx; h < hight; h += workgroup_size) {
      let value = ${h}(${he.get("batch", "channel", "h")});
      sum += value;
      squared_sum += value * value;
    }
    workgroup_shared[local_idx] = ${k}(sum, squared_sum);
    workgroupBarrier();

    for (var currSize = workgroup_size >> 1;  currSize > 0; currSize = currSize >> 1) {
      if (local_idx < currSize) {
        workgroup_shared[local_idx] = workgroup_shared[local_idx] + workgroup_shared[local_idx + currSize];
      }
      workgroupBarrier();
    }
    if (local_idx == 0) {
      let sum_final = ${Gs("workgroup_shared[0][0]", p)} / f32(hight * ${p});
      let squared_sum_final = ${Gs("workgroup_shared[0][1]", p)} / f32(hight * ${p});

      let inv_std_dev = inverseSqrt(squared_sum_final - sum_final * sum_final + f32(${u}));
      let channel_scale = inv_std_dev * f32(scale[channel]);
      let channel_shift = f32(bias[channel]) - sum_final * channel_scale;
      output[workgroup_index] = vec2f(channel_scale, channel_shift);
    }
  }`;
          };
          return e.compute({ name: "InstanceNormComputeChannelScaleShift", shaderCache: { hint: `${p};${u};${d}`, inputDependencies: V }, getRunData: () => ({ outputs: [{ dims: B, dataType: 1 }], dispatchGroup: { x: C }, programUniforms: Z }), getShaderSource: ee }, { inputs: [t, s, n], outputs: [-1] })[0];
        }, ha = (e, t, s) => {
          let n = t[0].dims, i = n, a = 2, o = n[0], u = n[1], p = ze.sizeFromDimension(n, a), h = qt(p), k = ze.size(i) / h, C = pa(e, t[0], t[1], t[2], o, p, u, s.epsilon), d = [o, u, p / h], z = [o, u], B = ["type", "none"], V = (Z) => {
            let ee = qe("x", t[0].dataType, d.length, h), X = qe("scale_shift", 1, z.length, 2), he = It("output", t[0].dataType, d.length, h), pe = [ee, X, he];
            return `
  ${Z.registerUniform("output_size", "u32").declareVariables(...pe)}
  ${Z.mainStart()}
  ${Z.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let outputIndices = ${he.offsetToIndices("global_idx")};
      let batch = outputIndices[0];
      let channel = outputIndices[1];
      let scale_shift = ${X.getByIndices("vec2<u32>(batch, channel)")};
      let value = ${ee.getByOffset("global_idx")} * ${he.type.value}(scale_shift.x) + ${he.type.value}(scale_shift.y);
      ${he.setByOffset("global_idx", "value")};
  }`;
          };
          e.compute({ name: "InstanceNormalization", shaderCache: { hint: `${h}`, inputDependencies: B }, getRunData: () => ({ outputs: [{ dims: i, dataType: t[0].dataType }], dispatchGroup: { x: Math.ceil(k / 64) }, programUniforms: [{ type: 12, data: k }, ...yt(d, z, d)] }), getShaderSource: V }, { inputs: [t[0], C] });
        }, yd = (e, t, s) => {
          let n = t[0].dims, i = n, a = n[0], o = n[n.length - 1], u = ze.sizeFromDimension(n, 1) / o, p = qt(o), h = ze.size(i) / p, k = [{ type: 12, data: u }, { type: 12, data: Math.floor(o / p) }], C = ["type", "type"], d = !1, z = [0, n.length - 1];
          for (let ee = 0; ee < n.length - 2; ee++) d = d || n[ee + 1] !== 1, z.push(ee + 1);
          d = d && n[n.length - 1] !== 1;
          let B = d ? e.compute(cr(e.inputs[0], z), { inputs: [e.inputs[0]], outputs: [-1] })[0] : e.inputs[0].reshape(Array.from({ length: n.length }, (ee, X) => n[z[X]])), V = pa(e, B, t[1], t[2], a, u, o, s.epsilon), Z = (ee) => {
            let X = fs(t[0].dataType), he = p === 1 ? "vec2f" : `mat${p}x2f`, pe = (Le) => {
              let Ye = Le === 0 ? "x" : "y", at = p === 1 ? "f32" : `vec${p}f`;
              switch (p) {
                case 1:
                  return `${X}(${at}(scale.${Ye}))`;
                case 2:
                  return `vec2<${X}>(${at}(scale[0].${Ye}, scale[1].${Ye}))`;
                case 4:
                  return `vec4<${X}>(${at}(scale[0].${Ye}, scale[1].${Ye}, scale[2].${Ye}, scale[3].${Ye}))`;
                default:
                  throw new Error(`Not supported compoents ${p}`);
              }
            }, Me = qe("input", t[0].dataType, t[0].dims, p), Oe = It("output", t[0].dataType, i, p);
            return `
  @group(0) @binding(0) var<storage, read> input : array<${Me.type.storage}>;
  @group(0) @binding(1) var<storage, read> scale_input : array<${he}>;
  @group(0) @binding(2) var<storage, read_write> output : array<${Oe.type.storage}>;
  struct Uniforms {H: u32, C : u32};
  @group(0) @binding(3) var<uniform> uniforms: Uniforms;

  ${ee.mainStart()}
    let current_image_number = global_idx / (uniforms.C * uniforms.H);
    let current_channel_number = global_idx % uniforms.C;

    let scale_offset = current_image_number * uniforms.C + current_channel_number;
    let scale = scale_input[scale_offset];
    output[global_idx] = fma(input[global_idx], ${pe(0)}, ${pe(1)});
  }`;
          };
          e.compute({ name: "InstanceNormalizationNHWC", shaderCache: { hint: `${p}`, inputDependencies: C }, getRunData: () => ({ outputs: [{ dims: i, dataType: t[0].dataType }], dispatchGroup: { x: Math.ceil(h / 64) }, programUniforms: k }), getShaderSource: Z }, { inputs: [t[0], V] });
        }, Md = (e, t) => {
          t.format === "NHWC" ? yd(e, e.inputs, t) : ha(e, e.inputs, t);
        };
      }), vd, xd, ma, mp = g(() => {
        zt(), Ft(), Yt(), vd = (e) => {
          if (!e || e.length < 2) throw new Error("layerNorm requires at least 2 inputs.");
        }, xd = (e, t, s) => {
          let n = t.simplified, i = e[0].dims, a = e[1], o = !n && e[2], u = i, p = ze.normalizeAxis(t.axis, i.length), h = ze.sizeToDimension(i, p), k = ze.sizeFromDimension(i, p), C = ze.size(a.dims), d = o ? ze.size(o.dims) : 0;
          if (C !== k || o && d !== k) throw new Error(`Size of X.shape()[axis:] == ${k}.
       Size of scale and bias (if provided) must match this.
       Got scale size of ${C} and bias size of ${d}`);
          let z = [];
          for (let Me = 0; Me < i.length; ++Me) Me < p ? z.push(i[Me]) : z.push(1);
          let B = qt(k), V = ["type", "type"], Z = [{ type: 12, data: h }, { type: 1, data: k }, { type: 12, data: Math.floor(k / B) }, { type: 1, data: t.epsilon }];
          o && V.push("type");
          let ee = s > 1, X = s > 2, he = (Me) => {
            let Oe = fs(e[0].dataType), Le = [qe("x", e[0].dataType, e[0].dims, B), qe("scale", a.dataType, a.dims, B)];
            o && Le.push(qe("bias", o.dataType, o.dims, B)), Le.push(It("output", e[0].dataType, u, B)), ee && Le.push(It("mean_data_output", 1, z)), X && Le.push(It("inv_std_output", 1, z));
            let Ye = [{ name: "norm_count", type: "u32" }, { name: "norm_size", type: "f32" }, { name: "norm_size_vectorized", type: "u32" }, { name: "epsilon", type: "f32" }];
            return `
  ${Me.registerUniforms(Ye).declareVariables(...Le)}
  ${Me.mainStart()}
    ${Me.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.norm_count")}
    let offset = global_idx * uniforms.norm_size_vectorized;
    var mean_vector = ${Ls("f32", B)};
    var mean_square_vector = ${Ls("f32", B)};

    for (var h: u32 = 0u; h < uniforms.norm_size_vectorized; h++) {
      let value = ${$s(Oe, B, "x[h + offset]")};
      mean_vector += value;
      mean_square_vector += value * value;
    }
    let mean = ${Gs("mean_vector", B)} / uniforms.norm_size;
    let inv_std_dev = inverseSqrt(${Gs("mean_square_vector", B)} / uniforms.norm_size ${n ? "" : "- mean * mean"} + uniforms.epsilon);

    for (var j: u32 = 0; j < uniforms.norm_size_vectorized; j++) {
      let f32input = ${$s(Oe, B, "x[j + offset]")};
      let f32scale = ${$s(Oe, B, "scale[j]")};
      output[j + offset] = ${Le[0].type.value}((f32input ${n ? "" : "- mean"}) * inv_std_dev * f32scale
        ${o ? `+ ${$s(Oe, B, "bias[j]")}` : ""}
      );
    }

    ${ee ? "mean_data_output[global_idx] = mean" : ""};
    ${X ? "inv_std_output[global_idx] = inv_std_dev" : ""};
  }`;
          }, pe = [{ dims: u, dataType: e[0].dataType }];
          return ee && pe.push({ dims: z, dataType: 1 }), X && pe.push({ dims: z, dataType: 1 }), { name: "LayerNormalization", shaderCache: { hint: `${B};${s};${n}`, inputDependencies: V }, getRunData: () => ({ outputs: pe, dispatchGroup: { x: Math.ceil(h / 64) }, programUniforms: Z }), getShaderSource: he };
        }, ma = (e, t) => {
          vd(e.inputs), e.compute(xd(e.inputs, t, e.outputCount));
        };
      }), Td, Pd, fp = g(() => {
        Ft(), pi(), Ro(), Td = (e) => {
          if (!e || e.length !== 2) throw new Error("MatMul requires 2 inputs.");
          if (e[0].dims[e[0].dims.length - 1] !== e[1].dims[e[1].dims.length - 2]) throw new Error("shared dimension does not match.");
        }, Pd = (e) => {
          Td(e.inputs);
          let t = Ws.calcShape(e.inputs[0].dims, e.inputs[1].dims, !0);
          if (!t) throw new Error("Can't use matmul on the given tensors");
          let s = t[t.length - 1], n = e.inputs[0].dims[e.inputs[0].dims.length - 1];
          if (s < 8 && n < 8) e.compute(Do(e.inputs, { activation: "" }, t));
          else {
            let i = t[t.length - 2], a = ze.size(e.inputs[0].dims.slice(0, -2)), o = ze.size(e.inputs[1].dims.slice(0, -2));
            if (a !== 1 && i === 1 && o === 1) {
              let u = e.inputs[0].reshape([1, a, n]), p = e.inputs[1].reshape([1, n, s]), h = [1, a, s], k = [u, p];
              e.compute(mi(k, { activation: "" }, t, h), { inputs: k });
            } else e.compute(mi(e.inputs, { activation: "" }, t));
          }
        };
      }), Ed, Cd, kd, Sd, $d, Ad = g(() => {
        zt(), Ft(), rs(), Yt(), Ed = (e, t) => {
          if (e.length < 3 || e.length > 4) throw new Error("MatMulNBits requires 3 or 4 inputs");
          let s = e[0], n = s.dims.length;
          if (s.dims[n - 1] !== t.k) throw new Error("The last dim of input shape does not match the k value");
          let i = Math.floor((t.k + t.blockSize - 1) / t.blockSize), a = t.blockSize / 8 * t.bits, o = e[1];
          if (!ze.areEqual(o.dims, [t.n, i, a])) throw new Error("The second inputs must be 3D tensor with shape N X nBlocksPerCol X blobSize");
          let u = e[2].dims;
          if (ze.size(u) !== t.n * i) throw new Error("scales input size error.");
          if (e.length === 4) {
            let p = e[3].dims, h = t.bits > 4 ? t.n * i : t.n * Math.floor((i + 1) / 2);
            if (ze.size(p) !== h) throw new Error("zeroPoints input size error.");
          }
        }, Cd = (e, t) => {
          let s = e[0].dims, n = s.length, i = s[n - 2], a = t.k, o = t.n, u = s.slice(0, n - 2), p = ze.size(u), h = e[1].dims[2] / 4, k = e[0].dataType, C = qt(t.k), d = qt(h), z = qt(o), B = u.concat([i, o]), V = i > 1 && o / z % 2 === 0 ? 2 : 1, Z = ze.size(B) / z / V, ee = 64, X = [], he = [p, i, a / C], pe = ze.convertShape(e[1].dims).slice();
          pe.splice(-1, 1, h / d), X.push(...yt(he)), X.push(...yt(pe)), X.push(...yt(e[2].dims)), e.length === 4 && X.push(...yt(ze.convertShape(e[3].dims)));
          let Me = [p, i, o / z];
          X.push(...yt(Me));
          let Oe = (Le) => {
            let Ye = he.length, at = qe("a", e[0].dataType, Ye, C), Pt = qe("b", 12, pe.length, d), Xt = qe("scales", e[2].dataType, e[2].dims.length), Zt = [at, Pt, Xt], bt = e.length === 4 ? qe("zero_points", 12, e[3].dims.length) : void 0;
            bt && Zt.push(bt);
            let ss = Me.length, St = It("output", e[0].dataType, ss, z), Ot = fs(e[0].dataType), bs = (() => {
              switch (C) {
                case 1:
                  return `array<${Ot}, 8>`;
                case 2:
                  return `mat4x2<${Ot}>`;
                case 4:
                  return `mat2x4<${Ot}>`;
                default:
                  throw new Error(`${C}-component is not supported.`);
              }
            })(), Ht = () => {
              let ot = `
          // reuse a data
            var input_offset = ${at.indicesToOffset(`${at.type.indices}(batch, row, word_offset)`)};
            var a_data: ${bs};
            for (var j: u32 = 0; j < ${8 / C}; j++) {
              a_data[j] = ${at.getByOffset("input_offset")};
              input_offset++;
            }
          `;
              for (let Et = 0; Et < z * V; Et++) ot += `
            b_value = ${d === 1 ? `b${Et}_data` : `b${Et}_data[i]`};
            b_value_lower = unpack4xU8(b_value & b_mask);
            b_value_upper = unpack4xU8((b_value >> 4) & b_mask);
            b_quantized_values = ${bs}(${Array.from({ length: 4 }, (ps, Ns) => `${Ot}(b_value_lower[${Ns}]), ${Ot}(b_value_upper[${Ns}])`).join(", ")});
            b_dequantized_values = ${C === 1 ? `${bs}(${Array.from({ length: 8 }, (ps, Ns) => `(b_quantized_values[${Ns}] - ${bt ? `zero_point${Et}` : "zero_point"}) * scale${Et}`).join(", ")});` : `(b_quantized_values - ${bs}(${Array(8).fill(`${bt ? `zero_point${Et}` : "zero_point"}`).join(",")})) * scale${Et};`};
            workgroup_shared[local_id.x * ${V} + ${Math.floor(Et / z)}]${z > 1 ? `[${Et % z}]` : ""} += ${Array.from({ length: 8 / C }, (ps, Ns) => `${C === 1 ? `a_data[${Ns}] * b_dequantized_values[${Ns}]` : `dot(a_data[${Ns}], b_dequantized_values[${Ns}])`}`).join(" + ")};
          `;
              return ot;
            }, Rt = () => {
              let ot = `
            var col_index = col * ${z};
            ${bt ? `
            let zero_point_bytes_per_col = (nBlocksPerCol + 1) / 2;
            var zero_point_byte_count: u32;
            var zero_point_word_index: u32;
            var zero_point_byte_offset: u32;
            let zero_point_nibble_offset: u32 = block & 0x1u;
            var zero_point_bits_offset: u32;
            var zero_point_word: u32;` : `
            // The default zero point is 8 for unsigned 4-bit quantization.
            let zero_point = ${Ot}(8);`}
            `;
              for (let Et = 0; Et < z * V; Et++) ot += `
            let scale${Et} = ${Xt.getByOffset("col_index * nBlocksPerCol + block")};
            ${bt ? `
            zero_point_byte_count = col_index * zero_point_bytes_per_col + (block >> 0x1u);
            zero_point_word_index = zero_point_byte_count >> 0x2u;
            zero_point_byte_offset = zero_point_byte_count & 0x3u;
            zero_point_bits_offset = (zero_point_byte_offset << 3) + (zero_point_nibble_offset << 2);
            zero_point_word = ${bt.getByOffset("zero_point_word_index")} >> zero_point_bits_offset;
            let zero_point${Et} = ${Ot}((zero_point_word) & 0xFu);` : ""}
            col_index += 1;`;
              return ot;
            }, _s = () => {
              let ot = `col_index = col * ${z};`;
              for (let Et = 0; Et < z * V; Et++) ot += `
            let b${Et}_data = ${Pt.getByIndices(`${Pt.type.indices}(col_index, block, word)`)};
            col_index += 1;`;
              return ot += `
            var b_value: u32;
            let b_mask: u32 = 0x0F0F0F0Fu;
            var b_value_lower: vec4<u32>;
            var b_value_upper: vec4<u32>;
            var b_quantized_values: ${bs};
            var b_dequantized_values: ${bs};`, ot;
            };
            return `
        var<workgroup> workgroup_shared: array<${St.type.value}, ${V * ee}>;
        ${Le.declareVariables(...Zt, St)}
        ${Le.mainStart([ee, 1, 1])}
          let output_indices = ${St.offsetToIndices(`(global_idx / ${ee}) * ${V}`)};
          let col = output_indices[2];
          let row = output_indices[1];
          let batch = output_indices[0];
          let nBlocksPerCol = uniforms.b_shape[1];

          for (var block = local_id.x; block < nBlocksPerCol; block += ${ee}) {
            //process one block
            var word_offset: u32 = block * ${t.blockSize / C};
            ${Rt()}
            for (var word: u32 = 0; word < ${h}; word += ${d}) {
              ${_s()}
              for (var i: u32 = 0; i < ${d}; i++) {
                ${Ht()}
                word_offset += ${8 / C};
              }
            }
          }
          workgroupBarrier();

          if (local_id.x < ${V}) {
            var output_value: ${St.type.value} = ${St.type.value}(0);
            var workgroup_shared_offset: u32 = local_id.x;
            for (var b: u32 = 0u; b < ${ee}u; b++) {
              output_value += workgroup_shared[workgroup_shared_offset];
              workgroup_shared_offset += ${V};
            }
            ${St.setByIndices(`${St.type.indices}(batch, row, col + local_id.x)`, "output_value")};
          }
        }`;
          };
          return { name: "MatMulNBits", shaderCache: { hint: `${t.blockSize};${t.bits};${C};${d};${z};${V};${ee}`, inputDependencies: Array(e.length).fill("rank") }, getRunData: () => ({ outputs: [{ dims: B, dataType: k }], dispatchGroup: { x: Z }, programUniforms: X }), getShaderSource: Oe };
        }, kd = (e, t) => {
          let s = e[0].dims, n = s.length, i = s[n - 2], a = t.k, o = t.n, u = s.slice(0, n - 2), p = ze.size(u), h = e[1].dims[2] / 4, k = e[0].dataType, C = qt(t.k), d = qt(h), z = u.concat([i, o]), B = 128, V = o % 8 === 0 ? 8 : o % 4 === 0 ? 4 : 1, Z = B / V, ee = Z * d * 8, X = ee / C, he = ee / t.blockSize, pe = ze.size(z) / V, Me = [], Oe = [p, i, a / C], Le = ze.convertShape(e[1].dims).slice();
          Le.splice(-1, 1, h / d), Me.push(...yt(Oe)), Me.push(...yt(Le)), Me.push(...yt(e[2].dims)), e.length === 4 && Me.push(...yt(ze.convertShape(e[3].dims)));
          let Ye = [p, i, o];
          Me.push(...yt(Ye));
          let at = (Pt) => {
            let Xt = Oe.length, Zt = qe("a", e[0].dataType, Xt, C), bt = qe("b", 12, Le.length, d), ss = qe("scales", e[2].dataType, e[2].dims.length), St = [Zt, bt, ss], Ot = e.length === 4 ? qe("zero_points", 12, e[3].dims.length) : void 0;
            Ot && St.push(Ot);
            let bs = Ye.length, Ht = It("output", e[0].dataType, bs), Rt = fs(e[0].dataType), _s = () => {
              switch (C) {
                case 1:
                  return `
          let a_data0 = vec4<${Rt}>(sub_a[word_offset], sub_a[word_offset + 1], sub_a[word_offset + 2], sub_a[word_offset + 3]);
          let a_data1 = vec4<${Rt}>(sub_a[word_offset + 4], sub_a[word_offset + 5], sub_a[word_offset + 6], sub_a[word_offset + 7]);`;
                case 2:
                  return `
          let a_data0 = vec4<${Rt}>(sub_a[word_offset], sub_a[word_offset + 1]);
          let a_data1 = vec4<${Rt}>(sub_a[word_offset + 2], sub_a[word_offset + 3]);`;
                case 4:
                  return `
          let a_data0 = sub_a[word_offset];
          let a_data1 = sub_a[word_offset + 1];`;
                default:
                  throw new Error(`${C}-component is not supported.`);
              }
            };
            return `
        var<workgroup> sub_a: array<${Zt.type.value}, ${X}>;
        var<workgroup> inter_results: array<array<${Ht.type.value}, ${Z}>, ${V}>;
        ${Pt.declareVariables(...St, Ht)}
        ${Pt.mainStart([Z, V, 1])}
          let output_indices = ${Ht.offsetToIndices(`workgroup_index * ${V}`)};
          let col = output_indices[2];
          let row = output_indices[1];
          let batch = output_indices[0];
          let n_blocks_per_col = uniforms.b_shape[1];
          let num_tiles =  (n_blocks_per_col - 1) / ${he} + 1;

          // Loop over shared dimension.
          for (var tile: u32 = 0; tile < num_tiles; tile += 1) {
            let a_col_start = tile * ${X};
            // load one tile A data into shared memory.
            for (var a_offset = local_idx; a_offset < ${X}; a_offset += ${B})
            {
              let a_col = a_col_start + a_offset;
              if (a_col < uniforms.a_shape[2])
              {
                sub_a[a_offset] = ${Zt.getByIndices(`${Zt.type.indices}(batch, row, a_col)`)};
              } else {
                sub_a[a_offset] = ${Zt.type.value}(0);
              }
            }
            workgroupBarrier();

            // each thread process one block
            let b_row = col + local_id.y;
            let block = tile * ${he} + local_id.x;
            ${Ot ? `
            let zero_point_bytes_per_col = (n_blocks_per_col + 1) / 2;
            let zero_point_byte_count = b_row * zero_point_bytes_per_col + (block >> 0x1u);
            let zero_point_word_index = zero_point_byte_count >> 0x2u;
            let zero_point_byte_offset = zero_point_byte_count & 0x3u;
            let zero_point_nibble_offset: u32 = block & 0x1u;
            let zero_point_bits_offset = (zero_point_byte_offset << 3) + (zero_point_nibble_offset << 2);
            let zero_point_word = ${Ot.getByOffset("zero_point_word_index")} >> zero_point_bits_offset;
            let zero_point = ${Rt}((zero_point_word) & 0xFu);` : `
            // The default zero point is 8 for unsigned 4-bit quantization.
            let zero_point = ${Rt}(8);`}
            let scale = ${ss.getByOffset("b_row * n_blocks_per_col + block")};
            let b_data = ${bt.getByIndices(`${bt.type.indices}(b_row, block, 0)`)};
            var word_offset = local_id.x * ${t.blockSize / C};
            for (var i: u32 = 0; i < ${d}; i++) {
              ${_s()}
              let b_value = ${d === 1 ? "b_data" : "b_data[i]"};
              let b_value_lower = unpack4xU8(b_value & 0x0F0F0F0Fu);
              let b_value_upper = unpack4xU8((b_value >> 4) & 0x0F0F0F0Fu);
              let b_quantized_values = mat2x4<${Rt}>(${Array.from({ length: 4 }, (ot, Et) => `${Rt}(b_value_lower[${Et}]), ${Rt}(b_value_upper[${Et}])`).join(", ")});
              let b_dequantized_values = (b_quantized_values - mat2x4<${Rt}>(${Array(8).fill("zero_point").join(",")})) * scale;
              inter_results[local_id.y][local_id.x] += ${Array.from({ length: 2 }, (ot, Et) => `${`dot(a_data${Et}, b_dequantized_values[${Et}])`}`).join(" + ")};
              word_offset += ${8 / C};
            }
            workgroupBarrier();
          }

          if (local_idx < ${V}) {
            var output_value: ${Ht.type.value} = ${Ht.type.value}(0);
            for (var b = 0u; b < ${Z}; b++) {
              output_value += inter_results[local_idx][b];
            }
            if (col + local_idx < uniforms.output_shape[2])
            {
              ${Ht.setByIndices(`${Ht.type.indices}(batch, row, col + local_idx)`, "output_value")}
            }
          }
        }`;
          };
          return { name: "BlockwiseMatMulNBits32", shaderCache: { hint: `${t.blockSize};${C};${d};${Z};${V}`, inputDependencies: Array(e.length).fill("rank") }, getRunData: () => ({ outputs: [{ dims: z, dataType: k }], dispatchGroup: { x: pe }, programUniforms: Me }), getShaderSource: at };
        }, Sd = (e, t) => {
          Ed(e.inputs, t), t.blockSize === 32 && e.adapterInfo.isVendor("intel") && e.adapterInfo.isArchitecture("gen-12lp") ? e.compute(kd(e.inputs, t)) : e.compute(Cd(e.inputs, t));
        }, $d = (e) => Bt(e);
      }), Id, Fd, fa, Od, Dd, gs, _p, gp, wp, Ld = g(() => {
        zt(), Ft(), Yt(), Id = (e) => {
          if (!e || e.length < 1) throw new Error("Too few inputs");
          if (e[0].dataType !== 1 && e[0].dataType !== 10) throw new Error("Input type must be float or float16.");
          if (e.length >= 2) {
            let t = e[0].dims.length * 2 === e[1].dims[0];
            if (e.length === 4 && (t = e[3].dims[0] * 2 === e[1].dims[0]), !t) throw new Error("The pads should be a 1D tensor of shape [2 * input_rank] or [2 * num_axes].");
          }
        }, Fd = (e, t, s) => {
          let n = "";
          for (let i = t - 1; i >= 0; --i) n += `
            k = i32(${e.indicesGet("indices", i)}) - ${$t("uniforms.pads", i, s)};
            if (k < 0) {
              break;
            }
            if (k >= i32(${$t("uniforms.x_shape", i, t)})) {
              break;
            }
            offset += k * i32(${$t("uniforms.x_strides", i, t)});
        `;
          return `
          value = ${e.type.value}(uniforms.constant_value);
          for (var i = 0; i < 1; i++) {
            var offset = 0;
            var k = 0;
            ${n}
            value = x[offset];
          }
      `;
        }, fa = (e, t, s) => {
          let n = "";
          for (let i = t - 1; i >= 0; --i) n += `
                k = i32(${e.indicesGet("indices", i)}) - ${$t("uniforms.pads", i, s)};
                if (k < 0) {
                  k = -k;
                }
                {
                  let _2n_1 = 2 * (i32(${$t("uniforms.x_shape", i, t)}) - 1);
                  k = k % _2n_1;
                  if(k >= i32(${$t("uniforms.x_shape", i, t)})) {
                    k = _2n_1 - k;
                  }
                }
                offset += k * i32(${$t("uniforms.x_strides", i, t)});
            `;
          return `
              var offset = 0;
              var k = 0;
              ${n}
              value = x[offset];
          `;
        }, Od = (e, t, s) => {
          let n = "";
          for (let i = t - 1; i >= 0; --i) n += `
                k = i32(${e.indicesGet("indices", i)}) - ${$t("uniforms.pads", i, s)};
                if (k < 0) {
                  k = 0;
                }
                if (k >= i32(${$t("uniforms.x_shape", i, t)})) {
                  k = i32(${$t("uniforms.x_shape", i, t)}) - 1;
                }
                offset += k * i32(${$t("uniforms.x_strides", i, t)});
            `;
          return `
              var offset = 0;
              var k = 0;
              ${n}
              value = x[offset];
          `;
        }, Dd = (e, t, s) => {
          let n = "";
          for (let i = t - 1; i >= 0; --i) n += `
                k = i32(${e.indicesGet("indices", i)}) - ${$t("uniforms.pads", i, s)};
                if (k < 0)  {
                  k += i32(${$t("uniforms.x_shape", i, t)}]);
                }
                if (k >= i32(${$t("uniforms.x_shape", i, t)})) {
                  k -= i32(${$t("uniforms.x_shape", i, t)});
                }
                offset += k * i32(${$t("uniforms.x_strides", i, t)});
            `;
          return `
              var offset = 0;
              var k = 0;
              ${n}
              value = x[offset];
          `;
        }, gs = (e, t, s) => {
          switch (s.mode) {
            case 0:
              return Fd(e, t, s.pads.length);
            case 1:
              return fa(e, t, s.pads.length);
            case 2:
              return Od(e, t, s.pads.length);
            case 3:
              return Dd(e, t, s.pads.length);
            default:
              throw new Error("Invalid mode");
          }
        }, _p = (e, t) => {
          let s = ze.padShape(e[0].dims.slice(), t.pads), n = e[0].dims, i = ze.size(s), a = [{ type: 12, data: i }, { type: 6, data: t.pads }], o = e.length >= 3 && e[2].data;
          t.mode === 0 && a.push({ type: o ? e[2].dataType : 1, data: t.value }), a.push(...yt(e[0].dims, s));
          let u = ["rank"], p = (h) => {
            let k = It("output", e[0].dataType, s.length), C = qe("x", e[0].dataType, n.length), d = C.type.value, z = gs(k, n.length, t), B = [{ name: "output_size", type: "u32" }, { name: "pads", type: "i32", length: t.pads.length }];
            return t.mode === 0 && B.push({ name: "constant_value", type: o ? d : "f32" }), `
            ${h.registerUniforms(B).declareVariables(C, k)}
            ${h.mainStart()}
            ${h.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

            let indices = ${k.offsetToIndices("global_idx")};

            var value = ${d}(0);
            ${z}
            output[global_idx] = value;
        }`;
          };
          return { name: "Pad", shaderCache: { hint: `${t.mode}${o}`, inputDependencies: u }, getRunData: () => ({ outputs: [{ dims: s, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(ze.size(s) / 64) }, programUniforms: a }), getShaderSource: p };
        }, gp = (e, t) => {
          if (e.length > 1) {
            let s = e[1].getBigInt64Array(), n = e.length >= 3 && e[2].data ? e[2].dataType === 10 ? e[2].getUint16Array()[0] : e[2].getFloat32Array()[0] : 0, i = e[0].dims.length, a = new Int32Array(2 * i).fill(0);
            if (e.length >= 4) {
              let u = e[3].getBigInt64Array();
              for (let p = 0; p < u.length; p++) a[Number(u[p])] = Number(s[p]), a[Number(u[p]) + i] = Number(s[p + u.length]);
            } else s.forEach((u, p) => a[Number(p)] = Number(u));
            let o = [];
            return a.forEach((u) => o.push(u)), { mode: t.mode, value: n, pads: o };
          } else return t;
        }, wp = (e, t) => {
          Id(e.inputs);
          let s = gp(e.inputs, t);
          e.compute(_p(e.inputs, s), { inputs: [0] });
        };
      }), Wn, _a, ga, wa, Ci, ya, yp, Ma, ba, va, Mp, zd, Bd, Rd, xa, Nd, jd, Ud, Vd, bp = g(() => {
        We(), zt(), Ft(), Yt(), Wn = (e) => {
          if (F.webgpu.validateInputContent && (!e || e.length !== 1)) throw new Error("Pool ops requires 1 input.");
        }, _a = (e, t, s) => {
          let n = t.format === "NHWC", i = e.dims.slice();
          n && i.splice(1, 0, i.pop());
          let a = Object.hasOwnProperty.call(t, "dilations"), o = t.kernelShape.slice(), u = t.strides.slice(), p = a ? t.dilations.slice() : [], h = t.pads.slice();
          Js.adjustPoolAttributes(s, i, o, u, p, h);
          let k = Js.computePoolOutputShape(s, i, u, p, o, h, t.autoPad), C = Object.assign({}, t);
          a ? Object.assign(C, { kernelShape: o, strides: u, pads: h, dilations: p, cacheKey: t.cacheKey }) : Object.assign(C, { kernelShape: o, strides: u, pads: h, cacheKey: t.cacheKey });
          let d = k.slice();
          return d.push(d.splice(1, 1)[0]), [C, n ? d : k];
        }, ga = (e, t) => {
          let s = t.format === "NHWC", n = ze.size(e), i = ze.size(t.kernelShape), a = [{ type: 12, data: n }, { type: 12, data: i }], o = [{ name: "outputSize", type: "u32" }, { name: "kernelSize", type: "u32" }];
          if (t.kernelShape.length <= 2) {
            let u = t.kernelShape[t.kernelShape.length - 1], p = t.strides[t.strides.length - 1], h = t.pads[t.pads.length / 2 - 1], k = t.pads[t.pads.length - 1], C = !!(h + k);
            a.push({ type: 12, data: u }, { type: 12, data: p }, { type: 12, data: h }, { type: 12, data: k }), o.push({ name: "kw", type: "u32" }, { name: "sw", type: "u32" }, { name: "pwStart", type: "u32" }, { name: "pwEnd", type: "u32" });
            let d = !1;
            if (t.kernelShape.length === 2) {
              let z = t.kernelShape[t.kernelShape.length - 2], B = t.strides[t.strides.length - 2], V = t.pads[t.pads.length / 2 - 2], Z = t.pads[t.pads.length - 2];
              d = !!(V + Z), a.push({ type: 12, data: z }, { type: 12, data: B }, { type: 12, data: V }, { type: 12, data: Z }), o.push({ name: "kh", type: "u32" }, { name: "sh", type: "u32" }, { name: "phStart", type: "u32" }, { name: "phEnd", type: "u32" });
            }
            return [a, o, !0, C, d];
          } else {
            if (s) throw new Error("Pooling with kernelShape.length > 2 is not supported for NHWC format.");
            let u = ze.computeStrides(t.kernelShape);
            a.push({ type: 12, data: u }, { type: 12, data: t.pads }, { type: 12, data: t.strides }), o.push({ name: "kernelStrides", type: "u32", length: u.length }, { name: "pads", type: "u32", length: t.pads.length }, { name: "strides", type: "u32", length: t.strides.length });
            let p = t.pads.reduce((h, k) => h + k);
            return [a, o, !!p, !1, !1];
          }
        }, wa = (e, t, s, n, i, a, o, u, p, h, k, C) => {
          let d = i.format === "NHWC", z = t.type.value, B = It("output", t.type.tensor, n);
          if (i.kernelShape.length <= 2) {
            let V = "", Z = "", ee = "", X = s - (d ? 2 : 1);
            if (k ? V = `
                for (var i: u32 = 0u; i < uniforms.kw; i++) {
                  xIndices[${X}] = indices[${X}] * uniforms.sw - uniforms.pwStart + i;
                  if (xIndices[${X}] < 0 || xIndices[${X}]
                      >= uniforms.x_shape[${X}]) {
                    pad++;
                    continue;
                  }
                  let x_val = x[${t.indicesToOffset("xIndices")}];
                  ${a}
                }` : V = `
                for (var i: u32 = 0u; i < uniforms.kw; i++) {
                  xIndices[${X}] = indices[${X}] * uniforms.sw - uniforms.pwStart + i;
                  let x_val = x[${t.indicesToOffset("xIndices")}];
                  ${a}
                }`, i.kernelShape.length === 2) {
              let he = s - (d ? 3 : 2);
              C ? Z = `
                for (var j: u32 = 0u; j < uniforms.kh; j++) {
                  xIndices[${he}] = indices[${he}] * uniforms.sh - uniforms.phStart + j;
                  if (xIndices[${he}] < 0 || xIndices[${he}] >= uniforms.x_shape[${he}]) {
                    pad += i32(uniforms.kw);
                    continue;
                  }
              ` : Z = `
                for (var j: u32 = 0u; j < uniforms.kh; j++) {
                  xIndices[${he}] = indices[${he}] * uniforms.sh - uniforms.phStart + j;
                `, ee = `
              }
            `;
            }
            return `
            ${e.registerUniforms(p).declareVariables(t, B)}

            ${e.mainStart()}
              ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

              let indices = ${B.offsetToIndices("global_idx")};
              var xIndices = ${B.offsetToIndices("global_idx")};

              var value = ${z}(${u});
              var pad = 0;
              ${Z}
              ${V}
              ${ee}
              ${o}

              output[global_idx] = value;
            }`;
          } else {
            if (d) throw new Error("Pooling with kernelShape.length > 2 is not supported for NHWC format.");
            let V = i.kernelShape.length, Z = i.pads.length, ee = "";
            return h ? ee = `
                if (xIndices[j] >= uniforms.x_shape[j]) {
                  pad++;
                  isPad = true;
                  break;
                }
              }
              if (!isPad) {
                let x_val = x[${t.indicesToOffset("xIndices")}];
                ${a}
              }` : ee = `
              }
              let x_val = x[${t.indicesToOffset("xIndices")}];
              ${a}
            `, `
            ${e.registerUniforms(p).declareVariables(t, B)}

            ${e.mainStart()}
              ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
              let indices = ${B.offsetToIndices("global_idx")};
              var xIndices = ${B.offsetToIndices("global_idx")};

              var offsets: array<u32, ${V}>;

              var value = ${z}(${u});
              var pad = 0;
              var isPad = false;

              for (var i: u32 = 0u; i < uniforms.kernelSize; i++) {
                var offset = i;
                for (var j = 0u; j < ${V - 1}u; j++) {
                  offsets[j] = offset / ${$t("uniforms.kernelStrides", "j", V)};
                  offset -= offsets[j] * ${$t("uniforms.kernelStrides", "j", V)};
                }
                offsets[${V - 1}] = offset;

                isPad = false;
                for (var j = ${s - V}u; j < ${s}u; j++) {
                  xIndices[j] = indices[j] * ${$t("uniforms.strides", `j - ${s - V}u`, V)}
                    + offsets[j - ${s - V}u] - ${$t("uniforms.pads", "j - 2u", Z)};
                  ${ee}
              }
              ${o}

              output[global_idx] = value;
            }`;
          }
        }, Ci = (e) => `${e.format};${e.ceilMode};${e.autoPad};${e.kernelShape.length}`, ya = (e) => `${Ci(e)};${e.countIncludePad}`, yp = (e) => `${Ci(e)};${e.storageOrder};${e.dilations}`, Ma = (e) => ({ format: e.format, autoPad: ["NOTSET", "VALID", "SAME_UPPER", "SAME_LOWER"][e.auto_pad], ceilMode: e.ceil_mode, kernelShape: e.kernel_shape, strides: e.strides, pads: e.pads }), ba = (e, t, s, n) => {
          let [i, a] = _a(t, n, s), o = qe("x", t.dataType, t.dims.length), u = o.type.value, p = "value += x_val;", h = "";
          i.countIncludePad ? h += `value /= ${u}(uniforms.kernelSize);` : h += `value /= ${u}(i32(uniforms.kernelSize) - pad);`;
          let [k, C, d, z, B] = ga(a, i);
          k.push(...yt(t.dims, a));
          let V = ["rank"];
          return { name: e, shaderCache: { hint: `${n.cacheKey};${d};${z};${B}`, inputDependencies: V }, getRunData: () => ({ outputs: [{ dims: a, dataType: t.dataType }], dispatchGroup: { x: Math.ceil(ze.size(a) / 64) }, programUniforms: k }), getShaderSource: (Z) => wa(Z, o, t.dims.length, a.length, i, p, h, 0, C, d, z, B) };
        }, va = (e) => {
          let t = e.count_include_pad !== 0, s = Ma(e);
          if (s.ceilMode !== 0) throw new Error("using ceil() in shape computation is not yet supported for AveragePool");
          let n = { countIncludePad: t, ...s, cacheKey: "" };
          return { ...n, cacheKey: ya(n) };
        }, Mp = (e, t) => {
          Wn(e.inputs), e.compute(ba("AveragePool", e.inputs[0], !1, t));
        }, zd = { autoPad: "", ceilMode: 0, countIncludePad: !1, kernelShape: [], strides: [], pads: [], storageOrder: 0, dilations: [] }, Bd = (e) => {
          let t = e.format;
          return { format: t, ...zd, cacheKey: t };
        }, Rd = (e, t) => {
          Wn(e.inputs), e.compute(ba("GlobalAveragePool", e.inputs[0], !0, t));
        }, xa = (e, t, s, n) => {
          let [i, a] = _a(t, n, s), o = `
      value = max(x_val, value);
    `, u = "", p = qe("x", t.dataType, t.dims.length), h = ["rank"], [k, C, d, z, B] = ga(a, i);
          return k.push(...yt(t.dims, a)), { name: e, shaderCache: { hint: `${n.cacheKey};${d};${z};${B}`, inputDependencies: h }, getRunData: () => ({ outputs: [{ dims: a, dataType: t.dataType }], dispatchGroup: { x: Math.ceil(ze.size(a) / 64) }, programUniforms: k }), getShaderSource: (V) => wa(V, p, t.dims.length, a.length, i, o, u, t.dataType === 10 ? -65504 : -1e5, C, d, z, B) };
        }, Nd = (e, t) => {
          Wn(e.inputs), e.compute(xa("MaxPool", e.inputs[0], !1, t));
        }, jd = (e) => {
          let t = e.storage_order, s = e.dilations, n = Ma(e);
          if (t !== 0) throw new Error("column major storage order is not yet supported for MaxPool");
          if (n.ceilMode !== 0) throw new Error("using ceil() in shape computation is not yet supported for MaxPool");
          let i = { storageOrder: t, dilations: s, ...n, cacheKey: "" };
          return { ...i, cacheKey: yp(i) };
        }, Ud = (e) => {
          let t = e.format;
          return { format: t, ...zd, cacheKey: t };
        }, Vd = (e, t) => {
          Wn(e.inputs), e.compute(xa("GlobalMaxPool", e.inputs[0], !0, t));
        };
      }), Wd, Gd, Kd, Hd, Qp = g(() => {
        zt(), Ft(), rs(), Yt(), Wd = (e, t) => {
          if (e.length < 2 || e.length > 3) throw new Error("DequantizeLinear requires 2 or 3 inputs.");
          if (e.length === 3 && e[1].dims === e[2].dims) throw new Error("x-scale and x-zero-point must have the same shape.");
          if (e.length === 3 && e[0].dataType !== e[2].dataType) throw new Error("x and x-zero-point must have the same data type.");
          if (e[0].dataType === 6 && e.length > 2) throw new Error("In the case of dequantizing int32 there is no zero point.");
          if (e[1].dims.length !== 0 && e[1].dims.length !== 1 && e[1].dims.length !== e[0].dims.length) throw new Error("scale input must be a scalar, a 1D tensor, or have the same rank as the input tensor.");
          if (e.length > 2) {
            if (e[0].dataType !== e[2].dataType) throw new Error("x and x-zero-point must have the same data type.");
            if (e[1].dims.length !== e[2].dims.length) throw new Error("scale and zero-point inputs must have the same rank.");
            if (!e[1].dims.map((s, n) => s === e[2].dims[n]).reduce((s, n) => s && n, !0)) throw new Error("scale and zero-point inputs must have the same shape.");
          }
          if (t.blockSize > 0) {
            if (e[1].dims.length === 0 || e[1].dims.length === 1 && e[1].dims[0] === 1) throw new Error("blockSize must be set only for block quantization.");
            if (!e[1].dims.map((i, a) => a === t.axis || i === e[0].dims[a]).reduce((i, a) => i && a, !0)) throw new Error("For block qunatization, scale input shape to match the input shape except for the axis");
            if (e[1].dims.length !== e[0].dims.length) throw new Error("For block qunatization the scale input rank must be the same as the x rank.");
            let s = e[0].dims[t.axis], n = e[1].dims[t.axis];
            if (t.blockSize < Math.ceil(s / n) || t.blockSize > Math.ceil(s / (n - 1) - 1)) throw new Error("blockSize must be with in the range [ceil(dI / Si), ceil(dI / (Si - 1) - 1)].");
          }
        }, Gd = (e, t) => {
          let s = ze.normalizeAxis(t.axis, e[0].dims.length), n = e[0].dataType, i = n === 3, a = e[0].dims, o = e[1].dataType, u = ze.size(a), p = n === 3 || n === 2, h = p ? [Math.ceil(ze.size(e[0].dims) / 4)] : e[0].dims, k = e[1].dims, C = e.length > 2 ? e[2] : void 0, d = C ? p ? [Math.ceil(ze.size(C.dims) / 4)] : C.dims : void 0, z = k.length === 0 || k.length === 1 && k[0] === 1, B = z === !1 && k.length === 1, V = qt(u), Z = z && (!p || V === 4), ee = Z ? V : 1, X = Z && !p ? V : 1, he = qe("input", p ? 12 : n, h.length, X), pe = qe("scale", o, k.length), Me = C ? qe("zero_point", p ? 12 : n, d.length) : void 0, Oe = It("output", o, a.length, ee), Le = [he, pe];
          Me && Le.push(Me);
          let Ye = [h, k];
          C && Ye.push(d);
          let at = [{ type: 12, data: u / ee }, { type: 12, data: s }, { type: 12, data: t.blockSize }, ...yt(...Ye, a)], Pt = (Xt) => {
            let Zt = [{ name: "output_size", type: "u32" }, { name: "axis", type: "u32" }, { name: "block_size", type: "u32" }];
            return `
      ${Xt.registerUniforms(Zt).declareVariables(...Le, Oe)}
      ${Xt.mainStart()}
          ${Xt.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          let output_indices = ${Oe.offsetToIndices("global_idx")};

          // Set input x
          ${p ? `
            let input = ${he.getByOffset("global_idx / 4")};
            let x_vec = ${i ? "unpack4xI8(input)" : "unpack4xU8(input)"};
            let x_value = ${ee === 1 ? "x_vec[global_idx % 4]" : "x_vec"};` : `let x_value = ${he.getByOffset("global_idx")};`};

          // Set scale input
          ${z ? `let scale_value= ${pe.getByOffset("0")}` : B ? `
            let scale_index = ${Oe.indicesGet("output_indices", "uniforms.axis")};
            let scale_value= ${pe.getByOffset("scale_index")};` : `
            var scale_indices: ${pe.type.indices} = output_indices;
            let index = ${pe.indicesGet("scale_indices", "uniforms.axis")} / uniforms.block_size;
            ${pe.indicesSet("scale_indices", "uniforms.axis", "index")};
            let scale_value= ${pe.getByIndices("scale_indices")};`};

          // Set zero-point input
          ${Me ? z ? p ? `
                let zero_point_input = ${Me.getByOffset("0")};
                let zero_point_vec =  ${i ? "unpack4xI8(zero_point_input)" : "unpack4xU8(zero_point_input)"};
                let zero_point_value= zero_point_vec[0]` : `let zero_point_value = ${Me.getByOffset("0")}` : B ? p ? `
                let zero_point_index = ${Oe.indicesGet("output_indices", "uniforms.axis")};
                let zero_point_input = ${Me.getByOffset("zero_point_index / 4")};
                let zero_point_vec =  ${i ? "unpack4xI8(zero_point_input)" : "unpack4xU8(zero_point_input)"};
                let zero_point_value = zero_point_vec[zero_point_index % 4]` : `
                let zero_point_index = ${Oe.indicesGet("output_indices", "uniforms.axis")};
                let zero_point_value = ${Me.getByOffset("zero_point_index")};` : p ? `
                let zero_point_offset = ${pe.indicesToOffset("scale_indices")};
                let zero_point_input = ${Me.getByOffset("zero_point_offset / 4")};
                let zero_point_vec = ${i ? "unpack4xI8(zero_point_input)" : "unpack4xU8(zero_point_input)"};
                let zero_point_value = zero_point_vec[zero_point_offset % 4];` : `let zero_point_value = ${Me.getByIndices("scale_indices")};` : `let zero_point_value = ${p ? i ? "i32" : "u32" : he.type.value}(0);`};
      // Compute and write output
      ${Oe.setByOffset("global_idx", `${Oe.type.value}(x_value - zero_point_value) * scale_value`)};
      }`;
          };
          return { name: "DequantizeLinear", shaderCache: { hint: t.cacheKey, inputDependencies: Me ? ["rank", "rank", "rank"] : ["rank", "rank"] }, getShaderSource: Pt, getRunData: () => ({ outputs: [{ dims: a, dataType: o }], dispatchGroup: { x: Math.ceil(u / ee / 64), y: 1, z: 1 }, programUniforms: at }) };
        }, Kd = (e, t) => {
          Wd(e.inputs, t), e.compute(Gd(e.inputs, t));
        }, Hd = (e) => Bt({ axis: e.axis, blockSize: e.blockSize });
      }), qd, Qd, Xd, vp = g(() => {
        We(), zt(), Yt(), qd = (e, t, s) => {
          let n = e === t, i = e < t && s < 0, a = e > t && s > 0;
          if (n || i || a) throw new Error("Range these inputs' contents are invalid.");
        }, Qd = (e, t, s, n) => {
          let i = Math.abs(Math.ceil((t - e) / s)), a = [i], o = i, u = [{ type: 12, data: o }, { type: n, data: e }, { type: n, data: s }, ...yt(a)], p = (h) => {
            let k = It("output", n, a.length), C = k.type.value, d = [{ name: "outputSize", type: "u32" }, { name: "start", type: C }, { name: "delta", type: C }];
            return `
        ${h.registerUniforms(d).declareVariables(k)}
        ${h.mainStart()}
        ${h.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
        output[global_idx] = uniforms.start + ${C}(global_idx) * uniforms.delta;
      }`;
          };
          return { name: "Range", shaderCache: { hint: `${n}` }, getShaderSource: p, getRunData: () => ({ outputs: [{ dims: a, dataType: n }], dispatchGroup: { x: Math.ceil(o / 64) }, programUniforms: u }) };
        }, Xd = (e) => {
          let t = 0, s = 0, n = 0;
          e.inputs[0].dataType === 6 ? (t = e.inputs[0].getInt32Array()[0], s = e.inputs[1].getInt32Array()[0], n = e.inputs[2].getInt32Array()[0]) : e.inputs[0].dataType === 1 && (t = e.inputs[0].getFloat32Array()[0], s = e.inputs[1].getFloat32Array()[0], n = e.inputs[2].getFloat32Array()[0]), F.webgpu.validateInputContent && qd(t, s, n), e.compute(Qd(t, s, n, e.inputs[0].dataType), { inputs: [] });
        };
      }), Yd, Jd, xp, Ta, Tp = g(() => {
        zt(), Ft(), rs(), Yt(), Yd = (e, t, s, n) => {
          if (e !== "none" && n !== "i32" && n !== "u32" && n !== "f32") throw new Error(`Input ${n} is not supported with reduction ${e}.`);
          let i = `{
                var oldValue = 0;
                loop {
                  let newValueF32 =`, a = `;
                  let newValue = bitcast<i32>(newValueF32);
                  let res = atomicCompareExchangeWeak(&${t}, oldValue, newValue);
                  if res.exchanged {
                    break;
                  }
                  oldValue = res.old_value;
                }
              }`;
          switch (e) {
            case "none":
              return `${t}=${s};`;
            case "add":
              return n === "i32" || n === "u32" ? `atomicAdd(&${t}, bitcast<${n}>(${s}));` : `
              ${i}bitcast<${n}>(oldValue) + (${s})${a}`;
            case "max":
              return n === "i32" || n === "u32" ? `atomicMax(&${t}, bitcast<${n}>(${s}));` : `
                ${i}max(bitcast<f32>(oldValue), (${s}))${a}`;
            case "min":
              return n === "i32" || n === "u32" ? `atomicMin(&${t}, bitcast<${n}>(${s}));` : `${i}min(bitcast<${n}>(oldValue), (${s}))${a}`;
            case "mul":
              return `${i}(bitcast<${n}>(oldValue) * (${s}))${a}`;
            default:
              throw new Error(`Reduction ${e} is not supported.`);
          }
        }, Jd = (e, t) => {
          let s = e[0].dims, n = e[1].dims, i = s, a = 1, o = Math.ceil(ze.size(n) / a), u = n[n.length - 1], p = ze.sizeFromDimension(s, u), h = [{ type: 12, data: o }, { type: 12, data: u }, { type: 12, data: p }, ...yt(e[1].dims, e[2].dims, i)], k = (C) => {
            let d = qe("indices", e[1].dataType, e[1].dims.length), z = qe("updates", e[2].dataType, e[2].dims.length, a), B = t.reduction !== "none" && t.reduction !== "" ? Oa("output", e[0].dataType, i.length) : It("output", e[0].dataType, i.length, a);
            return `
      ${C.registerUniform("output_size", "u32").registerUniform("last_index_dimension", "u32").registerUniform("num_updates_elements", "u32").declareVariables(d, z, B)}
      ${C.mainStart()}
        ${C.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
  var data_offset = 0u;
  let indices_start = uniforms.last_index_dimension * global_idx;
  let indices_end = indices_start + uniforms.last_index_dimension;
  for (var i = indices_start; i < indices_end; i++) {
    var index = i32(indices[i].x);
    ${e[0].dims.length === 1 ? `
    let element_count_dim = uniforms.output_strides;
    let dim_value = uniforms.output_shape;` : `
    let element_count_dim = uniforms.output_strides[i - indices_start];
    let dim_value = uniforms.output_shape[i - indices_start + uniforms.last_index_dimension];`}
    if (index >= 0) {
      if (index >= i32(dim_value)) {
        index = i32(dim_value - 1);
      }
    } else {
      if (index < -i32(dim_value)) {
        index = 0;
      } else {
        index += i32(dim_value);
      }
    }
    data_offset += u32((u32(index) * element_count_dim));
  }

  for (var i = 0u; i < uniforms.num_updates_elements; i++) {
    let value = updates[uniforms.num_updates_elements * global_idx + i];
    ${Yd(t.reduction, "output[data_offset + i]", "value", B.type.value)}
  }

      }`;
          };
          return { name: "ScatterND", shaderCache: { hint: `${t.cacheKey}_${t.reduction}`, inputDependencies: ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: i, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(o / 64) }, programUniforms: h }), getShaderSource: k };
        }, xp = (e) => Bt({ reduction: e.reduction }), Ta = (e, t) => {
          e.compute(Jd(e.inputs, t), { inputs: [e.inputs[1], e.inputs[2]], outputs: [] });
        };
      }), Zd, ec, tc, sc, rc, nc, ic, oc, ac, lc, uc, Pa, dc, cc, pc, hc, mc, fc, _c, Pp = g(() => {
        zt(), Ft(), rs(), Yt(), Zd = (e, t) => {
          if (e.every((s) => s > 0 || (() => {
            throw new Error("Resize requires scales input values to be positive");
          })), e.length > 0) {
            if (t.mode === "linear") {
              if (!(e.length === 2 || e.length === 3 || e.length === 4 && e[0] === 1 && e[1] === 1 || e.length === 4 && e[0] === 1 && e[3] === 1 || e.length === 5 && e[0] === 1 && e[1] === 1)) throw new Error(`For linear mode, Resize requires scales to be 2D, 3D, 4D with either two outermost or one innermost and
            one outermost scale values equal to 1, or 5D with two outermost scale values equal to 1`);
            } else if (t.mode === "cubic" && !(e.length === 2 || e.length === 4 && e[0] === 1 && e[1] === 1 || e.length === 4 && e[0] === 1 && e[3] === 1)) throw new Error("Resize requires scales input size to be 2 or 4 for cubic mode");
          }
        }, ec = (e, t, s) => {
          t.every((i) => i >= 0 && i < s || (() => {
            throw new Error("Resize requires axes input values to be positive and less than rank");
          }));
          let n = new Array(s).fill(1);
          return t.forEach((i, a) => n[i] = e[a]), n;
        }, tc = (e, t, s, n, i, a) => {
          let [o, u, p] = s > 10 ? [1, 2, 3] : [-1, e.length > 1 ? 1 : -1, -1], h = e[0].dims.length;
          if (o > 0 && e.length > o && e[o].dims.length > 0) e[o].getFloat32Array().forEach((k) => a.push(k));
          else if (t.coordinateTransformMode === "tf_crop_and_resize") throw new Error("Resize requires RoI input to be specified when coordinateTransformMode is tfCropAndResize");
          if (u > 0 && e.length > u && e[u].dims.length === 1 && e[u].dims[0] > 0) {
            if (e[u].getFloat32Array().forEach((k) => n.push(k)), n.length !== 0 && n.length !== h && s >= 18 && n.length !== t.axes.length) throw new Error("Resize requires scales input size to be same as input rank or axes size for opset 18 and up");
            Zd(n, t), t.axes.length > 0 && ec(n, t.axes, h).forEach((k, C) => n[C] = k);
          }
          if (p > 0 && e.length > p && e[p].dims.length === 1 && e[p].dims[0] > 0 && (e[p].getBigInt64Array().forEach((k) => i.push(Number(k))), i.length !== 0 && i.length !== h && s >= 18 && i.length !== t.axes.length)) throw new Error("Resize requires sizes input size to be same as input rank or axes size for opset 18 and up");
          if (t.axes.length > 0) {
            if (n.length !== 0 && n.length !== t.axes.length) throw new Error('Resize requires "scales" input size to be of axes rank when axes attributes is specified');
            if (i.length !== 0 && i.length !== t.axes.length) throw new Error('Resize requires "sizes" input size to be of rank axes rank when axes attributes is specified');
          }
          if (typeof n < "u" && typeof i < "u" && n.length > 0 && i.length > h) throw new Error("Resize requires only of scales or sizes to be specified");
        }, sc = (e, t) => `fn getOriginalCoordinateFromResizedCoordinate(xResized: u32, xScale: f32, lengthResized: u32,
     lengthOriginal: u32, roiStart: f32, roiEnd: f32) -> ${t} { ` + (() => {
          switch (e) {
            case "asymmetric":
              return `return ${t}(xResized) / ${t}(xScale);`;
            case "pytorch_half_pixel":
              return `if (lengthResized > 1) {
                    return (${t}(xResized) + 0.5) / ${t}(xScale) - 0.5;
                  } else {
                    return 0.0;
                  }`;
            case "tf_half_pixel_for_nn":
              return `return (${t}(xResized) + 0.5) / ${t}(xScale);`;
            case "align_corners":
              return `if (lengthResized == 1) {
                    return 0.0;
                  } else {
                    // The whole part and the fractional part are calculated separately due to inaccuracy of floating
                    // point division. As an example, f32(21) / f32(7) may evaluate to 2.99... instead of 3, causing an
                    // offset-by-one error later in floor().
                    let whole = ${t}(xResized * (lengthOriginal - 1) / (lengthResized - 1));
                    let fract =
                        ${t}(xResized * (lengthOriginal - 1) % (lengthResized - 1)) / ${t}(lengthResized - 1);
                    return whole + fract;
                  }`;
            case "tf_crop_and_resize":
              return `if (lengthResized > 1) {
                    return ${t}(roiStart) * ${t}(lengthOriginal - 1) +
                        (${t}(xResized) * ${t}(roiEnd - roiStart) * ${t}(lengthOriginal - 1)) /
                        ${t}(lengthResized - 1);
                  } else {
                    return 0.5 * ${t}(roiStart + roiEnd) * ${t}(lengthOriginal - 1);
                  }`;
            case "half_pixel_symmetric":
              return `const outputWidth = ${t}xScale * ${t}(lengthResized);
                  const adjustment = ${t}(lengthResized) / outputWidth;
                  const center = ${t}(lengthOriginal) / 2;
                  const offset = center * (1 - adjustment);
                  return offset + ((${t}(xResized) + 0.5) / ${t}(xScale)) - 0.5;`;
            case "half_pixel":
              return `return ((${t}(xResized) + 0.5) / ${t}(xScale)) - 0.5;`;
            default:
              throw new Error(`Coordinate transform mode ${e} is not supported`);
          }
        })() + "}", rc = (e, t, s) => `fn getNearestPixelFromOriginal(xOriginal: ${s}, isDownSample: bool) -> ${s} {` + (() => {
          switch (e) {
            case "round_prefer_ceil":
              return "if (fract(xOriginal) == 0.5) {             return ceil(xOriginal);           } else {             return round(xOriginal);           }";
            case "floor":
              return "return floor(xOriginal);";
            case "ceil":
              return "return ceil(xOriginal);";
            case "round_prefer_floor":
              return "if (fract(xOriginal) == 0.5) {                     return floor(xOriginal);                   } else {                     return round(xOriginal);                   }";
            case "simple":
            default:
              if (t < 11) return "if (isDownSample)                     {                       return ceil(xOriginal);                     } else {                       return xOriginal;                     }";
              throw new Error(`Nearest mode ${e} is not supported`);
          }
        })() + "}", nc = (e, t, s) => {
          let n = new Array(s).fill(0).concat(new Array(s).fill(1)), i = e.length === 0 ? n : e.slice();
          return t.length > 0 ? (t.forEach((a, o) => {
            n[a] = i[o], n[o + s] = i[t.length + o];
          }), n) : i;
        }, ic = (e, t, s, n) => {
          let i = [];
          if (s.length > 0) if (n.length > 0) {
            if (e.forEach((a) => i.push(a)), Math.max(...n) > e.length) throw new Error("axes is out of bound");
            n.forEach((a, o) => i[a] = s[o]);
          } else s.forEach((a) => i.push(a));
          else {
            if (t.length === 0) throw new Error("Resize requires either scales or sizes.");
            i = e.map((a, o) => Math.round(a * t[o]));
          }
          return i;
        }, oc = (e, t, s) => {
          let n = (() => {
            switch (s.keepAspectRatioPolicy) {
              case "not_larger":
                return s.axes.length > 0 ? Math.min(...s.axes.map((a) => t[a]), Number.MAX_VALUE) : Math.min(...t, Number.MAX_VALUE);
              case "not_smaller":
                return s.axes.length > 0 ? Math.max(...s.axes.map((a) => t[a]), Number.MIN_VALUE) : Math.max(...t, Number.MIN_VALUE);
              default:
                throw new Error(`Keep aspect ratio policy ${s.keepAspectRatioPolicy} is not supported`);
            }
          })();
          t.fill(1, 0, t.length);
          let i = e.slice();
          return s.axes.length > 0 ? (s.axes.forEach((a) => t[a] = n), s.axes.forEach((a) => i[a] = Math.round(e[a] * t[a]))) : (t.fill(n, 0, t.length), i.forEach((a, o) => i[o] = Math.round(a * t[o]))), i;
        }, ac = (e, t, s, n, i) => `
    fn calculateOriginalIndicesFromOutputIndices(output_indices: ${e.type.indices}) -> array<${e.type.value}, ${s.length}> {
      var original_indices: array<${e.type.value}, ${s.length}>;
      for (var i:u32 = 0; i < ${s.length}; i++) {
        var output_index = ${e.indicesGet("output_indices", "i")};
        var scale = ${$t("uniforms.scales", "i", n)};
        var roi_low = ${$t("uniforms.roi", "i", i)};
        var roi_hi = ${$t("uniforms.roi", `i + ${t.length}`, i)};
        if (scale == 1.0) {
          original_indices[i] = ${e.type.value}(output_index);
        } else {
          var input_shape_i = ${$t("uniforms.input_shape", "i", t.length)};
          var output_shape_i = ${$t("uniforms.output_shape", "i", s.length)};
          original_indices[i] = getOriginalCoordinateFromResizedCoordinate(output_index, scale, output_shape_i,
                                                                           input_shape_i, roi_low, roi_hi);
        }
      }
      return original_indices;
    }`, lc = (e, t, s, n, i, a, o) => `
    fn calculateInputIndicesFromOutputIndices(output_indices: ${t.type.indices}) -> ${e.type.indices} {
      var input_indices: ${e.type.indices};
      for (var i:u32 = 0; i < ${n.length}; i++) {
        var output_index = ${t.indicesGet("output_indices", "i")};
        var input_index: u32;
        var scale = ${$t("uniforms.scales", "i", i)};
        if (scale == 1.0) {
          input_index = output_index;
        } else {
          var roi_low = ${$t("uniforms.roi", "i", a)};
          var roi_hi = ${$t("uniforms.roi", `i + ${s.length}`, a)};
          var input_shape_i = ${$t("uniforms.input_shape", "i", s.length)};
          var output_shape_i = ${$t("uniforms.output_shape", "i", n.length)};
          var original_idx = getOriginalCoordinateFromResizedCoordinate(output_index, scale, output_shape_i,
                                                                        input_shape_i, roi_low, roi_hi);
          if (!${o} || (original_idx >= 0 && original_idx < ${t.type.value}(input_shape_i))) {
            if (original_idx < 0) {
              input_index = 0;
            } else if (original_idx > ${t.type.value}(input_shape_i - 1)) {
              input_index = input_shape_i - 1;
            } else {
              input_index = u32(getNearestPixelFromOriginal(original_idx, scale < 1));
            }
          } else {
            input_index = u32(original_idx);
          }
        }
        ${e.indicesSet("input_indices", "i", " input_index")}
      }
      return input_indices;
    }`, uc = (e, t) => `
    fn checkInputIndices(input_indices: ${e.type.indices}) -> bool {
      for (var i:u32 = 0; i < ${t.length}; i++) {
        var input_index = ${e.indicesGet("input_indices", "i")};
        if (input_index < 0 || input_index >= ${$t("uniforms.input_shape", "i", t.length)}) {
          return false;
        }
      }
      return true;
    }`, Pa = (e, t, s, n) => e.rank > n ? `
    ${e.indicesSet("input_indices", t, "channel")};
    ${e.indicesSet("input_indices", s, "batch")};
` : "", dc = (e, t, s, n, i) => {
          let [a, o, u, p] = s.length === 2 ? [-1, 0, 1, -1] : [0, 2, 3, 1], h = e.type.value;
          return `
    fn getInputValue(batch: u32, channel: u32, row: u32, col: u32) -> ${h} {
      var input_indices: ${e.type.indices};
      ${e.indicesSet("input_indices", o, `max(0, min(row, ${s[o]} - 1))`)};
      ${e.indicesSet("input_indices", u, `max(0, min(col, ${s[u]} - 1))`)};
      ${Pa(e, p, a, 2)}
      return ${e.getByIndices("input_indices")};
    }

    fn bilinearInterpolation(output_indices: ${t.type.indices}) -> ${h} {
      var originalIndices = calculateOriginalIndicesFromOutputIndices(output_indices);
      var row:${h} = originalIndices[${o}];
      var col:${h} = originalIndices[${u}];
      ${n ? `if (row < 0 || row > (${s[o]} - 1) || col < 0 || col > (${s[u]} - 1)) {
        return ${i};
      }` : ""};
      row = max(0, min(row, ${s[o]} - 1));
      col = max(0, min(col, ${s[u]} - 1));
      var row1: u32 = u32(row);
      var col1: u32 = u32(col);
      var row2: u32 = u32(row + 1);
      var col2: u32 = u32(col + 1);
      var channel: u32 = ${s.length > 2 ? `u32(originalIndices[${p}])` : "0"};
      var batch: u32 =  ${s.length > 2 ? `u32(originalIndices[${a}])` : "0"};
      var x11: ${h} = getInputValue(batch, channel, row1, col1);
      var x12: ${h} = getInputValue(batch, channel, row1, col2);
      var x21: ${h} = getInputValue(batch, channel, row2, col1);
      var x22: ${h} = getInputValue(batch, channel, row2, col2);
      var dx1: ${h} = abs(row - ${h}(row1));
      var dx2: ${h} = abs(${h}(row2) - row);
      var dy1: ${h} = abs(col - ${h}(col1));
      var dy2: ${h} = abs(${h}(col2) - col);
      if (row1 == row2) {
        dx1 = 0.5;
        dx2 = 0.5;
      }
      if (col1 == col2) {
        dy1 = 0.5;
        dy2 = 0.5;
      }
      return (x11 * dx2 * dy2 + x12 * dx2 * dy1 + x21 * dx1 * dy2 + x22 * dx1 * dy1);
    }`;
        }, cc = (e, t, s, n, i, a, o, u, p, h) => {
          let k = s.length === 2, [C, d] = k ? [0, 1] : [2, 3], z = e.type.value, B = (V) => {
            let Z = V === C ? "row" : "col";
            return `
      fn ${Z}CubicInterpolation(input_indices: ${e.type.indices}, output_indices: ${t.type.indices}) -> ${z} {
        var output_index = ${t.indicesGet("output_indices", V)};
        var originalIdx: ${z} = getOriginalCoordinateFromResizedCoordinate(output_index, ${i[V]},
        ${n[V]}, ${s[V]}, ${a[V]}, ${a[V]} + ${s.length});
        var fractOriginalIdx: ${z} = originalIdx - floor(originalIdx);
        var coefs = getCubicInterpolationCoefs(fractOriginalIdx);

        if (${u} && (originalIdx < 0 || originalIdx > (${s[V]} - 1))) {
          return ${p};
        }
        var data: array<${z}, 4> = array<${z}, 4>(0.0, 0.0, 0.0, 0.0);
        for (var i: i32 = -1; i < 3; i++) {
          var ${Z}: ${z} = originalIdx + ${z}(i);
          if (${Z} < 0 || ${Z} >= ${s[V]}) {
            ${h ? `coefs[i + 1] = 0.0;
                        continue;` : u ? `return ${p};` : `${Z} = max(0, min(${Z}, ${s[V]} - 1));`};
          }
        var input_indices_copy: ${e.type.indices} = input_indices;
          ${e.indicesSet("input_indices_copy", V, `u32(${Z})`)};
          data[i + 1] = ${V === C ? e.getByIndices("input_indices_copy") : "rowCubicInterpolation(input_indices_copy, output_indices)"};
        }
        return cubicInterpolation1D(data, coefs);
      }`;
          };
          return `
    ${B(C)};
    ${B(d)};
  fn getCubicInterpolationCoefs(s: ${z}) -> array<${z}, 4> {
    var absS = abs(s);
    var coeffs: array<${z}, 4> = array<${z}, 4>(0.0, 0.0, 0.0, 0.0);
    var oneMinusAbsS: ${z} = 1.0 - absS;
    var twoMinusAbsS: ${z} = 2.0 - absS;
    var onePlusAbsS: ${z} = 1.0 + absS;
    coeffs[0] = ((${o} * onePlusAbsS - 5 * ${o}) * onePlusAbsS + 8 * ${o}) * onePlusAbsS - 4 * ${o};
    coeffs[1] = ((${o} + 2) * absS - (${o} + 3)) * absS * absS + 1;
    coeffs[2] = ((${o} + 2) * oneMinusAbsS - (${o} + 3)) * oneMinusAbsS * oneMinusAbsS + 1;
    coeffs[3] = ((${o} * twoMinusAbsS - 5 * ${o}) * twoMinusAbsS + 8 * ${o}) * twoMinusAbsS - 4 * ${o};
    return coeffs;
  }

  fn cubicInterpolation1D(x: array<${z}, 4>, coefs: array<${z}, 4>) -> ${z} {
    var coefsSum: ${z} = coefs[0] + coefs[1] + coefs[2] + coefs[3];
    return (x[0] * coefs[0] + x[1] * coefs[1]+ x[2] * coefs[2]+ x[3] * coefs[3]) / coefsSum;
  }

  fn bicubicInterpolation(output_indices: ${t.type.indices}) -> ${z} {
    var input_indices: ${e.type.indices} = output_indices;
    return colCubicInterpolation(input_indices, output_indices);
  }
    `;
        }, pc = (e, t, s, n, i) => {
          let [a, o, u, p, h] = s.length === 3 ? [-1, 0, 1, 2, -1] : [0, 2, 3, 4, 1], k = e.type.value;
          return `
    fn getInputValue(batch: u32, channel: u32, depth:u32, height: u32, width: u32) -> ${k} {
      var input_indices: ${e.type.indices};
      ${e.indicesSet("input_indices", o, `max(0, min(depth, ${s[o]} - 1))`)};
      ${e.indicesSet("input_indices", u, `max(0, min(height, ${s[u]} - 1))`)};
      ${e.indicesSet("input_indices", p, `max(0, min(width, ${s[p]} - 1))`)};
      ${Pa(e, h, a, 3)}
      return ${e.getByIndices("input_indices")};
    }

    fn trilinearInterpolation(output_indices: ${t.type.indices}) -> ${k} {
      var originalIndices = calculateOriginalIndicesFromOutputIndices(output_indices);
      var depth:${k} = originalIndices[${o}];
      var height:${k} = originalIndices[${u}];
      var width:${k} = originalIndices[${p}];
      ${n ? `if (depth < 0 || depth > (${s[o]} - 1) || height < 0 || height > (${s[u]} - 1) || width < 0 || (width > ${s[p]} - 1)) {
      return ${i};
        }` : ""};

    depth = max(0, min(depth, ${s[o]} - 1));
      height = max(0, min(height, ${s[u]} - 1));
      width = max(0, min(width, ${s[p]} - 1));
      var depth1: u32 = u32(depth);
      var height1: u32 = u32(height);
      var width1: u32 = u32(width);
      var depth2: u32 = u32(depth + 1);
      var height2: u32 = u32(height + 1);
      var width2: u32 = u32(width + 1);
      var channel: u32 = ${s.length > 3 ? `u32(originalIndices[${h}])` : "0"};
      var batch: u32 =  ${s.length > 3 ? `u32(originalIndices[${a}])` : "0"};

      var x111: ${k} = getInputValue(batch, channel, depth1, height1, width1);
      var x112: ${k} = getInputValue(batch, channel, depth1, height1, width2);
      var x121: ${k} = getInputValue(batch, channel, depth1, height2, width1);
      var x122: ${k} = getInputValue(batch, channel, depth1, height2, width2);
      var x211: ${k} = getInputValue(batch, channel, depth2, height1, width1);
      var x212: ${k} = getInputValue(batch, channel, depth2, height1, width2);
      var x221: ${k} = getInputValue(batch, channel, depth2, height2, width1);
      var x222: ${k} = getInputValue(batch, channel, depth2, height2, width2);
      var dx1: ${k} = abs(depth - ${k}(depth1));
      var dx2: ${k} = abs(${k}(depth2) - depth);
      var dy1: ${k} = abs(height - ${k}(height1));
      var dy2: ${k} = abs(${k}(height2) - height);
      var dz1: ${k} = abs(width - ${k}(width1));
      var dz2: ${k} = abs(${k}(width2) - width);
      if (depth1 == depth2) {
        dx1 = 0.5;
        dx2 = 0.5;
      }
      if (height1 == height2) {
        dy1 = 0.5;
        dy2 = 0.5;
      }
      if (width1 == width2) {
        dz1 = 0.5;
        dz2 = 0.5;
      }
      return (x111 * dx2 * dy2 * dz2 + x112 * dx2 * dy2 * dz1 + x121 * dx2 * dy1 *dz2 + x122 * dx2 * dy1 * dz1 +
              x211 * dx1 * dy2 * dz2 + x212 * dx1 * dy2 * dz1 + x221 * dx1 * dy1 *dz2 + x222 * dx1 * dy1 * dz1);
    }`;
        }, hc = (e, t, s, n, i, a) => {
          let o = e.dims, u = nc(a, t.axes, o.length), p = ic(o, n, i, t.axes), h = n.slice();
          n.length === 0 && (h = o.map((X, he) => X === 0 ? 1 : p[he] / X), t.keepAspectRatioPolicy !== "stretch" && (p = oc(o, h, t)));
          let k = It("output", e.dataType, p.length), C = qe("input", e.dataType, o.length), d = ze.size(p), z = o.length === p.length && o.every((X, he) => X === p[he]), B = t.coordinateTransformMode === "tf_crop_and_resize", V = t.extrapolationValue, Z = C.type.value, ee = (X) => `
      ${z ? "" : `
      ${sc(t.coordinateTransformMode, Z)};
      ${(() => {
            switch (t.mode) {
              case "nearest":
                return `
              ${uc(C, o)};
              ${rc(t.nearestMode, s, Z)};
              ${lc(C, k, o, p, h.length, u.length, B)};
              `;
              case "linear":
                return `
              ${ac(k, o, p, h.length, u.length)};
              ${(() => {
                  if (o.length === 2 || o.length === 4) return `${dc(C, k, o, B, V)}`;
                  if (o.length === 3 || o.length === 5) return `${pc(C, k, o, B, V)}`;
                  throw Error("Linear mode only supports input dims 2, 3, 4 and 5 are supported in linear mode.");
                })()};
            `;
              case "cubic":
                return `
            ${(() => {
                  if (o.length === 2 || o.length === 4) return `${cc(C, k, o, p, h, u, t.cubicCoeffA, B, t.extrapolationValue, t.excludeOutside)}`;
                  throw Error("Cubic mode only supports input dims 2 and 4 are supported in linear mode.");
                })()};
            `;
              default:
                throw Error("Invalid resize mode");
            }
          })()};
      `}
      ${X.registerUniform("output_size", "u32").registerUniform("scales", "f32", h.length).registerUniform("roi", "f32", u.length).declareVariables(C, k)}
      ${X.mainStart()}
        ${X.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
        ${z ? "output[global_idx] = input[global_idx];" : `
        let output_indices = ${k.offsetToIndices("global_idx")};
        var input_indices: ${C.type.indices};
        ${(() => {
            switch (t.mode) {
              case "nearest":
                return `input_indices = calculateInputIndicesFromOutputIndices(output_indices);
                if (checkInputIndices(input_indices)) {
                  output[global_idx] = ${C.getByIndices("input_indices")};
                } else {
                  output[global_idx] = ${t.extrapolationValue};
                }`;
              case "linear":
                return `output[global_idx] = ${o.length === 2 || o.length === 4 ? "bilinearInterpolation" : "trilinearInterpolation"}(output_indices);`;
              case "cubic":
                return "output[global_idx] = bicubicInterpolation(output_indices);";
              default:
                throw Error(`Unsupported resize mode: ${t.mode}`);
            }
          })()};
`}
      }`;
          return { name: "Resize", shaderCache: { hint: `${t.cacheKey}|${s}|${h.length > 0 ? h : ""}|${i.length > 0 ? i : ""}|${u.length > 0 ? u : ""}|${z}|${o}`, inputDependencies: ["rank"] }, getShaderSource: ee, getRunData: () => ({ outputs: [{ dims: p, dataType: e.dataType }], dispatchGroup: { x: Math.ceil(d / 64) }, programUniforms: [{ type: 12, data: d }, { type: 1, data: h }, { type: 1, data: u }, ...yt(o, p)] }) };
        }, mc = (e) => {
          let t = e.customDataBuffer;
          return new Uint32Array(t, t.byteOffset, 1)[0];
        }, fc = (e, t) => {
          let s = [], n = [], i = [], a = mc(e);
          if (t.antialias !== 0) throw Error("Only default value (0) for Antialias attribute is supported");
          tc(e.inputs, t, a, s, n, i), e.compute(hc(e.inputs[0], t, a, s, n, i), { inputs: [0] });
        }, _c = (e) => {
          let t = e.antialias, s = e.axes, n = e.coordinateTransformMode, i = e.cubicCoeffA, a = e.excludeOutside !== 0, o = e.extrapolationValue, u = e.keepAspectRatioPolicy, p = e.mode, h = e.nearestMode === "" ? "simple" : e.nearestMode;
          return Bt({ antialias: t, axes: s, coordinateTransformMode: n, cubicCoeffA: i, excludeOutside: a, extrapolationValue: o, keepAspectRatioPolicy: u, mode: p, nearestMode: h });
        };
      }), gc, wc, yc, Ep = g(() => {
        zt(), Ft(), rs(), Yt(), gc = (e, t) => {
          let [s, n, i, a] = e, { numHeads: o, rotaryEmbeddingDim: u } = t;
          if (s.dims.length !== 3 && s.dims.length !== 4) throw new Error(`Input 'x' is expected to have 3 or 4 dimensions, got ${s.dims.length}`);
          if (!ze.areEqual(n.dims, []) && !ze.areEqual(n.dims, [1]) && n.dims.length !== 2) throw new Error(`Input 'position_ids' is expected to have 0, 1, or 2 dimensions, got ${n.dims.length}`);
          if (i.dims.length !== 2) throw new Error(`Input 'cos_cache' is expected to have 2 dimensions, got ${i.dims.length}`);
          if (a.dims.length !== 2) throw new Error(`Input 'sin_cache' is expected to have 2 dimensions, got ${a.dims.length}`);
          if (!ze.areEqual(i.dims, a.dims)) throw new Error("Inputs 'cos_cache' and 'sin_cache' are expected to have the same shape");
          if (u > 0 && o === 0) throw new Error("num_heads must be provided if rotary_embedding_dim is specified");
          let p = s.dims[0], h = s.dims[s.dims.length - 2], k = i.dims[0], C = ze.sizeFromDimension(s.dims, 1) / h, d = u === 0 ? i.dims[1] * 2 : C / o;
          if (u > d) throw new Error("rotary_embedding_dim must be less than or equal to head_size");
          if (n.dims.length === 2) {
            if (p !== n.dims[0]) throw new Error(`Input 'position_ids' dimension 0 should be of size batch_size, got ${n.dims[0]}`);
            if (h !== n.dims[1]) throw new Error(`Input 'position_ids' dimension 1 should be of size sequence_length, got ${n.dims[1]}`);
          }
          if (d / 2 !== i.dims[1] && u / 2 !== i.dims[1]) throw new Error(`Input 'cos_cache' dimension 1 should be same as head_size / 2 or rotary_embedding_dim / 2, got ${i.dims[1]}`);
          if (h > k) throw new Error("Updating cos_cache and sin_cache in RotaryEmbedding is not currently supported");
        }, wc = (e, t) => {
          let { interleaved: s, numHeads: n, rotaryEmbeddingDim: i, scale: a } = t, o = e[0].dims[0], u = ze.sizeFromDimension(e[0].dims, 1), p = e[0].dims[e[0].dims.length - 2], h = u / p, k = e[2].dims[1], C = i === 0 ? k * 2 : h / n, d = new Array(o, p, h / C, C - k), z = ze.computeStrides(d), B = [{ type: 1, data: a }, { type: 12, data: d }, { type: 12, data: z }, ...e[0].dims.length === 3 ? new Array({ type: 12, data: [u, h, C, 1] }) : [], ...e[0].dims.length === 4 ? new Array({ type: 12, data: [u, C, p * C, 1] }) : [], ...yt(e[0].dims, e[1].dims, e[2].dims, e[3].dims, e[0].dims)], V = (Z) => {
            let ee = qe("input", e[0].dataType, e[0].dims.length), X = qe("position_ids", e[1].dataType, e[1].dims.length), he = qe("cos_cache", e[2].dataType, e[2].dims.length), pe = qe("sin_cache", e[3].dataType, e[3].dims.length), Me = It("output", e[0].dataType, e[0].dims.length);
            return Z.registerUniforms([{ name: "scale", type: "f32" }, { name: "global_shape", type: "u32", length: d.length }, { name: "global_strides", type: "u32", length: z.length }, { name: "input_output_strides", type: "u32", length: z.length }]), `
        ${Z.declareVariables(ee, X, he, pe, Me)}

        ${Z.mainStart(ir)}
          let half_rotary_emb_dim = uniforms.${he.name}_shape[1];
          let bsnh = global_idx / uniforms.global_strides % uniforms.global_shape;
          let size = uniforms.global_shape[0] * uniforms.global_strides[0];
          ${Z.guardAgainstOutOfBoundsWorkgroupSizes("size")}

          if (bsnh[3] < half_rotary_emb_dim) {
            let position_ids_idx =
                ${X.broadcastedIndicesToOffset("bsnh.xy", It("", X.type.tensor, 2))};
            let position_id =
                u32(${X.getByOffset("position_ids_idx")}) + select(0, bsnh[1], position_ids_idx == 0);
            let i = dot(bsnh, uniforms.input_output_strides) + select(0, bsnh[3], ${s});
            let j = i + select(half_rotary_emb_dim, 1, ${s});
            let re = ${ee.getByOffset("i")} * ${he.get("position_id", "bsnh[3]")} -
                ${ee.getByOffset("j")} * ${pe.get("position_id", "bsnh[3]")};
            ${Me.setByOffset("i", "re")}
            let im = ${ee.getByOffset("i")} * ${pe.get("position_id", "bsnh[3]")} +
                ${ee.getByOffset("j")} * ${he.get("position_id", "bsnh[3]")};
            ${Me.setByOffset("j", "im")}
          } else {
            let k = dot(bsnh, uniforms.input_output_strides) + half_rotary_emb_dim;
            ${Me.setByOffset("k", ee.getByOffset("k"))}
          }
        }`;
          };
          return { name: "RotaryEmbedding", shaderCache: { hint: Bt({ interleaved: s }).cacheKey, inputDependencies: ["rank", "rank", "rank", "rank"] }, getShaderSource: V, getRunData: () => ({ outputs: [{ dims: e[0].dims, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(ze.size(d) / ir) }, programUniforms: B }) };
        }, yc = (e, t) => {
          gc(e.inputs, t), e.compute(wc(e.inputs, t));
        };
      }), Mc, bc, vc, Xp = g(() => {
        zt(), Ft(), Yt(), Mc = (e) => {
          if (!e || e.length < 3) throw new Error("layerNorm requires at least 3 inputs.");
          let t = e[0], s = e[1], n = e[2];
          if (t.dataType !== s.dataType || t.dataType !== n.dataType) throw new Error("All inputs must have the same data type");
          if (t.dims.length !== 3 && t.dims.length !== 2) throw new Error("Input must be 2D or 3D");
          if (s.dims.length !== 3 && s.dims.length !== 2) throw new Error("Skip must be 2D or 3D");
          let i = t.dims[t.dims.length - 1], a = t.dims[t.dims.length - 2];
          if (s.dims[s.dims.length - 1] !== i) throw new Error("Skip must have the same hidden size as input");
          if (s.dims[s.dims.length - 2] !== a) throw new Error("Skip must have the same sequence length as input");
          if (n.dims.length !== 1) throw new Error("Gamma must be 1D");
          if (n.dims[n.dims.length - 1] !== i) throw new Error("Gamma must have the same hidden size as input");
          if (e.length > 3) {
            let o = e[3];
            if (o.dims.length !== 1) throw new Error("Beta must be 1D");
            if (o.dims[o.dims.length - 1] !== i) throw new Error("Beta must have the same hidden size as input");
          }
          if (e.length > 4) {
            let o = e[4];
            if (o.dims.length !== 1) throw new Error("Bias must be 1D");
            if (o.dims[o.dims.length - 1] !== i) throw new Error("Bias must have the same hidden size as input");
          }
        }, bc = (e, t, s, n) => {
          let i = t.simplified, a = e[0].dims, o = ze.size(a), u = a, p = o, h = a.slice(-1)[0], k = n ? a.slice(0, -1).concat(1) : [], C = !i && e.length > 3, d = e.length > 4, z = n && s > 1, B = n && s > 2, V = s > 3, Z = 64, ee = qt(h), X = [{ type: 12, data: p }, { type: 12, data: ee }, { type: 12, data: h }, { type: 1, data: t.epsilon }], he = (Me) => {
            let Oe = [{ name: "output_size", type: "u32" }, { name: "components", type: "u32" }, { name: "hidden_size", type: "u32" }, { name: "epsilon", type: "f32" }], Le = [qe("x", e[0].dataType, e[0].dims, ee), qe("skip", e[1].dataType, e[1].dims, ee), qe("gamma", e[2].dataType, e[2].dims, ee)];
            C && Le.push(qe("beta", e[3].dataType, e[3].dims, ee)), d && Le.push(qe("bias", e[4].dataType, e[4].dims, ee)), Le.push(It("output", e[0].dataType, u, ee)), z && Le.push(It("mean_output", 1, k)), B && Le.push(It("inv_std_output", 1, k)), V && Le.push(It("input_skip_bias_sum", e[0].dataType, u, ee));
            let Ye = fs(e[0].dataType), at = fs(1, ee);
            return `

      ${Me.registerUniforms(Oe).declareVariables(...Le)}
      var<workgroup> sum_shared : array<${at}, ${Z}>;
      var<workgroup> sum_squared_shared : array<${at}, ${Z}>;

      ${Me.mainStart([Z, 1, 1])}
        let ix = local_id.x;
        let iy = global_id.x / ${Z};

        let hidden_size_vectorized: u32 = uniforms.hidden_size / uniforms.components;
        var stride = hidden_size_vectorized / ${Z};
        let offset = ix * stride + iy * hidden_size_vectorized;
        let offset1d = stride * ix;
        if (ix == ${Z - 1}) {
          stride = hidden_size_vectorized - stride * ix;
        }
        for (var i: u32 = 0; i < stride; i++) {
          let skip_value = skip[offset + i];
          let bias_value = ${d ? "bias[offset1d + i]" : Ye + "(0.0)"};
          let input_value = x[offset + i];
          let value = input_value + skip_value + bias_value;
          ${V ? "input_skip_bias_sum[offset + i] = value;" : ""}
          output[offset + i] = value;
          let f32_value = ${$s(Ye, ee, "value")};
          sum_shared[ix] += f32_value;
          sum_squared_shared[ix] += f32_value * f32_value;
        }
        workgroupBarrier();

        var reduce_size : u32 = ${Z};
        for (var curr_size = reduce_size >> 1;  curr_size > 0; curr_size = reduce_size >> 1) {
          reduce_size = curr_size + (reduce_size & 1);
          if (ix < curr_size) {
            sum_shared[ix] += sum_shared[ix + reduce_size];
            sum_squared_shared[ix] += sum_squared_shared[ix + reduce_size];
          }
          workgroupBarrier();
        }

        let sum = sum_shared[0];
        let square_sum = sum_squared_shared[0];
        let mean = ${Gs("sum", ee)} / f32(uniforms.hidden_size);
        let inv_std_dev = inverseSqrt(${Gs("square_sum", ee)} / f32(uniforms.hidden_size) ${i ? "" : "- mean * mean"} + uniforms.epsilon);
        ${z ? "mean_output[global_idx] = mean;" : ""}
        ${B ? "inv_std_output[global_idx] = inv_std_dev;" : ""}

        for (var i: u32 = 0; i < stride; i++) {
          output[offset + i] = (output[offset + i] ${i ? "" : `- ${Ye}(mean)`}) *
            ${Ye}(inv_std_dev) * gamma[offset1d + i]
            ${C ? "+ beta[offset1d + i]" : ""};
        }
      }`;
          }, pe = [{ dims: u, dataType: e[0].dataType }];
          return s > 1 && pe.push({ dims: k, dataType: 1 }), s > 2 && pe.push({ dims: k, dataType: 1 }), s > 3 && pe.push({ dims: a, dataType: e[0].dataType }), { name: "SkipLayerNormalization", shaderCache: { hint: `${ee};${z};${B};${V}`, inputDependencies: e.map((Me, Oe) => "type") }, getShaderSource: he, getRunData: () => ({ outputs: pe, dispatchGroup: { x: Math.ceil(p / h) }, programUniforms: X }) };
        }, vc = (e, t) => {
          Mc(e.inputs);
          let s = [0];
          e.outputCount > 1 && s.push(-3), e.outputCount > 2 && s.push(-3), e.outputCount > 3 && s.push(3), e.compute(bc(e.inputs, t, e.outputCount, !1), { outputs: s });
        };
      }), Qt, Gn, Hs, qs, tr, un, Cp, xc, kp = g(() => {
        zt(), Ft(), rs(), Yt(), Qt = (e, t) => {
          if (!e || e.length < 1) throw new Error("too few inputs");
          if (t.axes.length !== 0) {
            if (t.axes.length !== t.starts.length || t.axes.length !== t.ends.length) throw new Error("axes, starts and ends must have the same length");
          } else if (t.starts.length !== t.ends.length) throw new Error("starts and ends must have the same length");
          e.slice(1).forEach((s, n) => {
            if (e[n + 1].dataType !== 6 && e[n + 1].dataType !== 7) throw new Error(`Input ${n} must be an array of int32 or int64`);
          });
        }, Gn = (e, t) => {
          let s = [];
          if (e.length > t) if (e[t].dataType === 7) e[t].getBigInt64Array().forEach((n) => s.push(Number(n)));
          else if (e[t].dataType === 6) e[t].getInt32Array().forEach((n) => s.push(Number(n)));
          else throw new Error(`Input ${t} must be an array of int32 or int64`);
          return s;
        }, Hs = (e, t) => {
          if (e.length > 1) {
            let s = Gn(e, 1), n = Gn(e, 2), i = Gn(e, 3);
            return i.length === 0 && (i = [...Array(e[0].dims.length).keys()]), Bt({ starts: s, ends: n, axes: i });
          } else return t;
        }, qs = (e, t, s, n, i) => {
          let a = e;
          return e < 0 && (a += s[n[t]]), i[t] < 0 ? Math.max(0, Math.min(a, s[n[t]] - 1)) : Math.max(0, Math.min(a, s[n[t]]));
        }, tr = (e, t, s) => `fn calculateInputIndices(output_indices: ${t.type.indices}) -> ${e.type.indices} {
          var input_indices: ${e.type.indices};
          var carry = 0u;
          for (var i = ${s.length}; i >= 0; i--) {
            let input_shape_i = ${$t("uniforms.input_shape", "i", s.length)};
            let steps_i = ${$t("uniforms.steps", "i", s.length)};
            let signs_i = ${$t("uniforms.signs", "i", s.length)};
            let starts_i = ${$t("uniforms.starts", "i", s.length)};
            var output_index = ${t.indicesGet("output_indices", "i")};
            var input_index = output_index * steps_i + starts_i + carry;
            carry = input_index / input_shape_i;
            input_index = input_index % input_shape_i;
            if (signs_i < 0) {
              input_index = input_shape_i - input_index - 1u + starts_i;
            }
            ${e.indicesSet("input_indices", "i", "input_index")};
          }
          return input_indices;
      }`, un = (e, t) => {
          let s = e[0].dims, n = ze.size(s), i = t.axes.length > 0 ? ze.normalizeAxes(t.axes, s.length) : [...Array(s.length).keys()], a = Gn(e, 4);
          a.forEach((ee) => ee !== 0 || (() => {
            throw new Error("step cannot be 0");
          })), a.length === 0 && (a = Array(i.length).fill(1));
          let o = t.starts.map((ee, X) => qs(ee, X, s, i, a)), u = t.ends.map((ee, X) => qs(ee, X, s, i, a));
          if (i.length !== o.length || i.length !== u.length) throw new Error("start, ends and axes should have the same number of elements");
          if (i.length !== s.length) for (let ee = 0; ee < s.length; ++ee) i.includes(ee) || (o.splice(ee, 0, 0), u.splice(ee, 0, s[ee]), a.splice(ee, 0, 1));
          let p = a.map((ee) => Math.sign(ee));
          a.forEach((ee, X, he) => {
            if (ee < 0) {
              let pe = (u[X] - o[X]) / ee, Me = o[X], Oe = Me + pe * a[X];
              o[X] = Oe, u[X] = Me, he[X] = -ee;
            }
          });
          let h = s.slice(0);
          i.forEach((ee, X) => {
            h[ee] = Math.ceil((u[ee] - o[ee]) / a[ee]);
          });
          let k = { dims: h, dataType: e[0].dataType }, C = It("output", e[0].dataType, h.length), d = qe("input", e[0].dataType, e[0].dims.length), z = ze.size(h), B = [{ name: "outputSize", type: "u32" }, { name: "starts", type: "u32", length: o.length }, { name: "signs", type: "i32", length: p.length }, { name: "steps", type: "u32", length: a.length }], V = [{ type: 12, data: z }, { type: 12, data: o }, { type: 6, data: p }, { type: 12, data: a }, ...yt(e[0].dims, h)], Z = (ee) => `
      ${ee.registerUniforms(B).declareVariables(d, C)}
        ${tr(d, C, s)}
        ${ee.mainStart()}
          ${ee.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
          let output_indices = ${C.offsetToIndices("global_idx")};
          let input_indices = calculateInputIndices(output_indices);
          ${C.setByOffset("global_idx", d.getByIndices("input_indices"))}
      }`;
          return { name: "Slice", shaderCache: { hint: `${p.length}_${o.length}_${a.length}`, inputDependencies: ["rank"] }, getShaderSource: Z, getRunData: () => ({ outputs: [k], dispatchGroup: { x: Math.ceil(n / 64) }, programUniforms: V }) };
        }, Cp = (e, t) => {
          Qt(e.inputs, t);
          let s = Hs(e.inputs, t);
          e.compute(un(e.inputs, s), { inputs: [0] });
        }, xc = (e) => {
          let t = e.starts, s = e.ends, n = e.axes;
          return Bt({ starts: t, ends: s, axes: n });
        };
      }), f, T, N, ge, De = g(() => {
        zt(), Ft(), rs(), Gr(), Yt(), f = (e) => {
          if (!e || e.length !== 1) throw new Error("Softmax op requires 1 input.");
        }, T = (e, t) => {
          let s = e.inputs[0], n = s.dims, i = ze.size(n), a = n.length, o = ze.normalizeAxis(t.axis, a), u = o < n.length - 1, p, h = [];
          u ? (h = Array.from({ length: a }, (Le, Ye) => Ye), h[o] = a - 1, h[a - 1] = o, p = e.compute(cr(s, h), { inputs: [s], outputs: [-1] })[0]) : p = s;
          let k = p.dims, C = k[a - 1], d = i / C, z = qt(C), B = C / z, V = 64;
          d === 1 && (V = 256);
          let Z = (Le, Ye) => Ye === 4 ? `max(max(${Le}.x, ${Le}.y), max(${Le}.z, ${Le}.w))` : Ye === 2 ? `max(${Le}.x, ${Le}.y)` : Ye === 3 ? `max(max(${Le}.x, ${Le}.y), ${Le}.z)` : Le, ee = qe("x", p.dataType, p.dims, z), X = It("result", p.dataType, p.dims, z), he = ee.type.value, pe = fs(p.dataType) === "f32" ? `var threadMax = ${he}(-3.402823e+38f);` : `var threadMax = ${he}(-65504.0h);`, Me = (Le) => `
      var<workgroup> rowMaxShared : ${he};
      var<workgroup> rowSumShared : ${he};
      var<workgroup> threadShared : array<${he}, ${V}>;

      fn getValue(row: i32, col: i32, row_stride: i32) -> ${he} {
        let index = row * row_stride + col;
        return x[index];
      }

      fn setValue(row: i32, col: i32, row_stride: i32, value: ${he}) {
        let index = row * row_stride + col;
        result[index] = value;
      }
      ${Le.registerUniform("packedCols", "i32").declareVariables(ee, X)}
      ${Le.mainStart(V)}
        let gindex = i32(global_idx);
        let lindex = i32(local_idx);
        const wg = ${V};
        let row = gindex / wg;
        let cols = uniforms.packedCols;
        let row_stride : i32 = uniforms.packedCols;

        // find the rows max
        ${pe}
        for (var col = lindex; col < cols; col += wg) {
          let value = getValue(row, col, row_stride);
          threadMax = max(threadMax, value);
        }
        if (lindex < cols) {
          threadShared[lindex] = threadMax;
        }
        workgroupBarrier();

        var reduceSize = min(cols, wg);
        for (var currSize = reduceSize >> 1;  currSize > 0; currSize = reduceSize >> 1) {
          reduceSize = currSize + (reduceSize & 1);
          if (lindex < currSize) {
            threadShared[lindex] = max(threadShared[lindex], threadShared[lindex + reduceSize]);
          }
          workgroupBarrier();
        }
        if (lindex == 0) {
          rowMaxShared = ${he}(${Z("threadShared[0]", z)});
        }
        workgroupBarrier();

        // find the rows sum
        var threadSum = ${he}(0.0);
        for (var col = lindex; col < cols; col += wg) {
          let subExp = exp(getValue(row, col, row_stride) - rowMaxShared);
          threadSum += subExp;
        }
        threadShared[lindex] = threadSum;
        workgroupBarrier();

        for (var currSize = wg >> 1;  currSize > 0; currSize = currSize >> 1) {
          if (lindex < currSize) {
            threadShared[lindex] = threadShared[lindex] + threadShared[lindex + currSize];
          }
          workgroupBarrier();
        }
        if (lindex == 0) {
          rowSumShared = ${he}(${Gs("threadShared[0]", z)});
        }
        workgroupBarrier();

        // calculate final value for each element in the row
        for (var col = lindex; col < cols; col += wg) {
          let value = exp(getValue(row, col, row_stride) - rowMaxShared) / rowSumShared;
          setValue(row, col, row_stride, value);
        }
      }`, Oe = e.compute({ name: "Softmax", shaderCache: { hint: `${z};${V}`, inputDependencies: ["type"] }, getRunData: () => ({ outputs: [{ dims: k, dataType: p.dataType }], dispatchGroup: { x: d }, programUniforms: [{ type: 6, data: B }] }), getShaderSource: Me }, { inputs: [p], outputs: [u ? -1 : 0] })[0];
          u && e.compute(cr(Oe, h), { inputs: [Oe] });
        }, N = (e, t) => {
          f(e.inputs), T(e, t);
        }, ge = (e) => Bt({ axis: e.axis });
      }), Ie, et, rt, _t, Mt, jt = g(() => {
        zt(), Ft(), Yt(), Ie = (e) => Array.from(e.getBigInt64Array(), Number), et = (e) => {
          if (!e || e.length !== 2) throw new Error("Tile requires 2 inputs.");
          if (e[0].dataType !== 1 && e[0].dataType !== 10 && e[0].dataType !== 6 && e[0].dataType !== 12) throw new Error("Tile only support float, float16, int32, and uint32 data types");
          if (e[1].dataType !== 7) throw new Error("Tile `repeats` input should be of int64 data type");
          if (e[1].dims.length !== 1) throw new Error("Tile `repeats` input should be 1-D");
          if (Ie(e[1]).length !== e[0].dims.length) throw new Error("Tile `repeats` input should have same number of elements as rank of input data tensor");
        }, rt = (e, t) => {
          let s = [];
          for (let n = 0; n < e.length; ++n) s.push(e[n] * t[n]);
          return s;
        }, _t = (e, t) => {
          let s = e[0].dims, n = t ?? Ie(e[1]), i = rt(s, n), a = ze.size(i), o = e[0].dataType, u = qe("input", o, s.length), p = It("output", o, i.length), h = (k) => `
      const inputShape = ${u.indices(...s)};
      ${k.registerUniform("output_size", "u32").declareVariables(u, p)}
      ${k.mainStart()}
      ${k.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let output_indices = ${p.offsetToIndices("global_idx")};
      var input_indices: ${u.type.indices};
      for (var i = 0; i < ${s.length}; i++) {
        let input_dim_i = ${u.indicesGet("uniforms.input_shape", "i")};
        let input_dim_value = ${p.indicesGet("output_indices", "i")}  % input_dim_i;

        ${u.indicesSet("input_indices", "i", "input_dim_value")}
      }
      ${p.setByOffset("global_idx", u.getByIndices("input_indices"))}
    }`;
          return { name: "Tile", shaderCache: { hint: `${n}`, inputDependencies: ["rank"] }, getRunData: () => ({ outputs: [{ dims: i, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(a / 64) }, programUniforms: [{ type: 12, data: a }, ...yt(e[0].dims, i)] }), getShaderSource: h };
        }, Mt = (e) => {
          et(e.inputs), e.compute(_t(e.inputs), { inputs: [0] });
        };
      }), Vt, Lt, Gt, ts = g(() => {
        zt(), Ft(), Yt(), Vt = (e, t, s, n, i) => {
          let a = It("output_data", i, s.length, 4), o = qe("a_data", t[1].dataType, t[1].dims.length, 4), u = qe("b_data", t[2].dataType, t[2].dims.length, 4), p = qe("c_data", t[0].dataType, t[0].dims.length, 4), h, k = (C, d, z) => `select(${d}, ${C}, ${z})`;
          if (!n) h = a.setByOffset("global_idx", k(o.getByOffset("global_idx"), u.getByOffset("global_idx"), p.getByOffset("global_idx")));
          else {
            let C = (d, z, B = "") => {
              let V = `a_data[index_a${z}][component_a${z}]`, Z = `b_data[index_b${z}][component_b${z}]`, ee = `bool(c_data[index_c${z}] & (0xffu << (component_c${z} * 8)))`;
              return `
            let output_indices${z} = ${a.offsetToIndices(`global_idx * 4u + ${z}u`)};
            let offset_a${z} = ${o.broadcastedIndicesToOffset(`output_indices${z}`, a)};
            let offset_b${z} = ${u.broadcastedIndicesToOffset(`output_indices${z}`, a)};
            let offset_c${z} = ${p.broadcastedIndicesToOffset(`output_indices${z}`, a)};
            let index_a${z} = offset_a${z} / 4u;
            let index_b${z} = offset_b${z} / 4u;
            let index_c${z} = offset_c${z} / 4u;
            let component_a${z} = offset_a${z} % 4u;
            let component_b${z} = offset_b${z} % 4u;
            let component_c${z} = offset_c${z} % 4u;
            ${d}[${z}] = ${B}(${k(V, Z, ee)});
          `;
            };
            i === 9 ? h = `
            var data = vec4<u32>(0);
            ${C("data", 0, "u32")}
            ${C("data", 1, "u32")}
            ${C("data", 2, "u32")}
            ${C("data", 3, "u32")}
            output_data[global_idx] = dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(data));` : h = `
            ${C("output_data[global_idx]", 0)}
            ${C("output_data[global_idx]", 1)}
            ${C("output_data[global_idx]", 2)}
            ${C("output_data[global_idx]", 3)}
          `;
          }
          return `
        ${e.registerUniform("vec_size", "u32").declareVariables(p, o, u, a)}
        ${e.mainStart()}
        ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
        ${h}
      }`;
        }, Lt = (e) => {
          let t = e[1].dims, s = e[2].dims, n = e[0].dims, i = e[1].dataType, a = !(ze.areEqual(t, s) && ze.areEqual(s, n)), o = t, u = ze.size(t);
          if (a) {
            let h = Ws.calcShape(Ws.calcShape(t, s, !1), n, !1);
            if (!h) throw new Error("Can't perform where op on the given tensors");
            o = h, u = ze.size(o);
          }
          let p = Math.ceil(u / 4);
          return { name: "Where", shaderCache: { inputDependencies: ["rank", "rank", "rank"] }, getShaderSource: (h) => Vt(h, e, o, a, i), getRunData: () => ({ outputs: [{ dims: o, dataType: i }], dispatchGroup: { x: Math.ceil(u / 64 / 4) }, programUniforms: [{ type: 12, data: p }, ...yt(n, t, s, o)] }) };
        }, Gt = (e) => {
          e.compute(Lt(e.inputs));
        };
      }), ns, Jt = g(() => {
        Vc(), oo(), Wc(), Gc(), Kc(), Hc(), du(), Yc(), Zc(), ep(), tp(), sp(), rp(), np(), ip(), op(), lp(), up(), qp(), oa(), hp(), bd(), mp(), fp(), Ad(), hd(), Ld(), bp(), Qp(), vp(), Tp(), ai(), Pp(), Ep(), Xp(), kp(), De(), da(), jt(), Gr(), To(), ts(), ns = /* @__PURE__ */ new Map([["Abs", [vl]], ["Acos", [po]], ["Acosh", [xl]], ["Add", [Eo]], ["ArgMax", [to, so]], ["ArgMin", [ml, so]], ["Asin", [Tl]], ["Asinh", [ho]], ["Atan", [Pl]], ["Atanh", [El]], ["Attention", [gl]], ["AveragePool", [Mp, va]], ["BatchNormalization", [lo]], ["BiasAdd", [bl]], ["BiasSplitGelu", [Po]], ["Cast", [Cl, mo]], ["Ceil", [Sl]], ["Clip", [fo]], ["Concat", [lu, uu]], ["Conv", [wi, Ho]], ["ConvTranspose", [Cu, Xo]], ["Cos", [$l]], ["Cosh", [_o]], ["CumSum", [ku, Su]], ["DepthToSpace", [Iu, Fu]], ["DequantizeLinear", [Kd, Hd]], ["Div", [Zl]], ["Einsum", [zu, Bu]], ["Elu", [Al, Rn]], ["Equal", [eu]], ["Erf", [Il]], ["Exp", [go]], ["Expand", [Uu]], ["FastGelu", [Vu]], ["Floor", [Fl]], ["FusedConv", [wi, Ho]], ["Gather", [Ku, Gu]], ["GatherElements", [ed, Zu]], ["GatherBlockQuantized", [Xu, Yu]], ["GatherND", [xi, qu]], ["Gelu", [Ol]], ["Gemm", [Ti, rd]], ["GlobalAveragePool", [Rd, Bd]], ["GlobalMaxPool", [Vd, Ud]], ["Greater", [ru]], ["GreaterOrEqual", [ko]], ["GridSample", [ud, dd]], ["GroupQueryAttention", [ca]], ["HardSigmoid", [Rl, Mo]], ["InstanceNormalization", [Md]], ["LayerNormalization", [ma]], ["LeakyRelu", [wo, Rn]], ["Less", [nu]], ["LessOrEqual", [iu]], ["Log", [Kl]], ["MatMul", [Pd]], ["MatMulNBits", [Sd, $d]], ["MaxPool", [Nd, jd]], ["Mul", [tu]], ["MultiHeadAttention", [dp, pd]], ["Neg", [Ll]], ["Not", [Dl]], ["Pad", [wp]], ["Pow", [su]], ["QuickGelu", [ql, Rn]], ["Range", [Xd]], ["Reciprocal", [yo]], ["ReduceMin", [dl]], ["ReduceMean", [al]], ["ReduceMax", [Ji]], ["ReduceSum", [Zi]], ["ReduceProd", [cl]], ["ReduceL1", [Yi]], ["ReduceL2", [ll]], ["ReduceLogSum", [hl]], ["ReduceLogSumExp", [ul]], ["ReduceSumSquare", [pl]], ["Relu", [zl]], ["Resize", [fc, _c]], ["RotaryEmbedding", [yc]], ["ScatterND", [Ta, xp]], ["Sigmoid", [Bl]], ["Sin", [Nl]], ["Sinh", [bo]], ["Slice", [Cp, xc]], ["SkipLayerNormalization", [vc]], ["Split", [gd, wd]], ["Sqrt", [jl]], ["Softmax", [N, ge]], ["Sub", [Co]], ["Tan", [Ul]], ["Tanh", [Vl]], ["ThresholdedRelu", [Gl, Rn]], ["Tile", [Mt]], ["Transpose", [Na, Vi]], ["Where", [Gt]]]);
      }), os, As = g(() => {
        We(), Pe(), Yt(), os = class {
          constructor(e) {
            this.backend = e, this.repo = /* @__PURE__ */ new Map(), this.attributesBound = !1;
          }
          getArtifact(e) {
            return this.repo.get(e);
          }
          setArtifact(e, t) {
            this.repo.set(e, t);
          }
          run(e, t, s, n, i) {
            Ne(e.programInfo.name);
            let a = this.backend.device, o = this.backend.getComputePassEncoder();
            this.backend.writeTimestamp(this.backend.pendingDispatchNumber * 2);
            let u = [];
            for (let h of t) u.push({ binding: u.length, resource: { buffer: h.buffer } });
            for (let h of s) u.push({ binding: u.length, resource: { buffer: h.buffer } });
            i && u.push({ binding: u.length, resource: i });
            let p = a.createBindGroup({ layout: e.computePipeline.getBindGroupLayout(0), entries: u, label: e.programInfo.name });
            if (this.backend.sessionStatus === "capturing") {
              let h = { kernelId: this.backend.currentKernelId, computePipeline: e.computePipeline, bindGroup: p, dispatchGroup: n };
              this.backend.capturedCommandList.get(this.backend.currentSessionId).push(h);
            }
            o.setPipeline(e.computePipeline), o.setBindGroup(0, p), o.dispatchWorkgroups(...n), this.backend.writeTimestamp(this.backend.pendingDispatchNumber * 2 + 1), this.backend.pendingDispatchNumber++, (this.backend.pendingDispatchNumber >= this.backend.maxDispatchNumber || this.backend.queryType === "at-passes") && this.backend.endComputePass(), this.backend.pendingDispatchNumber >= this.backend.maxDispatchNumber && this.backend.flush(), Re(e.programInfo.name);
          }
          dispose() {
          }
          build(e, t) {
            Ne(e.name);
            let s = this.backend.device, n = [];
            [{ feature: "shader-f16", extension: "f16" }, { feature: "subgroups", extension: "subgroups" }, { feature: "subgroups-f16", extension: "subgroups_f16" }].forEach((h) => {
              s.features.has(h.feature) && n.push(`enable ${h.extension};`);
            });
            let i = La(t, this.backend.device.limits), a = e.getShaderSource(i), o = `${n.join(`
`)}
${i.additionalImplementations}
${a}`, u = s.createShaderModule({ code: o, label: e.name });
            as("verbose", () => `[WebGPU] ${e.name} shader code: ${o}`);
            let p = s.createComputePipeline({ compute: { module: u, entryPoint: "main" }, layout: "auto", label: e.name });
            return Re(e.name), { programInfo: e, computePipeline: p, uniformVariablesInfo: i.variablesInfo };
          }
          normalizeDispatchGroupSize(e) {
            let t = typeof e == "number" ? e : e.x, s = typeof e == "number" ? 1 : e.y || 1, n = typeof e == "number" ? 1 : e.z || 1, i = this.backend.device.limits.maxComputeWorkgroupsPerDimension;
            if (t <= i && s <= i && n <= i) return [t, s, n];
            let a = t * s * n, o = Math.ceil(Math.sqrt(a));
            if (o > i) {
              if (o = Math.ceil(Math.cbrt(a)), o > i) throw new Error("Total dispatch size exceeds WebGPU maximum.");
              return [o, o, o];
            } else return [o, o, 1];
          }
        };
      }), xs, cs, Es, Is, Zs, Ys = g(() => {
        We(), zt(), Pe(), Q(), us(), Jt(), As(), xs = (e, t) => {
          if (t.length !== e.length) throw new Error(`inputDependencies length ${t.length} is not equal to inputTensors length ${e.length}.`);
          let s = [];
          for (let n = 0; n < e.length; ++n) {
            let i = e[n].dataType;
            switch (t[n]) {
              case "none": {
                s.push("");
                break;
              }
              case "type": {
                s.push(`${i}`);
                break;
              }
              case "rank": {
                let a = e[n].dims.length;
                s.push(`${i};${a}`);
                break;
              }
              case "dims": {
                let a = e[n].dims.join(",");
                s.push(`${i};${a}`);
                break;
              }
              default:
                throw new Error(`unsupported input dependency: ${t[n]}`);
            }
          }
          return s.join("|");
        }, cs = (e, t, s) => {
          var i, a;
          let n = e.name;
          return (i = e.shaderCache) != null && i.hint && (n += "[" + e.shaderCache.hint + "]"), n += ":" + s + `:${xs(t, ((a = e.shaderCache) == null ? void 0 : a.inputDependencies) ?? new Array(t.length).fill("dims"))}`, n;
        }, Es = class {
          constructor(e) {
            e && (this.architecture = e.architecture, this.vendor = e.vendor);
          }
          isArchitecture(e) {
            return this.architecture === e;
          }
          isVendor(e) {
            return this.vendor === e;
          }
        }, Is = class {
          constructor(e) {
            this.subgroupsSupported = e.features.has("subgroups"), this.subgroupsF16Supported = e.features.has("subgroups");
            let t = e.limits;
            !this.subgroupsSupported || !t.minSubgroupSize || !t.maxSubgroupSize ? this.subgroupSizeRange = void 0 : this.subgroupSizeRange = [t.minSubgroupSize, t.maxSubgroupSize];
          }
        }, Zs = class {
          constructor() {
            this.currentSessionId = null, this.currentKernelId = null, this.commandEncoder = null, this.computePassEncoder = null, this.maxDispatchNumber = 16, this.pendingDispatchNumber = 0, this.pendingKernels = [], this.pendingQueries = /* @__PURE__ */ new Map(), this.sessionStatus = "default", this.capturedCommandList = /* @__PURE__ */ new Map(), this.capturedPendingKernels = /* @__PURE__ */ new Map(), this.sessionExternalDataMapping = /* @__PURE__ */ new Map();
          }
          get currentKernelCustomData() {
            if (this.currentKernelId === null) throw new Error("currentKernelCustomData(): currentKernelId is null. (should not happen)");
            let e = this.kernelCustomData.get(this.currentKernelId);
            return e || (e = {}, this.kernelCustomData.set(this.currentKernelId, e)), e;
          }
          async initialize(e, t) {
            this.env = e;
            let s = [], n = { requiredLimits: { maxComputeWorkgroupStorageSize: t.limits.maxComputeWorkgroupStorageSize, maxComputeWorkgroupsPerDimension: t.limits.maxComputeWorkgroupsPerDimension, maxStorageBufferBindingSize: t.limits.maxStorageBufferBindingSize, maxBufferSize: t.limits.maxBufferSize, maxComputeInvocationsPerWorkgroup: t.limits.maxComputeInvocationsPerWorkgroup, maxComputeWorkgroupSizeX: t.limits.maxComputeWorkgroupSizeX, maxComputeWorkgroupSizeY: t.limits.maxComputeWorkgroupSizeY, maxComputeWorkgroupSizeZ: t.limits.maxComputeWorkgroupSizeZ }, requiredFeatures: s }, i = (a) => t.features.has(a) && s.push(a) && !0;
            i("chromium-experimental-timestamp-query-inside-passes") || i("timestamp-query"), i("shader-f16"), i("subgroups") && i("subgroups-f16"), this.device = await t.requestDevice(n), this.deviceInfo = new Is(this.device), this.adapterInfo = new Es(t.info || await t.requestAdapterInfo()), this.gpuDataManager = ms(this), this.programManager = new os(this), this.kernels = /* @__PURE__ */ new Map(), this.kernelPersistentData = /* @__PURE__ */ new Map(), this.kernelCustomData = /* @__PURE__ */ new Map(), vn(e.logLevel, !!e.debug), this.device.onuncapturederror = (a) => {
              a.error instanceof GPUValidationError && console.error(`An uncaught WebGPU validation error was raised: ${a.error.message}`);
            }, Object.defineProperty(this.env.webgpu, "device", { value: this.device, writable: !1, enumerable: !0, configurable: !1 }), Object.defineProperty(this.env.webgpu, "adapter", { value: t, writable: !1, enumerable: !0, configurable: !1 }), this.setQueryType();
          }
          dispose() {
            typeof this.querySet < "u" && this.querySet.destroy(), this.gpuDataManager.dispose();
          }
          getCommandEncoder() {
            return this.commandEncoder || (this.commandEncoder = this.device.createCommandEncoder()), this.commandEncoder;
          }
          getComputePassEncoder() {
            if (!this.computePassEncoder) {
              let e = this.getCommandEncoder(), t = {};
              this.queryType === "at-passes" && (t.timestampWrites = { querySet: this.querySet, beginningOfPassWriteIndex: this.pendingDispatchNumber * 2, endOfPassWriteIndex: this.pendingDispatchNumber * 2 + 1 }), this.computePassEncoder = e.beginComputePass(t);
            }
            return this.computePassEncoder;
          }
          endComputePass() {
            this.computePassEncoder && (this.computePassEncoder.end(), this.computePassEncoder = null);
          }
          flush() {
            if (!this.commandEncoder) return;
            Ne(), this.endComputePass();
            let e;
            this.queryType !== "none" && (this.commandEncoder.resolveQuerySet(this.querySet, 0, this.pendingDispatchNumber * 2, this.queryResolveBuffer, 0), e = this.device.createBuffer({ size: this.pendingDispatchNumber * 2 * 8, usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST }), this.pendingQueries.set(e, this.pendingKernels), this.pendingKernels = [], this.commandEncoder.copyBufferToBuffer(this.queryResolveBuffer, 0, e, 0, this.pendingDispatchNumber * 2 * 8)), this.device.queue.submit([this.commandEncoder.finish()]), this.gpuDataManager.refreshPendingBuffers(), this.commandEncoder = null, this.pendingDispatchNumber = 0, this.queryType !== "none" && e.mapAsync(GPUMapMode.READ).then(() => {
              var n;
              let t = new BigUint64Array(e.getMappedRange()), s = this.pendingQueries.get(e);
              for (let i = 0; i < t.length / 2; i++) {
                let a = s[i], o = a.kernelId, u = this.kernels.get(o), p = u.kernelType, h = u.kernelName, k = a.programName, C = a.inputTensorViews, d = a.outputTensorViews, z = t[i * 2], B = t[i * 2 + 1];
                typeof this.queryTimeBase > "u" && (this.queryTimeBase = z);
                let V = Number(z - this.queryTimeBase), Z = Number(B - this.queryTimeBase);
                if (!Number.isSafeInteger(V) || !Number.isSafeInteger(Z)) throw new RangeError("incorrect timestamp range");
                if ((n = this.env.webgpu.profiling) != null && n.ondata) this.env.webgpu.profiling.ondata({ version: 1, inputsMetadata: C.map((ee) => ({ dims: ee.dims, dataType: mr(ee.dataType) })), outputsMetadata: d.map((ee) => ({ dims: ee.dims, dataType: mr(ee.dataType) })), kernelId: o, kernelType: p, kernelName: h, programName: k, startTime: V, endTime: Z });
                else {
                  let ee = "";
                  C.forEach((he, pe) => {
                    ee += `input[${pe}]: [${he.dims}] | ${mr(he.dataType)}, `;
                  });
                  let X = "";
                  d.forEach((he, pe) => {
                    X += `output[${pe}]: [${he.dims}] | ${mr(he.dataType)}, `;
                  }), console.log(`[profiling] kernel "${o}|${p}|${h}|${k}" ${ee}${X}execution time: ${Z - V} ns`);
                }
                Ue("GPU", `${k}::${z}::${B}`);
              }
              e.unmap(), this.pendingQueries.delete(e);
            }), Re();
          }
          run(e, t, s, n, i, a) {
            Ne(e.name);
            let o = [];
            for (let X = 0; X < t.length; ++X) {
              let he = t[X].data;
              if (he === 0) continue;
              let pe = this.gpuDataManager.get(he);
              if (!pe) throw new Error(`no GPU data for input: ${he}`);
              o.push(pe);
            }
            let { outputs: u, dispatchGroup: p, programUniforms: h } = e.getRunData(t), k = s.length === 0 ? u.map((X, he) => he) : s;
            if (k.length !== u.length) throw new Error(`Output size ${k.length} must be equal to ${u.length}.`);
            let C = [], d = [];
            for (let X = 0; X < u.length; ++X) {
              if (!Number.isInteger(k[X]) || k[X] < -3 || k[X] >= a) throw new Error(`Invalid output index: ${k[X]}`);
              if (k[X] === -3) continue;
              let he = k[X] === -1, pe = k[X] === -2, Me = he || pe ? i(u[X].dataType, u[X].dims) : n(k[X], u[X].dataType, u[X].dims);
              if (C.push(Me), Me.data === 0) continue;
              let Oe = this.gpuDataManager.get(Me.data);
              if (!Oe) throw new Error(`no GPU data for output: ${Me.data}`);
              if (he && this.temporaryData.push(Oe), pe) {
                let Le = this.kernelPersistentData.get(this.currentKernelId);
                Le || (Le = [], this.kernelPersistentData.set(this.currentKernelId, Le)), Le.push(Oe);
              }
              d.push(Oe);
            }
            if (o.length !== t.length || d.length !== C.length) {
              if (d.length === 0) return Re(e.name), C;
              throw new Error(`Program ${e.name} has zero-sized tensor(s) in inputs or outputs. This is not supported now.`);
            }
            let z;
            if (h) {
              let X = 0, he = [];
              h.forEach((Le) => {
                let Ye = typeof Le.data == "number" ? [Le.data] : Le.data;
                if (Ye.length === 0) return;
                let at = Le.type === 10 ? 2 : 4, Pt, Xt;
                Le.type === 10 ? (Xt = Ye.length > 4 ? 16 : Ye.length > 2 ? 8 : Ye.length * at, Pt = Ye.length > 4 ? 16 : at * Ye.length) : (Xt = Ye.length <= 2 ? Ye.length * at : 16, Pt = 16), X = Math.ceil(X / Xt) * Xt, he.push(X);
                let Zt = Le.type === 10 ? 8 : 4;
                X += Ye.length > 4 ? Math.ceil(Ye.length / Zt) * Pt : Ye.length * at;
              });
              let pe = 16;
              X = Math.ceil(X / pe) * pe;
              let Me = new ArrayBuffer(X);
              h.forEach((Le, Ye) => {
                let at = he[Ye], Pt = typeof Le.data == "number" ? [Le.data] : Le.data;
                if (Le.type === 6) new Int32Array(Me, at, Pt.length).set(Pt);
                else if (Le.type === 12) new Uint32Array(Me, at, Pt.length).set(Pt);
                else if (Le.type === 10) new Uint16Array(Me, at, Pt.length).set(Pt);
                else if (Le.type === 1) new Float32Array(Me, at, Pt.length).set(Pt);
                else throw new Error(`Unsupported uniform type: ${mr(Le.type)}`);
              });
              let Oe = this.gpuDataManager.create(X, GPUBufferUsage.COPY_DST | GPUBufferUsage.UNIFORM);
              this.device.queue.writeBuffer(Oe.buffer, 0, Me, 0, X), this.gpuDataManager.release(Oe.id), z = { offset: 0, size: X, buffer: Oe.buffer };
            }
            let B = this.programManager.normalizeDispatchGroupSize(p), V = B[1] === 1 && B[2] === 1, Z = cs(e, t, V), ee = this.programManager.getArtifact(Z);
            if (ee || (ee = this.programManager.build(e, B), this.programManager.setArtifact(Z, ee), as("info", () => `[artifact] key: ${Z}, programName: ${e.name}`)), h && ee.uniformVariablesInfo) {
              if (h.length !== ee.uniformVariablesInfo.length) throw new Error(`Uniform variables count mismatch: expect ${ee.uniformVariablesInfo.length}, got ${h.length} in program "${ee.programInfo.name}".`);
              for (let X = 0; X < h.length; X++) {
                let he = h[X], pe = he.type, Me = typeof he.data == "number" ? 1 : he.data.length, [Oe, Le] = ee.uniformVariablesInfo[X];
                if (pe !== Oe || Me !== Le) throw new Error(`Uniform variable ${X} mismatch: expect type ${Oe} with size ${Le}, got type ${pe} with size ${Me} in program "${ee.programInfo.name}".`);
              }
            }
            if (as("info", () => `[ProgramManager] run "${e.name}" (key=${Z}) with ${B[0]}x${B[1]}x${B[2]}`), this.queryType !== "none" || this.sessionStatus === "capturing") {
              let X = { kernelId: this.currentKernelId, programName: ee.programInfo.name, inputTensorViews: t, outputTensorViews: C };
              this.pendingKernels.push(X), this.sessionStatus === "capturing" && this.capturedPendingKernels.get(this.currentSessionId).push(X);
            }
            return this.programManager.run(ee, o, d, B, z), Re(e.name), C;
          }
          upload(e, t) {
            this.gpuDataManager.upload(e, t);
          }
          memcpy(e, t) {
            this.gpuDataManager.memcpy(e, t);
          }
          async download(e, t) {
            await this.gpuDataManager.download(e, t);
          }
          alloc(e) {
            return this.gpuDataManager.create(e).id;
          }
          free(e) {
            return this.gpuDataManager.release(e);
          }
          createKernel(e, t, s, n) {
            let i = ns.get(e);
            if (!i) throw new Error(`kernel not implemented: ${e}`);
            let a = { kernelType: e, kernelName: n, kernelEntry: i[0], attributes: [i[1], s] };
            this.kernels.set(t, a);
          }
          releaseKernel(e) {
            let t = this.kernelPersistentData.get(e);
            if (t) {
              for (let s of t) this.gpuDataManager.release(s.id);
              this.kernelPersistentData.delete(e);
            }
            this.kernelCustomData.delete(e), this.kernels.delete(e);
          }
          computeKernel(e, t, s) {
            let n = this.kernels.get(e);
            if (!n) throw new Error(`kernel not created: ${e}`);
            let i = n.kernelType, a = n.kernelName, o = n.kernelEntry, u = n.attributes;
            if (this.currentKernelId !== null) throw new Error(`kernel "[${i}] ${a}" is not allowed to be called recursively`);
            this.currentKernelId = e, u[0] && (u[1] = u[0](u[1]), u[0] = void 0), as("info", () => `[WebGPU] Start to run kernel "[${i}] ${a}"...`);
            let p = this.env.debug;
            this.temporaryData = [];
            try {
              return p && this.device.pushErrorScope("validation"), o(t, u[1]), 0;
            } catch (h) {
              return s.push(Promise.resolve(`[WebGPU] Kernel "[${i}] ${a}" failed. ${h}`)), 1;
            } finally {
              p && s.push(this.device.popErrorScope().then((h) => h ? `GPU validation error for kernel "[${i}] ${a}": ${h.message}` : null));
              for (let h of this.temporaryData) this.gpuDataManager.release(h.id);
              this.temporaryData = [], this.currentKernelId = null;
            }
          }
          registerBuffer(e, t, s, n) {
            let i = this.sessionExternalDataMapping.get(e);
            i || (i = /* @__PURE__ */ new Map(), this.sessionExternalDataMapping.set(e, i));
            let a = i.get(t), o = this.gpuDataManager.registerExternalBuffer(s, n, a);
            return i.set(t, [o, s]), o;
          }
          unregisterBuffers(e) {
            let t = this.sessionExternalDataMapping.get(e);
            t && (t.forEach((s) => this.gpuDataManager.unregisterExternalBuffer(s[0])), this.sessionExternalDataMapping.delete(e));
          }
          getBuffer(e) {
            let t = this.gpuDataManager.get(e);
            if (!t) throw new Error(`no GPU data for buffer: ${e}`);
            return t.buffer;
          }
          createDownloader(e, t, s) {
            return async () => {
              let n = await xt(this, e, t);
              return P(n.buffer, s);
            };
          }
          writeTimestamp(e) {
            this.queryType === "inside-passes" && this.computePassEncoder.writeTimestamp(this.querySet, e);
          }
          setQueryType() {
            var e;
            this.queryType = "none", (((e = this.env.webgpu.profiling) == null ? void 0 : e.mode) === "default" || (typeof this.env.trace > "u" ? this.env.wasm.trace : this.env.trace)) && (this.device.features.has("chromium-experimental-timestamp-query-inside-passes") ? this.queryType = "inside-passes" : this.device.features.has("timestamp-query") && (this.queryType = "at-passes"), this.queryType !== "none" && typeof this.querySet > "u" && (this.querySet = this.device.createQuerySet({ type: "timestamp", count: this.maxDispatchNumber * 2 }), this.queryResolveBuffer = this.device.createBuffer({ size: this.maxDispatchNumber * 2 * 8, usage: GPUBufferUsage.COPY_SRC | GPUBufferUsage.QUERY_RESOLVE })));
          }
          captureBegin() {
            as("info", "captureBegin"), this.capturedCommandList.get(this.currentSessionId) || this.capturedCommandList.set(this.currentSessionId, []), this.capturedPendingKernels.get(this.currentSessionId) || this.capturedPendingKernels.set(this.currentSessionId, []), this.flush(), this.sessionStatus = "capturing";
          }
          captureEnd() {
            as("info", "captureEnd"), this.flush(), this.sessionStatus = "default";
          }
          replay() {
            as("info", "replay"), this.sessionStatus = "replaying";
            let e = this.capturedCommandList.get(this.currentSessionId), t = this.capturedPendingKernels.get(this.currentSessionId), s = e.length;
            this.pendingKernels = [];
            for (let n = 0; n < s; n++) {
              let i = this.getComputePassEncoder(), a = e[n];
              this.writeTimestamp(this.pendingDispatchNumber * 2), i.setPipeline(a.computePipeline), i.setBindGroup(0, a.bindGroup), i.dispatchWorkgroups(...a.dispatchGroup), this.writeTimestamp(this.pendingDispatchNumber * 2 + 1), this.pendingDispatchNumber++, this.queryType !== "none" && this.pendingKernels.push(t[n]), (this.pendingDispatchNumber >= this.maxDispatchNumber || this.queryType === "at-passes") && this.endComputePass(), this.pendingDispatchNumber >= this.maxDispatchNumber && this.flush();
            }
            this.flush(), this.sessionStatus = "default";
          }
          onCreateSession() {
            this.gpuDataManager.onCreateSession();
          }
          onReleaseSession(e) {
            this.unregisterBuffers(e), this.capturedCommandList.has(e) && this.capturedCommandList.delete(e), this.capturedPendingKernels.has(e) && this.capturedPendingKernels.delete(e), this.gpuDataManager.onReleaseSession(e);
          }
          onRunStart(e) {
            this.currentSessionId = e, this.setQueryType();
          }
        };
      }), Mr, dn, Ea, or, br, ki, Si, $i, Ca = g(() => {
        Pe(), Mr = 1, dn = () => Mr++, Ea = /* @__PURE__ */ new Map([["float32", 32], ["float16", 16], ["int32", 32], ["uint32", 32], ["int64", 64], ["uint64", 64], ["int8", 8], ["uint8", 8], ["int4", 4], ["uint4", 4]]), or = (e, t) => {
          let s = Ea.get(e);
          if (!s) throw new Error("Unsupported data type.");
          return t.length > 0 ? Math.ceil(t.reduce((n, i) => n * i) * s / 8) : 0;
        }, br = class {
          constructor(e) {
            this.sessionId = e.sessionId, this.mlContext = e.context, this.mlTensor = e.tensor, this.dataType = e.dataType, this.tensorShape = e.shape;
          }
          get tensor() {
            return this.mlTensor;
          }
          get type() {
            return this.dataType;
          }
          get shape() {
            return this.tensorShape;
          }
          get byteLength() {
            return or(this.dataType, this.tensorShape);
          }
          destroy() {
            as("verbose", () => "[WebNN] TensorWrapper.destroy"), this.mlTensor.destroy();
          }
          write(e) {
            this.mlContext.writeTensor(this.mlTensor, e);
          }
          async read(e) {
            return e ? this.mlContext.readTensor(this.mlTensor, e) : this.mlContext.readTensor(this.mlTensor);
          }
          canReuseTensor(e, t, s) {
            return this.mlContext === e && this.dataType === t && this.tensorShape.length === s.length && this.tensorShape.every((n, i) => n === s[i]);
          }
        }, ki = class {
          constructor(e, t) {
            this.tensorManager = e, this.wrapper = t;
          }
          get tensorWrapper() {
            return this.wrapper;
          }
          releaseTensor() {
            this.tensorWrapper && (this.tensorManager.releaseTensor(this.tensorWrapper), this.wrapper = void 0);
          }
          async ensureTensor(e, t, s, n) {
            if (this.wrapper) {
              if (this.wrapper.canReuseTensor(e, t, s)) return this.wrapper.tensor;
              if (n) {
                if (this.wrapper.byteLength !== or(t, s)) throw new Error("Unable to copy data to tensor with different size.");
                this.activeUpload = new Uint8Array(await this.wrapper.read());
              }
              this.tensorManager.releaseTensor(this.wrapper);
            }
            let i = typeof MLTensorUsage > "u" ? void 0 : MLTensorUsage.READ | MLTensorUsage.WRITE;
            return this.wrapper = await this.tensorManager.getCachedTensor(t, s, i, !0, !0), n && this.activeUpload && (this.wrapper.write(this.activeUpload), this.activeUpload = void 0), this.wrapper.tensor;
          }
          upload(e) {
            if (this.wrapper) if (e.byteLength === this.wrapper.byteLength) {
              this.wrapper.write(e);
              return;
            } else as("verbose", () => "Data size does not match tensor size. Releasing tensor."), this.releaseTensor();
            this.activeUpload ? this.activeUpload.set(e) : this.activeUpload = new Uint8Array(e);
          }
          async download(e) {
            if (this.activeUpload) if (e) {
              e instanceof ArrayBuffer ? new Uint8Array(e).set(this.activeUpload) : new Uint8Array(e.buffer, e.byteOffset, e.byteLength).set(this.activeUpload);
              return;
            } else return this.activeUpload.buffer;
            if (!this.wrapper) throw new Error("Tensor has not been created.");
            return e ? this.wrapper.read(e) : this.wrapper.read();
          }
        }, Si = class {
          constructor(e) {
            this.backend = e, this.tensorTrackersById = /* @__PURE__ */ new Map(), this.freeTensors = [], this.externalTensors = /* @__PURE__ */ new Set();
          }
          reserveTensorId() {
            let e = dn();
            return this.tensorTrackersById.set(e, new ki(this)), e;
          }
          releaseTensorId(e) {
            let t = this.tensorTrackersById.get(e);
            t && (this.tensorTrackersById.delete(e), t.tensorWrapper && this.releaseTensor(t.tensorWrapper));
          }
          async ensureTensor(e, t, s, n) {
            as("verbose", () => `[WebNN] TensorManager.ensureTensor {tensorId: ${e}, dataType: ${t}, shape: ${s}, copyOld: ${n}}`);
            let i = this.tensorTrackersById.get(e);
            if (!i) throw new Error("Tensor not found.");
            return i.ensureTensor(this.backend.currentContext, t, s, n);
          }
          upload(e, t) {
            let s = this.tensorTrackersById.get(e);
            if (!s) throw new Error("Tensor not found.");
            s.upload(t);
          }
          async download(e, t) {
            as("verbose", () => `[WebNN] TensorManager.download {tensorId: ${e}, dstBuffer: ${t == null ? void 0 : t.byteLength}}`);
            let s = this.tensorTrackersById.get(e);
            if (!s) throw new Error("Tensor not found.");
            return s.download(t);
          }
          releaseTensorsForSession(e) {
            for (let t of this.freeTensors) t.sessionId === e && t.destroy();
            this.freeTensors = this.freeTensors.filter((t) => t.sessionId !== e);
          }
          registerTensor(e, t, s, n) {
            let i = dn(), a = new br({ sessionId: this.backend.currentSessionId, context: e, tensor: t, dataType: s, shape: n });
            return this.tensorTrackersById.set(i, new ki(this, a)), this.externalTensors.add(a), i;
          }
          async getCachedTensor(e, t, s, n, i) {
            let a = this.backend.currentSessionId, o = this.backend.currentContext;
            for (let [p, h] of this.freeTensors.entries()) if (h.canReuseTensor(o, e, t)) {
              as("verbose", () => `[WebNN] Reusing tensor {dataType: ${e}, shape: ${t}}`);
              let k = this.freeTensors.splice(p, 1)[0];
              return k.sessionId = a, k;
            }
            as("verbose", () => `[WebNN] MLContext.createTensor {dataType: ${e}, shape: ${t}}`);
            let u = await o.createTensor({ dataType: e, shape: t, dimensions: t, usage: s, writable: n, readable: i });
            return new br({ sessionId: a, context: o, tensor: u, dataType: e, shape: t });
          }
          releaseTensor(e) {
            this.externalTensors.has(e) && this.externalTensors.delete(e), this.freeTensors.push(e);
          }
        }, $i = (...e) => new Si(...e);
      }), Ts, Rs, Hr, Tn = g(() => {
        zt(), lr(), Q(), Ca(), Pe(), Ts = /* @__PURE__ */ new Map([[1, "float32"], [10, "float16"], [6, "int32"], [12, "uint32"], [7, "int64"], [13, "uint64"], [22, "int4"], [21, "uint4"], [3, "int8"], [2, "uint8"], [9, "uint8"]]), Rs = (e, t) => {
          if (e === t) return !0;
          if (e === void 0 || t === void 0) return !1;
          let s = Object.keys(e).sort(), n = Object.keys(t).sort();
          return s.length === n.length && s.every((i, a) => i === n[a] && e[i] === t[i]);
        }, Hr = class {
          constructor(e) {
            this.tensorManager = $i(this), this.mlContextBySessionId = /* @__PURE__ */ new Map(), this.sessionIdsByMLContext = /* @__PURE__ */ new Map(), this.mlContextCache = [], vn(e.logLevel, !!e.debug);
          }
          get currentSessionId() {
            if (this.activeSessionId === void 0) throw new Error("No active session");
            return this.activeSessionId;
          }
          onRunStart(e) {
            this.activeSessionId = e;
          }
          async createMLContext(e) {
            if (e instanceof GPUDevice) {
              let s = this.mlContextCache.findIndex((n) => n.gpuDevice === e);
              if (s !== -1) return this.mlContextCache[s].mlContext;
              {
                let n = await navigator.ml.createContext(e);
                return this.mlContextCache.push({ gpuDevice: e, mlContext: n }), n;
              }
            } else if (e === void 0) {
              let s = this.mlContextCache.findIndex((n) => n.options === void 0 && n.gpuDevice === void 0);
              if (s !== -1) return this.mlContextCache[s].mlContext;
              {
                let n = await navigator.ml.createContext();
                return this.mlContextCache.push({ mlContext: n }), n;
              }
            }
            let t = this.mlContextCache.findIndex((s) => Rs(s.options, e));
            if (t !== -1) return this.mlContextCache[t].mlContext;
            {
              let s = await navigator.ml.createContext(e);
              return this.mlContextCache.push({ options: e, mlContext: s }), s;
            }
          }
          get currentContext() {
            let e = this.getMLContext(this.currentSessionId);
            if (!e) throw new Error(`No MLContext found for session ${this.currentSessionId}`);
            return e;
          }
          registerMLContext(e, t) {
            this.mlContextBySessionId.set(e, t);
            let s = this.sessionIdsByMLContext.get(t);
            s || (s = /* @__PURE__ */ new Set(), this.sessionIdsByMLContext.set(t, s)), s.add(e);
          }
          onReleaseSession(e) {
            let t = this.mlContextBySessionId.get(e);
            if (!t) return;
            this.tensorManager.releaseTensorsForSession(e), this.mlContextBySessionId.delete(e);
            let s = this.sessionIdsByMLContext.get(t);
            if (s.delete(e), s.size === 0) {
              this.sessionIdsByMLContext.delete(t);
              let n = this.mlContextCache.findIndex((i) => i.mlContext === t);
              n !== -1 && this.mlContextCache.splice(n, 1);
            }
          }
          getMLContext(e) {
            return this.mlContextBySessionId.get(e);
          }
          reserveTensorId() {
            return this.tensorManager.reserveTensorId();
          }
          releaseTensorId(e) {
            as("verbose", () => `[WebNN] releaseTensorId {tensorId: ${e}}`), this.tensorManager.releaseTensorId(e);
          }
          async ensureTensor(e, t, s, n) {
            let i = Ts.get(t);
            if (!i) throw new Error(`Unsupported ONNX data type: ${t}`);
            return this.tensorManager.ensureTensor(e, i, s, n);
          }
          uploadTensor(e, t) {
            if (!Ms().shouldTransferToMLTensor) throw new Error("Trying to upload to a MLTensor while shouldTransferToMLTensor is false");
            as("verbose", () => `[WebNN] uploadTensor {tensorId: ${e}, data: ${t.byteLength}}`), this.tensorManager.upload(e, t);
          }
          async downloadTensor(e, t) {
            return this.tensorManager.download(e, t);
          }
          createMLTensorDownloader(e, t) {
            return async () => {
              let s = await this.tensorManager.download(e);
              return P(s, t);
            };
          }
          registerMLTensor(e, t, s) {
            let n = Ts.get(t);
            if (!n) throw new Error(`Unsupported ONNX data type: ${t}`);
            let i = this.tensorManager.registerTensor(this.currentContext, e, n, s);
            return as("verbose", () => `[WebNN] registerMLTensor {tensor: ${e}, dataType: ${n}, dimensions: ${s}} -> {tensorId: ${i}}`), i;
          }
          registerMLConstant(e, t, s, n, i, a) {
            if (!a) throw new Error("External mounted files are not available.");
            let o = e;
            e.startsWith("./") && (o = e.substring(2));
            let u = a.get(o);
            if (!u) throw new Error(`File with name ${o} not found in preloaded files.`);
            if (t + s > u.byteLength) throw new Error("Out of bounds: data offset and length exceed the external file data size.");
            let p = u.slice(t, t + s).buffer, h;
            switch (i.dataType) {
              case "float32":
                h = new Float32Array(p);
                break;
              case "float16":
                h = new Uint16Array(p);
                break;
              case "int32":
                h = new Int32Array(p);
                break;
              case "uint32":
                h = new Uint32Array(p);
                break;
              case "int64":
                h = new BigInt64Array(p);
                break;
              case "uint64":
                h = new BigUint64Array(p);
                break;
              case "int8":
                h = new Int8Array(p);
                break;
              case "int4":
              case "uint4":
              case "uint8":
                h = new Uint8Array(p);
                break;
              default:
                throw new Error(`Unsupported data type: ${i.dataType} in creating WebNN Constant from external data.`);
            }
            return as("verbose", () => `[WebNN] registerMLConstant {dataType: ${i.dataType}, shape: ${i.shape}}}`), n.constant(i, h);
          }
          flush() {
          }
        };
      }), Pn = {};
      v(Pn, { init: () => Sa });
      var En, ka, Sa, Tc = g(() => {
        zt(), Ys(), Pe(), Ft(), Tn(), En = class uf {
          constructor(t, s, n, i) {
            this.module = t, this.dataType = s, this.data = n, this.dims = i;
          }
          getFloat32Array() {
            if (this.dataType !== 1) throw new Error("Invalid data type");
            let t = ze.size(this.dims);
            return t === 0 ? new Float32Array() : new Float32Array(this.module.HEAP8.buffer, this.data, t);
          }
          getBigInt64Array() {
            if (this.dataType !== 7) throw new Error("Invalid data type");
            let t = ze.size(this.dims);
            return t === 0 ? new BigInt64Array() : new BigInt64Array(this.module.HEAP8.buffer, this.data, t);
          }
          getInt32Array() {
            if (this.dataType !== 6) throw new Error("Invalid data type");
            let t = ze.size(this.dims);
            return t === 0 ? new Int32Array() : new Int32Array(this.module.HEAP8.buffer, this.data, t);
          }
          getUint16Array() {
            if (this.dataType !== 10 && this.dataType !== 4) throw new Error("Invalid data type");
            let t = ze.size(this.dims);
            return t === 0 ? new Uint16Array() : new Uint16Array(this.module.HEAP8.buffer, this.data, t);
          }
          reshape(t) {
            if (ze.size(t) !== ze.size(this.dims)) throw new Error("Invalid new shape");
            return new uf(this.module, this.dataType, this.data, t);
          }
        }, ka = class {
          constructor(e, t, s) {
            this.module = e, this.backend = t, this.customDataOffset = 0, this.customDataSize = 0, this.adapterInfo = t.adapterInfo, this.deviceInfo = t.deviceInfo;
            let n = e.PTR_SIZE, i = s / e.PTR_SIZE, a = n === 4 ? "i32" : "i64";
            this.opKernelContext = Number(e.getValue(n * i++, a));
            let o = Number(e.getValue(n * i++, a));
            this.outputCount = Number(e.getValue(n * i++, a)), this.customDataOffset = Number(e.getValue(n * i++, "*")), this.customDataSize = Number(e.getValue(n * i++, a));
            let u = [];
            for (let p = 0; p < o; p++) {
              let h = Number(e.getValue(n * i++, a)), k = Number(e.getValue(n * i++, "*")), C = Number(e.getValue(n * i++, a)), d = [];
              for (let z = 0; z < C; z++) d.push(Number(e.getValue(n * i++, a)));
              u.push(new En(e, h, k, d));
            }
            this.inputs = u;
          }
          get kernelCustomData() {
            return this.backend.currentKernelCustomData;
          }
          get customDataBuffer() {
            return this.module.HEAPU8.subarray(this.customDataOffset, this.customDataOffset + this.customDataSize);
          }
          compute(e, t) {
            var o;
            let s = ((o = t == null ? void 0 : t.inputs) == null ? void 0 : o.map((u) => typeof u == "number" ? this.inputs[u] : u)) ?? this.inputs, n = (t == null ? void 0 : t.outputs) ?? [], i = (u, p, h) => new En(this.module, p, this.output(u, h), h), a = (u, p) => {
              let h = ur(u, p);
              if (!h) throw new Error(`Unsupported data type: ${u}`);
              let k = h > 0 ? this.backend.gpuDataManager.create(h).id : 0;
              return new En(this.module, u, k, p);
            };
            return this.backend.run(e, s, n, i, a, this.outputCount);
          }
          output(e, t) {
            let s = this.module.stackSave();
            try {
              let n = this.module.PTR_SIZE, i = n === 4 ? "i32" : "i64", a = this.module.stackAlloc((1 + t.length) * n);
              this.module.setValue(a, t.length, i);
              for (let o = 0; o < t.length; o++) this.module.setValue(a + n * (o + 1), t[o], i);
              return this.module._JsepOutput(this.opKernelContext, e, a);
            } catch (n) {
              throw new Error(`Failed to generate kernel's output[${e}] with dims [${t}]. If you are running with pre-allocated output, please make sure the output type/dims are correct. Error: ${n}`);
            } finally {
              this.module.stackRestore(s);
            }
          }
        }, Sa = async (e, t, s, n) => {
          let i = t.jsepInit;
          if (!i) throw new Error("Failed to initialize JSEP. The WebAssembly module is not built with JSEP support.");
          if (e === "webgpu") {
            let a = new Zs();
            await a.initialize(s, n), i("webgpu", [a, (o) => a.alloc(Number(o)), (o) => a.free(o), (o, u, p, h = !1) => {
              if (h) as("verbose", () => `[WebGPU] jsepCopyGpuToGpu: src=${Number(o)}, dst=${Number(u)}, size=${Number(p)}`), a.memcpy(Number(o), Number(u));
              else {
                as("verbose", () => `[WebGPU] jsepCopyCpuToGpu: dataOffset=${Number(o)}, gpuDataId=${Number(u)}, size=${Number(p)}`);
                let k = t.HEAPU8.subarray(Number(o >>> 0), Number(o >>> 0) + Number(p));
                a.upload(Number(u), k);
              }
            }, async (o, u, p) => {
              as("verbose", () => `[WebGPU] jsepCopyGpuToCpu: gpuDataId=${o}, dataOffset=${u}, size=${p}`), await a.download(Number(o), () => t.HEAPU8.subarray(Number(u) >>> 0, Number(u + p) >>> 0));
            }, (o, u, p) => a.createKernel(o, Number(u), p, t.UTF8ToString(t._JsepGetNodeName(Number(u)))), (o) => a.releaseKernel(o), (o, u, p, h) => {
              as("verbose", () => `[WebGPU] jsepRun: sessionHandle=${p}, kernel=${o}, contextDataOffset=${u}`);
              let k = new ka(t, a, Number(u));
              return a.computeKernel(Number(o), k, h);
            }, () => a.captureBegin(), () => a.captureEnd(), () => a.replay()]);
          } else {
            let a = new Hr(s);
            i("webnn", [a, () => a.reserveTensorId(), (o) => a.releaseTensorId(o), async (o, u, p, h) => a.ensureTensor(o, u, p, h), (o, u) => {
              a.uploadTensor(o, u);
            }, async (o, u) => a.downloadTensor(o, u)]);
          }
        };
      }), Sp, Ai, Kn, Lr, Pc, Hn, qn, Ec, Cc, kc, qr, hr, Th = g(() => {
        si(), ri(), zt(), lr(), _n(), Fn(), Sp = (e, t) => {
          Ms()._OrtInit(e, t) !== 0 && es("Can't initialize onnxruntime.");
        }, Ai = async (e) => {
          Sp(e.wasm.numThreads, Zr(e.logLevel));
        }, Kn = async (e, t) => {
          {
            let s = (Tc(), y(Pn)).init;
            if (t === "webgpu") {
              if (typeof navigator > "u" || !navigator.gpu) throw new Error("WebGPU is not supported in current environment");
              let n = e.webgpu.adapter;
              if (n) {
                if (typeof n.limits != "object" || typeof n.features != "object" || typeof n.requestDevice != "function") throw new Error("Invalid GPU adapter set in `env.webgpu.adapter`. It must be a GPUAdapter object.");
              } else {
                let i = e.webgpu.powerPreference;
                if (i !== void 0 && i !== "low-power" && i !== "high-performance") throw new Error(`Invalid powerPreference setting: "${i}"`);
                let a = e.webgpu.forceFallbackAdapter;
                if (a !== void 0 && typeof a != "boolean") throw new Error(`Invalid forceFallbackAdapter setting: "${a}"`);
                if (n = await navigator.gpu.requestAdapter({ powerPreference: i, forceFallbackAdapter: a }), !n) throw new Error('Failed to get GPU adapter. You may need to enable flag "--enable-unsafe-webgpu" if you are using Chrome.');
              }
              await s("webgpu", Ms(), e, n);
            }
            if (t === "webnn") {
              if (typeof navigator > "u" || !navigator.ml) throw new Error("WebNN is not supported in current environment");
              await s("webnn", Ms(), e);
            }
          }
        }, Lr = /* @__PURE__ */ new Map(), Pc = (e) => {
          let t = Ms(), s = t.stackSave();
          try {
            let n = t.PTR_SIZE, i = t.stackAlloc(2 * n);
            t._OrtGetInputOutputCount(e, i, i + n) !== 0 && es("Can't get session input/output count.");
            let a = n === 4 ? "i32" : "i64";
            return [Number(t.getValue(i, a)), Number(t.getValue(i + n, a))];
          } finally {
            t.stackRestore(s);
          }
        }, Hn = (e) => {
          let t = Ms(), s = t._malloc(e.byteLength);
          if (s === 0) throw new Error(`Can't create a session. failed to allocate a buffer of size ${e.byteLength}.`);
          return t.HEAPU8.set(e, s), [s, e.byteLength];
        }, qn = async (e, t) => {
          var C, d, z;
          let s, n, i = Ms();
          Array.isArray(e) ? [s, n] = e : e.buffer === i.HEAPU8.buffer ? [s, n] = [e.byteOffset, e.byteLength] : [s, n] = Hn(e);
          let a = 0, o = 0, u = 0, p = [], h = [], k = [];
          try {
            if ([o, p] = In(t), (t == null ? void 0 : t.externalData) && i.mountExternalData) {
              let Me = [];
              for (let Oe of t.externalData) {
                let Le = typeof Oe == "string" ? Oe : Oe.path;
                Me.push(bn(typeof Oe == "string" ? Oe : Oe.data).then((Ye) => {
                  i.mountExternalData(Le, Ye);
                }));
              }
              await Promise.all(Me);
            }
            for (let Me of (t == null ? void 0 : t.executionProviders) ?? []) if ((typeof Me == "string" ? Me : Me.name) === "webnn") {
              if (i.shouldTransferToMLTensor = !1, typeof Me != "string") {
                let Oe = Me, Le = Oe == null ? void 0 : Oe.context, Ye = Oe == null ? void 0 : Oe.gpuDevice, at = Oe == null ? void 0 : Oe.deviceType, Pt = Oe == null ? void 0 : Oe.powerPreference;
                Le ? i.currentContext = Le : Ye ? i.currentContext = await i.jsepCreateMLContext(Ye) : i.currentContext = await i.jsepCreateMLContext({ deviceType: at, powerPreference: Pt });
              } else i.currentContext = await i.jsepCreateMLContext();
              break;
            }
            a = await i._OrtCreateSession(s, n, o), a === 0 && es("Can't create a session."), (C = i.jsepOnCreateSession) == null || C.call(i), i.currentContext && (i.jsepRegisterMLContext(a, i.currentContext), i.currentContext = void 0, i.shouldTransferToMLTensor = !0);
            let [B, V] = Pc(a), Z = !!(t != null && t.enableGraphCapture), ee = [], X = [], he = [];
            for (let Me = 0; Me < B; Me++) {
              let Oe = i._OrtGetInputName(a, Me);
              Oe === 0 && es("Can't get an input name."), h.push(Oe), ee.push(i.UTF8ToString(Oe));
            }
            for (let Me = 0; Me < V; Me++) {
              let Oe = i._OrtGetOutputName(a, Me);
              Oe === 0 && es("Can't get an output name."), k.push(Oe);
              let Le = i.UTF8ToString(Oe);
              X.push(Le);
              {
                if (Z && (t == null ? void 0 : t.preferredOutputLocation) === void 0) {
                  he.push("gpu-buffer");
                  continue;
                }
                let Ye = typeof (t == null ? void 0 : t.preferredOutputLocation) == "string" ? t.preferredOutputLocation : ((d = t == null ? void 0 : t.preferredOutputLocation) == null ? void 0 : d[Le]) ?? "cpu";
                if (Ye !== "cpu" && Ye !== "cpu-pinned" && Ye !== "gpu-buffer" && Ye !== "ml-tensor") throw new Error(`Not supported preferred output location: ${Ye}.`);
                if (Z && Ye !== "gpu-buffer") throw new Error(`Not supported preferred output location: ${Ye}. Only 'gpu-buffer' location is supported when enableGraphCapture is true.`);
                he.push(Ye);
              }
            }
            let pe = null;
            return he.some((Me) => Me === "gpu-buffer" || Me === "ml-tensor") && (u = i._OrtCreateBinding(a), u === 0 && es("Can't create IO binding."), pe = { handle: u, outputPreferredLocations: he, outputPreferredLocationsEncoded: he.map((Me) => Mn(Me)) }), Lr.set(a, [a, h, k, pe, Z, !1]), [a, ee, X];
          } catch (B) {
            throw h.forEach((V) => i._OrtFree(V)), k.forEach((V) => i._OrtFree(V)), u !== 0 && i._OrtReleaseBinding(u) !== 0 && es("Can't release IO binding."), a !== 0 && i._OrtReleaseSession(a) !== 0 && es("Can't release session."), B;
          } finally {
            i._free(s), o !== 0 && i._OrtReleaseSessionOptions(o) !== 0 && es("Can't release session options."), p.forEach((B) => i._free(B)), (z = i.unmountExternalData) == null || z.call(i);
          }
        }, Ec = (e) => {
          var p;
          let t = Ms(), s = Lr.get(e);
          if (!s) throw new Error(`cannot release session. invalid session id: ${e}`);
          let [n, i, a, o, u] = s;
          o && (u && t._OrtClearBoundOutputs(o.handle) !== 0 && es("Can't clear bound outputs."), t._OrtReleaseBinding(o.handle) !== 0 && es("Can't release IO binding.")), (p = t.jsepOnReleaseSession) == null || p.call(t, e), i.forEach((h) => t._OrtFree(h)), a.forEach((h) => t._OrtFree(h)), t._OrtReleaseSession(n) !== 0 && es("Can't release session."), Lr.delete(e);
        }, Cc = (e, t, s, n, i, a = !1) => {
          if (!e) {
            t.push(0);
            return;
          }
          let o = Ms(), u = o.PTR_SIZE, p = e[0], h = e[1], k = e[3], C, d;
          if (p === "string" && (k === "gpu-buffer" || k === "ml-tensor")) throw new Error("String tensor is not supported on GPU.");
          if (a && k !== "gpu-buffer") throw new Error(`External buffer must be provided for input/output index ${i} when enableGraphCapture is true.`);
          if (k === "gpu-buffer") {
            let V = e[2].gpuBuffer;
            d = ur(Vr(p), h);
            let Z = o.jsepRegisterBuffer;
            if (!Z) throw new Error('Tensor location "gpu-buffer" is not supported without using WebGPU.');
            C = Z(n, i, V, d);
          } else if (k === "ml-tensor") {
            let V = e[2].mlTensor;
            d = ur(Vr(p), h);
            let Z = o.jsepRegisterMLTensor;
            if (!Z) throw new Error('Tensor location "ml-tensor" is not supported without using WebNN.');
            C = Z(V, Vr(p), h);
          } else {
            let V = e[2];
            if (Array.isArray(V)) {
              d = u * V.length, C = o._malloc(d), s.push(C);
              for (let Z = 0; Z < V.length; Z++) {
                if (typeof V[Z] != "string") throw new TypeError(`tensor data at index ${Z} is not a string`);
                o.setValue(C + Z * u, Fs(V[Z], s), "*");
              }
            } else d = V.byteLength, C = o._malloc(d), s.push(C), o.HEAPU8.set(new Uint8Array(V.buffer, V.byteOffset, d), C);
          }
          let z = o.stackSave(), B = o.stackAlloc(4 * h.length);
          try {
            h.forEach((Z, ee) => o.setValue(B + ee * u, Z, u === 4 ? "i32" : "i64"));
            let V = o._OrtCreateTensor(Vr(p), C, d, B, h.length, Mn(k));
            V === 0 && es(`Can't create tensor for input/output. session=${n}, index=${i}.`), t.push(V);
          } finally {
            o.stackRestore(z);
          }
        }, kc = async (e, t, s, n, i, a) => {
          var Xt, Zt;
          let o = Ms(), u = o.PTR_SIZE, p = Lr.get(e);
          if (!p) throw new Error(`cannot run inference. invalid session id: ${e}`);
          let h = p[0], k = p[1], C = p[2], d = p[3], z = p[4], B = p[5], V = t.length, Z = n.length, ee = 0, X = [], he = [], pe = [], Me = [], Oe = o.stackSave(), Le = o.stackAlloc(V * u), Ye = o.stackAlloc(V * u), at = o.stackAlloc(Z * u), Pt = o.stackAlloc(Z * u);
          try {
            (Xt = o.jsepOnRunStart) == null || Xt.call(o, h), [ee, X] = jr(a);
            for (let St = 0; St < V; St++) Cc(s[St], he, Me, e, t[St], z);
            for (let St = 0; St < Z; St++) Cc(i[St], pe, Me, e, V + n[St], z);
            for (let St = 0; St < V; St++) o.setValue(Le + St * u, he[St], "*"), o.setValue(Ye + St * u, k[t[St]], "*");
            for (let St = 0; St < Z; St++) o.setValue(at + St * u, pe[St], "*"), o.setValue(Pt + St * u, C[n[St]], "*");
            if (d && !B) {
              let { handle: St, outputPreferredLocations: Ot, outputPreferredLocationsEncoded: bs } = d;
              if (k.length !== V) throw new Error(`input count from feeds (${V}) is expected to be always equal to model's input count (${k.length}).`);
              for (let Ht = 0; Ht < V; Ht++) {
                let Rt = t[Ht];
                await o._OrtBindInput(St, k[Rt], he[Ht]) !== 0 && es(`Can't bind input[${Ht}] for session=${e}.`);
              }
              for (let Ht = 0; Ht < Z; Ht++) {
                let Rt = n[Ht];
                (Zt = i[Ht]) != null && Zt[3] ? o._OrtBindOutput(St, C[Rt], pe[Ht], 0) !== 0 && es(`Can't bind pre-allocated output[${Ht}] for session=${e}.`) : o._OrtBindOutput(St, C[Rt], 0, bs[Rt]) !== 0 && es(`Can't bind output[${Ht}] to ${Ot[Ht]} for session=${e}.`);
              }
              Lr.set(e, [h, k, C, d, z, !0]);
            }
            let bt;
            d ? bt = await o._OrtRunWithBinding(h, d.handle, Z, at, ee) : bt = await o._OrtRun(h, Ye, Le, V, Pt, Z, at, ee), bt !== 0 && es("failed to call OrtRun().");
            let ss = [];
            for (let St = 0; St < Z; St++) {
              let Ot = Number(o.getValue(at + St * u, "*"));
              if (Ot === pe[St]) {
                ss.push(i[St]);
                continue;
              }
              let bs = o.stackSave(), Ht = o.stackAlloc(4 * u), Rt = !1, _s, ot = 0;
              try {
                o._OrtGetTensorData(Ot, Ht, Ht + u, Ht + 2 * u, Ht + 3 * u) !== 0 && es(`Can't access output tensor data on index ${St}.`);
                let Et = u === 4 ? "i32" : "i64", ps = Number(o.getValue(Ht, Et));
                ot = o.getValue(Ht + u, "*");
                let Ns = o.getValue(Ht + u * 2, "*"), xr = Number(o.getValue(Ht + u * 3, Et)), er = [];
                for (let zs = 0; zs < xr; zs++) er.push(Number(o.getValue(Ns + zs * u, Et)));
                o._OrtFree(Ns) !== 0 && es("Can't free memory for tensor dims.");
                let cn = er.reduce((zs, Ps) => zs * Ps, 1);
                _s = mr(ps);
                let Oi = d == null ? void 0 : d.outputPreferredLocations[n[St]];
                if (_s === "string") {
                  if (Oi === "gpu-buffer" || Oi === "ml-tensor") throw new Error("String tensor is not supported on GPU.");
                  let zs = [];
                  for (let Ps = 0; Ps < cn; Ps++) {
                    let Xn = o.getValue(ot + Ps * u, "*"), Ac = o.getValue(ot + (Ps + 1) * u, "*"), th = Ps === cn - 1 ? void 0 : Ac - Xn;
                    zs.push(o.UTF8ToString(Xn, th));
                  }
                  ss.push([_s, er, zs, "cpu"]);
                } else if (Oi === "gpu-buffer" && cn > 0) {
                  let zs = o.jsepGetBuffer;
                  if (!zs) throw new Error('preferredLocation "gpu-buffer" is not supported without using WebGPU.');
                  let Ps = zs(ot), Xn = ur(ps, cn);
                  if (Xn === void 0 || !wn(_s)) throw new Error(`Unsupported data type: ${_s}`);
                  Rt = !0, ss.push([_s, er, { gpuBuffer: Ps, download: o.jsepCreateDownloader(Ps, Xn, _s), dispose: () => {
                    o._OrtReleaseTensor(Ot) !== 0 && es("Can't release tensor.");
                  } }, "gpu-buffer"]);
                } else if (Oi === "ml-tensor" && cn > 0) {
                  let zs = o.jsepEnsureTensor;
                  if (!zs) throw new Error('preferredLocation "ml-tensor" is not supported without using WebNN.');
                  if (ur(ps, cn) === void 0 || !yn(_s)) throw new Error(`Unsupported data type: ${_s}`);
                  let Ps = await zs(ot, ps, er, !1);
                  Rt = !0, ss.push([_s, er, { mlTensor: Ps, download: o.jsepCreateMLTensorDownloader(ot, _s), dispose: () => {
                    o.jsepReleaseTensorId(ot), o._OrtReleaseTensor(Ot);
                  } }, "ml-tensor"]);
                } else {
                  let zs = gn(_s), Ps = new zs(cn);
                  new Uint8Array(Ps.buffer, Ps.byteOffset, Ps.byteLength).set(o.HEAPU8.subarray(ot, ot + Ps.byteLength)), ss.push([_s, er, Ps, "cpu"]);
                }
              } finally {
                o.stackRestore(bs), _s === "string" && ot && o._free(ot), Rt || o._OrtReleaseTensor(Ot);
              }
            }
            return d && !z && (o._OrtClearBoundOutputs(d.handle) !== 0 && es("Can't clear bound outputs."), Lr.set(e, [h, k, C, d, z, !1])), ss;
          } finally {
            o.stackRestore(Oe), he.forEach((bt) => o._OrtReleaseTensor(bt)), pe.forEach((bt) => o._OrtReleaseTensor(bt)), Me.forEach((bt) => o._free(bt)), ee !== 0 && o._OrtReleaseRunOptions(ee), X.forEach((bt) => o._free(bt));
          }
        }, qr = (e) => {
          let t = Ms(), s = Lr.get(e);
          if (!s) throw new Error("invalid session id");
          let n = s[0], i = t._OrtEndProfiling(n);
          i === 0 && es("Can't get an profile file name."), t._OrtFree(i);
        }, hr = (e) => {
          let t = [];
          for (let s of e) {
            let n = s[2];
            !Array.isArray(n) && "buffer" in n && t.push(n.buffer);
          }
          return t;
        };
      }), Qn, vr, $a, Sc, $c, $p, Yp, Ap, Ii, Fi, Ph, Eh, Ch, kh, Sh, $h, Ah, Ih, Fh = g(() => {
        We(), Th(), lr(), Rr(), Qn = () => !!F.wasm.proxy && typeof document < "u", $a = !1, Sc = !1, $c = !1, Ap = /* @__PURE__ */ new Map(), Ii = (e, t) => {
          let s = Ap.get(e);
          s ? s.push(t) : Ap.set(e, [t]);
        }, Fi = () => {
          if ($a || !Sc || $c || !vr) throw new Error("worker not ready");
        }, Ph = (e) => {
          switch (e.data.type) {
            case "init-wasm":
              $a = !1, e.data.err ? ($c = !0, Yp[1](e.data.err)) : (Sc = !0, Yp[0]()), $p && (URL.revokeObjectURL($p), $p = void 0);
              break;
            case "init-ep":
            case "copy-from":
            case "create":
            case "release":
            case "run":
            case "end-profiling": {
              let t = Ap.get(e.data.type);
              e.data.err ? t.shift()[1](e.data.err) : t.shift()[0](e.data.out);
              break;
            }
          }
        }, Eh = async () => {
          if (!Sc) {
            if ($a) throw new Error("multiple calls to 'initWasm()' detected.");
            if ($c) throw new Error("previous call to 'initWasm()' failed.");
            if ($a = !0, Qn()) return new Promise((e, t) => {
              vr == null || vr.terminate(), Jr().then(([s, n]) => {
                var i;
                try {
                  vr = n, vr.onerror = (o) => t(o), vr.onmessage = Ph, Yp = [e, t];
                  let a = { type: "init-wasm", in: F };
                  !a.in.wasm.wasmPaths && (s || (i = import.meta.url) != null && i.startsWith("file:")) && (a.in.wasm.wasmPaths = { wasm: new URL(
                    /* asset import */
                    r(
                      /*! ort-wasm-simd-threaded.jsep.wasm */
                      "./node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm"
                    ),
                    r.b
                  ).href }), vr.postMessage(a), $p = s;
                } catch (a) {
                  t(a);
                }
              }, t);
            });
            try {
              await Ir(F.wasm), await Ai(F), Sc = !0;
            } catch (e) {
              throw $c = !0, e;
            } finally {
              $a = !1;
            }
          }
        }, Ch = async (e) => {
          if (Qn()) return Fi(), new Promise((t, s) => {
            Ii("init-ep", [t, s]);
            let n = { type: "init-ep", in: { epName: e, env: F } };
            vr.postMessage(n);
          });
          await Kn(F, e);
        }, kh = async (e) => Qn() ? (Fi(), new Promise((t, s) => {
          Ii("copy-from", [t, s]);
          let n = { type: "copy-from", in: { buffer: e } };
          vr.postMessage(n, [e.buffer]);
        })) : Hn(e), Sh = async (e, t) => {
          if (Qn()) {
            if (t != null && t.preferredOutputLocation) throw new Error('session option "preferredOutputLocation" is not supported for proxy.');
            return Fi(), new Promise((s, n) => {
              Ii("create", [s, n]);
              let i = { type: "create", in: { model: e, options: { ...t } } }, a = [];
              e instanceof Uint8Array && a.push(e.buffer), vr.postMessage(i, a);
            });
          } else return qn(e, t);
        }, $h = async (e) => {
          if (Qn()) return Fi(), new Promise((t, s) => {
            Ii("release", [t, s]);
            let n = { type: "release", in: e };
            vr.postMessage(n);
          });
          Ec(e);
        }, Ah = async (e, t, s, n, i, a) => {
          if (Qn()) {
            if (s.some((o) => o[3] !== "cpu")) throw new Error("input tensor on GPU is not supported for proxy.");
            if (i.some((o) => o)) throw new Error("pre-allocated output tensor is not supported for proxy.");
            return Fi(), new Promise((o, u) => {
              Ii("run", [o, u]);
              let p = s, h = { type: "run", in: { sessionId: e, inputIndices: t, inputs: p, outputIndices: n, options: a } };
              vr.postMessage(h, hr(p));
            });
          } else return kc(e, t, s, n, i, a);
        }, Ih = async (e) => {
          if (Qn()) return Fi(), new Promise((t, s) => {
            Ii("end-profiling", [t, s]);
            let n = { type: "end-profiling", in: e };
            vr.postMessage(n);
          });
          qr(e);
        };
      }), Jp, Oh, Dh, df = g(() => {
        We(), Fh(), zt(), Je(), Fn(), Jp = (e, t) => {
          switch (e.location) {
            case "cpu":
              return [e.type, e.dims, e.data, "cpu"];
            case "gpu-buffer":
              return [e.type, e.dims, { gpuBuffer: e.gpuBuffer }, "gpu-buffer"];
            case "ml-tensor":
              return [e.type, e.dims, { mlTensor: e.mlTensor }, "ml-tensor"];
            default:
              throw new Error(`invalid data location: ${e.location} for ${t()}`);
          }
        }, Oh = (e) => {
          switch (e[3]) {
            case "cpu":
              return new ae(e[0], e[2], e[1]);
            case "gpu-buffer": {
              let t = e[0];
              if (!wn(t)) throw new Error(`not supported data type: ${t} for deserializing GPU tensor`);
              let { gpuBuffer: s, download: n, dispose: i } = e[2];
              return ae.fromGpuBuffer(s, { dataType: t, dims: e[1], download: n, dispose: i });
            }
            case "ml-tensor": {
              let t = e[0];
              if (!yn(t)) throw new Error(`not supported data type: ${t} for deserializing MLTensor tensor`);
              let { mlTensor: s, download: n, dispose: i } = e[2];
              return ae.fromMLTensor(s, { dataType: t, dims: e[1], download: n, dispose: i });
            }
            default:
              throw new Error(`invalid data location: ${e[3]}`);
          }
        }, Dh = class {
          async fetchModelAndCopyToWasmMemory(e) {
            return kh(await bn(e));
          }
          async loadModel(e, t) {
            Ne();
            let s;
            typeof e == "string" ? s = await this.fetchModelAndCopyToWasmMemory(e) : s = e, [this.sessionId, this.inputNames, this.outputNames] = await Sh(s, t), Re();
          }
          async dispose() {
            return $h(this.sessionId);
          }
          async run(e, t, s) {
            Ne();
            let n = [], i = [];
            Object.entries(e).forEach((C) => {
              let d = C[0], z = C[1], B = this.inputNames.indexOf(d);
              if (B === -1) throw new Error(`invalid input '${d}'`);
              n.push(z), i.push(B);
            });
            let a = [], o = [];
            Object.entries(t).forEach((C) => {
              let d = C[0], z = C[1], B = this.outputNames.indexOf(d);
              if (B === -1) throw new Error(`invalid output '${d}'`);
              a.push(z), o.push(B);
            });
            let u = n.map((C, d) => Jp(C, () => `input "${this.inputNames[i[d]]}"`)), p = a.map((C, d) => C ? Jp(C, () => `output "${this.outputNames[o[d]]}"`) : null), h = await Ah(this.sessionId, i, u, o, p, s), k = {};
            for (let C = 0; C < h.length; C++) k[this.outputNames[o[C]]] = a[C] ?? Oh(h[C]);
            return Re(), k;
          }
          startProfiling() {
          }
          endProfiling() {
            Ih(this.sessionId);
          }
        };
      }), Lh = {};
      v(Lh, { OnnxruntimeWebAssemblyBackend: () => eh, initializeFlags: () => Zp, wasmBackend: () => zh });
      var Zp, eh, zh, cf = g(() => {
        We(), Fh(), df(), Zp = () => {
          if ((typeof F.wasm.initTimeout != "number" || F.wasm.initTimeout < 0) && (F.wasm.initTimeout = 0), F.wasm.simd === !1 && console.warn('Deprecated property "env.wasm.simd" is set to false. non-SIMD build is no longer provided, and this setting will be ignored.'), typeof F.wasm.proxy != "boolean" && (F.wasm.proxy = !1), typeof F.wasm.trace != "boolean" && (F.wasm.trace = !1), typeof F.wasm.numThreads != "number" || !Number.isInteger(F.wasm.numThreads) || F.wasm.numThreads <= 0) if (typeof self < "u" && !self.crossOriginIsolated) F.wasm.numThreads = 1;
          else {
            let e = typeof navigator > "u" ? R("node:os").cpus().length : navigator.hardwareConcurrency;
            F.wasm.numThreads = Math.min(4, Math.ceil((e || 1) / 2));
          }
        }, eh = class {
          async init(e) {
            Zp(), await Eh(), await Ch(e);
          }
          async createInferenceSessionHandler(e, t) {
            let s = new Dh();
            return await s.loadModel(e, t), Promise.resolve(s);
          }
        }, zh = new eh();
      });
      We(), We(), We();
      var pf = "1.21.0-dev.20250114-228dd16893", hf = Ae;
      {
        let e = (cf(), y(Lh)).wasmBackend;
        K("webgpu", e, 5), K("webnn", e, 5), K("cpu", e, 10), K("wasm", e, 10);
      }
      Object.defineProperty(F.versions, "web", { value: pf, enumerable: !0 });
      /**
       * @license
       * Copyright 2021 Google LLC. All Rights Reserved.
       * Licensed under the Apache License, Version 2.0 (the "License");
       * you may not use this file except in compliance with the License.
       * You may obtain a copy of the License at
       *
       * http://www.apache.org/licenses/LICENSE-2.0
       *
       * Unless required by applicable law or agreed to in writing, software
       * distributed under the License is distributed on an "AS IS" BASIS,
       * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       * See the License for the specific language governing permissions and
       * limitations under the License.
       * =============================================================================
       */
      /**
       * @license
       * Copyright 2020 Google LLC. All Rights Reserved.
       * Licensed under the Apache License, Version 2.0 (the "License");
       * you may not use this file except in compliance with the License.
       * You may obtain a copy of the License at
       *
       * http://www.apache.org/licenses/LICENSE-2.0
       *
       * Unless required by applicable law or agreed to in writing, software
       * distributed under the License is distributed on an "AS IS" BASIS,
       * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       * See the License for the specific language governing permissions and
       * limitations under the License.
       * =============================================================================
       */
      /**
       * @license
       * Copyright 2019 Google LLC. All Rights Reserved.
       * Licensed under the Apache License, Version 2.0 (the "License");
       * you may not use this file except in compliance with the License.
       * You may obtain a copy of the License at
       *
       * http://www.apache.org/licenses/LICENSE-2.0
       *
       * Unless required by applicable law or agreed to in writing, software
       * distributed under the License is distributed on an "AS IS" BASIS,
       * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       * See the License for the specific language governing permissions and
       * limitations under the License.
       * =============================================================================
       */
    }
  ),
  /***/
  "./src/backends/onnx.js": (
    /*!******************************!*\
      !*** ./src/backends/onnx.js ***!
      \******************************/
    /***/
    ($e, $, r) => {
      var _;
      r.r($), r.d($, {
        /* harmony export */
        Tensor: () => (
          /* reexport safe */
          R.Tensor
        ),
        /* harmony export */
        createInferenceSession: () => (
          /* binding */
          ie
        ),
        /* harmony export */
        deviceToExecutionProviders: () => (
          /* binding */
          K
        ),
        /* harmony export */
        isONNXProxy: () => (
          /* binding */
          q
        ),
        /* harmony export */
        isONNXTensor: () => (
          /* binding */
          W
        )
        /* harmony export */
      });
      var D = r(
        /*! ../env.js */
        "./src/env.js"
      ), U = r(
        /*! onnxruntime-node */
        "?2ce3"
      ), Y = r(
        /*! onnxruntime-web */
        "./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs?3a96"
      ), R = r(
        /*! onnxruntime-common */
        "./node_modules/onnxruntime-common/dist/esm/index.js"
      );
      const g = Object.freeze({
        auto: null,
        // Auto-detect based on device and environment
        gpu: null,
        // Auto-detect GPU
        cpu: "cpu",
        // CPU
        wasm: "wasm",
        // WebAssembly
        webgpu: "webgpu",
        // WebGPU
        cuda: "cuda",
        // CUDA
        dml: "dml",
        // DirectML
        webnn: { name: "webnn", deviceType: "cpu" },
        // WebNN (default)
        "webnn-npu": { name: "webnn", deviceType: "npu" },
        // WebNN NPU
        "webnn-gpu": { name: "webnn", deviceType: "gpu" },
        // WebNN GPU
        "webnn-cpu": { name: "webnn", deviceType: "cpu" }
        // WebNN CPU
      }), v = [];
      let M, y;
      const b = Symbol.for("onnxruntime");
      if (b in globalThis)
        y = globalThis[b];
      else if (D.apis.IS_NODE_ENV) {
        switch (y = U ?? (_ || (_ = r.t(U, 2))), process.platform) {
          case "win32":
            v.push("dml");
            break;
          case "linux":
            process.arch === "x64" && v.push("cuda");
            break;
        }
        v.push("cpu"), M = ["cpu"];
      } else
        y = Y, D.apis.IS_WEBNN_AVAILABLE && v.push("webnn-npu", "webnn-gpu", "webnn-cpu", "webnn"), D.apis.IS_WEBGPU_AVAILABLE && v.push("webgpu"), v.push("wasm"), M = ["wasm"];
      const I = y.InferenceSession;
      function K(A = null) {
        if (!A) return M;
        switch (A) {
          case "auto":
            return v;
          case "gpu":
            return v.filter(
              (S) => ["webgpu", "cuda", "dml", "webnn-gpu"].includes(S)
            );
        }
        if (v.includes(A))
          return [g[A] ?? A];
        throw new Error(`Unsupported device: "${A}". Should be one of: ${v.join(", ")}.`);
      }
      let se = null;
      async function ie(A, S, w) {
        se && await se;
        const x = I.create(A, S);
        se ?? (se = x);
        const F = await x;
        return F.config = w, F;
      }
      function W(A) {
        return A instanceof y.Tensor;
      }
      const j = y == null ? void 0 : y.env;
      j != null && j.wasm && (j.wasm.wasmPaths = `https://cdn.jsdelivr.net/npm/@huggingface/transformers@${D.env.version}/dist/`, j.wasm.proxy = !1, (typeof crossOriginIsolated > "u" || !crossOriginIsolated) && (j.wasm.numThreads = 1)), j != null && j.webgpu && (j.webgpu.powerPreference = "high-performance");
      function q() {
        var A;
        return (A = j == null ? void 0 : j.wasm) == null ? void 0 : A.proxy;
      }
      D.env.backends.onnx = j;
    }
  ),
  /***/
  "./src/base/feature_extraction_utils.js": (
    /*!**********************************************!*\
      !*** ./src/base/feature_extraction_utils.js ***!
      \**********************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        FeatureExtractor: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        validate_audio_inputs: () => (
          /* binding */
          R
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../utils/constants.js */
        "./src/utils/constants.js"
      ), D = r(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      ), U = r(
        /*! ../utils/hub.js */
        "./src/utils/hub.js"
      );
      class Y extends D.Callable {
        /**
         * Constructs a new FeatureExtractor instance.
         *
         * @param {Object} config The configuration for the feature extractor.
         */
        constructor(v) {
          super(), this.config = v;
        }
        /**
         * Instantiate one of the feature extractor classes of the library from a pretrained model.
         * 
         * The feature extractor class to instantiate is selected based on the `feature_extractor_type` property of
         * the config object (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained feature_extractor hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing feature_extractor files, e.g., `./my_model_directory/`.
         * @param {import('../utils/hub.js').PretrainedOptions} options Additional options for loading the feature_extractor.
         * 
         * @returns {Promise<FeatureExtractor>} A new instance of the Feature Extractor class.
         */
        static async from_pretrained(v, M) {
          const y = await (0, U.getModelJSON)(v, _.FEATURE_EXTRACTOR_NAME, !0, M);
          return new this(y);
        }
      }
      function R(g, v) {
        var M;
        if (!(g instanceof Float32Array || g instanceof Float64Array))
          throw new Error(
            `${v} expects input to be a Float32Array or a Float64Array, but got ${((M = g == null ? void 0 : g.constructor) == null ? void 0 : M.name) ?? typeof g} instead. If using the feature extractor directly, remember to use \`read_audio(url, sampling_rate)\` to obtain the raw audio data of the file/url.`
          );
      }
    }
  ),
  /***/
  "./src/base/image_processors_utils.js": (
    /*!********************************************!*\
      !*** ./src/base/image_processors_utils.js ***!
      \********************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ImageProcessor: () => (
          /* binding */
          A
        ),
        /* harmony export */
        center_to_corners_format: () => (
          /* binding */
          y
        ),
        /* harmony export */
        post_process_instance_segmentation: () => (
          /* binding */
          q
        ),
        /* harmony export */
        post_process_object_detection: () => (
          /* binding */
          b
        ),
        /* harmony export */
        post_process_panoptic_segmentation: () => (
          /* binding */
          j
        ),
        /* harmony export */
        post_process_semantic_segmentation: () => (
          /* binding */
          I
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      ), D = r(
        /*! ../utils/tensor.js */
        "./src/utils/tensor.js"
      ), U = r(
        /*! ../utils/maths.js */
        "./src/utils/maths.js"
      );
      r(
        /*! ../utils/image.js */
        "./src/utils/image.js"
      );
      var Y = r(
        /*! ../utils/core.js */
        "./src/utils/core.js"
      ), R = r(
        /*! ../utils/hub.js */
        "./src/utils/hub.js"
      ), g = r(
        /*! ../utils/constants.js */
        "./src/utils/constants.js"
      );
      function v(S, w, x = 0, F = null) {
        const le = S / w;
        let ne = (0, U.bankers_round)(le) * w;
        return F !== null && ne > F && (ne = Math.floor(le) * w), ne < x && (ne = Math.ceil(le) * w), ne;
      }
      function M([S, w], x) {
        return [
          Math.max(Math.floor(S / x), 1) * x,
          Math.max(Math.floor(w / x), 1) * x
        ];
      }
      function y([S, w, x, F]) {
        return [
          S - x / 2,
          w - F / 2,
          S + x / 2,
          w + F / 2
        ];
      }
      function b(S, w = 0.5, x = null, F = !1) {
        const le = S.logits, ne = S.pred_boxes, [be, _e, re] = le.dims;
        if (x !== null && x.length !== be)
          throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");
        let xe = [];
        for (let ce = 0; ce < be; ++ce) {
          let ke = x !== null ? x[ce] : null, Fe = {
            boxes: [],
            classes: [],
            scores: []
          }, Ee = le[ce], tt = ne[ce];
          for (let Ge = 0; Ge < _e; ++Ge) {
            let ye = Ee[Ge], J = [], de;
            if (F) {
              de = ye.sigmoid().data;
              for (let Ce = 0; Ce < de.length; ++Ce)
                de[Ce] > w && J.push(Ce);
            } else {
              let Ce = (0, U.max)(ye.data)[1];
              if (Ce === re - 1 || (de = (0, U.softmax)(ye.data), de[Ce] < w))
                continue;
              J.push(Ce);
            }
            for (const Ce of J) {
              let Be = tt[Ge].data;
              Be = y(Be), ke !== null && (Be = Be.map((Ze, te) => Ze * ke[(te + 1) % 2])), Fe.boxes.push(Be), Fe.classes.push(Ce), Fe.scores.push(de[Ce]);
            }
          }
          xe.push(Fe);
        }
        return xe;
      }
      function I(S, w = null) {
        const x = S.logits, F = x.dims[0];
        if (w !== null && w.length !== F)
          throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");
        const le = [];
        for (let ne = 0; ne < F; ++ne) {
          const be = w !== null ? w[ne] : null;
          let _e = x[ne];
          be !== null && (_e = (0, D.interpolate)(_e, be, "bilinear", !1));
          const [re, xe] = be ?? _e.dims.slice(-2), ce = new D.Tensor(
            "int32",
            new Int32Array(re * xe),
            [re, xe]
          ), ke = _e[0].data, Fe = ce.data;
          for (let Ge = 1; Ge < _e.dims[0]; ++Ge) {
            const ye = _e[Ge].data;
            for (let J = 0; J < ye.length; ++J)
              ye[J] > ke[J] && (ke[J] = ye[J], Fe[J] = Ge);
          }
          const Ee = new Array(_e.dims[0]);
          for (let Ge = 0; Ge < Fe.length; ++Ge) {
            const ye = Fe[Ge];
            Ee[ye] = ye;
          }
          const tt = Ee.filter((Ge) => Ge !== void 0);
          le.push({ segmentation: ce, labels: tt });
        }
        return le;
      }
      function K(S, w, x, F) {
        const le = [], ne = [], be = [];
        for (let _e = 0; _e < S.dims[0]; ++_e) {
          const re = S[_e], xe = w[_e], ce = (0, U.max)(re.data)[1];
          if (ce === F)
            continue;
          const Fe = (0, U.softmax)(re.data)[ce];
          Fe > x && (le.push(xe), ne.push(Fe), be.push(ce));
        }
        return [le, ne, be];
      }
      function se(S, w, x, F = 0.5, le = 0.8) {
        const ne = [];
        let be = 0, _e = 0;
        const re = w[x].data;
        for (let ce = 0; ce < S.length; ++ce)
          S[ce] === x && (ne.push(ce), ++be), re[ce] >= F && ++_e;
        let xe = be > 0 && _e > 0;
        return xe && (xe = be / _e > le), [xe, ne];
      }
      function ie(S, w, x, F, le, ne = null, be = null) {
        const [_e, re] = be ?? S[0].dims, xe = new D.Tensor(
          "int32",
          new Int32Array(_e * re),
          [_e, re]
        ), ce = [];
        if (be !== null)
          for (let Ge = 0; Ge < S.length; ++Ge)
            S[Ge] = (0, D.interpolate)(S[Ge], be, "bilinear", !1);
        const ke = new Int32Array(S[0].data.length), Fe = new Float32Array(S[0].data.length);
        for (let Ge = 0; Ge < S.length; ++Ge) {
          let ye = w[Ge];
          const J = S[Ge].data;
          for (let de = 0; de < J.length; ++de)
            J[de] *= ye, J[de] > Fe[de] && (ke[de] = Ge, Fe[de] = J[de]);
        }
        let Ee = 0;
        const tt = xe.data;
        for (let Ge = 0; Ge < x.length; ++Ge) {
          const ye = x[Ge], [J, de] = se(
            ke,
            S,
            Ge,
            F,
            le
          );
          if (J) {
            ++Ee;
            for (const Ce of de)
              tt[Ce] = Ee;
            ce.push({
              id: Ee,
              label_id: ye,
              // was_fused: should_fuse, TODO
              score: w[Ge]
            });
          }
        }
        return [xe, ce];
      }
      function W(S, w, x = 28, F = 3136, le = 1003520) {
        if (S < x || w < x)
          throw new Error(`height:${S} or width:${w} must be larger than factor:${x}`);
        if (Math.max(S, w) / Math.min(S, w) > 200)
          throw new Error(
            `absolute aspect ratio must be smaller than 200, got ${Math.max(S, w) / Math.min(S, w)}`
          );
        let ne = Math.round(S / x) * x, be = Math.round(w / x) * x;
        if (ne * be > le) {
          const _e = Math.sqrt(S * w / le);
          ne = Math.floor(S / _e / x) * x, be = Math.floor(w / _e / x) * x;
        } else if (ne * be < F) {
          const _e = Math.sqrt(F / (S * w));
          ne = Math.ceil(S * _e / x) * x, be = Math.ceil(w * _e / x) * x;
        }
        return [ne, be];
      }
      function j(S, w = 0.5, x = 0.5, F = 0.8, le = null, ne = null) {
        le === null && (console.warn("`label_ids_to_fuse` unset. No instance will be fused."), le = /* @__PURE__ */ new Set());
        const be = S.class_queries_logits ?? S.logits, re = (S.masks_queries_logits ?? S.pred_masks).sigmoid();
        let [xe, ce, ke] = be.dims;
        if (ke -= 1, ne !== null && ne.length !== xe)
          throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");
        let Fe = [];
        for (let Ee = 0; Ee < xe; ++Ee) {
          let tt = ne !== null ? ne[Ee] : null, Ge = be[Ee], ye = re[Ee], [J, de, Ce] = K(Ge, ye, w, ke);
          if (Ce.length === 0) {
            let [te, Ke] = tt ?? ye.dims.slice(-2), je = new D.Tensor(
              "int32",
              new Int32Array(te * Ke).fill(-1),
              [te, Ke]
            );
            Fe.push({
              segmentation: je,
              segments_info: []
            });
            continue;
          }
          let [Be, Ze] = ie(
            J,
            de,
            Ce,
            x,
            F,
            le,
            tt
          );
          Fe.push({
            segmentation: Be,
            segments_info: Ze
          });
        }
        return Fe;
      }
      function q(S, w = 0.5, x = null) {
        throw new Error("`post_process_instance_segmentation` is not yet implemented.");
      }
      class A extends _.Callable {
        /**
         * Constructs a new `ImageProcessor`.
         * @param {ImageProcessorConfig} config The configuration object.
         */
        constructor(w) {
          super(), this.image_mean = w.image_mean ?? w.mean, this.image_std = w.image_std ?? w.std, this.resample = w.resample ?? 2, this.do_rescale = w.do_rescale ?? !0, this.rescale_factor = w.rescale_factor ?? 0.00392156862745098, this.do_normalize = w.do_normalize, this.do_thumbnail = w.do_thumbnail, this.size = w.size ?? w.image_size, this.do_resize = w.do_resize ?? this.size !== void 0, this.size_divisibility = w.size_divisibility ?? w.size_divisor, this.do_center_crop = w.do_center_crop, this.crop_size = w.crop_size, this.do_convert_rgb = w.do_convert_rgb ?? !0, this.do_crop_margin = w.do_crop_margin, this.pad_size = w.pad_size, this.do_pad = w.do_pad, this.do_pad && !this.pad_size && this.size && this.size.width !== void 0 && this.size.height !== void 0 && (this.pad_size = this.size), this.do_flip_channel_order = w.do_flip_channel_order ?? !1, this.config = w;
        }
        /**
         * Resize the image to make a thumbnail. The image is resized so that no dimension is larger than any
         * corresponding dimension of the specified size.
         * @param {RawImage} image The image to be resized.
         * @param {{height:number, width:number}} size The size `{"height": h, "width": w}` to resize the image to.
         * @param {string | 0 | 1 | 2 | 3 | 4 | 5} [resample=2] The resampling filter to use.
         * @returns {Promise<RawImage>} The resized image.
         */
        async thumbnail(w, x, F = 2) {
          const le = w.height, ne = w.width, be = x.height, _e = x.width;
          let re = Math.min(le, be), xe = Math.min(ne, _e);
          return re === le && xe === ne ? w : (le > ne ? xe = Math.floor(ne * re / le) : ne > le && (re = Math.floor(le * xe / ne)), await w.resize(xe, re, { resample: F }));
        }
        /**
         * Crops the margin of the image. Gray pixels are considered margin (i.e., pixels with a value below the threshold).
         * @param {RawImage} image The image to be cropped.
         * @param {number} gray_threshold Value below which pixels are considered to be gray.
         * @returns {Promise<RawImage>} The cropped image.
         */
        async crop_margin(w, x = 200) {
          const F = w.clone().grayscale(), le = (0, U.min)(F.data)[0], be = (0, U.max)(F.data)[0] - le;
          if (be === 0)
            return w;
          const _e = x / 255;
          let re = F.width, xe = F.height, ce = 0, ke = 0;
          const Fe = F.data;
          for (let Ee = 0; Ee < F.height; ++Ee) {
            const tt = Ee * F.width;
            for (let Ge = 0; Ge < F.width; ++Ge)
              (Fe[tt + Ge] - le) / be < _e && (re = Math.min(re, Ge), xe = Math.min(xe, Ee), ce = Math.max(ce, Ge), ke = Math.max(ke, Ee));
          }
          return w = await w.crop([re, xe, ce, ke]), w;
        }
        /**
         * Pad the image by a certain amount.
         * @param {Float32Array} pixelData The pixel data to pad.
         * @param {number[]} imgDims The dimensions of the image (height, width, channels).
         * @param {{width:number; height:number}|number|'square'} padSize The dimensions of the padded image.
         * @param {Object} options The options for padding.
         * @param {'constant'|'symmetric'} [options.mode='constant'] The type of padding to add.
         * @param {boolean} [options.center=false] Whether to center the image.
         * @param {number|number[]} [options.constant_values=0] The constant value to use for padding.
         * @returns {[Float32Array, number[]]} The padded pixel data and image dimensions.
         */
        pad_image(w, x, F, {
          mode: le = "constant",
          center: ne = !1,
          constant_values: be = 0
        } = {}) {
          const [_e, re, xe] = x;
          let ce, ke;
          if (typeof F == "number" ? (ce = F, ke = F) : F === "square" ? ce = ke = Math.max(_e, re) : (ce = F.width, ke = F.height), ce !== re || ke !== _e) {
            const Fe = new Float32Array(ce * ke * xe);
            if (Array.isArray(be))
              for (let Ge = 0; Ge < Fe.length; ++Ge)
                Fe[Ge] = be[Ge % xe];
            else be !== 0 && Fe.fill(be);
            const [Ee, tt] = ne ? [Math.floor((ce - re) / 2), Math.floor((ke - _e) / 2)] : [0, 0];
            for (let Ge = 0; Ge < _e; ++Ge) {
              const ye = (Ge + tt) * ce, J = Ge * re;
              for (let de = 0; de < re; ++de) {
                const Ce = (ye + de + Ee) * xe, Be = (J + de) * xe;
                for (let Ze = 0; Ze < xe; ++Ze)
                  Fe[Ce + Ze] = w[Be + Ze];
              }
            }
            if (le === "symmetric") {
              if (ne)
                throw new Error("`center` padding is not supported when `mode` is set to `symmetric`.");
              const Ge = _e - 1, ye = re - 1;
              for (let J = 0; J < ke; ++J) {
                const de = J * ce, Ce = (0, Y.calculateReflectOffset)(J, Ge) * re;
                for (let Be = 0; Be < ce; ++Be) {
                  if (J < _e && Be < re) continue;
                  const Ze = (de + Be) * xe, te = (Ce + (0, Y.calculateReflectOffset)(Be, ye)) * xe;
                  for (let Ke = 0; Ke < xe; ++Ke)
                    Fe[Ze + Ke] = w[te + Ke];
                }
              }
            }
            w = Fe, x = [ke, ce, xe];
          }
          return [w, x];
        }
        /**
         * Rescale the image' pixel values by `this.rescale_factor`.
         * @param {Float32Array} pixelData The pixel data to rescale.
         * @returns {void}
         */
        rescale(w) {
          for (let x = 0; x < w.length; ++x)
            w[x] = this.rescale_factor * w[x];
        }
        /**
         * Find the target (width, height) dimension of the output image after
         * resizing given the input image and the desired size.
         * @param {RawImage} image The image to resize.
         * @param {any} size The size to use for resizing the image. 
         * @returns {[number, number]} The target (width, height) dimension of the output image after resizing.
         */
        get_resize_output_image_size(w, x) {
          const [F, le] = w.size;
          let ne, be;
          if (this.do_thumbnail) {
            const { height: _e, width: re } = x;
            ne = Math.min(_e, re);
          } else Number.isInteger(x) ? (ne = x, be = this.config.max_size ?? ne) : x !== void 0 && (ne = x.shortest_edge, be = x.longest_edge);
          if (ne !== void 0 || be !== void 0) {
            const _e = ne === void 0 ? 1 : Math.max(ne / F, ne / le), re = F * _e, xe = le * _e, ce = be === void 0 ? 1 : Math.min(be / re, be / xe);
            let ke = Math.floor(Number((re * ce).toFixed(2))), Fe = Math.floor(Number((xe * ce).toFixed(2)));
            return this.size_divisibility !== void 0 && ([ke, Fe] = M([ke, Fe], this.size_divisibility)), [ke, Fe];
          } else if (x !== void 0 && x.width !== void 0 && x.height !== void 0) {
            let _e = x.width, re = x.height;
            if (this.config.keep_aspect_ratio && this.config.ensure_multiple_of) {
              let xe = re / le, ce = _e / F;
              Math.abs(1 - ce) < Math.abs(1 - xe) ? xe = ce : ce = xe, re = v(xe * le, this.config.ensure_multiple_of), _e = v(ce * F, this.config.ensure_multiple_of);
            }
            return [_e, re];
          } else {
            if (this.size_divisibility !== void 0)
              return M([F, le], this.size_divisibility);
            if (x.min_pixels !== void 0 && x.max_pixels !== void 0) {
              const { min_pixels: _e, max_pixels: re } = x, xe = this.config.patch_size * this.config.merge_size;
              return W(le, F, xe, _e, re);
            } else
              throw new Error(`Could not resize image due to unsupported \`this.size\` option in config: ${JSON.stringify(x)}`);
          }
        }
        /**
         * Resizes the image.
         * @param {RawImage} image The image to resize.
         * @returns {Promise<RawImage>} The resized image.
         */
        async resize(w) {
          const [x, F] = this.get_resize_output_image_size(w, this.size);
          return await w.resize(x, F, {
            // @ts-expect-error TS2322
            resample: this.resample
          });
        }
        /**
         * @typedef {object} PreprocessedImage
         * @property {HeightWidth} original_size The original size of the image.
         * @property {HeightWidth} reshaped_input_size The reshaped input size of the image.
         * @property {Tensor} pixel_values The pixel values of the preprocessed image.
         */
        /**
         * Preprocesses the given image.
         *
         * @param {RawImage} image The image to preprocess.
         * @param {Object} overrides The overrides for the preprocessing options.
         * @returns {Promise<PreprocessedImage>} The preprocessed image.
         */
        async preprocess(w, {
          do_normalize: x = null,
          do_pad: F = null,
          do_convert_rgb: le = null,
          do_convert_grayscale: ne = null,
          do_flip_channel_order: be = null
        } = {}) {
          this.do_crop_margin && (w = await this.crop_margin(w));
          const [_e, re] = w.size;
          if (le ?? this.do_convert_rgb ? w = w.rgb() : ne && (w = w.grayscale()), this.do_resize && (w = await this.resize(w)), this.do_thumbnail && (w = await this.thumbnail(w, this.size, this.resample)), this.do_center_crop) {
            let Ee, tt;
            Number.isInteger(this.crop_size) ? (Ee = this.crop_size, tt = this.crop_size) : (Ee = this.crop_size.width, tt = this.crop_size.height), w = await w.center_crop(Ee, tt);
          }
          const xe = [w.height, w.width];
          let ce = Float32Array.from(w.data), ke = [w.height, w.width, w.channels];
          if (this.do_rescale && this.rescale(ce), x ?? this.do_normalize) {
            let Ee = this.image_mean;
            Array.isArray(this.image_mean) || (Ee = new Array(w.channels).fill(Ee));
            let tt = this.image_std;
            if (Array.isArray(this.image_std) || (tt = new Array(w.channels).fill(Ee)), Ee.length !== w.channels || tt.length !== w.channels)
              throw new Error(`When set to arrays, the length of \`image_mean\` (${Ee.length}) and \`image_std\` (${tt.length}) must match the number of channels in the image (${w.channels}).`);
            for (let Ge = 0; Ge < ce.length; Ge += w.channels)
              for (let ye = 0; ye < w.channels; ++ye)
                ce[Ge + ye] = (ce[Ge + ye] - Ee[ye]) / tt[ye];
          }
          if (F ?? this.do_pad) {
            if (this.pad_size)
              [ce, ke] = this.pad_image(ce, [w.height, w.width, w.channels], this.pad_size);
            else if (this.size_divisibility) {
              const [Ee, tt] = M([ke[1], ke[0]], this.size_divisibility);
              [ce, ke] = this.pad_image(ce, ke, { width: Ee, height: tt });
            }
          }
          if (be ?? this.do_flip_channel_order) {
            if (ke[2] !== 3)
              throw new Error("Flipping channel order is only supported for RGB images.");
            for (let Ee = 0; Ee < ce.length; Ee += 3) {
              const tt = ce[Ee];
              ce[Ee] = ce[Ee + 2], ce[Ee + 2] = tt;
            }
          }
          const Fe = new D.Tensor("float32", ce, ke).permute(2, 0, 1);
          return {
            original_size: [re, _e],
            reshaped_input_size: xe,
            pixel_values: Fe
          };
        }
        /**
         * Calls the feature extraction process on an array of images,
         * preprocesses each image, and concatenates the resulting
         * features into a single Tensor.
         * @param {RawImage[]} images The image(s) to extract features from.
         * @param {...any} args Additional arguments.
         * @returns {Promise<ImageProcessorResult>} An object containing the concatenated pixel values (and other metadata) of the preprocessed images.
         */
        async _call(w, ...x) {
          Array.isArray(w) || (w = [w]);
          const F = await Promise.all(w.map((ne) => this.preprocess(ne)));
          return {
            pixel_values: (0, D.stack)(F.map((ne) => ne.pixel_values), 0),
            // Original sizes of images
            original_sizes: F.map((ne) => ne.original_size),
            // Reshaped sizes of images, before padding or cropping
            reshaped_input_sizes: F.map((ne) => ne.reshaped_input_size)
          };
        }
        /**
         * Instantiate one of the processor classes of the library from a pretrained model.
         * 
         * The processor class to instantiate is selected based on the `image_processor_type` (or `feature_extractor_type`; legacy)
         * property of the config object (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.
         * @param {import('../utils/hub.js').PretrainedOptions} options Additional options for loading the processor.
         * 
         * @returns {Promise<ImageProcessor>} A new instance of the Processor class.
         */
        static async from_pretrained(w, x) {
          const F = await (0, R.getModelJSON)(w, g.IMAGE_PROCESSOR_NAME, !0, x);
          return new this(F);
        }
      }
    }
  ),
  /***/
  "./src/base/processing_utils.js": (
    /*!**************************************!*\
      !*** ./src/base/processing_utils.js ***!
      \**************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Processor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../utils/constants.js */
        "./src/utils/constants.js"
      ), D = r(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      ), U = r(
        /*! ../utils/hub.js */
        "./src/utils/hub.js"
      );
      class Y extends D.Callable {
        /**
         * Creates a new Processor with the given components
         * @param {Object} config 
         * @param {Record<string, Object>} components 
         */
        constructor(g, v) {
          super(), this.config = g, this.components = v;
        }
        /**
         * @returns {import('./image_processors_utils.js').ImageProcessor|undefined} The image processor of the processor, if it exists.
         */
        get image_processor() {
          return this.components.image_processor;
        }
        /**
         * @returns {PreTrainedTokenizer|undefined} The tokenizer of the processor, if it exists.
         */
        get tokenizer() {
          return this.components.tokenizer;
        }
        /**
         * @returns {import('./feature_extraction_utils.js').FeatureExtractor|undefined} The feature extractor of the processor, if it exists.
         */
        get feature_extractor() {
          return this.components.feature_extractor;
        }
        /**
         * @param {Parameters<PreTrainedTokenizer['apply_chat_template']>[0]} messages
         * @param {Parameters<PreTrainedTokenizer['apply_chat_template']>[1]} options
         * @returns {ReturnType<PreTrainedTokenizer['apply_chat_template']>}
         */
        apply_chat_template(g, v = {}) {
          if (!this.tokenizer)
            throw new Error("Unable to apply chat template without a tokenizer.");
          return this.tokenizer.apply_chat_template(g, {
            tokenize: !1,
            // default to false
            ...v
          });
        }
        /**
         * @param {Parameters<PreTrainedTokenizer['batch_decode']>} args
         * @returns {ReturnType<PreTrainedTokenizer['batch_decode']>}
         */
        batch_decode(...g) {
          if (!this.tokenizer)
            throw new Error("Unable to decode without a tokenizer.");
          return this.tokenizer.batch_decode(...g);
        }
        /**
         * @param {Parameters<PreTrainedTokenizer['decode']>} args
         * @returns {ReturnType<PreTrainedTokenizer['decode']>}
         */
        decode(...g) {
          if (!this.tokenizer)
            throw new Error("Unable to decode without a tokenizer.");
          return this.tokenizer.decode(...g);
        }
        /**
         * Calls the feature_extractor function with the given input.
         * @param {any} input The input to extract features from.
         * @param {...any} args Additional arguments.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(g, ...v) {
          for (const M of [this.image_processor, this.feature_extractor, this.tokenizer])
            if (M)
              return M(g, ...v);
          throw new Error("No image processor, feature extractor, or tokenizer found.");
        }
        /**
         * Instantiate one of the processor classes of the library from a pretrained model.
         * 
         * The processor class to instantiate is selected based on the `image_processor_type` (or `feature_extractor_type`; legacy)
         * property of the config object (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.
         * @param {PretrainedProcessorOptions} options Additional options for loading the processor.
         * 
         * @returns {Promise<Processor>} A new instance of the Processor class.
         */
        static async from_pretrained(g, v) {
          const [M, y] = await Promise.all([
            // TODO:
            this.uses_processor_config ? (0, U.getModelJSON)(g, _.PROCESSOR_NAME, !0, v) : {},
            Promise.all(
              this.classes.filter((b) => b in this).map(async (b) => {
                const I = await this[b].from_pretrained(g, v);
                return [b.replace(/_class$/, ""), I];
              })
            ).then(Object.fromEntries)
          ]);
          return new this(M, y);
        }
      }
      fe(Y, "classes", [
        "image_processor_class",
        "tokenizer_class",
        "feature_extractor_class"
      ]), fe(Y, "uses_processor_config", !1);
    }
  ),
  /***/
  "./src/configs.js": (
    /*!************************!*\
      !*** ./src/configs.js ***!
      \************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        AutoConfig: () => (
          /* binding */
          v
        ),
        /* harmony export */
        PretrainedConfig: () => (
          /* binding */
          g
        ),
        /* harmony export */
        getKeyValueShapes: () => (
          /* binding */
          R
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./utils/core.js */
        "./src/utils/core.js"
      ), D = r(
        /*! ./utils/hub.js */
        "./src/utils/hub.js"
      );
      async function U(M, y) {
        return await (0, D.getModelJSON)(M, "config.json", !0, y);
      }
      function Y(M) {
        const y = {};
        let b = {};
        switch (M.model_type) {
          // Sub-configs
          case "llava":
          case "paligemma":
          case "florence2":
          case "llava_onevision":
          case "idefics3":
            b = Y(M.text_config);
            break;
          case "moondream1":
            b = Y(M.phi_config);
            break;
          case "musicgen":
            b = Y(M.decoder);
            break;
          case "multi_modality":
            b = Y(M.language_config);
            break;
          // Decoder-only models
          case "gpt2":
          case "gptj":
          case "jais":
          case "codegen":
          case "gpt_bigcode":
            y.num_heads = "n_head", y.num_layers = "n_layer", y.hidden_size = "n_embd";
            break;
          case "gpt_neox":
          case "stablelm":
          case "opt":
          case "falcon":
            y.num_heads = "num_attention_heads", y.num_layers = "num_hidden_layers", y.hidden_size = "hidden_size";
            break;
          case "llama":
          case "olmo":
          case "olmo2":
          case "mobilellm":
          case "granite":
          case "cohere":
          case "mistral":
          case "starcoder2":
          case "qwen2":
          case "qwen2_vl":
          case "phi":
          case "phi3":
          case "phi3_v":
            y.num_heads = "num_key_value_heads", y.num_layers = "num_hidden_layers", y.hidden_size = "hidden_size", y.num_attention_heads = "num_attention_heads";
            break;
          case "gemma":
          case "gemma2":
          case "glm":
          case "helium":
            y.num_heads = "num_key_value_heads", y.num_layers = "num_hidden_layers", y.dim_kv = "head_dim";
            break;
          case "openelm":
            y.num_heads = "num_kv_heads", y.num_layers = "num_transformer_layers", y.dim_kv = "head_dim";
            break;
          case "gpt_neo":
          case "donut-swin":
            y.num_heads = "num_heads", y.num_layers = "num_layers", y.hidden_size = "hidden_size";
            break;
          case "bloom":
            y.num_heads = "n_head", y.num_layers = "n_layer", y.hidden_size = "hidden_size";
            break;
          case "mpt":
            y.num_heads = "n_heads", y.num_layers = "n_layers", y.hidden_size = "d_model";
            break;
          case "exaone":
            y.num_heads = "num_key_value_heads", y.num_layers = "num_layers", y.dim_kv = "head_dim", y.num_attention_heads = "num_attention_heads";
            break;
          // Encoder-decoder models
          case "t5":
          case "mt5":
          case "longt5":
            y.num_decoder_layers = "num_decoder_layers", y.num_decoder_heads = "num_heads", y.decoder_dim_kv = "d_kv", y.num_encoder_layers = "num_layers", y.num_encoder_heads = "num_heads", y.encoder_dim_kv = "d_kv";
            break;
          case "bart":
          case "mbart":
          case "marian":
          case "whisper":
          case "m2m_100":
          case "blenderbot":
          case "blenderbot-small":
          case "florence2_language":
            y.num_decoder_layers = "decoder_layers", y.num_decoder_heads = "decoder_attention_heads", y.decoder_hidden_size = "d_model", y.num_encoder_layers = "encoder_layers", y.num_encoder_heads = "encoder_attention_heads", y.encoder_hidden_size = "d_model";
            break;
          case "speecht5":
            y.num_decoder_layers = "decoder_layers", y.num_decoder_heads = "decoder_attention_heads", y.decoder_hidden_size = "hidden_size", y.num_encoder_layers = "encoder_layers", y.num_encoder_heads = "encoder_attention_heads", y.encoder_hidden_size = "hidden_size";
            break;
          case "trocr":
            y.num_encoder_layers = y.num_decoder_layers = "decoder_layers", y.num_encoder_heads = y.num_decoder_heads = "decoder_attention_heads", y.encoder_hidden_size = y.decoder_hidden_size = "d_model";
            break;
          case "musicgen_decoder":
            y.num_encoder_layers = y.num_decoder_layers = "num_hidden_layers", y.num_encoder_heads = y.num_decoder_heads = "num_attention_heads", y.encoder_hidden_size = y.decoder_hidden_size = "hidden_size";
            break;
          case "moonshine":
            y.num_decoder_layers = "decoder_num_hidden_layers", y.num_decoder_heads = "decoder_num_key_value_heads", y.num_encoder_layers = "encoder_num_hidden_layers", y.num_encoder_heads = "encoder_num_key_value_heads", y.encoder_hidden_size = y.decoder_hidden_size = "hidden_size";
            break;
          case "vision-encoder-decoder":
            const K = Y(M.decoder), se = "num_decoder_layers" in K, ie = (0, _.pick)(M, ["model_type", "is_encoder_decoder"]);
            return se ? (ie.num_decoder_layers = K.num_decoder_layers, ie.num_decoder_heads = K.num_decoder_heads, ie.decoder_hidden_size = K.decoder_hidden_size, ie.num_encoder_layers = K.num_encoder_layers, ie.num_encoder_heads = K.num_encoder_heads, ie.encoder_hidden_size = K.encoder_hidden_size) : (ie.num_layers = K.num_layers, ie.num_heads = K.num_heads, ie.hidden_size = K.hidden_size), ie;
        }
        const I = {
          ...b,
          ...(0, _.pick)(M, ["model_type", "multi_query", "is_encoder_decoder"])
        };
        for (const K in y)
          I[K] = M[y[K]];
        return I;
      }
      function R(M, {
        prefix: y = "past_key_values",
        batch_size: b = 1
      } = {}) {
        const I = {}, K = M.normalized_config;
        if (K.is_encoder_decoder && "num_encoder_heads" in K && "num_decoder_heads" in K) {
          const se = K.encoder_dim_kv ?? K.encoder_hidden_size / K.num_encoder_heads, ie = K.decoder_dim_kv ?? K.decoder_hidden_size / K.num_decoder_heads, W = [b, K.num_encoder_heads, 0, se], j = [b, K.num_decoder_heads, 0, ie];
          for (let q = 0; q < K.num_decoder_layers; ++q)
            I[`${y}.${q}.encoder.key`] = W, I[`${y}.${q}.encoder.value`] = W, I[`${y}.${q}.decoder.key`] = j, I[`${y}.${q}.decoder.value`] = j;
        } else {
          const se = K.num_heads, ie = K.num_layers, W = K.dim_kv ?? K.hidden_size / (K.num_attention_heads ?? se);
          if (K.model_type === "falcon") {
            const j = [b * se, 0, W];
            for (let q = 0; q < ie; ++q)
              I[`${y}.${q}.key`] = j, I[`${y}.${q}.value`] = j;
          } else if (K.multi_query) {
            const j = [b * se, 0, 2 * W];
            for (let q = 0; q < ie; ++q)
              I[`${y}.${q}.key_value`] = j;
          } else if (K.model_type === "bloom") {
            const j = [b * se, W, 0], q = [b * se, 0, W];
            for (let A = 0; A < ie; ++A)
              I[`${y}.${A}.key`] = j, I[`${y}.${A}.value`] = q;
          } else if (K.model_type === "openelm")
            for (let j = 0; j < ie; ++j) {
              const q = [b, se[j], 0, W];
              I[`${y}.${j}.key`] = q, I[`${y}.${j}.value`] = q;
            }
          else {
            const j = [b, se, 0, W];
            for (let q = 0; q < ie; ++q)
              I[`${y}.${q}.key`] = j, I[`${y}.${q}.value`] = j;
          }
        }
        return I;
      }
      class g {
        /**
         * Create a new PreTrainedTokenizer instance.
         * @param {Object} configJSON The JSON of the config.
         */
        constructor(y) {
          // NOTE: Typo in original
          /** @type {string|null} */
          fe(this, "model_type", null);
          /** @type {boolean} */
          fe(this, "is_encoder_decoder", !1);
          /** @type {number} */
          fe(this, "max_position_embeddings");
          /** @type {TransformersJSConfig} */
          fe(this, "transformers.js_config");
          Object.assign(this, y), this.normalized_config = Y(this);
        }
        /**
         * Loads a pre-trained config from the given `pretrained_model_name_or_path`. 
         * 
         * @param {string} pretrained_model_name_or_path The path to the pre-trained config.
         * @param {PretrainedOptions} options Additional options for loading the config.
         * @throws {Error} Throws an error if the config.json is not found in the `pretrained_model_name_or_path`.
         * 
         * @returns {Promise<PretrainedConfig>} A new instance of the `PretrainedConfig` class.
         */
        static async from_pretrained(y, {
          progress_callback: b = null,
          config: I = null,
          cache_dir: K = null,
          local_files_only: se = !1,
          revision: ie = "main"
        } = {}) {
          I && !(I instanceof g) && (I = new g(I));
          const W = I ?? await U(y, {
            progress_callback: b,
            config: I,
            cache_dir: K,
            local_files_only: se,
            revision: ie
          });
          return new this(W);
        }
      }
      class v {
        /** @type {typeof PretrainedConfig.from_pretrained} */
        static async from_pretrained(...y) {
          return g.from_pretrained(...y);
        }
      }
    }
  ),
  /***/
  "./src/env.js": (
    /*!********************!*\
      !*** ./src/env.js ***!
      \********************/
    /***/
    ($e, $, r) => {
      var F, le;
      r.r($), r.d($, {
        /* harmony export */
        apis: () => (
          /* binding */
          ie
        ),
        /* harmony export */
        env: () => (
          /* binding */
          w
        )
        /* harmony export */
      });
      var _ = r(
        /*! fs */
        "?569f"
      ), D = r(
        /*! path */
        "?3f59"
      ), U = r(
        /*! url */
        "?154a"
      );
      const Y = "3.3.2", R = typeof window < "u" && typeof window.document < "u", g = typeof self < "u" && ((F = self.constructor) == null ? void 0 : F.name) === "DedicatedWorkerGlobalScope", v = typeof self < "u" && "caches" in self, M = typeof navigator < "u" && "gpu" in navigator, y = typeof navigator < "u" && "ml" in navigator, b = typeof process < "u", I = b && ((le = process == null ? void 0 : process.release) == null ? void 0 : le.name) === "node", K = !x(_), se = !x(D), ie = Object.freeze({
        /** Whether we are running in a browser environment (and not a web worker) */
        IS_BROWSER_ENV: R,
        /** Whether we are running in a web worker environment */
        IS_WEBWORKER_ENV: g,
        /** Whether the Cache API is available */
        IS_WEB_CACHE_AVAILABLE: v,
        /** Whether the WebGPU API is available */
        IS_WEBGPU_AVAILABLE: M,
        /** Whether the WebNN API is available */
        IS_WEBNN_AVAILABLE: y,
        /** Whether the Node.js process API is available */
        IS_PROCESS_AVAILABLE: b,
        /** Whether we are running in a Node.js environment */
        IS_NODE_ENV: I,
        /** Whether the filesystem API is available */
        IS_FS_AVAILABLE: K,
        /** Whether the path API is available */
        IS_PATH_AVAILABLE: se
      }), W = K && se;
      let j = "./";
      if (W) {
        const ne = Object(import.meta).url;
        ne ? j = D.dirname(D.dirname(U.fileURLToPath(ne))) : typeof __dirname < "u" && (j = D.dirname(__dirname));
      }
      const q = W ? D.join(j, "/.cache/") : null, A = "/models/", S = W ? D.join(j, A) : A, w = {
        version: Y,
        /////////////////// Backends settings ///////////////////
        // NOTE: These will be populated later by the backends themselves.
        backends: {
          // onnxruntime-web/onnxruntime-node
          onnx: {}
        },
        /////////////////// Model settings ///////////////////
        allowRemoteModels: !0,
        remoteHost: "https://huggingface.co/",
        remotePathTemplate: "{model}/resolve/{revision}/",
        allowLocalModels: !(R || g),
        localModelPath: S,
        useFS: K,
        /////////////////// Cache settings ///////////////////
        useBrowserCache: v,
        useFSCache: K,
        cacheDir: q,
        useCustomCache: !1,
        customCache: null
        //////////////////////////////////////////////////////
      };
      function x(ne) {
        return Object.keys(ne).length === 0;
      }
    }
  ),
  /***/
  "./src/generation/configuration_utils.js": (
    /*!***********************************************!*\
      !*** ./src/generation/configuration_utils.js ***!
      \***********************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        GenerationConfig: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../utils/core.js */
        "./src/utils/core.js"
      );
      class D {
        /**
         * 
         * @param {GenerationConfig|import('../configs.js').PretrainedConfig} config 
         */
        constructor(Y) {
          // Parameters that control the length of the output
          /**
           * The maximum length the generated tokens can have.
           * Corresponds to the length of the input prompt + `max_new_tokens`.
           * Its effect is overridden by `max_new_tokens`, if also set.
           * @type {number}
           * @default 20
           */
          fe(this, "max_length", 20);
          /**
           * The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
           * @type {number}
           * @default null
           */
          fe(this, "max_new_tokens", null);
          /**
           * The minimum length of the sequence to be generated.
           * Corresponds to the length of the input prompt + `min_new_tokens`.
           * Its effect is overridden by `min_new_tokens`, if also set.
           * @type {number}
           * @default 0
           */
          fe(this, "min_length", 0);
          /**
           * The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt.
           * @type {number}
           * @default null
           */
          fe(this, "min_new_tokens", null);
          /**
           * Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:
           * - `true`, where the generation stops as soon as there are `num_beams` complete candidates;
           * - `false`, where an heuristic is applied and the generation stops when is it very unlikely to find better candidates;
           * - `"never"`, where the beam search procedure only stops when there cannot be better candidates (canonical beam search algorithm).
           * @type {boolean|"never"}
           * @default false
           */
          fe(this, "early_stopping", !1);
          /**
           * The maximum amount of time you allow the computation to run for in seconds.
           * Generation will still finish the current pass after allocated time has been passed.
           * @type {number}
           * @default null
           */
          fe(this, "max_time", null);
          // Parameters that control the generation strategy used
          /**
           * Whether or not to use sampling; use greedy decoding otherwise.
           * @type {boolean}
           * @default false
           */
          fe(this, "do_sample", !1);
          /**
           * Number of beams for beam search. 1 means no beam search.
           * @type {number}
           * @default 1
           */
          fe(this, "num_beams", 1);
          /**
           * Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.
           * See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.
           * @type {number}
           * @default 1
           */
          fe(this, "num_beam_groups", 1);
          /**
           * The values balance the model confidence and the degeneration penalty in contrastive search decoding.
           * @type {number}
           * @default null
           */
          fe(this, "penalty_alpha", null);
          /**
           * Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.
           * @type {boolean}
           * @default true
           */
          fe(this, "use_cache", !0);
          // Parameters for manipulation of the model output logits
          /**
           * The value used to modulate the next token probabilities.
           * @type {number}
           * @default 1.0
           */
          fe(this, "temperature", 1);
          /**
           * The number of highest probability vocabulary tokens to keep for top-k-filtering.
           * @type {number}
           * @default 50
           */
          fe(this, "top_k", 50);
          /**
           * If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation.
           * @type {number}
           * @default 1.0
           */
          fe(this, "top_p", 1);
          /**
           * Local typicality measures how similar the conditional probability of predicting a target token next is to the expected conditional probability of predicting a random token next, given the partial text already generated.
           * If set to float < 1, the smallest set of the most locally typical tokens with probabilities that add up to `typical_p` or higher are kept for generation.
           * See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.
           * @type {number}
           * @default 1.0
           */
          fe(this, "typical_p", 1);
          /**
           * If set to float strictly between 0 and 1, only tokens with a conditional probability greater than `epsilon_cutoff` will be sampled.
           * In the paper, suggested values range from 3e-4 to 9e-4, depending on the size of the model.
           * See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191) for more details.
           * @type {number}
           * @default 0.0
           */
          fe(this, "epsilon_cutoff", 0);
          /**
           * Eta sampling is a hybrid of locally typical sampling and epsilon sampling.
           * If set to float strictly between 0 and 1, a token is only considered if it is greater than either `eta_cutoff` or `sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits)))`.
           * The latter term is intuitively the expected next token probability, scaled by `sqrt(eta_cutoff)`. In the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.
           * See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191) for more details.
           * @type {number}
           * @default 0.0
           */
          fe(this, "eta_cutoff", 0);
          /**
           * This value is subtracted from a beam's score if it generates a token same as any beam from other group at a particular time.
           * Note that `diversity_penalty` is only effective if `group beam search` is enabled.
           * @type {number}
           * @default 0.0
           */
          fe(this, "diversity_penalty", 0);
          /**
           * The parameter for repetition penalty. 1.0 means no penalty.
           * See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
           * @type {number}
           * @default 1.0
           */
          fe(this, "repetition_penalty", 1);
          /**
           * The paramater for encoder_repetition_penalty.
           * An exponential penalty on sequences that are not in the original input.
           * 1.0 means no penalty.
           * @type {number}
           * @default 1.0
           */
          fe(this, "encoder_repetition_penalty", 1);
          /**
           * Exponential penalty to the length that is used with beam-based generation.
           * It is applied as an exponent to the sequence length, which in turn is used to divide the score of the sequence.
           * Since the score is the log likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while `length_penalty` < 0.0 encourages shorter sequences.
           * @type {number}
           * @default 1.0
           */
          fe(this, "length_penalty", 1);
          /**
           * If set to int > 0, all ngrams of that size can only occur once.
           * @type {number}
           * @default 0
           */
          fe(this, "no_repeat_ngram_size", 0);
          /**
           * List of token ids that are not allowed to be generated.
           * In order to get the token ids of the words that should not appear in the generated text, use
           * `tokenizer(bad_words, { add_prefix_space: true, add_special_tokens: false }).input_ids`.
           * @type {number[][]}
           * @default null
           */
          fe(this, "bad_words_ids", null);
          /**
           * List of token ids that must be generated.
           * If given a `number[][]`, this is treated as a simple list of words that must be included, the opposite to `bad_words_ids`.
           * If given `number[][][]`, this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081), where one can allow different forms of each word.
           * @type {number[][]|number[][][]}
           * @default null
           */
          fe(this, "force_words_ids", null);
          /**
           * Whether to renormalize the logits after applying all the logits processors or warpers (including the custom ones).
           * It's highly recommended to set this flag to `true` as the search algorithms suppose the score logits are normalized but some logit processors or warpers break the normalization.
           * @type {boolean}
           * @default false
           */
          fe(this, "renormalize_logits", !1);
          /**
           * Custom constraints that can be added to the generation to ensure that the output will contain the use of certain tokens as defined by `Constraint` objects, in the most sensible way possible.
           * @type {Object[]}
           * @default null
           */
          fe(this, "constraints", null);
          /**
           * The id of the token to force as the first generated token after the `decoder_start_token_id`.
           * Useful for multilingual models like mBART where the first generated token needs to be the target language token.
           * @type {number}
           * @default null
           */
          fe(this, "forced_bos_token_id", null);
          /**
           * The id of the token to force as the last generated token when `max_length` is reached.
           * Optionally, use a list to set multiple *end-of-sequence* tokens.
           * @type {number|number[]}
           * @default null
           */
          fe(this, "forced_eos_token_id", null);
          /**
           * Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to crash. Note that using `remove_invalid_values` can slow down generation.
           * @type {boolean}
           */
          fe(this, "remove_invalid_values", !1);
          /**
           * This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been generated.
           * The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where penalty starts and `decay_factor` represents the factor of exponential decay.
           * @type {[number, number]}
           * @default null
           */
          fe(this, "exponential_decay_length_penalty", null);
          /**
           * A list of tokens that will be suppressed at generation.
           * The `SuppressTokens` logit processor will set their log probs to `-inf` so that they are not sampled.
           * @type {number[]}
           * @default null
           */
          fe(this, "suppress_tokens", null);
          /**
           * A streamer that will be used to stream the generation.
           * @type {import('./streamers.js').TextStreamer}
           * @default null
           */
          fe(this, "streamer", null);
          /**
           * A list of tokens that will be suppressed at the beginning of the generation.
           * The `SuppressBeginTokens` logit processor will set their log probs to `-inf` so that they are not sampled.
           * @type {number[]}
           * @default null
           */
          fe(this, "begin_suppress_tokens", null);
          /**
           * A list of pairs of integers which indicates a mapping from generation indices to token indices that will be forced before sampling.
           * For example, `[[1, 123]]` means the second generated token will always be a token of index 123.
           * @type {[number, number][]}
           * @default null
           */
          fe(this, "forced_decoder_ids", null);
          /**
           * The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.
           * Higher guidance scale encourages the model to generate samples that are more closely linked to the input
           * prompt, usually at the expense of poorer quality.
           * @type {number}
           * @default null
           */
          fe(this, "guidance_scale", null);
          // Parameters that define the output variables of `generate`
          /**
           * The number of independently computed returned sequences for each element in the batch.
           * @type {number}
           * @default 1
           */
          fe(this, "num_return_sequences", 1);
          /**
           * Whether or not to return the attentions tensors of all attention layers.
           * See `attentions` under returned tensors for more details.
           * @type {boolean}
           * @default false
           */
          fe(this, "output_attentions", !1);
          /**
           * Whether or not to return the hidden states of all layers.
           * See `hidden_states` under returned tensors for more details.
           * @type {boolean}
           * @default false
           */
          fe(this, "output_hidden_states", !1);
          /**
           * Whether or not to return the prediction scores.
           * See `scores` under returned tensors for more details.
           * @type {boolean}
           * @default false
           */
          fe(this, "output_scores", !1);
          /**
           * Whether or not to return a `ModelOutput` instead of a plain tuple.
           * @type {boolean}
           * @default false
           */
          fe(this, "return_dict_in_generate", !1);
          // Special tokens that can be used at generation time
          /**
           * The id of the *padding* token.
           * @type {number}
           * @default null
           */
          fe(this, "pad_token_id", null);
          /**
           * The id of the *beginning-of-sequence* token.
           * @type {number}
           * @default null
           */
          fe(this, "bos_token_id", null);
          /**
           * The id of the *end-of-sequence* token.
           * Optionally, use a list to set multiple *end-of-sequence* tokens.
           * @type {number|number[]}
           * @default null
           */
          fe(this, "eos_token_id", null);
          // Generation parameters exclusive to encoder-decoder models
          /**
           * If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the `decoder_input_ids`.
           * @type {number}
           * @default 0
           */
          fe(this, "encoder_no_repeat_ngram_size", 0);
          /**
           * If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.
           * @type {number}
           * @default null
           */
          fe(this, "decoder_start_token_id", null);
          // Wild card
          /**
           * Additional generation kwargs will be forwarded to the `generate` function of the model.
           * Kwargs that are not present in `generate`'s signature will be used in the model forward pass.
           * @type {Object}
           * @default {}
           */
          fe(this, "generation_kwargs", {});
          Object.assign(this, (0, _.pick)(Y, Object.getOwnPropertyNames(this)));
        }
      }
    }
  ),
  /***/
  "./src/generation/logits_process.js": (
    /*!******************************************!*\
      !*** ./src/generation/logits_process.js ***!
      \******************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ClassifierFreeGuidanceLogitsProcessor: () => (
          /* binding */
          W
        ),
        /* harmony export */
        ForcedBOSTokenLogitsProcessor: () => (
          /* binding */
          g
        ),
        /* harmony export */
        ForcedEOSTokenLogitsProcessor: () => (
          /* binding */
          v
        ),
        /* harmony export */
        LogitsProcessor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        LogitsProcessorList: () => (
          /* binding */
          R
        ),
        /* harmony export */
        LogitsWarper: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        MinLengthLogitsProcessor: () => (
          /* binding */
          K
        ),
        /* harmony export */
        MinNewTokensLengthLogitsProcessor: () => (
          /* binding */
          se
        ),
        /* harmony export */
        NoBadWordsLogitsProcessor: () => (
          /* binding */
          ie
        ),
        /* harmony export */
        NoRepeatNGramLogitsProcessor: () => (
          /* binding */
          b
        ),
        /* harmony export */
        RepetitionPenaltyLogitsProcessor: () => (
          /* binding */
          I
        ),
        /* harmony export */
        SuppressTokensAtBeginLogitsProcessor: () => (
          /* binding */
          M
        ),
        /* harmony export */
        TemperatureLogitsWarper: () => (
          /* binding */
          j
        ),
        /* harmony export */
        TopKLogitsWarper: () => (
          /* binding */
          A
        ),
        /* harmony export */
        TopPLogitsWarper: () => (
          /* binding */
          q
        ),
        /* harmony export */
        WhisperTimeStampLogitsProcessor: () => (
          /* binding */
          y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      );
      r(
        /*! ../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var D = r(
        /*! ../utils/maths.js */
        "./src/utils/maths.js"
      );
      class U extends _.Callable {
        /**
         * Apply the processor to the input logits.
         *
         * @abstract
         * @param {bigint[][]} input_ids The input ids.
         * @param {Tensor} logits The logits to process.
         * @throws {Error} Throws an error if `_call` is not implemented in the subclass.
         */
        _call(w, x) {
          throw Error("`_call` should be implemented in a subclass");
        }
      }
      class Y extends _.Callable {
        /**
         * Apply the processor to the input logits.
         *
         * @abstract
         * @param {bigint[][]} input_ids The input ids.
         * @param {Tensor} logits The logits to process.
         * @throws {Error} Throws an error if `_call` is not implemented in the subclass.
         */
        _call(w, x) {
          throw Error("`_call` should be implemented in a subclass");
        }
      }
      class R extends _.Callable {
        /**
         * Constructs a new instance of `LogitsProcessorList`.
         */
        constructor() {
          super(), this.processors = [];
        }
        /**
         * Adds a new logits processor to the list.
         *
         * @param {LogitsProcessor} item The logits processor function to add.
         */
        push(w) {
          this.processors.push(w);
        }
        /**
         * Adds multiple logits processors to the list.
         *
         * @param {LogitsProcessor[]} items The logits processor functions to add.
         */
        extend(w) {
          this.processors.push(...w);
        }
        /**
         * Applies all logits processors in the list to a batch of logits, modifying them in-place.
         *
         * @param {bigint[][]} input_ids The input IDs for the language model.
         * @param {Tensor} logits
         */
        _call(w, x) {
          let F = x;
          for (const le of this.processors)
            F = le(w, F);
          return F;
        }
        [Symbol.iterator]() {
          return this.processors.values();
        }
      }
      class g extends U {
        /**
         * Create a ForcedBOSTokenLogitsProcessor.
         * @param {number} bos_token_id The ID of the beginning-of-sequence token to be forced.
         */
        constructor(w) {
          super(), this.bos_token_id = w;
        }
        /**
         * Apply the BOS token forcing to the logits.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The logits with BOS token forcing.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F)
            if (w[F].length === 1) {
              const le = (
                /** @type {Float32Array} */
                x[F].data
              );
              le.fill(-1 / 0), le[this.bos_token_id] = 0;
            }
          return x;
        }
      }
      class v extends U {
        /**
         * Create a ForcedEOSTokenLogitsProcessor.
         * @param {number} max_length The maximum length of the sequence to be generated.
         * @param {number|number[]} eos_token_id The id(s) of the *end-of-sequence* token.
         */
        constructor(w, x) {
          super(), this.max_length = w, this.eos_token_id = Array.isArray(x) ? x : [x];
        }
        /**
         * Apply the processor to input_ids and logits.
         * 
         * @param {bigint[][]} input_ids The input ids.
         * @param {Tensor} logits The logits tensor.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F)
            if (w[F].length === this.max_length - 1) {
              const le = (
                /** @type {Float32Array} */
                x[F].data
              );
              le.fill(-1 / 0);
              for (const ne of this.eos_token_id)
                le[ne] = 0;
            }
          return x;
        }
      }
      class M extends U {
        /**
         * Create a SuppressTokensAtBeginLogitsProcessor.
         * @param {number[]} begin_suppress_tokens The IDs of the tokens to suppress.
         * @param {number} begin_index The number of tokens to generate before suppressing tokens.
         */
        constructor(w, x) {
          super(), this.begin_suppress_tokens = w, this.begin_index = x;
        }
        /**
         * Apply the BOS token forcing to the logits.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The logits with BOS token forcing.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F)
            if (w[F].length === this.begin_index) {
              const le = (
                /** @type {Float32Array} */
                x[F].data
              );
              for (const ne of this.begin_suppress_tokens)
                le[ne] = -1 / 0;
            }
          return x;
        }
      }
      class y extends U {
        /**
         * Constructs a new WhisperTimeStampLogitsProcessor.
         * @param {import('../models/whisper/generation_whisper.js').WhisperGenerationConfig} generate_config The config object passed to the `generate()` method of a transformer model.
         * @param {number[]} init_tokens The initial tokens of the input sequence.
         */
        constructor(w, x) {
          super(), this.eos_token_id = Array.isArray(w.eos_token_id) ? w.eos_token_id[0] : w.eos_token_id, this.no_timestamps_token_id = w.no_timestamps_token_id, this.timestamp_begin = this.no_timestamps_token_id + 1, this.begin_index = x.length, x.at(-1) === this.no_timestamps_token_id && (this.begin_index -= 1), this.max_initial_timestamp_index = w.max_initial_timestamp_index;
        }
        /**
         * Modify the logits to handle timestamp tokens.
         * @param {bigint[][]} input_ids The input sequence of tokens.
         * @param {Tensor} logits The logits output by the model.
         * @returns {Tensor} The modified logits.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F) {
            const le = (
              /** @type {Float32Array} */
              x[F].data
            );
            if (le[this.no_timestamps_token_id] = -1 / 0, w[F].length === this.begin_index - 1) {
              le.fill(-1 / 0), le[this.timestamp_begin] = 0;
              continue;
            }
            const ne = w[F].slice(this.begin_index), be = ne.length >= 1 && ne[ne.length - 1] >= this.timestamp_begin, _e = ne.length < 2 || ne[ne.length - 2] >= this.timestamp_begin;
            if (be && (_e ? le.subarray(this.timestamp_begin).fill(-1 / 0) : le.subarray(0, this.eos_token_id).fill(-1 / 0)), w[F].length === this.begin_index && this.max_initial_timestamp_index !== null) {
              const ke = this.timestamp_begin + this.max_initial_timestamp_index;
              le.subarray(ke + 1).fill(-1 / 0);
            }
            const re = (0, D.log_softmax)(le), xe = Math.log(re.subarray(this.timestamp_begin).map(Math.exp).reduce((ke, Fe) => ke + Fe)), ce = (0, D.max)(re.subarray(0, this.timestamp_begin))[0];
            xe > ce && le.subarray(0, this.timestamp_begin).fill(-1 / 0);
          }
          return x;
        }
      }
      class b extends U {
        /**
         * Create a NoRepeatNGramLogitsProcessor.
         * @param {number} no_repeat_ngram_size The no-repeat-ngram size. All ngrams of this size can only occur once.
         */
        constructor(w) {
          super(), this.no_repeat_ngram_size = w;
        }
        /**
         * Generate n-grams from a sequence of token ids.
         * @param {bigint[]} prevInputIds List of previous input ids
         * @returns {Map<string, number[]>} Map of generated n-grams
         */
        getNgrams(w) {
          const x = w.length, F = [];
          for (let ne = 0; ne < x + 1 - this.no_repeat_ngram_size; ++ne) {
            const be = [];
            for (let _e = 0; _e < this.no_repeat_ngram_size; ++_e)
              be.push(w[ne + _e]);
            F.push(be.map(Number));
          }
          const le = /* @__PURE__ */ new Map();
          for (const ne of F) {
            const be = ne.slice(0, ne.length - 1), _e = JSON.stringify(be), re = le.get(_e) ?? [];
            re.push(ne[ne.length - 1]), le.set(_e, re);
          }
          return le;
        }
        /**
         * Generate n-grams from a sequence of token ids.
         * @param {Map<string, number[]>} bannedNgrams Map of banned n-grams
         * @param {bigint[]} prevInputIds List of previous input ids
         * @returns {number[]} Map of generated n-grams
         */
        getGeneratedNgrams(w, x) {
          const F = x.slice(x.length + 1 - this.no_repeat_ngram_size, x.length);
          return w.get(JSON.stringify(F.map(Number))) ?? [];
        }
        /**
         * Calculate banned n-gram tokens
         * @param {bigint[]} prevInputIds List of previous input ids
         * @returns {number[]} Map of generated n-grams
         */
        calcBannedNgramTokens(w) {
          const x = [];
          if (w.length + 1 < this.no_repeat_ngram_size)
            return x;
          {
            const F = this.getNgrams(w);
            return this.getGeneratedNgrams(F, w);
          }
        }
        /**
         * Apply the no-repeat-ngram processor to the logits.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The logits with no-repeat-ngram processing.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F) {
            const le = (
              /** @type {Float32Array} */
              x[F].data
            ), ne = this.calcBannedNgramTokens(w[F]);
            for (const be of ne)
              le[be] = -1 / 0;
          }
          return x;
        }
      }
      class I extends U {
        /**
         * Create a RepetitionPenaltyLogitsProcessor.
         * @param {number} penalty The parameter for repetition penalty.
         * - 1.0 means no penalty. Above 1.0 penalizes previously generated tokens.
         * - Between 0.0 and 1.0 rewards previously generated tokens.
         */
        constructor(w) {
          super(), this.penalty = w;
        }
        /**
         * Apply the repetition penalty to the logits.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The logits with repetition penalty processing.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F) {
            const le = (
              /** @type {Float32Array} */
              x[F].data
            );
            for (const ne of new Set(w[F])) {
              const be = Number(ne);
              le[be] < 0 ? le[be] *= this.penalty : le[be] /= this.penalty;
            }
          }
          return x;
        }
      }
      class K extends U {
        /**
         * Create a MinLengthLogitsProcessor.
         * @param {number} min_length The minimum length below which the score of `eos_token_id` is set to negative infinity.
         * @param {number|number[]} eos_token_id The ID/IDs of the end-of-sequence token.
         */
        constructor(w, x) {
          super(), this.min_length = w, this.eos_token_id = Array.isArray(x) ? x : [x];
        }
        /**
         * Apply logit processor.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F)
            if (w[F].length < this.min_length) {
              const le = (
                /** @type {Float32Array} */
                x[F].data
              );
              for (const ne of this.eos_token_id)
                le[ne] = -1 / 0;
            }
          return x;
        }
      }
      class se extends U {
        /**
         * Create a MinNewTokensLengthLogitsProcessor.
         * @param {number} prompt_length_to_skip The input tokens length.
         * @param {number} min_new_tokens The minimum *new* tokens length below which the score of `eos_token_id` is set to negative infinity.
         * @param {number|number[]} eos_token_id The ID/IDs of the end-of-sequence token.
         */
        constructor(w, x, F) {
          super(), this.prompt_length_to_skip = w, this.min_new_tokens = x, this.eos_token_id = Array.isArray(F) ? F : [F];
        }
        /**
         * Apply logit processor.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F)
            if (w[F].length - this.prompt_length_to_skip < this.min_new_tokens) {
              const ne = (
                /** @type {Float32Array} */
                x[F].data
              );
              for (const be of this.eos_token_id)
                ne[be] = -1 / 0;
            }
          return x;
        }
      }
      class ie extends U {
        /**
         * Create a `NoBadWordsLogitsProcessor`.
         * @param {number[][]} bad_words_ids List of list of token ids that are not allowed to be generated.
         * @param {number|number[]} eos_token_id The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.
         */
        constructor(w, x) {
          super(), this.bad_words_ids = w, this.eos_token_id = Array.isArray(x) ? x : [x];
        }
        /**
         * Apply logit processor.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(w, x) {
          for (let F = 0; F < w.length; ++F) {
            const le = (
              /** @type {Float32Array} */
              x[F].data
            ), ne = w[F];
            for (const be of this.bad_words_ids) {
              let _e = !0;
              for (let re = 1; re <= be.length - 1 && be.length < ne.length; ++re)
                if (be.at(-re - 1) != ne.at(-re)) {
                  _e = !1;
                  break;
                }
              _e && (le[be.at(-1)] = -1 / 0);
            }
          }
          return x;
        }
      }
      class W extends U {
        /**
         * Create a `ClassifierFreeGuidanceLogitsProcessor`.
         * @param {number} guidance_scale The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.
         * Higher guidance scale encourages the model to generate samples that are more closely linked to the input
         * prompt, usually at the expense of poorer quality.
         */
        constructor(w) {
          if (super(), w <= 1)
            throw new Error(
              `Require guidance scale >1 to use the classifier free guidance processor, got guidance scale ${w}.`
            );
          this.guidance_scale = w;
        }
        /**
         * Apply logit processor.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(w, x) {
          if (x.dims[0] !== 2 * w.length)
            throw new Error(
              `Logits should have twice the batch size of the input ids, the first half of batches corresponding to the conditional inputs, and the second half of batches corresponding to the unconditional inputs. Got batch size ${x.dims[0]} for the logits and ${w.length} for the input ids.`
            );
          const F = w.length, le = x.slice([0, F], null), ne = x.slice([F, x.dims[0]], null);
          for (let be = 0; be < ne.data.length; ++be)
            ne.data[be] += (le.data[be] - ne.data[be]) * this.guidance_scale;
          return ne;
        }
      }
      class j extends Y {
        /**
         * Create a `TemperatureLogitsWarper`.
         * @param {number} temperature Strictly positive float value used to modulate the logits distribution.
         * A value smaller than `1` decreases randomness (and vice versa), with `0` being equivalent to shifting
         * all probability mass to the most likely token.
         */
        constructor(w) {
          super(), this.temperature = w;
        }
        /**
         * Apply logit warper.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(w, x) {
          const F = (
            /** @type {Float32Array} */
            x.data
          );
          for (let le = 0; le < F.length; ++le)
            F[le] /= this.temperature;
          return x;
        }
      }
      class q extends Y {
        /**
         * Create a `TopPLogitsWarper`.
         * @param {number} top_p If set to < 1, only the smallest set of most probable tokens with
         * probabilities that add up to `top_p` or higher are kept for generation.
         * @param {Object} options Additional options for the top-p sampling.
         * @param {number} [options.filter_value=-Infinity] All filtered values will be set to this float value.
         * @param {number} [options.min_tokens_to_keep=1] Minimum number of tokens that cannot be filtered.
         */
        constructor(w, {
          filter_value: x = -1 / 0,
          min_tokens_to_keep: F = 1
        } = {}) {
          if (super(), w < 0 || w > 1)
            throw new Error(`\`top_p\` must be a float > 0 and < 1, but is ${w}`);
          if (!Number.isInteger(F) || F < 1)
            throw new Error(`\`min_tokens_to_keep\` must be a positive integer, but is ${F}`);
          this.top_p = w, this.filter_value = x, this.min_tokens_to_keep = F;
        }
      }
      class A extends Y {
        /**
         * Create a `TopKLogitsWarper`.
         * @param {number} top_k If set to > 0, only the top `top_k` tokens are kept for generation.
         * @param {Object} options Additional options for the top-k sampling.
         * @param {number} [options.filter_value=-Infinity] All filtered values will be set to this float value.
         * @param {number} [options.min_tokens_to_keep=1] Minimum number of tokens that cannot be filtered.
         */
        constructor(w, {
          filter_value: x = -1 / 0,
          min_tokens_to_keep: F = 1
        } = {}) {
          if (super(), !Number.isInteger(w) || w < 0)
            throw new Error(`\`top_k\` must be a positive integer, but is ${w}`);
          this.top_k = Math.max(w, F), this.filter_value = x;
        }
      }
    }
  ),
  /***/
  "./src/generation/logits_sampler.js": (
    /*!******************************************!*\
      !*** ./src/generation/logits_sampler.js ***!
      \******************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        LogitsSampler: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      ), D = r(
        /*! ../utils/tensor.js */
        "./src/utils/tensor.js"
      ), U = r(
        /*! ../utils/maths.js */
        "./src/utils/maths.js"
      );
      r(
        /*! ../generation/configuration_utils.js */
        "./src/generation/configuration_utils.js"
      );
      class Y extends _.Callable {
        /**
         * Creates a new Sampler object with the specified generation config.
         * @param {GenerationConfig} generation_config The generation config.
         */
        constructor(y) {
          super(), this.generation_config = y;
        }
        /**
         * Executes the sampler, using the specified logits.
         * @param {Tensor} logits
         * @returns {Promise<[bigint, number][]>}
         */
        async _call(y) {
          return this.sample(y);
        }
        /**
         * Abstract method for sampling the logits.
         * @param {Tensor} logits
         * @throws {Error} If not implemented in subclass.
         * @returns {Promise<[bigint, number][]>}
         */
        async sample(y) {
          throw Error("sample should be implemented in subclasses.");
        }
        /**
         * Returns the specified logits as an array, with temperature applied.
         * @param {Tensor} logits
         * @param {number} index
         * @returns {Float32Array}
         */
        getLogits(y, b) {
          let I = y.dims.at(-1), K = (
            /** @type {Float32Array} */
            y.data
          );
          if (b === -1)
            K = K.slice(-I);
          else {
            let se = b * I;
            K = K.slice(se, se + I);
          }
          return K;
        }
        /**
         * Selects an item randomly based on the specified probabilities.
         * @param {import("../transformers.js").DataArray} probabilities An array of probabilities to use for selection.
         * @returns {number} The index of the selected item.
         */
        randomSelect(y) {
          let b = 0;
          for (let K = 0; K < y.length; ++K)
            b += y[K];
          let I = Math.random() * b;
          for (let K = 0; K < y.length; ++K)
            if (I -= y[K], I <= 0)
              return K;
          return 0;
        }
        /**
         * Returns a Sampler object based on the specified options.
         * @param {GenerationConfig} generation_config An object containing options for the sampler.
         * @returns {LogitsSampler} A Sampler object.
         */
        static getSampler(y) {
          if (y.do_sample)
            return new g(y);
          if (y.num_beams > 1)
            return new v(y);
          if (y.num_return_sequences > 1)
            throw Error(`num_return_sequences has to be 1 when doing greedy search, but is ${y.num_return_sequences}.`);
          return new R(y);
        }
      }
      class R extends Y {
        /**
         * Sample the maximum probability of a given logits tensor.
         * @param {Tensor} logits
         * @returns {Promise<[bigint, number][]>} An array with a single tuple, containing the index of the maximum value and a meaningless score (since this is a greedy search).
         */
        async sample(y) {
          const b = (0, U.max)(y.data)[1];
          return [
            [BigInt(b), 0]
          ];
        }
      }
      class g extends Y {
        /**
         * Sample from the logits.
         * @param {Tensor} logits
         * @returns {Promise<[bigint, number][]>}
         */
        async sample(y) {
          let b = y.dims.at(-1);
          this.generation_config.top_k > 0 && (b = Math.min(this.generation_config.top_k, b));
          const [I, K] = await (0, D.topk)(y, b), se = (0, U.softmax)(
            /** @type {Float32Array} */
            I.data
          );
          return Array.from({ length: this.generation_config.num_beams }, () => {
            const ie = this.randomSelect(se);
            return [
              K.data[ie],
              // token id
              Math.log(se[ie])
              // score
            ];
          });
        }
      }
      class v extends Y {
        /**
         * Sample from the logits.
         * @param {Tensor} logits
         * @returns {Promise<[bigint, number][]>}
         */
        async sample(y) {
          let b = y.dims.at(-1);
          this.generation_config.top_k > 0 && (b = Math.min(this.generation_config.top_k, b));
          const [I, K] = await (0, D.topk)(y, b), se = (0, U.softmax)(
            /** @type {Float32Array} */
            I.data
          );
          return Array.from({ length: this.generation_config.num_beams }, (ie, W) => [
            K.data[W],
            // token id
            Math.log(se[W])
            // score
          ]);
        }
      }
    }
  ),
  /***/
  "./src/generation/stopping_criteria.js": (
    /*!*********************************************!*\
      !*** ./src/generation/stopping_criteria.js ***!
      \*********************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        EosTokenCriteria: () => (
          /* binding */
          R
        ),
        /* harmony export */
        InterruptableStoppingCriteria: () => (
          /* binding */
          g
        ),
        /* harmony export */
        MaxLengthCriteria: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        StoppingCriteria: () => (
          /* binding */
          D
        ),
        /* harmony export */
        StoppingCriteriaList: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      );
      class D extends _.Callable {
        /**
         * 
         * @param {number[][]} input_ids (`number[][]` of shape `(batch_size, sequence_length)`):
         * Indices of input sequence tokens in the vocabulary.
         * @param {number[][]} scores scores (`number[][]` of shape `(batch_size, config.vocab_size)`):
         * Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax
         * or scores for each vocabulary token after SoftMax.
         * @returns {boolean[]} A list of booleans indicating whether each sequence should be stopped.
         */
        _call(M, y) {
          throw Error("StoppingCriteria needs to be subclassed");
        }
      }
      class U extends _.Callable {
        /**
         * Constructs a new instance of `StoppingCriteriaList`.
         */
        constructor() {
          super(), this.criteria = [];
        }
        /**
         * Adds a new stopping criterion to the list.
         *
         * @param {StoppingCriteria} item The stopping criterion to add.
         */
        push(M) {
          this.criteria.push(M);
        }
        /**
         * Adds multiple stopping criteria to the list.
         *
         * @param {StoppingCriteria|StoppingCriteriaList|StoppingCriteria[]} items The stopping criteria to add.
         */
        extend(M) {
          M instanceof U ? M = M.criteria : M instanceof D && (M = [M]), this.criteria.push(...M);
        }
        _call(M, y) {
          const b = new Array(M.length).fill(!1);
          for (const I of this.criteria) {
            const K = I(M, y);
            for (let se = 0; se < b.length; ++se)
              b[se] || (b[se] = K[se]);
          }
          return b;
        }
        [Symbol.iterator]() {
          return this.criteria.values();
        }
      }
      class Y extends D {
        /**
         * 
         * @param {number} max_length The maximum length that the output sequence can have in number of tokens.
         * @param {number} [max_position_embeddings=null] The maximum model length, as defined by the model's `config.max_position_embeddings` attribute.
         */
        constructor(M, y = null) {
          super(), this.max_length = M, this.max_position_embeddings = y;
        }
        _call(M) {
          return M.map((y) => y.length >= this.max_length);
        }
      }
      class R extends D {
        /**
         * 
         * @param {number|number[]} eos_token_id The id of the *end-of-sequence* token.
         * Optionally, use a list to set multiple *end-of-sequence* tokens.
         */
        constructor(M) {
          super(), Array.isArray(M) || (M = [M]), this.eos_token_id = M;
        }
        /**
         * 
         * @param {number[][]} input_ids 
         * @param {number[][]} scores 
         * @returns {boolean[]}
         */
        _call(M, y) {
          return M.map((b) => {
            const I = b.at(-1);
            return this.eos_token_id.some((K) => I == K);
          });
        }
      }
      class g extends D {
        constructor() {
          super(), this.interrupted = !1;
        }
        interrupt() {
          this.interrupted = !0;
        }
        reset() {
          this.interrupted = !1;
        }
        _call(M, y) {
          return new Array(M.length).fill(this.interrupted);
        }
      }
    }
  ),
  /***/
  "./src/generation/streamers.js": (
    /*!*************************************!*\
      !*** ./src/generation/streamers.js ***!
      \*************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        BaseStreamer: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        TextStreamer: () => (
          /* binding */
          g
        ),
        /* harmony export */
        WhisperTextStreamer: () => (
          /* binding */
          v
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../utils/core.js */
        "./src/utils/core.js"
      ), D = r(
        /*! ../tokenizers.js */
        "./src/tokenizers.js"
      ), U = r(
        /*! ../env.js */
        "./src/env.js"
      );
      class Y {
        /**
         * Function that is called by `.generate()` to push new tokens
         * @param {bigint[][]} value 
         */
        put(y) {
          throw Error("Not implemented");
        }
        /**
         * Function that is called by `.generate()` to signal the end of generation
         */
        end() {
          throw Error("Not implemented");
        }
      }
      const R = U.apis.IS_PROCESS_AVAILABLE ? (M) => process.stdout.write(M) : (M) => console.log(M);
      class g extends Y {
        /**
         * 
         * @param {import('../tokenizers.js').PreTrainedTokenizer} tokenizer
         * @param {Object} options
         * @param {boolean} [options.skip_prompt=false] Whether to skip the prompt tokens
         * @param {boolean} [options.skip_special_tokens=true] Whether to skip special tokens when decoding
         * @param {function(string): void} [options.callback_function=null] Function to call when a piece of text is ready to display
         * @param {function(bigint[]): void} [options.token_callback_function=null] Function to call when a new token is generated
         * @param {Object} [options.decode_kwargs={}] Additional keyword arguments to pass to the tokenizer's decode method
         */
        constructor(y, {
          skip_prompt: b = !1,
          callback_function: I = null,
          token_callback_function: K = null,
          skip_special_tokens: se = !0,
          decode_kwargs: ie = {},
          ...W
        } = {}) {
          super(), this.tokenizer = y, this.skip_prompt = b, this.callback_function = I ?? R, this.token_callback_function = K, this.decode_kwargs = { skip_special_tokens: se, ...ie, ...W }, this.token_cache = [], this.print_len = 0, this.next_tokens_are_prompt = !0;
        }
        /**
         * Receives tokens, decodes them, and prints them to stdout as soon as they form entire words.
         * @param {bigint[][]} value 
         */
        put(y) {
          var se;
          if (y.length > 1)
            throw Error("TextStreamer only supports batch size of 1");
          if (this.skip_prompt && this.next_tokens_are_prompt) {
            this.next_tokens_are_prompt = !1;
            return;
          }
          const b = y[0];
          (se = this.token_callback_function) == null || se.call(this, b), this.token_cache = (0, _.mergeArrays)(this.token_cache, b);
          const I = this.tokenizer.decode(this.token_cache, this.decode_kwargs);
          let K;
          I.endsWith(`
`) ? (K = I.slice(this.print_len), this.token_cache = [], this.print_len = 0) : I.length > 0 && (0, D.is_chinese_char)(I.charCodeAt(I.length - 1)) ? (K = I.slice(this.print_len), this.print_len += K.length) : (K = I.slice(this.print_len, I.lastIndexOf(" ") + 1), this.print_len += K.length), this.on_finalized_text(K, !1);
        }
        /**
         * Flushes any remaining cache and prints a newline to stdout.
         */
        end() {
          let y;
          this.token_cache.length > 0 ? (y = this.tokenizer.decode(this.token_cache, this.decode_kwargs).slice(this.print_len), this.token_cache = [], this.print_len = 0) : y = "", this.next_tokens_are_prompt = !0, this.on_finalized_text(y, !0);
        }
        /**
         * Prints the new text to stdout. If the stream is ending, also prints a newline.
         * @param {string} text 
         * @param {boolean} stream_end 
         */
        on_finalized_text(y, b) {
          var I, K;
          y.length > 0 && ((I = this.callback_function) == null || I.call(this, y)), b && this.callback_function === R && U.apis.IS_PROCESS_AVAILABLE && ((K = this.callback_function) == null || K.call(this, `
`));
        }
      }
      class v extends g {
        /**
         * @param {import('../tokenizers.js').WhisperTokenizer} tokenizer
         * @param {Object} options
         * @param {boolean} [options.skip_prompt=false] Whether to skip the prompt tokens
         * @param {function(string): void} [options.callback_function=null] Function to call when a piece of text is ready to display
         * @param {function(bigint[]): void} [options.token_callback_function=null] Function to call when a new token is generated
         * @param {function(number): void} [options.on_chunk_start=null] Function to call when a new chunk starts
         * @param {function(number): void} [options.on_chunk_end=null] Function to call when a chunk ends
         * @param {function(): void} [options.on_finalize=null] Function to call when the stream is finalized
         * @param {number} [options.time_precision=0.02] Precision of the timestamps
         * @param {boolean} [options.skip_special_tokens=true] Whether to skip special tokens when decoding
         * @param {Object} [options.decode_kwargs={}] Additional keyword arguments to pass to the tokenizer's decode method
         */
        constructor(y, {
          skip_prompt: b = !1,
          callback_function: I = null,
          token_callback_function: K = null,
          on_chunk_start: se = null,
          on_chunk_end: ie = null,
          on_finalize: W = null,
          time_precision: j = 0.02,
          skip_special_tokens: q = !0,
          decode_kwargs: A = {}
        } = {}) {
          super(y, {
            skip_prompt: b,
            skip_special_tokens: q,
            callback_function: I,
            token_callback_function: K,
            decode_kwargs: A
          }), this.timestamp_begin = y.timestamp_begin, this.on_chunk_start = se, this.on_chunk_end = ie, this.on_finalize = W, this.time_precision = j, this.waiting_for_timestamp = !1;
        }
        /**
         * @param {bigint[][]} value 
         */
        put(y) {
          var I, K;
          if (y.length > 1)
            throw Error("WhisperTextStreamer only supports batch size of 1");
          const b = y[0];
          if (b.length === 1) {
            const se = Number(b[0]) - this.timestamp_begin;
            if (se >= 0) {
              const ie = se * this.time_precision;
              this.waiting_for_timestamp ? (I = this.on_chunk_end) == null || I.call(this, ie) : (K = this.on_chunk_start) == null || K.call(this, ie), this.waiting_for_timestamp = !this.waiting_for_timestamp, y = [[]];
            }
          }
          return super.put(y);
        }
        end() {
          var y;
          super.end(), (y = this.on_finalize) == null || y.call(this);
        }
      }
    }
  ),
  /***/
  "./src/models.js": (
    /*!***********************!*\
      !*** ./src/models.js ***!
      \***********************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ASTForAudioClassification: () => (
          /* binding */
          Ni
        ),
        /* harmony export */
        ASTModel: () => (
          /* binding */
          za
        ),
        /* harmony export */
        ASTPreTrainedModel: () => (
          /* binding */
          Yt
        ),
        /* harmony export */
        AlbertForMaskedLM: () => (
          /* binding */
          as
        ),
        /* harmony export */
        AlbertForQuestionAnswering: () => (
          /* binding */
          zn
        ),
        /* harmony export */
        AlbertForSequenceClassification: () => (
          /* binding */
          vn
        ),
        /* harmony export */
        AlbertModel: () => (
          /* binding */
          Ln
        ),
        /* harmony export */
        AlbertPreTrainedModel: () => (
          /* binding */
          Wr
        ),
        /* harmony export */
        AutoModel: () => (
          /* binding */
          Ta
        ),
        /* harmony export */
        AutoModelForAudioClassification: () => (
          /* binding */
          fc
        ),
        /* harmony export */
        AutoModelForAudioFrameClassification: () => (
          /* binding */
          Pp
        ),
        /* harmony export */
        AutoModelForCTC: () => (
          /* binding */
          mc
        ),
        /* harmony export */
        AutoModelForCausalLM: () => (
          /* binding */
          nc
        ),
        /* harmony export */
        AutoModelForDepthEstimation: () => (
          /* binding */
          Ep
        ),
        /* harmony export */
        AutoModelForDocumentQuestionAnswering: () => (
          /* binding */
          gc
        ),
        /* harmony export */
        AutoModelForImageClassification: () => (
          /* binding */
          lc
        ),
        /* harmony export */
        AutoModelForImageFeatureExtraction: () => (
          /* binding */
          vc
        ),
        /* harmony export */
        AutoModelForImageMatting: () => (
          /* binding */
          wc
        ),
        /* harmony export */
        AutoModelForImageSegmentation: () => (
          /* binding */
          uc
        ),
        /* harmony export */
        AutoModelForImageToImage: () => (
          /* binding */
          yc
        ),
        /* harmony export */
        AutoModelForMaskGeneration: () => (
          /* binding */
          hc
        ),
        /* harmony export */
        AutoModelForMaskedLM: () => (
          /* binding */
          ic
        ),
        /* harmony export */
        AutoModelForNormalEstimation: () => (
          /* binding */
          Mc
        ),
        /* harmony export */
        AutoModelForObjectDetection: () => (
          /* binding */
          cc
        ),
        /* harmony export */
        AutoModelForPoseEstimation: () => (
          /* binding */
          bc
        ),
        /* harmony export */
        AutoModelForQuestionAnswering: () => (
          /* binding */
          oc
        ),
        /* harmony export */
        AutoModelForSemanticSegmentation: () => (
          /* binding */
          Pa
        ),
        /* harmony export */
        AutoModelForSeq2SeqLM: () => (
          /* binding */
          ec
        ),
        /* harmony export */
        AutoModelForSequenceClassification: () => (
          /* binding */
          Tp
        ),
        /* harmony export */
        AutoModelForSpeechSeq2Seq: () => (
          /* binding */
          tc
        ),
        /* harmony export */
        AutoModelForTextToSpectrogram: () => (
          /* binding */
          sc
        ),
        /* harmony export */
        AutoModelForTextToWaveform: () => (
          /* binding */
          rc
        ),
        /* harmony export */
        AutoModelForTokenClassification: () => (
          /* binding */
          Zd
        ),
        /* harmony export */
        AutoModelForUniversalSegmentation: () => (
          /* binding */
          dc
        ),
        /* harmony export */
        AutoModelForVision2Seq: () => (
          /* binding */
          ac
        ),
        /* harmony export */
        AutoModelForXVector: () => (
          /* binding */
          _c
        ),
        /* harmony export */
        AutoModelForZeroShotObjectDetection: () => (
          /* binding */
          pc
        ),
        /* harmony export */
        BartForConditionalGeneration: () => (
          /* binding */
          Kt
        ),
        /* harmony export */
        BartForSequenceClassification: () => (
          /* binding */
          ms
        ),
        /* harmony export */
        BartModel: () => (
          /* binding */
          xt
        ),
        /* harmony export */
        BartPretrainedModel: () => (
          /* binding */
          ft
        ),
        /* harmony export */
        BaseModelOutput: () => (
          /* binding */
          je
        ),
        /* harmony export */
        BeitForImageClassification: () => (
          /* binding */
          rn
        ),
        /* harmony export */
        BeitModel: () => (
          /* binding */
          sn
        ),
        /* harmony export */
        BeitPreTrainedModel: () => (
          /* binding */
          du
        ),
        /* harmony export */
        BertForMaskedLM: () => (
          /* binding */
          Ue
        ),
        /* harmony export */
        BertForQuestionAnswering: () => (
          /* binding */
          Re
        ),
        /* harmony export */
        BertForSequenceClassification: () => (
          /* binding */
          Ve
        ),
        /* harmony export */
        BertForTokenClassification: () => (
          /* binding */
          Ne
        ),
        /* harmony export */
        BertModel: () => (
          /* binding */
          Te
        ),
        /* harmony export */
        BertPreTrainedModel: () => (
          /* binding */
          ae
        ),
        /* harmony export */
        BlenderbotForConditionalGeneration: () => (
          /* binding */
          Js
        ),
        /* harmony export */
        BlenderbotModel: () => (
          /* binding */
          ze
        ),
        /* harmony export */
        BlenderbotPreTrainedModel: () => (
          /* binding */
          Ws
        ),
        /* harmony export */
        BlenderbotSmallForConditionalGeneration: () => (
          /* binding */
          Xs
        ),
        /* harmony export */
        BlenderbotSmallModel: () => (
          /* binding */
          ks
        ),
        /* harmony export */
        BlenderbotSmallPreTrainedModel: () => (
          /* binding */
          Fr
        ),
        /* harmony export */
        BloomForCausalLM: () => (
          /* binding */
          Bl
        ),
        /* harmony export */
        BloomModel: () => (
          /* binding */
          zl
        ),
        /* harmony export */
        BloomPreTrainedModel: () => (
          /* binding */
          yo
        ),
        /* harmony export */
        CLIPModel: () => (
          /* binding */
          Qa
        ),
        /* harmony export */
        CLIPPreTrainedModel: () => (
          /* binding */
          tn
        ),
        /* harmony export */
        CLIPSegForImageSegmentation: () => (
          /* binding */
          sl
        ),
        /* harmony export */
        CLIPSegModel: () => (
          /* binding */
          tl
        ),
        /* harmony export */
        CLIPSegPreTrainedModel: () => (
          /* binding */
          Hi
        ),
        /* harmony export */
        CLIPTextModel: () => (
          /* binding */
          Nc
        ),
        /* harmony export */
        CLIPTextModelWithProjection: () => (
          /* binding */
          Xa
        ),
        /* harmony export */
        CLIPVisionModel: () => (
          /* binding */
          jc
        ),
        /* harmony export */
        CLIPVisionModelWithProjection: () => (
          /* binding */
          Ya
        ),
        /* harmony export */
        CamembertForMaskedLM: () => (
          /* binding */
          Xr
        ),
        /* harmony export */
        CamembertForQuestionAnswering: () => (
          /* binding */
          Yr
        ),
        /* harmony export */
        CamembertForSequenceClassification: () => (
          /* binding */
          Sr
        ),
        /* harmony export */
        CamembertForTokenClassification: () => (
          /* binding */
          $r
        ),
        /* harmony export */
        CamembertModel: () => (
          /* binding */
          Nt
        ),
        /* harmony export */
        CamembertPreTrainedModel: () => (
          /* binding */
          Tr
        ),
        /* harmony export */
        CausalLMOutput: () => (
          /* binding */
          un
        ),
        /* harmony export */
        CausalLMOutputWithPast: () => (
          /* binding */
          Cp
        ),
        /* harmony export */
        ChineseCLIPModel: () => (
          /* binding */
          _r
        ),
        /* harmony export */
        ChineseCLIPPreTrainedModel: () => (
          /* binding */
          Uc
        ),
        /* harmony export */
        ClapAudioModelWithProjection: () => (
          /* binding */
          pd
        ),
        /* harmony export */
        ClapModel: () => (
          /* binding */
          nr
        ),
        /* harmony export */
        ClapPreTrainedModel: () => (
          /* binding */
          oa
        ),
        /* harmony export */
        ClapTextModelWithProjection: () => (
          /* binding */
          cd
        ),
        /* harmony export */
        CodeGenForCausalLM: () => (
          /* binding */
          ml
        ),
        /* harmony export */
        CodeGenModel: () => (
          /* binding */
          eo
        ),
        /* harmony export */
        CodeGenPreTrainedModel: () => (
          /* binding */
          ai
        ),
        /* harmony export */
        CohereForCausalLM: () => (
          /* binding */
          Tl
        ),
        /* harmony export */
        CohereModel: () => (
          /* binding */
          xl
        ),
        /* harmony export */
        CoherePreTrainedModel: () => (
          /* binding */
          po
        ),
        /* harmony export */
        ConvBertForMaskedLM: () => (
          /* binding */
          kt
        ),
        /* harmony export */
        ConvBertForQuestionAnswering: () => (
          /* binding */
          ys
        ),
        /* harmony export */
        ConvBertForSequenceClassification: () => (
          /* binding */
          At
        ),
        /* harmony export */
        ConvBertForTokenClassification: () => (
          /* binding */
          is
        ),
        /* harmony export */
        ConvBertModel: () => (
          /* binding */
          vt
        ),
        /* harmony export */
        ConvBertPreTrainedModel: () => (
          /* binding */
          mt
        ),
        /* harmony export */
        ConvNextForImageClassification: () => (
          /* binding */
          Qo
        ),
        /* harmony export */
        ConvNextModel: () => (
          /* binding */
          Pu
        ),
        /* harmony export */
        ConvNextPreTrainedModel: () => (
          /* binding */
          qo
        ),
        /* harmony export */
        ConvNextV2ForImageClassification: () => (
          /* binding */
          Yo
        ),
        /* harmony export */
        ConvNextV2Model: () => (
          /* binding */
          Eu
        ),
        /* harmony export */
        ConvNextV2PreTrainedModel: () => (
          /* binding */
          Xo
        ),
        /* harmony export */
        DPTForDepthEstimation: () => (
          /* binding */
          Xc
        ),
        /* harmony export */
        DPTModel: () => (
          /* binding */
          _u
        ),
        /* harmony export */
        DPTPreTrainedModel: () => (
          /* binding */
          Go
        ),
        /* harmony export */
        DebertaForMaskedLM: () => (
          /* binding */
          Ar
        ),
        /* harmony export */
        DebertaForQuestionAnswering: () => (
          /* binding */
          ar
        ),
        /* harmony export */
        DebertaForSequenceClassification: () => (
          /* binding */
          Br
        ),
        /* harmony export */
        DebertaForTokenClassification: () => (
          /* binding */
          Rr
        ),
        /* harmony export */
        DebertaModel: () => (
          /* binding */
          Jr
        ),
        /* harmony export */
        DebertaPreTrainedModel: () => (
          /* binding */
          dr
        ),
        /* harmony export */
        DebertaV2ForMaskedLM: () => (
          /* binding */
          Dt
        ),
        /* harmony export */
        DebertaV2ForQuestionAnswering: () => (
          /* binding */
          Ir
        ),
        /* harmony export */
        DebertaV2ForSequenceClassification: () => (
          /* binding */
          Vs
        ),
        /* harmony export */
        DebertaV2ForTokenClassification: () => (
          /* binding */
          Nr
        ),
        /* harmony export */
        DebertaV2Model: () => (
          /* binding */
          Tt
        ),
        /* harmony export */
        DebertaV2PreTrainedModel: () => (
          /* binding */
          it
        ),
        /* harmony export */
        DecisionTransformerModel: () => (
          /* binding */
          fp
        ),
        /* harmony export */
        DecisionTransformerPreTrainedModel: () => (
          /* binding */
          Pd
        ),
        /* harmony export */
        DeiTForImageClassification: () => (
          /* binding */
          zo
        ),
        /* harmony export */
        DeiTModel: () => (
          /* binding */
          hu
        ),
        /* harmony export */
        DeiTPreTrainedModel: () => (
          /* binding */
          hi
        ),
        /* harmony export */
        DepthAnythingForDepthEstimation: () => (
          /* binding */
          wu
        ),
        /* harmony export */
        DepthAnythingPreTrainedModel: () => (
          /* binding */
          gu
        ),
        /* harmony export */
        DepthProForDepthEstimation: () => (
          /* binding */
          Ho
        ),
        /* harmony export */
        DepthProPreTrainedModel: () => (
          /* binding */
          _i
        ),
        /* harmony export */
        DetrForObjectDetection: () => (
          /* binding */
          nn
        ),
        /* harmony export */
        DetrForSegmentation: () => (
          /* binding */
          Ks
        ),
        /* harmony export */
        DetrModel: () => (
          /* binding */
          Ao
        ),
        /* harmony export */
        DetrObjectDetectionOutput: () => (
          /* binding */
          Io
        ),
        /* harmony export */
        DetrPreTrainedModel: () => (
          /* binding */
          Or
        ),
        /* harmony export */
        DetrSegmentationOutput: () => (
          /* binding */
          Fo
        ),
        /* harmony export */
        Dinov2ForImageClassification: () => (
          /* binding */
          Zc
        ),
        /* harmony export */
        Dinov2Model: () => (
          /* binding */
          Cu
        ),
        /* harmony export */
        Dinov2PreTrainedModel: () => (
          /* binding */
          Jo
        ),
        /* harmony export */
        Dinov2WithRegistersForImageClassification: () => (
          /* binding */
          Su
        ),
        /* harmony export */
        Dinov2WithRegistersModel: () => (
          /* binding */
          ku
        ),
        /* harmony export */
        Dinov2WithRegistersPreTrainedModel: () => (
          /* binding */
          Zo
        ),
        /* harmony export */
        DistilBertForMaskedLM: () => (
          /* binding */
          _n
        ),
        /* harmony export */
        DistilBertForQuestionAnswering: () => (
          /* binding */
          es
        ),
        /* harmony export */
        DistilBertForSequenceClassification: () => (
          /* binding */
          Fs
        ),
        /* harmony export */
        DistilBertForTokenClassification: () => (
          /* binding */
          Pr
        ),
        /* harmony export */
        DistilBertModel: () => (
          /* binding */
          lr
        ),
        /* harmony export */
        DistilBertPreTrainedModel: () => (
          /* binding */
          Ms
        ),
        /* harmony export */
        DonutSwinModel: () => (
          /* binding */
          Tu
        ),
        /* harmony export */
        DonutSwinPreTrainedModel: () => (
          /* binding */
          Jc
        ),
        /* harmony export */
        EfficientNetForImageClassification: () => (
          /* binding */
          wd
        ),
        /* harmony export */
        EfficientNetModel: () => (
          /* binding */
          gd
        ),
        /* harmony export */
        EfficientNetPreTrainedModel: () => (
          /* binding */
          Pi
        ),
        /* harmony export */
        ElectraForMaskedLM: () => (
          /* binding */
          sr
        ),
        /* harmony export */
        ElectraForQuestionAnswering: () => (
          /* binding */
          Us
        ),
        /* harmony export */
        ElectraForSequenceClassification: () => (
          /* binding */
          kr
        ),
        /* harmony export */
        ElectraForTokenClassification: () => (
          /* binding */
          Qr
        ),
        /* harmony export */
        ElectraModel: () => (
          /* binding */
          Ds
        ),
        /* harmony export */
        ElectraPreTrainedModel: () => (
          /* binding */
          Cs
        ),
        /* harmony export */
        EsmForMaskedLM: () => (
          /* binding */
          Sn
        ),
        /* harmony export */
        EsmForSequenceClassification: () => (
          /* binding */
          $n
        ),
        /* harmony export */
        EsmForTokenClassification: () => (
          /* binding */
          An
        ),
        /* harmony export */
        EsmModel: () => (
          /* binding */
          si
        ),
        /* harmony export */
        EsmPreTrainedModel: () => (
          /* binding */
          jr
        ),
        /* harmony export */
        ExaoneForCausalLM: () => (
          /* binding */
          oo
        ),
        /* harmony export */
        ExaoneModel: () => (
          /* binding */
          gl
        ),
        /* harmony export */
        ExaonePreTrainedModel: () => (
          /* binding */
          io
        ),
        /* harmony export */
        FalconForCausalLM: () => (
          /* binding */
          dd
        ),
        /* harmony export */
        FalconModel: () => (
          /* binding */
          ud
        ),
        /* harmony export */
        FalconPreTrainedModel: () => (
          /* binding */
          ia
        ),
        /* harmony export */
        FastViTForImageClassification: () => (
          /* binding */
          eu
        ),
        /* harmony export */
        FastViTModel: () => (
          /* binding */
          Zl
        ),
        /* harmony export */
        FastViTPreTrainedModel: () => (
          /* binding */
          Eo
        ),
        /* harmony export */
        Florence2ForConditionalGeneration: () => (
          /* binding */
          Wa
        ),
        /* harmony export */
        Florence2PreTrainedModel: () => (
          /* binding */
          Va
        ),
        /* harmony export */
        GLPNForDepthEstimation: () => (
          /* binding */
          xu
        ),
        /* harmony export */
        GLPNModel: () => (
          /* binding */
          Yc
        ),
        /* harmony export */
        GLPNPreTrainedModel: () => (
          /* binding */
          wi
        ),
        /* harmony export */
        GPT2LMHeadModel: () => (
          /* binding */
          nl
        ),
        /* harmony export */
        GPT2Model: () => (
          /* binding */
          rl
        ),
        /* harmony export */
        GPT2PreTrainedModel: () => (
          /* binding */
          qi
        ),
        /* harmony export */
        GPTBigCodeForCausalLM: () => (
          /* binding */
          hl
        ),
        /* harmony export */
        GPTBigCodeModel: () => (
          /* binding */
          pl
        ),
        /* harmony export */
        GPTBigCodePreTrainedModel: () => (
          /* binding */
          Zi
        ),
        /* harmony export */
        GPTJForCausalLM: () => (
          /* binding */
          cl
        ),
        /* harmony export */
        GPTJModel: () => (
          /* binding */
          dl
        ),
        /* harmony export */
        GPTJPreTrainedModel: () => (
          /* binding */
          Ji
        ),
        /* harmony export */
        GPTNeoForCausalLM: () => (
          /* binding */
          al
        ),
        /* harmony export */
        GPTNeoModel: () => (
          /* binding */
          wr
        ),
        /* harmony export */
        GPTNeoPreTrainedModel: () => (
          /* binding */
          Xi
        ),
        /* harmony export */
        GPTNeoXForCausalLM: () => (
          /* binding */
          ul
        ),
        /* harmony export */
        GPTNeoXModel: () => (
          /* binding */
          ll
        ),
        /* harmony export */
        GPTNeoXPreTrainedModel: () => (
          /* binding */
          Yi
        ),
        /* harmony export */
        Gemma2ForCausalLM: () => (
          /* binding */
          kl
        ),
        /* harmony export */
        Gemma2Model: () => (
          /* binding */
          Cl
        ),
        /* harmony export */
        Gemma2PreTrainedModel: () => (
          /* binding */
          mo
        ),
        /* harmony export */
        GemmaForCausalLM: () => (
          /* binding */
          El
        ),
        /* harmony export */
        GemmaModel: () => (
          /* binding */
          Pl
        ),
        /* harmony export */
        GemmaPreTrainedModel: () => (
          /* binding */
          ho
        ),
        /* harmony export */
        GlmForCausalLM: () => (
          /* binding */
          Bn
        ),
        /* harmony export */
        GlmModel: () => (
          /* binding */
          _l
        ),
        /* harmony export */
        GlmPreTrainedModel: () => (
          /* binding */
          no
        ),
        /* harmony export */
        GraniteForCausalLM: () => (
          /* binding */
          vl
        ),
        /* harmony export */
        GraniteModel: () => (
          /* binding */
          ds
        ),
        /* harmony export */
        GranitePreTrainedModel: () => (
          /* binding */
          co
        ),
        /* harmony export */
        GroundingDinoForObjectDetection: () => (
          /* binding */
          $u
        ),
        /* harmony export */
        GroundingDinoPreTrainedModel: () => (
          /* binding */
          ep
        ),
        /* harmony export */
        GroupViTModel: () => (
          /* binding */
          yr
        ),
        /* harmony export */
        GroupViTPreTrainedModel: () => (
          /* binding */
          Jl
        ),
        /* harmony export */
        HeliumForCausalLM: () => (
          /* binding */
          fl
        ),
        /* harmony export */
        HeliumModel: () => (
          /* binding */
          li
        ),
        /* harmony export */
        HeliumPreTrainedModel: () => (
          /* binding */
          ro
        ),
        /* harmony export */
        HieraForImageClassification: () => (
          /* binding */
          Ro
        ),
        /* harmony export */
        HieraModel: () => (
          /* binding */
          mi
        ),
        /* harmony export */
        HieraPreTrainedModel: () => (
          /* binding */
          Bo
        ),
        /* harmony export */
        HubertForCTC: () => (
          /* binding */
          Yu
        ),
        /* harmony export */
        HubertForSequenceClassification: () => (
          /* binding */
          lp
        ),
        /* harmony export */
        HubertModel: () => (
          /* binding */
          Xu
        ),
        /* harmony export */
        HubertPreTrainedModel: () => (
          /* binding */
          ap
        ),
        /* harmony export */
        IJepaForImageClassification: () => (
          /* binding */
          Gl
        ),
        /* harmony export */
        IJepaModel: () => (
          /* binding */
          Wl
        ),
        /* harmony export */
        IJepaPreTrainedModel: () => (
          /* binding */
          ci
        ),
        /* harmony export */
        Idefics3ForConditionalGeneration: () => (
          /* binding */
          Wi
        ),
        /* harmony export */
        Idefics3PreTrainedModel: () => (
          /* binding */
          Ha
        ),
        /* harmony export */
        ImageMattingOutput: () => (
          /* binding */
          xc
        ),
        /* harmony export */
        JAISLMHeadModel: () => (
          /* binding */
          ol
        ),
        /* harmony export */
        JAISModel: () => (
          /* binding */
          il
        ),
        /* harmony export */
        JAISPreTrainedModel: () => (
          /* binding */
          Qi
        ),
        /* harmony export */
        JinaCLIPModel: () => (
          /* binding */
          oi
        ),
        /* harmony export */
        JinaCLIPPreTrainedModel: () => (
          /* binding */
          ii
        ),
        /* harmony export */
        JinaCLIPTextModel: () => (
          /* binding */
          Ki
        ),
        /* harmony export */
        JinaCLIPVisionModel: () => (
          /* binding */
          gr
        ),
        /* harmony export */
        LlamaForCausalLM: () => (
          /* binding */
          Vc
        ),
        /* harmony export */
        LlamaModel: () => (
          /* binding */
          so
        ),
        /* harmony export */
        LlamaPreTrainedModel: () => (
          /* binding */
          to
        ),
        /* harmony export */
        LlavaForConditionalGeneration: () => (
          /* binding */
          ni
        ),
        /* harmony export */
        LlavaOnevisionForConditionalGeneration: () => (
          /* binding */
          ja
        ),
        /* harmony export */
        LlavaPreTrainedModel: () => (
          /* binding */
          Gr
        ),
        /* harmony export */
        LongT5ForConditionalGeneration: () => (
          /* binding */
          Se
        ),
        /* harmony export */
        LongT5Model: () => (
          /* binding */
          ve
        ),
        /* harmony export */
        LongT5PreTrainedModel: () => (
          /* binding */
          ue
        ),
        /* harmony export */
        M2M100ForConditionalGeneration: () => (
          /* binding */
          sa
        ),
        /* harmony export */
        M2M100Model: () => (
          /* binding */
          Lu
        ),
        /* harmony export */
        M2M100PreTrainedModel: () => (
          /* binding */
          ta
        ),
        /* harmony export */
        MBartForCausalLM: () => (
          /* binding */
          rr
        ),
        /* harmony export */
        MBartForConditionalGeneration: () => (
          /* binding */
          Bt
        ),
        /* harmony export */
        MBartForSequenceClassification: () => (
          /* binding */
          rs
        ),
        /* harmony export */
        MBartModel: () => (
          /* binding */
          Os
        ),
        /* harmony export */
        MBartPreTrainedModel: () => (
          /* binding */
          us
        ),
        /* harmony export */
        MPNetForMaskedLM: () => (
          /* binding */
          Zr
        ),
        /* harmony export */
        MPNetForQuestionAnswering: () => (
          /* binding */
          Mn
        ),
        /* harmony export */
        MPNetForSequenceClassification: () => (
          /* binding */
          wn
        ),
        /* harmony export */
        MPNetForTokenClassification: () => (
          /* binding */
          yn
        ),
        /* harmony export */
        MPNetModel: () => (
          /* binding */
          gn
        ),
        /* harmony export */
        MPNetPreTrainedModel: () => (
          /* binding */
          ur
        ),
        /* harmony export */
        MT5ForConditionalGeneration: () => (
          /* binding */
          gt
        ),
        /* harmony export */
        MT5Model: () => (
          /* binding */
          pt
        ),
        /* harmony export */
        MT5PreTrainedModel: () => (
          /* binding */
          Qe
        ),
        /* harmony export */
        MarianMTModel: () => (
          /* binding */
          Du
        ),
        /* harmony export */
        MarianModel: () => (
          /* binding */
          Ou
        ),
        /* harmony export */
        MarianPreTrainedModel: () => (
          /* binding */
          Mi
        ),
        /* harmony export */
        MaskFormerForInstanceSegmentation: () => (
          /* binding */
          vu
        ),
        /* harmony export */
        MaskFormerModel: () => (
          /* binding */
          bu
        ),
        /* harmony export */
        MaskFormerPreTrainedModel: () => (
          /* binding */
          gi
        ),
        /* harmony export */
        MaskedLMOutput: () => (
          /* binding */
          qs
        ),
        /* harmony export */
        MgpstrForSceneTextRecognition: () => (
          /* binding */
          $d
        ),
        /* harmony export */
        MgpstrModelOutput: () => (
          /* binding */
          kd
        ),
        /* harmony export */
        MgpstrPreTrainedModel: () => (
          /* binding */
          Sd
        ),
        /* harmony export */
        MistralForCausalLM: () => (
          /* binding */
          od
        ),
        /* harmony export */
        MistralModel: () => (
          /* binding */
          id
        ),
        /* harmony export */
        MistralPreTrainedModel: () => (
          /* binding */
          ra
        ),
        /* harmony export */
        MobileBertForMaskedLM: () => (
          /* binding */
          ri
        ),
        /* harmony export */
        MobileBertForQuestionAnswering: () => (
          /* binding */
          mr
        ),
        /* harmony export */
        MobileBertForSequenceClassification: () => (
          /* binding */
          Vr
        ),
        /* harmony export */
        MobileBertModel: () => (
          /* binding */
          In
        ),
        /* harmony export */
        MobileBertPreTrainedModel: () => (
          /* binding */
          Ur
        ),
        /* harmony export */
        MobileLLMForCausalLM: () => (
          /* binding */
          yl
        ),
        /* harmony export */
        MobileLLMModel: () => (
          /* binding */
          wl
        ),
        /* harmony export */
        MobileLLMPreTrainedModel: () => (
          /* binding */
          ao
        ),
        /* harmony export */
        MobileNetV1ForImageClassification: () => (
          /* binding */
          pa
        ),
        /* harmony export */
        MobileNetV1Model: () => (
          /* binding */
          hp
        ),
        /* harmony export */
        MobileNetV1PreTrainedModel: () => (
          /* binding */
          ca
        ),
        /* harmony export */
        MobileNetV2ForImageClassification: () => (
          /* binding */
          Md
        ),
        /* harmony export */
        MobileNetV2Model: () => (
          /* binding */
          yd
        ),
        /* harmony export */
        MobileNetV2PreTrainedModel: () => (
          /* binding */
          ha
        ),
        /* harmony export */
        MobileNetV3ForImageClassification: () => (
          /* binding */
          xd
        ),
        /* harmony export */
        MobileNetV3Model: () => (
          /* binding */
          vd
        ),
        /* harmony export */
        MobileNetV3PreTrainedModel: () => (
          /* binding */
          bd
        ),
        /* harmony export */
        MobileNetV4ForImageClassification: () => (
          /* binding */
          Td
        ),
        /* harmony export */
        MobileNetV4Model: () => (
          /* binding */
          mp
        ),
        /* harmony export */
        MobileNetV4PreTrainedModel: () => (
          /* binding */
          ma
        ),
        /* harmony export */
        MobileViTForImageClassification: () => (
          /* binding */
          nu
        ),
        /* harmony export */
        MobileViTModel: () => (
          /* binding */
          ru
        ),
        /* harmony export */
        MobileViTPreTrainedModel: () => (
          /* binding */
          Co
        ),
        /* harmony export */
        MobileViTV2ForImageClassification: () => (
          /* binding */
          Hc
        ),
        /* harmony export */
        MobileViTV2Model: () => (
          /* binding */
          iu
        ),
        /* harmony export */
        MobileViTV2PreTrainedModel: () => (
          /* binding */
          ko
        ),
        /* harmony export */
        ModelOutput: () => (
          /* binding */
          Ke
        ),
        /* harmony export */
        ModernBertForMaskedLM: () => (
          /* binding */
          ct
        ),
        /* harmony export */
        ModernBertForSequenceClassification: () => (
          /* binding */
          lt
        ),
        /* harmony export */
        ModernBertForTokenClassification: () => (
          /* binding */
          ht
        ),
        /* harmony export */
        ModernBertModel: () => (
          /* binding */
          dt
        ),
        /* harmony export */
        ModernBertPreTrainedModel: () => (
          /* binding */
          st
        ),
        /* harmony export */
        Moondream1ForConditionalGeneration: () => (
          /* binding */
          Ua
        ),
        /* harmony export */
        MoonshineForConditionalGeneration: () => (
          /* binding */
          Na
        ),
        /* harmony export */
        MoonshineModel: () => (
          /* binding */
          cr
        ),
        /* harmony export */
        MoonshinePreTrainedModel: () => (
          /* binding */
          Ui
        ),
        /* harmony export */
        MptForCausalLM: () => (
          /* binding */
          Nl
        ),
        /* harmony export */
        MptModel: () => (
          /* binding */
          Rl
        ),
        /* harmony export */
        MptPreTrainedModel: () => (
          /* binding */
          Mo
        ),
        /* harmony export */
        MultiModalityCausalLM: () => (
          /* binding */
          Cd
        ),
        /* harmony export */
        MultiModalityPreTrainedModel: () => (
          /* binding */
          Ed
        ),
        /* harmony export */
        MusicgenForCausalLM: () => (
          /* binding */
          pp
        ),
        /* harmony export */
        MusicgenForConditionalGeneration: () => (
          /* binding */
          Ei
        ),
        /* harmony export */
        MusicgenModel: () => (
          /* binding */
          cp
        ),
        /* harmony export */
        MusicgenPreTrainedModel: () => (
          /* binding */
          da
        ),
        /* harmony export */
        NomicBertModel: () => (
          /* binding */
          oe
        ),
        /* harmony export */
        NomicBertPreTrainedModel: () => (
          /* binding */
          L
        ),
        /* harmony export */
        OPTForCausalLM: () => (
          /* binding */
          Ul
        ),
        /* harmony export */
        OPTModel: () => (
          /* binding */
          jl
        ),
        /* harmony export */
        OPTPreTrainedModel: () => (
          /* binding */
          bo
        ),
        /* harmony export */
        Olmo2ForCausalLM: () => (
          /* binding */
          Gc
        ),
        /* harmony export */
        Olmo2Model: () => (
          /* binding */
          bl
        ),
        /* harmony export */
        Olmo2PreTrainedModel: () => (
          /* binding */
          uo
        ),
        /* harmony export */
        OlmoForCausalLM: () => (
          /* binding */
          Ml
        ),
        /* harmony export */
        OlmoModel: () => (
          /* binding */
          Wc
        ),
        /* harmony export */
        OlmoPreTrainedModel: () => (
          /* binding */
          lo
        ),
        /* harmony export */
        OpenELMForCausalLM: () => (
          /* binding */
          $l
        ),
        /* harmony export */
        OpenELMModel: () => (
          /* binding */
          Sl
        ),
        /* harmony export */
        OpenELMPreTrainedModel: () => (
          /* binding */
          fo
        ),
        /* harmony export */
        OwlViTForObjectDetection: () => (
          /* binding */
          au
        ),
        /* harmony export */
        OwlViTModel: () => (
          /* binding */
          ou
        ),
        /* harmony export */
        OwlViTPreTrainedModel: () => (
          /* binding */
          So
        ),
        /* harmony export */
        Owlv2ForObjectDetection: () => (
          /* binding */
          uu
        ),
        /* harmony export */
        Owlv2Model: () => (
          /* binding */
          lu
        ),
        /* harmony export */
        Owlv2PreTrainedModel: () => (
          /* binding */
          $o
        ),
        /* harmony export */
        PaliGemmaForConditionalGeneration: () => (
          /* binding */
          Ka
        ),
        /* harmony export */
        PaliGemmaPreTrainedModel: () => (
          /* binding */
          Ga
        ),
        /* harmony export */
        PatchTSMixerForPrediction: () => (
          /* binding */
          Dd
        ),
        /* harmony export */
        PatchTSMixerModel: () => (
          /* binding */
          Od
        ),
        /* harmony export */
        PatchTSMixerPreTrainedModel: () => (
          /* binding */
          fa
        ),
        /* harmony export */
        PatchTSTForPrediction: () => (
          /* binding */
          Fd
        ),
        /* harmony export */
        PatchTSTModel: () => (
          /* binding */
          Id
        ),
        /* harmony export */
        PatchTSTPreTrainedModel: () => (
          /* binding */
          Ad
        ),
        /* harmony export */
        Phi3ForCausalLM: () => (
          /* binding */
          Ll
        ),
        /* harmony export */
        Phi3Model: () => (
          /* binding */
          Dl
        ),
        /* harmony export */
        Phi3PreTrainedModel: () => (
          /* binding */
          wo
        ),
        /* harmony export */
        Phi3VForCausalLM: () => (
          /* binding */
          pr
        ),
        /* harmony export */
        Phi3VPreTrainedModel: () => (
          /* binding */
          qa
        ),
        /* harmony export */
        PhiForCausalLM: () => (
          /* binding */
          Ol
        ),
        /* harmony export */
        PhiModel: () => (
          /* binding */
          Fl
        ),
        /* harmony export */
        PhiPreTrainedModel: () => (
          /* binding */
          go
        ),
        /* harmony export */
        PreTrainedModel: () => (
          /* binding */
          te
        ),
        /* harmony export */
        PretrainedMixin: () => (
          /* binding */
          gs
        ),
        /* harmony export */
        PvtForImageClassification: () => (
          /* binding */
          To
        ),
        /* harmony export */
        PvtModel: () => (
          /* binding */
          ql
        ),
        /* harmony export */
        PvtPreTrainedModel: () => (
          /* binding */
          xo
        ),
        /* harmony export */
        PyAnnoteForAudioFrameClassification: () => (
          /* binding */
          ju
        ),
        /* harmony export */
        PyAnnoteModel: () => (
          /* binding */
          Nu
        ),
        /* harmony export */
        PyAnnotePreTrainedModel: () => (
          /* binding */
          bi
        ),
        /* harmony export */
        QuestionAnsweringModelOutput: () => (
          /* binding */
          tr
        ),
        /* harmony export */
        Qwen2ForCausalLM: () => (
          /* binding */
          Al
        ),
        /* harmony export */
        Qwen2Model: () => (
          /* binding */
          Rn
        ),
        /* harmony export */
        Qwen2PreTrainedModel: () => (
          /* binding */
          _o
        ),
        /* harmony export */
        Qwen2VLForConditionalGeneration: () => (
          /* binding */
          Il
        ),
        /* harmony export */
        Qwen2VLPreTrainedModel: () => (
          /* binding */
          ui
        ),
        /* harmony export */
        RTDetrForObjectDetection: () => (
          /* binding */
          Nn
        ),
        /* harmony export */
        RTDetrModel: () => (
          /* binding */
          qc
        ),
        /* harmony export */
        RTDetrObjectDetectionOutput: () => (
          /* binding */
          Do
        ),
        /* harmony export */
        RTDetrPreTrainedModel: () => (
          /* binding */
          Oo
        ),
        /* harmony export */
        ResNetForImageClassification: () => (
          /* binding */
          Qc
        ),
        /* harmony export */
        ResNetModel: () => (
          /* binding */
          mu
        ),
        /* harmony export */
        ResNetPreTrainedModel: () => (
          /* binding */
          No
        ),
        /* harmony export */
        RoFormerForMaskedLM: () => (
          /* binding */
          Ae
        ),
        /* harmony export */
        RoFormerForQuestionAnswering: () => (
          /* binding */
          ut
        ),
        /* harmony export */
        RoFormerForSequenceClassification: () => (
          /* binding */
          We
        ),
        /* harmony export */
        RoFormerForTokenClassification: () => (
          /* binding */
          Je
        ),
        /* harmony export */
        RoFormerModel: () => (
          /* binding */
          me
        ),
        /* harmony export */
        RoFormerPreTrainedModel: () => (
          /* binding */
          H
        ),
        /* harmony export */
        RobertaForMaskedLM: () => (
          /* binding */
          fr
        ),
        /* harmony export */
        RobertaForQuestionAnswering: () => (
          /* binding */
          yt
        ),
        /* harmony export */
        RobertaForSequenceClassification: () => (
          /* binding */
          fs
        ),
        /* harmony export */
        RobertaForTokenClassification: () => (
          /* binding */
          Ss
        ),
        /* harmony export */
        RobertaModel: () => (
          /* binding */
          ir
        ),
        /* harmony export */
        RobertaPreTrainedModel: () => (
          /* binding */
          Ft
        ),
        /* harmony export */
        SamImageSegmentationOutput: () => (
          /* binding */
          Un
        ),
        /* harmony export */
        SamModel: () => (
          /* binding */
          yi
        ),
        /* harmony export */
        SamPreTrainedModel: () => (
          /* binding */
          tp
        ),
        /* harmony export */
        SapiensForDepthEstimation: () => (
          /* binding */
          fi
        ),
        /* harmony export */
        SapiensForNormalEstimation: () => (
          /* binding */
          Mu
        ),
        /* harmony export */
        SapiensForSemanticSegmentation: () => (
          /* binding */
          yu
        ),
        /* harmony export */
        SapiensPreTrainedModel: () => (
          /* binding */
          Ko
        ),
        /* harmony export */
        SegformerForImageClassification: () => (
          /* binding */
          hd
        ),
        /* harmony export */
        SegformerForSemanticSegmentation: () => (
          /* binding */
          md
        ),
        /* harmony export */
        SegformerModel: () => (
          /* binding */
          dp
        ),
        /* harmony export */
        SegformerPreTrainedModel: () => (
          /* binding */
          ln
        ),
        /* harmony export */
        Seq2SeqLMOutput: () => (
          /* binding */
          Xp
        ),
        /* harmony export */
        SequenceClassifierOutput: () => (
          /* binding */
          Qt
        ),
        /* harmony export */
        SiglipModel: () => (
          /* binding */
          Ja
        ),
        /* harmony export */
        SiglipPreTrainedModel: () => (
          /* binding */
          Gi
        ),
        /* harmony export */
        SiglipTextModel: () => (
          /* binding */
          Za
        ),
        /* harmony export */
        SiglipVisionModel: () => (
          /* binding */
          el
        ),
        /* harmony export */
        SpeechT5ForSpeechToText: () => (
          /* binding */
          Er
        ),
        /* harmony export */
        SpeechT5ForTextToSpeech: () => (
          /* binding */
          Dr
        ),
        /* harmony export */
        SpeechT5HifiGan: () => (
          /* binding */
          on
        ),
        /* harmony export */
        SpeechT5Model: () => (
          /* binding */
          qp
        ),
        /* harmony export */
        SpeechT5PreTrainedModel: () => (
          /* binding */
          Ti
        ),
        /* harmony export */
        SqueezeBertForMaskedLM: () => (
          /* binding */
          Fn
        ),
        /* harmony export */
        SqueezeBertForQuestionAnswering: () => (
          /* binding */
          Dn
        ),
        /* harmony export */
        SqueezeBertForSequenceClassification: () => (
          /* binding */
          On
        ),
        /* harmony export */
        SqueezeBertModel: () => (
          /* binding */
          bn
        ),
        /* harmony export */
        SqueezeBertPreTrainedModel: () => (
          /* binding */
          zt
        ),
        /* harmony export */
        StableLmForCausalLM: () => (
          /* binding */
          _d
        ),
        /* harmony export */
        StableLmModel: () => (
          /* binding */
          fd
        ),
        /* harmony export */
        StableLmPreTrainedModel: () => (
          /* binding */
          ua
        ),
        /* harmony export */
        Starcoder2ForCausalLM: () => (
          /* binding */
          ld
        ),
        /* harmony export */
        Starcoder2Model: () => (
          /* binding */
          ad
        ),
        /* harmony export */
        Starcoder2PreTrainedModel: () => (
          /* binding */
          na
        ),
        /* harmony export */
        StyleTextToSpeech2Model: () => (
          /* binding */
          rd
        ),
        /* harmony export */
        StyleTextToSpeech2PreTrainedModel: () => (
          /* binding */
          sd
        ),
        /* harmony export */
        Swin2SRForImageSuperResolution: () => (
          /* binding */
          fu
        ),
        /* harmony export */
        Swin2SRModel: () => (
          /* binding */
          Wo
        ),
        /* harmony export */
        Swin2SRPreTrainedModel: () => (
          /* binding */
          Vo
        ),
        /* harmony export */
        SwinForImageClassification: () => (
          /* binding */
          jn
        ),
        /* harmony export */
        SwinModel: () => (
          /* binding */
          Uo
        ),
        /* harmony export */
        SwinPreTrainedModel: () => (
          /* binding */
          jo
        ),
        /* harmony export */
        T5ForConditionalGeneration: () => (
          /* binding */
          Q
        ),
        /* harmony export */
        T5Model: () => (
          /* binding */
          P
        ),
        /* harmony export */
        T5PreTrainedModel: () => (
          /* binding */
          Pe
        ),
        /* harmony export */
        TableTransformerForObjectDetection: () => (
          /* binding */
          pu
        ),
        /* harmony export */
        TableTransformerModel: () => (
          /* binding */
          cu
        ),
        /* harmony export */
        TableTransformerObjectDetectionOutput: () => (
          /* binding */
          Lo
        ),
        /* harmony export */
        TableTransformerPreTrainedModel: () => (
          /* binding */
          pi
        ),
        /* harmony export */
        TokenClassifierOutput: () => (
          /* binding */
          Hs
        ),
        /* harmony export */
        TrOCRForCausalLM: () => (
          /* binding */
          nd
        ),
        /* harmony export */
        TrOCRPreTrainedModel: () => (
          /* binding */
          an
        ),
        /* harmony export */
        UniSpeechForCTC: () => (
          /* binding */
          np
        ),
        /* harmony export */
        UniSpeechForSequenceClassification: () => (
          /* binding */
          Wu
        ),
        /* harmony export */
        UniSpeechModel: () => (
          /* binding */
          Vu
        ),
        /* harmony export */
        UniSpeechPreTrainedModel: () => (
          /* binding */
          vi
        ),
        /* harmony export */
        UniSpeechSatForAudioFrameClassification: () => (
          /* binding */
          Hu
        ),
        /* harmony export */
        UniSpeechSatForCTC: () => (
          /* binding */
          Ku
        ),
        /* harmony export */
        UniSpeechSatForSequenceClassification: () => (
          /* binding */
          ip
        ),
        /* harmony export */
        UniSpeechSatModel: () => (
          /* binding */
          Gu
        ),
        /* harmony export */
        UniSpeechSatPreTrainedModel: () => (
          /* binding */
          Vn
        ),
        /* harmony export */
        ViTForImageClassification: () => (
          /* binding */
          vo
        ),
        /* harmony export */
        ViTMAEModel: () => (
          /* binding */
          Xl
        ),
        /* harmony export */
        ViTMAEPreTrainedModel: () => (
          /* binding */
          Ql
        ),
        /* harmony export */
        ViTMSNForImageClassification: () => (
          /* binding */
          Yl
        ),
        /* harmony export */
        ViTMSNModel: () => (
          /* binding */
          Kc
        ),
        /* harmony export */
        ViTMSNPreTrainedModel: () => (
          /* binding */
          Po
        ),
        /* harmony export */
        ViTModel: () => (
          /* binding */
          Vl
        ),
        /* harmony export */
        ViTPreTrainedModel: () => (
          /* binding */
          di
        ),
        /* harmony export */
        VisionEncoderDecoderModel: () => (
          /* binding */
          Vi
        ),
        /* harmony export */
        VitMatteForImageMatting: () => (
          /* binding */
          su
        ),
        /* harmony export */
        VitMattePreTrainedModel: () => (
          /* binding */
          tu
        ),
        /* harmony export */
        VitPoseForPoseEstimation: () => (
          /* binding */
          Hl
        ),
        /* harmony export */
        VitPosePreTrainedModel: () => (
          /* binding */
          Kl
        ),
        /* harmony export */
        VitsModel: () => (
          /* binding */
          la
        ),
        /* harmony export */
        VitsModelOutput: () => (
          /* binding */
          kp
        ),
        /* harmony export */
        VitsPreTrainedModel: () => (
          /* binding */
          aa
        ),
        /* harmony export */
        Wav2Vec2BertForCTC: () => (
          /* binding */
          op
        ),
        /* harmony export */
        Wav2Vec2BertForSequenceClassification: () => (
          /* binding */
          Qu
        ),
        /* harmony export */
        Wav2Vec2BertModel: () => (
          /* binding */
          qu
        ),
        /* harmony export */
        Wav2Vec2BertPreTrainedModel: () => (
          /* binding */
          xi
        ),
        /* harmony export */
        Wav2Vec2ForAudioFrameClassification: () => (
          /* binding */
          Ru
        ),
        /* harmony export */
        Wav2Vec2ForCTC: () => (
          /* binding */
          Bu
        ),
        /* harmony export */
        Wav2Vec2ForSequenceClassification: () => (
          /* binding */
          sp
        ),
        /* harmony export */
        Wav2Vec2Model: () => (
          /* binding */
          zu
        ),
        /* harmony export */
        Wav2Vec2PreTrainedModel: () => (
          /* binding */
          Kr
        ),
        /* harmony export */
        WavLMForAudioFrameClassification: () => (
          /* binding */
          td
        ),
        /* harmony export */
        WavLMForCTC: () => (
          /* binding */
          Zu
        ),
        /* harmony export */
        WavLMForSequenceClassification: () => (
          /* binding */
          ed
        ),
        /* harmony export */
        WavLMForXVector: () => (
          /* binding */
          up
        ),
        /* harmony export */
        WavLMModel: () => (
          /* binding */
          Ju
        ),
        /* harmony export */
        WavLMPreTrainedModel: () => (
          /* binding */
          xn
        ),
        /* harmony export */
        WeSpeakerResNetModel: () => (
          /* binding */
          rp
        ),
        /* harmony export */
        WeSpeakerResNetPreTrainedModel: () => (
          /* binding */
          Uu
        ),
        /* harmony export */
        WhisperForConditionalGeneration: () => (
          /* binding */
          Ra
        ),
        /* harmony export */
        WhisperModel: () => (
          /* binding */
          Ba
        ),
        /* harmony export */
        WhisperPreTrainedModel: () => (
          /* binding */
          ji
        ),
        /* harmony export */
        XLMForQuestionAnswering: () => (
          /* binding */
          en
        ),
        /* harmony export */
        XLMForSequenceClassification: () => (
          /* binding */
          Gs
        ),
        /* harmony export */
        XLMForTokenClassification: () => (
          /* binding */
          $t
        ),
        /* harmony export */
        XLMModel: () => (
          /* binding */
          Ls
        ),
        /* harmony export */
        XLMPreTrainedModel: () => (
          /* binding */
          qt
        ),
        /* harmony export */
        XLMRobertaForMaskedLM: () => (
          /* binding */
          Oa
        ),
        /* harmony export */
        XLMRobertaForQuestionAnswering: () => (
          /* binding */
          La
        ),
        /* harmony export */
        XLMRobertaForSequenceClassification: () => (
          /* binding */
          Ri
        ),
        /* harmony export */
        XLMRobertaForTokenClassification: () => (
          /* binding */
          Da
        ),
        /* harmony export */
        XLMRobertaModel: () => (
          /* binding */
          It
        ),
        /* harmony export */
        XLMRobertaPreTrainedModel: () => (
          /* binding */
          qe
        ),
        /* harmony export */
        XLMWithLMHeadModel: () => (
          /* binding */
          $s
        ),
        /* harmony export */
        XVectorOutput: () => (
          /* binding */
          Gn
        ),
        /* harmony export */
        YolosForObjectDetection: () => (
          /* binding */
          Iu
        ),
        /* harmony export */
        YolosModel: () => (
          /* binding */
          Au
        ),
        /* harmony export */
        YolosObjectDetectionOutput: () => (
          /* binding */
          Fu
        ),
        /* harmony export */
        YolosPreTrainedModel: () => (
          /* binding */
          ea
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./configs.js */
        "./src/configs.js"
      ), D = r(
        /*! ./backends/onnx.js */
        "./src/backends/onnx.js"
      ), U = r(
        /*! ./utils/dtypes.js */
        "./src/utils/dtypes.js"
      ), Y = r(
        /*! ./utils/generic.js */
        "./src/utils/generic.js"
      ), R = r(
        /*! ./utils/core.js */
        "./src/utils/core.js"
      ), g = r(
        /*! ./utils/hub.js */
        "./src/utils/hub.js"
      ), v = r(
        /*! ./utils/constants.js */
        "./src/utils/constants.js"
      ), M = r(
        /*! ./generation/logits_process.js */
        "./src/generation/logits_process.js"
      ), y = r(
        /*! ./generation/configuration_utils.js */
        "./src/generation/configuration_utils.js"
      ), b = r(
        /*! ./utils/tensor.js */
        "./src/utils/tensor.js"
      ), I = r(
        /*! ./utils/image.js */
        "./src/utils/image.js"
      ), K = r(
        /*! ./utils/maths.js */
        "./src/utils/maths.js"
      ), se = r(
        /*! ./generation/stopping_criteria.js */
        "./src/generation/stopping_criteria.js"
      ), ie = r(
        /*! ./generation/logits_sampler.js */
        "./src/generation/logits_sampler.js"
      ), W = r(
        /*! ./env.js */
        "./src/env.js"
      ), j = r(
        /*! ./models/whisper/generation_whisper.js */
        "./src/models/whisper/generation_whisper.js"
      ), q = r(
        /*! ./models/whisper/common_whisper.js */
        "./src/models/whisper/common_whisper.js"
      );
      const A = {
        EncoderOnly: 0,
        EncoderDecoder: 1,
        Seq2Seq: 2,
        Vision2Seq: 3,
        DecoderOnly: 4,
        MaskGeneration: 5,
        ImageTextToText: 6,
        Musicgen: 7,
        MultiModality: 8,
        Phi3V: 9
      }, S = /* @__PURE__ */ new Map(), w = /* @__PURE__ */ new Map(), x = /* @__PURE__ */ new Map();
      async function F(f, T, N) {
        var xs;
        const ge = ((xs = N.config) == null ? void 0 : xs["transformers.js_config"]) ?? {};
        let De = N.device ?? ge.device;
        De && typeof De != "string" && (De.hasOwnProperty(T) ? De = De[T] : (console.warn(`device not specified for "${T}". Using the default device.`), De = null));
        const Ie = (
          /** @type {import("./utils/devices.js").DeviceType} */
          De ?? (W.apis.IS_NODE_ENV ? "cpu" : "wasm")
        ), et = (0, D.deviceToExecutionProviders)(Ie);
        let rt = N.dtype ?? ge.dtype;
        if (typeof rt != "string" && (rt && rt.hasOwnProperty(T) ? rt = rt[T] : (rt = U.DEFAULT_DEVICE_DTYPE_MAPPING[Ie] ?? U.DATA_TYPES.fp32, console.warn(`dtype not specified for "${T}". Using the default dtype (${rt}) for this device (${Ie}).`))), rt === U.DATA_TYPES.auto) {
          let cs = ge.dtype;
          typeof cs != "string" && (cs = cs[T]), cs && cs !== U.DATA_TYPES.auto && U.DATA_TYPES.hasOwnProperty(cs) ? rt = cs : rt = U.DEFAULT_DEVICE_DTYPE_MAPPING[Ie] ?? U.DATA_TYPES.fp32;
        }
        const _t = (
          /** @type {import("./utils/dtypes.js").DataType} */
          rt
        );
        if (U.DEFAULT_DTYPE_SUFFIX_MAPPING.hasOwnProperty(_t)) {
          if (_t === U.DATA_TYPES.fp16 && Ie === "webgpu" && !await (0, U.isWebGpuFp16Supported)())
            throw new Error(`The device (${Ie}) does not support fp16.`);
        } else throw new Error(`Invalid dtype: ${_t}. Should be one of: ${Object.keys(U.DATA_TYPES).join(", ")}`);
        const Mt = ge.kv_cache_dtype ? typeof ge.kv_cache_dtype == "string" ? ge.kv_cache_dtype : ge.kv_cache_dtype[_t] ?? "float32" : void 0;
        if (Mt && !["float32", "float16"].includes(Mt))
          throw new Error(`Invalid kv_cache_dtype: ${Mt}. Should be one of: float32, float16`);
        const jt = {
          dtype: _t,
          kv_cache_dtype: Mt
        }, Vt = U.DEFAULT_DTYPE_SUFFIX_MAPPING[_t], Lt = `${N.subfolder ?? ""}/${T}${Vt}.onnx`, Gt = { ...N.session_options };
        Gt.executionProviders ?? (Gt.executionProviders = et);
        const ts = ge.free_dimension_overrides;
        ts ? Gt.freeDimensionOverrides ?? (Gt.freeDimensionOverrides = ts) : Ie.startsWith("webnn") && !Gt.freeDimensionOverrides && console.warn(
          'WebNN does not currently support dynamic shapes and requires `free_dimension_overrides` to be set in config.json as a field within "transformers.js_config". When `free_dimension_overrides` is not set, you may experience significant performance degradation.'
        );
        const ns = (0, g.getModelFile)(f, Lt, !0, N), Jt = N.use_external_data_format ?? ge.use_external_data_format;
        let os = [];
        if (Jt && (Jt === !0 || typeof Jt == "object" && Jt.hasOwnProperty(T) && Jt[T] === !0)) {
          if (W.apis.IS_NODE_ENV)
            throw new Error("External data format is not yet supported in Node.js");
          const cs = `${T}${Vt}.onnx_data`, Es = `${N.subfolder ?? ""}/${cs}`;
          os.push(new Promise(async (Is, Zs) => {
            const Ys = await (0, g.getModelFile)(f, Es, !0, N);
            Is({ path: cs, data: Ys });
          }));
        } else Gt.externalData !== void 0 && (os = Gt.externalData.map(async (cs) => {
          if (typeof cs.data == "string") {
            const Es = await (0, g.getModelFile)(f, cs.data, !0, N);
            return { ...cs, data: Es };
          }
          return cs;
        }));
        if (os.length > 0 && (Gt.externalData = await Promise.all(os)), Ie === "webgpu") {
          const cs = (0, _.getKeyValueShapes)(N.config, {
            prefix: "present"
          });
          if (Object.keys(cs).length > 0 && !(0, D.isONNXProxy)()) {
            const Es = {};
            for (const Is in cs)
              Es[Is] = "gpu-buffer";
            Gt.preferredOutputLocation = Es;
          }
        }
        return { buffer: await ns, session_options: Gt, session_config: jt };
      }
      async function le(f, T, N) {
        return Object.fromEntries(await Promise.all(
          Object.keys(T).map(async (ge) => {
            const { buffer: De, session_options: Ie, session_config: et } = await F(f, T[ge], N), rt = await (0, D.createInferenceSession)(De, Ie, et);
            return [ge, rt];
          })
        ));
      }
      async function ne(f, T, N) {
        return Object.fromEntries(await Promise.all(
          Object.keys(T).map(async (ge) => {
            const De = await (0, g.getModelJSON)(f, T[ge], !1, N);
            return [ge, De];
          })
        ));
      }
      function be(f, T) {
        const N = /* @__PURE__ */ Object.create(null), ge = [];
        for (const et of f.inputNames) {
          const rt = T[et];
          if (!(rt instanceof b.Tensor)) {
            ge.push(et);
            continue;
          }
          N[et] = (0, D.isONNXProxy)() ? rt.clone() : rt;
        }
        if (ge.length > 0)
          throw new Error(
            `An error occurred during model execution: "Missing the following inputs: ${ge.join(", ")}.`
          );
        const De = Object.keys(T).length, Ie = f.inputNames.length;
        if (De > Ie) {
          let et = Object.keys(T).filter((rt) => !f.inputNames.includes(rt));
          console.warn(`WARNING: Too many inputs were provided (${De} > ${Ie}). The following inputs will be ignored: "${et.join(", ")}".`);
        }
        return N;
      }
      async function _e(f, T) {
        const N = be(f, T);
        try {
          const ge = Object.fromEntries(Object.entries(N).map(([Ie, et]) => [Ie, et.ort_tensor]));
          let De = await f.run(ge);
          return De = re(De), De;
        } catch (ge) {
          const De = Object.fromEntries(Object.entries(N).map(([Ie, { type: et, dims: rt, data: _t }]) => [Ie, {
            // Extract these properties from the underlying ORT tensor
            type: et,
            dims: rt,
            data: _t
          }]));
          throw console.error(`An error occurred during model execution: "${ge}".`), console.error("Inputs given to model:", De), ge;
        }
      }
      function re(f) {
        for (let T in f)
          (0, D.isONNXTensor)(f[T]) ? f[T] = new b.Tensor(f[T]) : typeof f[T] == "object" && re(f[T]);
        return f;
      }
      function xe(f) {
        if (f instanceof b.Tensor)
          return f;
        if (f.length === 0)
          throw Error("items must be non-empty");
        if (Array.isArray(f[0])) {
          if (f.some((T) => T.length !== f[0].length))
            throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.");
          return new b.Tensor(
            "int64",
            BigInt64Array.from(f.flat().map((T) => BigInt(T))),
            [f.length, f[0].length]
          );
        } else
          return new b.Tensor(
            "int64",
            BigInt64Array.from(f.map((T) => BigInt(T))),
            [1, f.length]
          );
      }
      function ce(f) {
        return new b.Tensor("bool", [f], [1]);
      }
      async function ke(f, T) {
        let { encoder_outputs: N, input_ids: ge, decoder_input_ids: De, ...Ie } = T;
        if (!N) {
          const rt = (0, R.pick)(T, f.sessions.model.inputNames);
          N = (await Fe(f, rt)).last_hidden_state;
        }
        return Ie.input_ids = De, Ie.encoder_hidden_states = N, f.sessions.decoder_model_merged.inputNames.includes("encoder_attention_mask") && (Ie.encoder_attention_mask = T.attention_mask), await Ee(f, Ie, !0);
      }
      async function Fe(f, T) {
        const N = f.sessions.model, ge = (0, R.pick)(T, N.inputNames);
        if (N.inputNames.includes("inputs_embeds") && !ge.inputs_embeds) {
          if (!T.input_ids)
            throw new Error("Both `input_ids` and `inputs_embeds` are missing in the model inputs.");
          ge.inputs_embeds = await f.encode_text({ input_ids: T.input_ids });
        }
        if (N.inputNames.includes("token_type_ids") && !ge.token_type_ids) {
          if (!ge.input_ids)
            throw new Error("Both `input_ids` and `token_type_ids` are missing in the model inputs.");
          ge.token_type_ids = (0, b.zeros_like)(ge.input_ids);
        }
        if (N.inputNames.includes("pixel_mask") && !ge.pixel_mask) {
          if (!ge.pixel_values)
            throw new Error("Both `pixel_values` and `pixel_mask` are missing in the model inputs.");
          const De = ge.pixel_values.dims;
          ge.pixel_mask = (0, b.ones)([De[0], De[2], De[3]]);
        }
        return await _e(N, ge);
      }
      async function Ee(f, T, N = !1) {
        const ge = f.sessions[N ? "decoder_model_merged" : "model"], { past_key_values: De, ...Ie } = T;
        if (ge.inputNames.includes("use_cache_branch") && (Ie.use_cache_branch = ce(!!De)), ge.inputNames.includes("position_ids") && Ie.attention_mask && !Ie.position_ids) {
          const rt = f.config.model_type === "paligemma" ? 1 : 0;
          Ie.position_ids = J(Ie, De, rt);
        }
        f.addPastKeyValues(Ie, De);
        const et = (0, R.pick)(Ie, ge.inputNames);
        return await _e(ge, et);
      }
      function tt({
        image_token_id: f,
        inputs_embeds: T,
        image_features: N,
        input_ids: ge,
        attention_mask: De
      }) {
        const Ie = ge.tolist().map(
          (Mt) => Mt.reduce((jt, Vt, Lt) => (Vt == f && jt.push(Lt), jt), [])
        ), et = Ie.reduce((Mt, jt) => Mt + jt.length, 0), rt = N.dims[0];
        if (et !== rt)
          throw new Error(`Image features and image tokens do not match: tokens: ${et}, features ${rt}`);
        let _t = 0;
        for (let Mt = 0; Mt < Ie.length; ++Mt) {
          const jt = Ie[Mt], Vt = T[Mt];
          for (let Lt = 0; Lt < jt.length; ++Lt)
            Vt[jt[Lt]].data.set(N[_t++].data);
        }
        return { inputs_embeds: T, attention_mask: De };
      }
      async function Ge(f, {
        // Produced by the tokenizer/processor:
        input_ids: T = null,
        attention_mask: N = null,
        pixel_values: ge = null,
        // Used during generation:
        position_ids: De = null,
        inputs_embeds: Ie = null,
        past_key_values: et = null,
        // Generic generation parameters
        generation_config: rt = null,
        logits_processor: _t = null,
        // TODO: needed?
        ...Mt
      }) {
        if (!Ie) {
          if (Ie = await f.encode_text({ input_ids: T, ...Mt }), ge && T.dims[1] !== 1) {
            const Vt = await f.encode_image({ pixel_values: ge, ...Mt });
            ({ inputs_embeds: Ie, attention_mask: N } = f._merge_input_ids_with_image_features({
              image_features: Vt,
              inputs_embeds: Ie,
              input_ids: T,
              attention_mask: N
            }));
          } else if (et && ge && T.dims[1] === 1) {
            const Vt = T.dims[1], Lt = Object.values(et)[0].dims.at(-2);
            N = (0, b.cat)([
              (0, b.ones)([T.dims[0], Lt]),
              N.slice(null, [N.dims[1] - Vt, N.dims[1]])
            ], 1);
          }
        }
        if (!De && f.config.model_type === "qwen2_vl") {
          const { image_grid_thw: Vt, video_grid_thw: Lt } = Mt;
          [De] = f.get_rope_index(T, Vt, Lt, N);
        }
        return await Ee(f, {
          inputs_embeds: Ie,
          past_key_values: et,
          attention_mask: N,
          position_ids: De,
          generation_config: rt,
          logits_processor: _t
        }, !0);
      }
      function ye(f, T = 0) {
        const [N, ge] = f.dims, De = f.data, Ie = new BigInt64Array(De.length);
        for (let et = 0; et < N; ++et) {
          const rt = et * ge;
          let _t = BigInt(T);
          for (let Mt = 0; Mt < ge; ++Mt) {
            const jt = rt + Mt;
            De[jt] === 0n ? Ie[jt] = BigInt(1) : (Ie[jt] = _t, _t += De[jt]);
          }
        }
        return { data: Ie, dims: f.dims };
      }
      function J(f, T = null, N = 0) {
        const { input_ids: ge, inputs_embeds: De, attention_mask: Ie } = f, { data: et, dims: rt } = ye(Ie, N);
        let _t = new b.Tensor("int64", et, rt);
        if (T) {
          const Mt = -(ge ?? De).dims.at(1);
          _t = _t.slice(null, [Mt, null]);
        }
        return _t;
      }
      function de(f, T, N, ge) {
        if (N.past_key_values) {
          const De = Object.values(N.past_key_values)[0].dims.at(-2), { input_ids: Ie, attention_mask: et } = N;
          if (!(et && et.dims[1] > Ie.dims[1])) {
            if (De < Ie.dims[1])
              N.input_ids = Ie.slice(null, [De, null]);
            else if (
              // NOTE: Only used by VLMs (!= so that null matches undefined)
              f.config.image_token_index != null && // Equivalent to `self.config.image_token_index in input_ids` (== so that int matches bigint)
              Ie.data.some((rt) => rt == f.config.image_token_index)
            ) {
              const rt = f.config.num_image_tokens;
              if (!rt)
                throw new Error("`num_image_tokens` is missing in the model configuration.");
              const _t = Ie.dims[1] - (De - rt);
              N.input_ids = Ie.slice(null, [-_t, null]), N.attention_mask = (0, b.ones)([1, De + _t]);
            }
          }
        }
        return N;
      }
      function Ce(f, T, N, ge) {
        return N.past_key_values && (T = T.map((De) => [De.at(-1)])), {
          ...N,
          decoder_input_ids: xe(T)
        };
      }
      function Be(f, ...T) {
        return f.config.is_encoder_decoder ? Ce(f, ...T) : de(f, ...T);
      }
      function Ze(f, T, N, ge) {
        const De = !!N.past_key_values;
        return ge.guidance_scale !== null && ge.guidance_scale > 1 && (De ? N.input_ids = (0, b.cat)([
          N.input_ids,
          N.input_ids
        ], 0) : (N.input_ids = (0, b.cat)([
          N.input_ids,
          (0, b.full_like)(N.input_ids, BigInt(ge.pad_token_id))
        ], 0), N.attention_mask = (0, b.cat)([
          N.attention_mask,
          (0, b.full_like)(N.attention_mask, 0n)
        ], 0))), (De || !N.pixel_values) && (N.pixel_values = (0, b.full)([0, 0, 3, 384, 384], 1)), De && (N.images_seq_mask = new b.Tensor(
          "bool",
          new Array(1).fill(!0).fill(!1, 0, 1),
          [1, 1]
        ), N.images_emb_mask = new b.Tensor(
          "bool",
          new Array(0).fill(!1),
          [1, 1, 0]
        )), N;
      }
      class te extends Y.Callable {
        /**
         * Creates a new instance of the `PreTrainedModel` class.
         * @param {import('./configs.js').PretrainedConfig} config The model configuration.
         * @param {Record<string, any>} sessions The inference sessions for the model.
         * @param {Record<string, Object>} configs Additional configuration files (e.g., generation_config.json).
         */
        constructor(N, ge, De) {
          super();
          fe(this, "main_input_name", "input_ids");
          fe(this, "forward_params", ["input_ids", "attention_mask"]);
          this.config = N, this.sessions = ge, this.configs = De;
          const Ie = x.get(this.constructor), et = S.get(Ie);
          switch (this.can_generate = !1, this._forward = null, this._prepare_inputs_for_generation = null, et) {
            case A.DecoderOnly:
              this.can_generate = !0, this._forward = Ee, this._prepare_inputs_for_generation = de;
              break;
            case A.Seq2Seq:
            case A.Vision2Seq:
            case A.Musicgen:
              this.can_generate = !0, this._forward = ke, this._prepare_inputs_for_generation = Ce;
              break;
            case A.EncoderDecoder:
              this._forward = ke;
              break;
            case A.ImageTextToText:
              this.can_generate = !0, this._forward = Ge, this._prepare_inputs_for_generation = Be;
              break;
            case A.Phi3V:
              this.can_generate = !0, this._prepare_inputs_for_generation = Be;
              break;
            case A.MultiModality:
              this.can_generate = !0, this._prepare_inputs_for_generation = Ze;
              break;
            default:
              this._forward = Fe;
              break;
          }
          this.can_generate && this.forward_params.push("past_key_values"), this.custom_config = this.config["transformers.js_config"] ?? {};
        }
        /**
        * Disposes of all the ONNX sessions that were created during inference.
        * @returns {Promise<unknown[]>} An array of promises, one for each ONNX session that is being disposed.
        * @todo Use https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/FinalizationRegistry
        */
        async dispose() {
          var ge;
          const N = [];
          for (const De of Object.values(this.sessions))
            (ge = De == null ? void 0 : De.handler) != null && ge.dispose && N.push(De.handler.dispose());
          return await Promise.all(N);
        }
        /**
         * Instantiate one of the model classes of the library from a pretrained model.
         * 
         * The model class to instantiate is selected based on the `model_type` property of the config object
         * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing model weights, e.g., `./my_model_directory/`.
         * @param {import('./utils/hub.js').PretrainedModelOptions} options Additional options for loading the model.
         * 
         * @returns {Promise<PreTrainedModel>} A new instance of the `PreTrainedModel` class.
         */
        static async from_pretrained(N, {
          progress_callback: ge = null,
          config: De = null,
          cache_dir: Ie = null,
          local_files_only: et = !1,
          revision: rt = "main",
          model_file_name: _t = null,
          subfolder: Mt = "onnx",
          device: jt = null,
          dtype: Vt = null,
          use_external_data_format: Lt = null,
          session_options: Gt = {}
        } = {}) {
          let ts = {
            progress_callback: ge,
            config: De,
            cache_dir: Ie,
            local_files_only: et,
            revision: rt,
            model_file_name: _t,
            subfolder: Mt,
            device: jt,
            dtype: Vt,
            use_external_data_format: Lt,
            session_options: Gt
          };
          const ns = x.get(this), Jt = S.get(ns);
          De = ts.config = await _.AutoConfig.from_pretrained(N, ts);
          let os;
          if (Jt === A.DecoderOnly)
            os = await Promise.all([
              le(N, {
                model: ts.model_file_name ?? "model"
              }, ts),
              ne(N, {
                generation_config: "generation_config.json"
              }, ts)
            ]);
          else if (Jt === A.Seq2Seq || Jt === A.Vision2Seq)
            os = await Promise.all([
              le(N, {
                model: "encoder_model",
                decoder_model_merged: "decoder_model_merged"
              }, ts),
              ne(N, {
                generation_config: "generation_config.json"
              }, ts)
            ]);
          else if (Jt === A.MaskGeneration)
            os = await Promise.all([
              le(N, {
                model: "vision_encoder",
                prompt_encoder_mask_decoder: "prompt_encoder_mask_decoder"
              }, ts)
            ]);
          else if (Jt === A.EncoderDecoder)
            os = await Promise.all([
              le(N, {
                model: "encoder_model",
                decoder_model_merged: "decoder_model_merged"
              }, ts)
            ]);
          else if (Jt === A.ImageTextToText) {
            const As = {
              embed_tokens: "embed_tokens",
              vision_encoder: "vision_encoder",
              decoder_model_merged: "decoder_model_merged"
            };
            De.is_encoder_decoder && (As.model = "encoder_model"), os = await Promise.all([
              le(N, As, ts),
              ne(N, {
                generation_config: "generation_config.json"
              }, ts)
            ]);
          } else if (Jt === A.Musicgen)
            os = await Promise.all([
              le(N, {
                model: "text_encoder",
                decoder_model_merged: "decoder_model_merged",
                encodec_decode: "encodec_decode"
              }, ts),
              ne(N, {
                generation_config: "generation_config.json"
              }, ts)
            ]);
          else if (Jt === A.MultiModality)
            os = await Promise.all([
              le(N, {
                prepare_inputs_embeds: "prepare_inputs_embeds",
                model: "language_model",
                lm_head: "lm_head",
                gen_head: "gen_head",
                gen_img_embeds: "gen_img_embeds",
                image_decode: "image_decode"
              }, ts),
              ne(N, {
                generation_config: "generation_config.json"
              }, ts)
            ]);
          else if (Jt === A.Phi3V)
            os = await Promise.all([
              le(N, {
                prepare_inputs_embeds: "prepare_inputs_embeds",
                model: "model",
                vision_encoder: "vision_encoder"
              }, ts),
              ne(N, {
                generation_config: "generation_config.json"
              }, ts)
            ]);
          else {
            if (Jt !== A.EncoderOnly) {
              const As = ns ?? (De == null ? void 0 : De.model_type);
              As !== "custom" && console.warn(`Model type for '${As}' not found, assuming encoder-only architecture. Please report this at ${v.GITHUB_ISSUE_URL}.`);
            }
            os = await Promise.all([
              le(N, {
                model: ts.model_file_name ?? "model"
              }, ts)
            ]);
          }
          return new this(De, ...os);
        }
        /**
         * Runs the model with the provided inputs
         * @param {Object} model_inputs Object containing input tensors
         * @returns {Promise<Object>} Object containing output tensors
         */
        async _call(N) {
          return await this.forward(N);
        }
        /**
         * Forward method for a pretrained model. If not overridden by a subclass, the correct forward method
         * will be chosen based on the model type.
         * @param {Object} model_inputs The input data to the model in the format specified in the ONNX model.
         * @returns {Promise<Object>} The output data from the model in the format specified in the ONNX model.
         * @throws {Error} This method must be implemented in subclasses.
         */
        async forward(N) {
          return await this._forward(this, N);
        }
        /**
         * Get the model's generation config, if it exists.
         * @returns {GenerationConfig|null} The model's generation config if it exists, otherwise `null`.
         */
        get generation_config() {
          var N;
          return ((N = this.configs) == null ? void 0 : N.generation_config) ?? null;
        }
        /**
         * This function returns a [`LogitsProcessorList`] list object that contains all relevant [`LogitsWarper`]
         * instances used for multinomial sampling.
         * @param {GenerationConfig} generation_config The generation config.
         * @returns {LogitsProcessorList} generation_config 
         */
        _get_logits_warper(N) {
          const ge = new M.LogitsProcessorList();
          return N.temperature !== null && N.temperature !== 1 && ge.push(new M.TemperatureLogitsWarper(N.temperature)), N.top_k !== null && N.top_k !== 0 && ge.push(new M.TopKLogitsWarper(N.top_k)), N.top_p !== null && N.top_p < 1 && ge.push(new M.TopPLogitsWarper(N.top_p)), ge;
        }
        /**
         * @param {GenerationConfig} generation_config 
         * @param {number} input_ids_seq_length The starting sequence length for the input ids.
         * @returns {LogitsProcessorList}
         * @private
         */
        _get_logits_processor(N, ge, De = null) {
          const Ie = new M.LogitsProcessorList();
          if (N.repetition_penalty !== null && N.repetition_penalty !== 1 && Ie.push(new M.RepetitionPenaltyLogitsProcessor(N.repetition_penalty)), N.no_repeat_ngram_size !== null && N.no_repeat_ngram_size > 0 && Ie.push(new M.NoRepeatNGramLogitsProcessor(N.no_repeat_ngram_size)), N.bad_words_ids !== null && Ie.push(new M.NoBadWordsLogitsProcessor(N.bad_words_ids, N.eos_token_id)), N.min_length !== null && N.eos_token_id !== null && N.min_length > 0 && Ie.push(new M.MinLengthLogitsProcessor(N.min_length, N.eos_token_id)), N.min_new_tokens !== null && N.eos_token_id !== null && N.min_new_tokens > 0 && Ie.push(new M.MinNewTokensLengthLogitsProcessor(
            ge,
            N.min_new_tokens,
            N.eos_token_id
          )), N.forced_bos_token_id !== null && Ie.push(new M.ForcedBOSTokenLogitsProcessor(N.forced_bos_token_id)), N.forced_eos_token_id !== null && Ie.push(new M.ForcedEOSTokenLogitsProcessor(
            N.max_length,
            N.forced_eos_token_id
          )), N.begin_suppress_tokens !== null) {
            const et = ge > 1 || N.forced_bos_token_id === null ? ge : ge + 1;
            Ie.push(new M.SuppressTokensAtBeginLogitsProcessor(N.begin_suppress_tokens, et));
          }
          return N.guidance_scale !== null && N.guidance_scale > 1 && Ie.push(new M.ClassifierFreeGuidanceLogitsProcessor(N.guidance_scale)), De !== null && Ie.extend(De), Ie;
        }
        /**
         * This function merges multiple generation configs together to form a final generation config to be used by the model for text generation.
         * It first creates an empty `GenerationConfig` object, then it applies the model's own `generation_config` property to it. Finally, if a `generation_config` object was passed in the arguments, it overwrites the corresponding properties in the final config with those of the passed config object.
         * @param {GenerationConfig|null} generation_config A `GenerationConfig` object containing generation parameters.
         * @param {Object} kwargs Additional generation parameters to be used in place of those in the `generation_config` object.
         * @returns {GenerationConfig} The final generation config object to be used by the model for text generation.
         */
        _prepare_generation_config(N, ge, De = y.GenerationConfig) {
          const Ie = { ...this.config };
          for (const rt of ["decoder", "generator", "text_config"])
            rt in Ie && Object.assign(Ie, Ie[rt]);
          const et = new De(Ie);
          return Object.assign(et, this.generation_config ?? {}), N && Object.assign(et, N), ge && Object.assign(et, (0, R.pick)(ge, Object.getOwnPropertyNames(et))), et;
        }
        /**
         * 
         * @param {GenerationConfig} generation_config 
         * @param {StoppingCriteriaList} [stopping_criteria=null] 
         */
        _get_stopping_criteria(N, ge = null) {
          const De = new se.StoppingCriteriaList();
          return N.max_length !== null && De.push(new se.MaxLengthCriteria(
            N.max_length,
            this.config.max_position_embeddings ?? null
          )), N.eos_token_id !== null && De.push(new se.EosTokenCriteria(N.eos_token_id)), ge && De.extend(ge), De;
        }
        /**
         * Confirms that the model class is compatible with generation.
         * If not, raises an exception that points to the right class to use.
         */
        _validate_model_class() {
          if (!this.can_generate) {
            const N = [
              ya,
              // MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING, // TODO
              va,
              Ci,
              Ld
            ], ge = x.get(this.constructor), De = /* @__PURE__ */ new Set(), Ie = this.config.model_type;
            for (const rt of N) {
              const _t = rt.get(Ie);
              _t && De.add(_t[0]);
            }
            let et = `The current model class (${ge}) is not compatible with \`.generate()\`, as it doesn't have a language model head.`;
            throw De.size > 0 && (et += ` Please use the following class instead: ${[...De].join(", ")}`), Error(et);
          }
        }
        prepare_inputs_for_generation(...N) {
          return this._prepare_inputs_for_generation(this, ...N);
        }
        /**
         * 
         * @param {Object} inputs
         * @param {bigint[][]} inputs.generated_input_ids
         * @param {Object} inputs.outputs
         * @param {Object} inputs.model_inputs
         * @param {boolean} inputs.is_encoder_decoder
         * @returns {Object} The updated model inputs for the next generation iteration.
         */
        _update_model_kwargs_for_generation({ generated_input_ids: N, outputs: ge, model_inputs: De, is_encoder_decoder: Ie }) {
          return De.past_key_values = this.getPastKeyValues(ge, De.past_key_values), De.input_ids = new b.Tensor("int64", N.flat(), [N.length, 1]), Ie || (De.attention_mask = (0, b.cat)(
            [
              De.attention_mask,
              (0, b.ones)([De.attention_mask.dims[0], 1])
            ],
            1
          )), De.position_ids = null, De;
        }
        /**
         * This function extracts the model-specific `inputs` for generation.
         * @param {Object} params
         * @param {Tensor} [params.inputs=null]
         * @param {number} [params.bos_token_id=null]
         * @param {Record<string, Tensor|number[]>} [params.model_kwargs]
         * @returns {{inputs_tensor: Tensor, model_inputs: Record<string, Tensor>, model_input_name: string}} The model-specific inputs for generation.
         */
        _prepare_model_inputs({ inputs: N, bos_token_id: ge, model_kwargs: De }) {
          const Ie = (0, R.pick)(De, this.forward_params), et = this.main_input_name;
          if (et in Ie) {
            if (N)
              throw new Error(
                "`inputs`: {inputs}` were passed alongside {input_name} which is not allowed. Make sure to either pass {inputs} or {input_name}=..."
              );
          } else
            Ie[et] = N;
          return { inputs_tensor: Ie[et], model_inputs: Ie, model_input_name: et };
        }
        async _prepare_encoder_decoder_kwargs_for_generation({ inputs_tensor: N, model_inputs: ge, model_input_name: De, generation_config: Ie }) {
          if (this.sessions.model.inputNames.includes("inputs_embeds") && !ge.inputs_embeds && "_prepare_inputs_embeds" in this) {
            const { input_ids: rt, pixel_values: _t, attention_mask: Mt, ...jt } = ge, Vt = await this._prepare_inputs_embeds(ge);
            ge = {
              ...jt,
              ...(0, R.pick)(Vt, ["inputs_embeds", "attention_mask"])
            };
          }
          let { last_hidden_state: et } = await Fe(this, ge);
          if (Ie.guidance_scale !== null && Ie.guidance_scale > 1)
            et = (0, b.cat)([
              et,
              (0, b.full_like)(et, 0)
            ], 0), "attention_mask" in ge && (ge.attention_mask = (0, b.cat)([
              ge.attention_mask,
              (0, b.zeros_like)(ge.attention_mask)
            ], 0));
          else if (ge.decoder_input_ids) {
            const rt = xe(ge.decoder_input_ids).dims[0];
            if (rt !== et.dims[0]) {
              if (et.dims[0] !== 1)
                throw new Error(
                  `The encoder outputs have a different batch size (${et.dims[0]}) than the decoder inputs (${rt}).`
                );
              et = (0, b.cat)(Array.from({ length: rt }, () => et), 0);
            }
          }
          return ge.encoder_outputs = et, ge;
        }
        /**
         * Prepares `decoder_input_ids` for generation with encoder-decoder models
         * @param {*} param0 
         */
        _prepare_decoder_input_ids_for_generation({ batch_size: N, model_input_name: ge, model_kwargs: De, decoder_start_token_id: Ie, bos_token_id: et, generation_config: rt }) {
          let { decoder_input_ids: _t, ...Mt } = De;
          if (!(_t instanceof b.Tensor)) {
            if (_t)
              Array.isArray(_t[0]) || (_t = Array.from({
                length: N
              }, () => _t));
            else if (Ie ?? (Ie = et), this.config.model_type === "musicgen")
              _t = Array.from({
                // @ts-expect-error TS2339
                length: N * this.config.decoder.num_codebooks
              }, () => [Ie]);
            else if (Array.isArray(Ie)) {
              if (Ie.length !== N)
                throw new Error(
                  `\`decoder_start_token_id\` expcted to have length ${N} but got ${Ie.length}`
                );
              _t = Ie;
            } else
              _t = Array.from({
                length: N
              }, () => [Ie]);
            _t = xe(_t);
          }
          return De.decoder_attention_mask = (0, b.ones_like)(_t), { input_ids: _t, model_inputs: Mt };
        }
        /**
         * Generates sequences of token ids for models with a language modeling head.
         * @param {import('./generation/parameters.js').GenerationFunctionParameters} options
         * @returns {Promise<ModelOutput|Tensor>} The output of the model, which can contain the generated token ids, attentions, and scores.
         */
        async generate({
          inputs: N = null,
          generation_config: ge = null,
          logits_processor: De = null,
          stopping_criteria: Ie = null,
          streamer: et = null,
          // inputs_attention_mask = null,
          ...rt
        }) {
          this._validate_model_class(), ge = this._prepare_generation_config(ge, rt);
          let { inputs_tensor: _t, model_inputs: Mt, model_input_name: jt } = this._prepare_model_inputs({
            inputs: N,
            model_kwargs: rt
          });
          const Vt = this.config.is_encoder_decoder;
          Vt && ("encoder_outputs" in Mt || (Mt = await this._prepare_encoder_decoder_kwargs_for_generation(
            { inputs_tensor: _t, model_inputs: Mt, model_input_name: jt, generation_config: ge }
          )));
          let Lt;
          Vt ? { input_ids: Lt, model_inputs: Mt } = this._prepare_decoder_input_ids_for_generation({
            batch_size: Mt[jt].dims.at(0),
            model_input_name: jt,
            model_kwargs: Mt,
            decoder_start_token_id: ge.decoder_start_token_id,
            bos_token_id: ge.bos_token_id,
            generation_config: ge
          }) : Lt = Mt[jt];
          let Gt = Lt.dims.at(-1);
          ge.max_new_tokens !== null && (ge.max_length = Gt + ge.max_new_tokens);
          const ts = this._get_logits_processor(
            ge,
            Gt,
            De
          ), ns = this._get_stopping_criteria(
            ge,
            Ie
          ), Jt = Mt[jt].dims.at(0), os = ie.LogitsSampler.getSampler(ge), As = new Array(Jt).fill(0), xs = Lt.tolist();
          et && et.put(xs);
          let cs, Es = {};
          for (; ; ) {
            if (Mt = this.prepare_inputs_for_generation(xs, Mt, ge), cs = await this.forward(Mt), ge.output_attentions && ge.return_dict_in_generate) {
              const or = this.getAttentions(cs);
              for (const br in or)
                br in Es || (Es[br] = []), Es[br].push(or[br]);
            }
            const Ys = cs.logits.slice(null, -1, null), Mr = ts(xs, Ys), dn = [];
            for (let or = 0; or < Mr.dims.at(0); ++or) {
              const br = Mr[or], ki = await os(br);
              for (const [Si, $i] of ki) {
                const Ca = BigInt(Si);
                As[or] += $i, xs[or].push(Ca), dn.push([Ca]);
                break;
              }
            }
            if (et && et.put(dn), ns(xs).every((or) => or))
              break;
            Mt = this._update_model_kwargs_for_generation({
              generated_input_ids: dn,
              outputs: cs,
              model_inputs: Mt,
              is_encoder_decoder: Vt
            });
          }
          et && et.end();
          const Is = this.getPastKeyValues(cs, Mt.past_key_values, !0), Zs = new b.Tensor("int64", xs.flat(), [xs.length, xs[0].length]);
          if (ge.return_dict_in_generate)
            return {
              sequences: Zs,
              past_key_values: Is,
              ...Es
              // TODO:
              // scores,
              // logits,
            };
          for (const Ys of Object.values(cs))
            Ys.location === "gpu-buffer" && Ys.dispose();
          return Zs;
        }
        /**
         * Returns an object containing past key values from the given decoder results object.
         *
         * @param {Object} decoderResults The decoder results object.
         * @param {Object} pastKeyValues The previous past key values.
         * @returns {Object} An object containing past key values.
         */
        getPastKeyValues(N, ge, De = !1) {
          const Ie = /* @__PURE__ */ Object.create(null);
          for (const et in N)
            if (et.startsWith("present")) {
              const rt = et.replace("present", "past_key_values"), _t = et.includes("encoder");
              if (_t && ge ? Ie[rt] = ge[rt] : Ie[rt] = N[et], ge && (!_t || De)) {
                const Mt = ge[rt];
                Mt.location === "gpu-buffer" && Mt.dispose();
              }
            }
          return Ie;
        }
        /**
         * Returns an object containing attentions from the given model output object.
         *
         * @param {Object} model_output The output of the model.
         * @returns {{cross_attentions?: Tensor[]}} An object containing attentions.
         */
        getAttentions(N) {
          const ge = {};
          for (const De of ["cross_attentions", "encoder_attentions", "decoder_attentions"])
            for (const Ie in N)
              Ie.startsWith(De) && (De in ge || (ge[De] = []), ge[De].push(N[Ie]));
          return ge;
        }
        /**
         * Adds past key values to the decoder feeds object. If pastKeyValues is null, creates new tensors for past key values.
         *
         * @param {Object} decoderFeeds The decoder feeds object to add past key values to.
         * @param {Object} pastKeyValues An object containing past key values.
         */
        addPastKeyValues(N, ge) {
          var De, Ie, et;
          if (ge)
            Object.assign(N, ge);
          else {
            const rt = this.sessions.decoder_model_merged ?? this.sessions.model, _t = ((De = rt == null ? void 0 : rt.config) == null ? void 0 : De.kv_cache_dtype) ?? "float32", Mt = _t === "float16" ? new Uint16Array() : [], jt = ((et = (Ie = N[this.main_input_name] ?? N.attention_mask) == null ? void 0 : Ie.dims) == null ? void 0 : et[0]) ?? 1, Vt = (0, _.getKeyValueShapes)(this.config, { batch_size: jt });
            for (const Lt in Vt)
              N[Lt] = new b.Tensor(_t, Mt, Vt[Lt]);
          }
        }
        async encode_image({ pixel_values: N }) {
          const ge = (await _e(this.sessions.vision_encoder, { pixel_values: N })).image_features;
          return this.config.num_image_tokens || (console.warn(
            `The number of image tokens was not set in the model configuration. Setting it to the number of features detected by the vision encoder (${ge.dims[1]}).`
          ), this.config.num_image_tokens = ge.dims[1]), ge;
        }
        async encode_text({ input_ids: N }) {
          return (await _e(this.sessions.embed_tokens, { input_ids: N })).inputs_embeds;
        }
      }
      class Ke {
      }
      class je extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.last_hidden_state Sequence of hidden-states at the output of the last layer of the model.
         * @param {Tensor} [output.hidden_states] Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
         * @param {Tensor} [output.attentions] Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.
         */
        constructor({ last_hidden_state: T, hidden_states: N = null, attentions: ge = null }) {
          super(), this.last_hidden_state = T, this.hidden_states = N, this.attentions = ge;
        }
      }
      class ae extends te {
      }
      class Te extends ae {
      }
      class Ue extends ae {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class Ve extends ae {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Ne extends ae {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class Re extends ae {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class st extends te {
      }
      class dt extends st {
      }
      class ct extends st {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class lt extends st {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class ht extends st {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class L extends te {
      }
      class oe extends L {
      }
      class H extends te {
      }
      class me extends H {
      }
      class Ae extends H {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class We extends H {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Je extends H {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class ut extends H {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class mt extends te {
      }
      class vt extends mt {
      }
      class kt extends mt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class At extends mt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class is extends mt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class ys extends mt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class Cs extends te {
      }
      class Ds extends Cs {
      }
      class sr extends Cs {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class kr extends Cs {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Qr extends Cs {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class Us extends Cs {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class Tr extends te {
      }
      class Nt extends Tr {
      }
      class Xr extends Tr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class Sr extends Tr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class $r extends Tr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class Yr extends Tr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class dr extends te {
      }
      class Jr extends dr {
      }
      class Ar extends dr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class Br extends dr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Rr extends dr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class ar extends dr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class it extends te {
      }
      class Tt extends it {
      }
      class Dt extends it {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class Vs extends it {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Nr extends it {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class Ir extends it {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class Ms extends te {
      }
      class lr extends Ms {
      }
      class Fs extends Ms {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Pr extends Ms {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class es extends Ms {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class _n extends Ms {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class jr extends te {
      }
      class si extends jr {
      }
      class Sn extends jr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class $n extends jr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class An extends jr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class Ur extends te {
      }
      class In extends Ur {
      }
      class ri extends Ur {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class Vr extends Ur {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class mr extends Ur {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class ur extends te {
      }
      class gn extends ur {
      }
      class Zr extends ur {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class wn extends ur {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class yn extends ur {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class Mn extends ur {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class zt extends te {
      }
      class bn extends zt {
      }
      class Fn extends zt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class On extends zt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Dn extends zt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class Wr extends te {
      }
      class Ln extends Wr {
      }
      class vn extends Wr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class zn extends Wr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class as extends Wr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class Pe extends te {
        constructor() {
          super(...arguments);
          fe(this, "forward_params", [
            "input_ids",
            "attention_mask",
            "encoder_outputs",
            "decoder_input_ids",
            "decoder_attention_mask",
            "past_key_values"
          ]);
        }
      }
      class P extends Pe {
      }
      class Q extends Pe {
      }
      class ue extends te {
      }
      class ve extends ue {
      }
      class Se extends ue {
      }
      class Qe extends te {
      }
      class pt extends Qe {
      }
      class gt extends Qe {
      }
      class ft extends te {
      }
      class xt extends ft {
      }
      class Kt extends ft {
      }
      class ms extends ft {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class us extends te {
      }
      class Os extends us {
      }
      class Bt extends us {
      }
      class rs extends us {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class rr extends us {
      }
      class Ws extends te {
      }
      class ze extends Ws {
      }
      class Js extends Ws {
      }
      class Fr extends te {
      }
      class ks extends Fr {
      }
      class Xs extends Fr {
      }
      class Ft extends te {
      }
      class ir extends Ft {
      }
      class fr extends Ft {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class fs extends Ft {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Ss extends Ft {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class yt extends Ft {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class qt extends te {
      }
      class Ls extends qt {
      }
      class $s extends qt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class Gs extends qt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class $t extends qt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class en extends qt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class qe extends te {
      }
      class It extends qe {
      }
      class Oa extends qe {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new qs(await super._call(T));
        }
      }
      class Ri extends qe {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Da extends qe {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class La extends qe {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new tr(await super._call(T));
        }
      }
      class Yt extends te {
      }
      class za extends Yt {
      }
      class Ni extends Yt {
      }
      class ji extends te {
        constructor() {
          super(...arguments);
          fe(this, "requires_attention_mask", !1);
          fe(this, "main_input_name", "input_features");
          fe(this, "forward_params", [
            "input_features",
            "attention_mask",
            "decoder_input_ids",
            "decoder_attention_mask",
            "past_key_values"
          ]);
        }
      }
      class Ba extends ji {
      }
      class Ra extends ji {
        _prepare_generation_config(T, N) {
          return (
            /** @type {WhisperGenerationConfig} */
            super._prepare_generation_config(T, N, j.WhisperGenerationConfig)
          );
        }
        /**
         * 
         * @param {WhisperGenerationConfig} generation_config 
         */
        _retrieve_init_tokens(T) {
          const N = [T.decoder_start_token_id];
          let ge = T.language;
          const De = T.task;
          if (T.is_multilingual) {
            ge || (console.warn("No language specified - defaulting to English (en)."), ge = "en");
            const et = `<|${(0, q.whisper_language_to_code)(ge)}|>`;
            N.push(T.lang_to_id[et]), N.push(T.task_to_id[De ?? "transcribe"]);
          } else if (ge || De)
            throw new Error(
              "Cannot specify `task` or `language` for an English-only model. If the model is intended to be multilingual, pass `is_multilingual=true` to generate, or update the generation config."
            );
          return !T.return_timestamps && T.no_timestamps_token_id && N.at(-1) !== T.no_timestamps_token_id ? N.push(T.no_timestamps_token_id) : T.return_timestamps && N.at(-1) === T.no_timestamps_token_id && (console.warn("<|notimestamps|> prompt token is removed from generation_config since `return_timestamps` is set to `true`."), N.pop()), N.filter((Ie) => Ie != null);
        }
        /**
         * Transcribes or translates log-mel input features to a sequence of auto-regressively generated token ids.
         * @param {import('./models/whisper/generation_whisper.js').WhisperGenerationFunctionParameters} options
         * @returns {Promise<ModelOutput|Tensor>} The output of the model, which can contain the generated token ids, attentions, and scores.
         */
        async generate({
          inputs: T = null,
          generation_config: N = null,
          logits_processor: ge = null,
          stopping_criteria: De = null,
          // Whisper-specific options (passed to kwargs)
          // prompt_ids = null,
          // language = null,
          // task = null,
          ...Ie
        }) {
          N = this._prepare_generation_config(N, Ie);
          const et = Ie.decoder_input_ids ?? this._retrieve_init_tokens(N);
          if (N.return_timestamps && (ge ?? (ge = new M.LogitsProcessorList()), ge.push(
            new M.WhisperTimeStampLogitsProcessor(N, et)
          )), N.begin_suppress_tokens && (ge ?? (ge = new M.LogitsProcessorList()), ge.push(
            new M.SuppressTokensAtBeginLogitsProcessor(N.begin_suppress_tokens, et.length)
          )), N.return_token_timestamps) {
            if (!N.alignment_heads)
              throw new Error(
                "Model generation config has no `alignment_heads`, token-level timestamps not available. See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config."
              );
            N.task === "translate" && console.warn("Token-level timestamps may not be reliable for task 'translate'."), N.output_attentions = !0, N.return_dict_in_generate = !0;
          }
          const rt = await super.generate({
            inputs: T,
            generation_config: N,
            logits_processor: ge,
            decoder_input_ids: et,
            ...Ie
          });
          return N.return_token_timestamps && (rt.token_timestamps = this._extract_token_timestamps(
            // @ts-expect-error TS2345
            rt,
            N.alignment_heads,
            N.num_frames
          )), rt;
        }
        /**
         * Calculates token-level timestamps using the encoder-decoder cross-attentions and
         * dynamic time-warping (DTW) to map each output token to a position in the input audio.
         * If `num_frames` is specified, the encoder-decoder cross-attentions will be cropped before applying DTW.
         * @param {Object} generate_outputs Outputs generated by the model
         * @param {Tensor[][]} generate_outputs.cross_attentions The cross attentions output by the model
         * @param {Tensor} generate_outputs.sequences The sequences output by the model
         * @param {number[][]} alignment_heads Alignment heads of the model
         * @param {number} [num_frames=null] Number of frames in the input audio.
         * @param {number} [time_precision=0.02] Precision of the timestamps in seconds
         * @returns {Tensor} tensor containing the timestamps in seconds for each predicted token
         */
        _extract_token_timestamps(T, N, ge = null, De = 0.02) {
          if (!T.cross_attentions)
            throw new Error(
              "Model outputs must contain cross attentions to extract timestamps. This is most likely because the model was not exported with `output_attentions=True`."
            );
          ge == null && console.warn(
            "`num_frames` has not been set, meaning the entire audio will be analyzed. This may lead to inaccurate token-level timestamps for short audios (< 30 seconds)."
          );
          let Ie = this.config.median_filter_width;
          Ie === void 0 && (console.warn("Model config has no `median_filter_width`, using default value of 7."), Ie = 7);
          const et = T.cross_attentions, rt = Array.from(
            { length: this.config.decoder_layers },
            // Concatenate the cross attentions for each layer across sequence length dimension.
            (ns, Jt) => (0, b.cat)(et.map((os) => os[Jt]), 2)
          ), _t = (0, b.stack)(N.map(([ns, Jt]) => {
            if (ns >= rt.length)
              throw new Error(`Layer index ${ns} is out of bounds for cross attentions (length ${rt.length}).`);
            return ge ? rt[ns].slice(null, Jt, null, [0, ge]) : rt[ns].slice(null, Jt);
          })).transpose(1, 0, 2, 3), [Mt, jt] = (0, b.std_mean)(_t, -2, 0, !0), Vt = _t.clone();
          for (let ns = 0; ns < Vt.dims[0]; ++ns) {
            const Jt = Vt[ns];
            for (let os = 0; os < Jt.dims[0]; ++os) {
              const As = Jt[os], xs = Mt[ns][os][0].data, cs = jt[ns][os][0].data;
              for (let Es = 0; Es < As.dims[0]; ++Es) {
                let Is = As[Es].data;
                for (let Zs = 0; Zs < Is.length; ++Zs)
                  Is[Zs] = (Is[Zs] - cs[Zs]) / xs[Zs];
                Is.set((0, K.medianFilter)(Is, Ie));
              }
            }
          }
          const Lt = [(0, b.mean)(Vt, 1)], Gt = T.sequences.dims, ts = new b.Tensor(
            "float32",
            new Float32Array(Gt[0] * Gt[1]),
            Gt
          );
          for (let ns = 0; ns < Gt[0]; ++ns) {
            const Jt = Lt[ns].neg().squeeze_(0), [os, As] = (0, K.dynamic_time_warping)(Jt.tolist()), xs = Array.from({ length: os.length - 1 }, (Is, Zs) => os[Zs + 1] - os[Zs]), cs = (0, R.mergeArrays)([1], xs).map((Is) => !!Is), Es = [];
            for (let Is = 0; Is < cs.length; ++Is)
              cs[Is] && Es.push(As[Is] * De);
            ts[ns].data.set(Es, 1);
          }
          return ts;
        }
      }
      class Ui extends te {
        constructor() {
          super(...arguments);
          fe(this, "requires_attention_mask", !1);
          fe(this, "main_input_name", "input_values");
          fe(this, "forward_params", [
            "input_values",
            "decoder_input_ids",
            "past_key_values"
          ]);
        }
      }
      class cr extends Ui {
      }
      class Na extends Ui {
      }
      class Vi extends te {
        constructor() {
          super(...arguments);
          fe(this, "main_input_name", "pixel_values");
          fe(this, "forward_params", [
            // Encoder inputs
            "pixel_values",
            // Decoder inpputs
            "decoder_input_ids",
            "encoder_hidden_states",
            "past_key_values"
          ]);
        }
      }
      class Gr extends te {
        constructor() {
          super(...arguments);
          fe(this, "forward_params", [
            "input_ids",
            "attention_mask",
            "pixel_values",
            "position_ids",
            "past_key_values"
          ]);
        }
      }
      class ni extends Gr {
        _merge_input_ids_with_image_features({
          inputs_embeds: T,
          image_features: N,
          input_ids: ge,
          attention_mask: De
        }) {
          const Ie = this.config.image_token_index, rt = ge.tolist().map((Lt) => Lt.findIndex((Gt) => Gt == Ie)), _t = rt.every((Lt) => Lt === -1), Mt = rt.every((Lt) => Lt !== -1);
          if (!_t && !Mt)
            throw new Error("Every input should contain either 0 or 1 image token.");
          if (_t)
            return {
              inputs_embeds: T,
              attention_mask: De
            };
          const jt = [], Vt = [];
          for (let Lt = 0; Lt < rt.length; ++Lt) {
            const Gt = rt[Lt], ts = T[Lt], ns = N[Lt], Jt = De[Lt];
            jt.push(
              (0, b.cat)([
                ts.slice([0, Gt]),
                ns,
                ts.slice([Gt + 1, ts.dims[0]])
              ], 0)
            ), Vt.push(
              (0, b.cat)([
                Jt.slice([0, Gt]),
                (0, b.ones)([ns.dims[0]]),
                Jt.slice([Gt + 1, Jt.dims[0]])
              ], 0)
            );
          }
          return {
            inputs_embeds: (0, b.stack)(jt, 0),
            attention_mask: (0, b.stack)(Vt, 0)
          };
        }
      }
      class ja extends ni {
      }
      class Ua extends ni {
      }
      class Va extends te {
        constructor() {
          super(...arguments);
          fe(this, "forward_params", [
            // Encoder inputs
            "input_ids",
            "inputs_embeds",
            "attention_mask",
            "pixel_values",
            // Decoder inputs
            "encoder_outputs",
            "decoder_input_ids",
            "decoder_inputs_embeds",
            "decoder_attention_mask",
            "past_key_values"
          ]);
          fe(this, "main_input_name", "inputs_embeds");
        }
      }
      class Wa extends Va {
        _merge_input_ids_with_image_features({
          inputs_embeds: T,
          image_features: N,
          input_ids: ge,
          attention_mask: De
        }) {
          return {
            inputs_embeds: (0, b.cat)([
              N,
              // image embeds
              T
              // task prefix embeds
            ], 1),
            attention_mask: (0, b.cat)([
              (0, b.ones)(N.dims.slice(0, 2)),
              // image attention mask
              De
              // task prefix attention mask
            ], 1)
          };
        }
        async _prepare_inputs_embeds({ input_ids: T, pixel_values: N, inputs_embeds: ge, attention_mask: De }) {
          if (!T && !N)
            throw new Error("Either `input_ids` or `pixel_values` should be provided.");
          let Ie, et;
          return T && (Ie = await this.encode_text({ input_ids: T })), N && (et = await this.encode_image({ pixel_values: N })), Ie && et ? { inputs_embeds: ge, attention_mask: De } = this._merge_input_ids_with_image_features({
            inputs_embeds: Ie,
            image_features: et,
            input_ids: T,
            attention_mask: De
          }) : ge = Ie || et, { inputs_embeds: ge, attention_mask: De };
        }
        async forward({
          input_ids: T,
          pixel_values: N,
          attention_mask: ge,
          decoder_input_ids: De,
          decoder_attention_mask: Ie,
          encoder_outputs: et,
          past_key_values: rt,
          inputs_embeds: _t,
          decoder_inputs_embeds: Mt
        }) {
          if (_t || ({ inputs_embeds: _t, attention_mask: ge } = await this._prepare_inputs_embeds({ input_ids: T, pixel_values: N, inputs_embeds: _t, attention_mask: ge })), !et) {
            let { last_hidden_state: Lt } = await Fe(this, { inputs_embeds: _t, attention_mask: ge });
            et = Lt;
          }
          if (!Mt) {
            if (!De)
              throw new Error("Either `decoder_input_ids` or `decoder_inputs_embeds` should be provided.");
            Mt = await this.encode_text({ input_ids: De });
          }
          return await Ee(this, {
            inputs_embeds: Mt,
            attention_mask: Ie,
            encoder_attention_mask: ge,
            encoder_hidden_states: et,
            past_key_values: rt
          }, !0);
        }
      }
      class Ga extends te {
        constructor() {
          super(...arguments);
          fe(this, "forward_params", [
            "input_ids",
            // 'inputs_embeds',
            "attention_mask",
            "pixel_values",
            "position_ids",
            "past_key_values"
          ]);
        }
      }
      class Ka extends Ga {
        _merge_input_ids_with_image_features(T) {
          const N = T.image_features.dims.at(-1), ge = T.image_features.view(-1, N);
          return tt({
            // @ts-ignore
            image_token_id: this.config.image_token_index,
            ...T,
            image_features: ge
          });
        }
      }
      class Ha extends te {
        constructor() {
          super(...arguments);
          fe(this, "forward_params", [
            "input_ids",
            "attention_mask",
            "pixel_values",
            "pixel_attention_mask",
            "position_ids",
            "past_key_values"
          ]);
        }
      }
      class Wi extends Ha {
        async encode_image({ pixel_values: T, pixel_attention_mask: N }) {
          return (await _e(this.sessions.vision_encoder, { pixel_values: T, pixel_attention_mask: N })).image_features;
        }
        _merge_input_ids_with_image_features(T) {
          const N = T.image_features.dims.at(-1), ge = T.image_features.view(-1, N);
          return tt({
            // @ts-ignore
            image_token_id: this.config.image_token_id,
            ...T,
            image_features: ge
          });
        }
      }
      class qa extends te {
        constructor() {
          super(...arguments);
          fe(this, "forward_params", [
            "input_ids",
            "inputs_embeds",
            "attention_mask",
            "position_ids",
            "pixel_values",
            "image_sizes",
            "past_key_values"
          ]);
        }
      }
      class pr extends qa {
        async forward({
          // Produced by the tokenizer/processor:
          input_ids: T = null,
          attention_mask: N = null,
          pixel_values: ge = null,
          image_sizes: De = null,
          // Used during generation:
          position_ids: Ie = null,
          inputs_embeds: et = null,
          past_key_values: rt = null,
          // Generic generation parameters
          generation_config: _t = null,
          logits_processor: Mt = null,
          // TODO: needed?
          ...jt
        }) {
          if (!et) {
            let Lt;
            if (ge && T.dims[1] !== 1) {
              if (!De)
                throw new Error("`image_sizes` must be provided when `pixel_values` is provided.");
              ({ image_features: Lt } = await _e(this.sessions.vision_encoder, {
                pixel_values: ge,
                image_sizes: De
              }));
            } else {
              const Gt = this.config.normalized_config.hidden_size;
              Lt = new b.Tensor(
                "float32",
                [],
                [0, Gt]
              );
            }
            ({ inputs_embeds: et } = await _e(this.sessions.prepare_inputs_embeds, {
              input_ids: T,
              image_features: Lt
            }));
          }
          return await Ee(this, {
            inputs_embeds: et,
            past_key_values: rt,
            attention_mask: N,
            position_ids: Ie,
            generation_config: _t,
            logits_processor: Mt
          }, !1);
        }
      }
      class tn extends te {
      }
      class Qa extends tn {
      }
      class Nc extends tn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "text_model"
          });
        }
      }
      class Xa extends tn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "text_model"
          });
        }
      }
      class jc extends tn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "vision_model"
          });
        }
      }
      class Ya extends tn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "vision_model"
          });
        }
      }
      class Gi extends te {
      }
      class Ja extends Gi {
      }
      class Za extends Gi {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "text_model"
          });
        }
      }
      class el extends tn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "vision_model"
          });
        }
      }
      class Uc extends te {
      }
      class _r extends Uc {
      }
      class ii extends te {
      }
      class oi extends ii {
        async forward(T) {
          const N = !T.input_ids, ge = !T.pixel_values;
          if (N && ge)
            throw new Error("Either `input_ids` or `pixel_values` should be provided.");
          if (N && (T.input_ids = (0, b.ones)([T.pixel_values.dims[0], 1])), ge) {
            const { image_size: Mt } = this.config.vision_config;
            T.pixel_values = (0, b.full)([0, 3, Mt, Mt], 0);
          }
          const { text_embeddings: De, image_embeddings: Ie, l2norm_text_embeddings: et, l2norm_image_embeddings: rt } = await super.forward(T), _t = {};
          return N || (_t.text_embeddings = De, _t.l2norm_text_embeddings = et), ge || (_t.image_embeddings = Ie, _t.l2norm_image_embeddings = rt), _t;
        }
      }
      class Ki extends ii {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "text_model"
          });
        }
      }
      class gr extends ii {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "vision_model"
          });
        }
      }
      class Hi extends te {
      }
      class tl extends Hi {
      }
      class sl extends Hi {
      }
      class qi extends te {
      }
      class rl extends qi {
      }
      class nl extends qi {
      }
      class Qi extends te {
      }
      class il extends Qi {
      }
      class ol extends Qi {
      }
      class Xi extends te {
      }
      class wr extends Xi {
      }
      class al extends Xi {
      }
      class Yi extends te {
      }
      class ll extends Yi {
      }
      class ul extends Yi {
      }
      class Ji extends te {
      }
      class dl extends Ji {
      }
      class cl extends Ji {
      }
      class Zi extends te {
      }
      class pl extends Zi {
      }
      class hl extends Zi {
      }
      class ai extends te {
      }
      class eo extends ai {
      }
      class ml extends ai {
      }
      class to extends te {
      }
      class so extends to {
      }
      class Vc extends to {
      }
      class ro extends te {
      }
      class li extends ro {
      }
      class fl extends ro {
      }
      class no extends te {
      }
      class _l extends no {
      }
      class Bn extends no {
      }
      class io extends te {
      }
      class gl extends io {
      }
      class oo extends io {
      }
      class ao extends te {
      }
      class wl extends ao {
      }
      class yl extends ao {
      }
      class lo extends te {
      }
      class Wc extends lo {
      }
      class Ml extends lo {
      }
      class uo extends te {
      }
      class bl extends uo {
      }
      class Gc extends uo {
      }
      class co extends te {
      }
      class ds extends co {
      }
      class vl extends co {
      }
      class po extends te {
      }
      class xl extends po {
      }
      class Tl extends po {
      }
      class ho extends te {
      }
      class Pl extends ho {
      }
      class El extends ho {
      }
      class mo extends te {
      }
      class Cl extends mo {
      }
      class kl extends mo {
      }
      class fo extends te {
      }
      class Sl extends fo {
      }
      class $l extends fo {
      }
      class _o extends te {
      }
      class Rn extends _o {
      }
      class Al extends _o {
      }
      class ui extends te {
        constructor() {
          super(...arguments);
          fe(this, "forward_params", [
            // Text inputs
            "input_ids",
            "attention_mask",
            "position_ids",
            "past_key_values",
            // Vision inputs
            "pixel_values",
            "image_grid_thw"
          ]);
        }
      }
      class Il extends ui {
        /**
         * Calculate the 3D rope index based on image and video's temporal, height and width in LLM.
         *
         * Explanation:
         *     Each embedding sequence contains vision embedding and text embedding or just contains text embedding.
         *
         *     For pure text embedding sequence, the rotary position embedding has no difference with mordern LLMs.
         *     Examples:
         *         input_ids: [T T T T T], here T is for text.
         *         temporal position_ids: [0, 1, 2, 3, 4]
         *         height position_ids: [0, 1, 2, 3, 4]
         *         width position_ids: [0, 1, 2, 3, 4]
         *
         *     For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part
         *     and 1D rotary position embeddin for text part.
         *     Examples:
         *         Assume we have a video input with 3 temporal patches, 2 height patches and 2 width patches.
         *         input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
         *         vision temporal position_ids: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]
         *         vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
         *         vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
         *         text temporal position_ids: [3, 4, 5, 6, 7]
         *         text height position_ids: [3, 4, 5, 6, 7]
         *         text width position_ids: [3, 4, 5, 6, 7]
         *         Here we calculate the text start position_ids as the max vision position_ids plus 1.
         * 
         * @param {Tensor} input_ids Indices of input sequence tokens in the vocabulary. Tensor of shape `(batch_size, sequence_length)`.
         * @param {Tensor} image_grid_thw (Optional) The temporal, height and width of feature shape of each image in LLM. Tensor of shape `(num_images, 3)`.
         * @param {Tensor} video_grid_thw (Optional) The temporal, height and width of feature shape of each video in LLM. Tensor of shape `(num_videos, 3)`.
         * @param {Tensor} attention_mask (Optional) Mask to avoid performing attention on padding token indices. Tensor of shape `(batch_size, sequence_length)`. Mask values selected in `[0, 1]`:
         * - 1 for tokens that are **not masked**,
         * - 0 for tokens that are **masked**.
         * @returns {[Tensor, Tensor]} [position_ids, mrope_position_deltas] with:
         * - position_ids: Tensor of shape `(3, batch_size, sequence_length)`.
         * - mrope_position_deltas: Tensor of shape `(batch_size)`.
         */
        get_rope_index(T, N, ge, De) {
          const { vision_config: Ie, image_token_id: et, video_token_id: rt, vision_start_token_id: _t } = this.config, Mt = Ie.spatial_merge_size ?? 2, jt = [];
          if (N || ge) {
            let Vt = T.tolist();
            De || (De = (0, b.ones_like)(T));
            const Lt = De.tolist(), Gt = Array.from({ length: 3 }, (As) => Array.from({ length: T.dims[0] }, (xs) => Array.from({ length: T.dims[1] }, (cs) => 1))), ts = N ? N.tolist() : [], ns = ge ? ge.tolist() : [];
            let Jt = 0, os = 0;
            for (let As = 0; As < Vt.length; ++As) {
              const xs = Vt[As].filter((Ts, Rs) => Lt[As][Rs] == 1), Es = xs.reduce((Ts, Rs, Hr) => (Rs == _t && Ts.push(Hr), Ts), []).map((Ts) => xs[Ts + 1]), Is = Es.filter((Ts) => Ts == et).length, Zs = Es.filter((Ts) => Ts == rt).length;
              let Ys = [], Mr = 0, dn = Is, Ea = Zs;
              for (let Ts = 0; Ts < Es.length; ++Ts) {
                const Rs = xs.findIndex((qr, hr) => hr > Mr && qr == et), Hr = xs.findIndex((qr, hr) => hr > Mr && qr == rt), Tn = dn > 0 && Rs !== -1 ? Rs : xs.length + 1, Pn = Ea > 0 && Hr !== -1 ? Hr : xs.length + 1;
                let En, ka, Sa, Tc;
                Tn < Pn ? ([ka, Sa, Tc] = ts[Jt], ++Jt, --dn, En = Tn) : ([ka, Sa, Tc] = ns[os], ++os, --Ea, En = Pn);
                const [Sp, Ai, Kn] = [
                  Number(ka),
                  Math.floor(Number(Sa) / Mt),
                  Math.floor(Number(Tc) / Mt)
                ], Lr = En - Mr, Pc = Ys.length > 0 ? (0, K.max)(Ys.at(-1))[0] + 1 : 0;
                Ys.push(
                  Array.from({ length: 3 * Lr }, (qr, hr) => Pc + hr % Lr)
                );
                const Hn = Lr + Pc, qn = Sp * Ai * Kn, Ec = Array.from({ length: qn }, (qr, hr) => Hn + Math.floor(hr / (Ai * Kn))), Cc = Array.from({ length: qn }, (qr, hr) => Hn + Math.floor(hr / Kn) % Ai), kc = Array.from({ length: qn }, (qr, hr) => Hn + hr % Kn);
                Ys.push([Ec, Cc, kc].flat()), Mr = En + qn;
              }
              if (Mr < xs.length) {
                const Ts = Ys.length > 0 ? (0, K.max)(Ys.at(-1))[0] + 1 : 0, Rs = xs.length - Mr;
                Ys.push(
                  Array.from({ length: 3 * Rs }, (Hr, Tn) => Ts + Tn % Rs)
                );
              }
              const or = Ys.reduce((Ts, Rs) => Ts + Rs.length, 0), br = new Array(or);
              let ki = 0;
              for (let Ts = 0; Ts < 3; ++Ts)
                for (let Rs = 0; Rs < Ys.length; ++Rs) {
                  const Hr = Ys[Rs], Tn = Hr.length / 3;
                  for (let Pn = Ts * Tn; Pn < (Ts + 1) * Tn; ++Pn)
                    br[ki++] = Hr[Pn];
                }
              let Si = 0;
              const $i = Lt[As];
              for (let Ts = 0; Ts < $i.length; ++Ts)
                if ($i[Ts] == 1) {
                  for (let Rs = 0; Rs < 3; ++Rs)
                    Gt[Rs][As][Ts] = br[Rs * or / 3 + Si];
                  ++Si;
                }
              const Ca = (0, K.max)(br)[0];
              jt.push(Ca + 1 - Vt[As].length);
            }
            return [
              new b.Tensor("int64", Gt.flat(1 / 0), [3, T.dims[0], T.dims[1]]),
              new b.Tensor("int64", jt, [jt.length, 1])
            ];
          } else if (De) {
            const { data: Vt, dims: Lt } = ye(De), Gt = BigInt64Array.from(
              { length: 3 * Vt.length },
              (ns, Jt) => Vt[Jt % Vt.length]
            ), ts = Array.from(
              { length: Lt[0] },
              (ns, Jt) => (0, K.max)(Vt.subarray(Lt[1] * Jt, Lt[1] * (Jt + 1)))[0] + 1n + BigInt(Lt[1])
            );
            return [
              new b.Tensor("int64", Gt, [3, ...Lt]),
              new b.Tensor("int64", ts, [ts.length, 1])
            ];
          } else {
            const [Vt, Lt] = T.dims, Gt = BigInt64Array.from(
              { length: 3 * Vt * Lt },
              (ts, ns) => BigInt(Math.floor(ns % Lt / Vt))
            );
            return [
              new b.Tensor("int64", Gt, [3, ...T.dims]),
              (0, b.zeros)([Vt, 1])
            ];
          }
        }
        async encode_image({ pixel_values: T, image_grid_thw: N }) {
          return (await _e(this.sessions.vision_encoder, { pixel_values: T, grid_thw: N })).image_features;
        }
        _merge_input_ids_with_image_features(T) {
          return tt({
            // @ts-ignore
            image_token_id: this.config.image_token_id,
            ...T
          });
        }
        prepare_inputs_for_generation(T, N, ge) {
          if (N.attention_mask && !N.position_ids)
            if (!N.past_key_values)
              [N.position_ids, N.rope_deltas] = this.get_rope_index(
                N.input_ids,
                N.image_grid_thw,
                N.video_grid_thw,
                N.attention_mask
              );
            else {
              N.pixel_values = null;
              const De = BigInt(Object.values(N.past_key_values)[0].dims.at(-2)), Ie = N.rope_deltas.map((et) => De + et);
              N.position_ids = (0, b.stack)([Ie, Ie, Ie], 0);
            }
          return N;
        }
      }
      class go extends te {
      }
      class Fl extends go {
      }
      class Ol extends go {
      }
      class wo extends te {
      }
      class Dl extends wo {
      }
      class Ll extends wo {
      }
      class yo extends te {
      }
      class zl extends yo {
      }
      class Bl extends yo {
      }
      class Mo extends te {
      }
      class Rl extends Mo {
      }
      class Nl extends Mo {
      }
      class bo extends te {
      }
      class jl extends bo {
      }
      class Ul extends bo {
      }
      class di extends te {
      }
      class Vl extends di {
      }
      class vo extends di {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class ci extends te {
      }
      class Wl extends ci {
      }
      class Gl extends ci {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Kl extends te {
      }
      class Hl extends Kl {
      }
      class xo extends te {
      }
      class ql extends xo {
      }
      class To extends xo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Ql extends te {
      }
      class Xl extends Ql {
      }
      class Po extends te {
      }
      class Kc extends Po {
      }
      class Yl extends Po {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Jl extends te {
      }
      class yr extends Jl {
      }
      class Eo extends te {
      }
      class Zl extends Eo {
      }
      class eu extends Eo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class tu extends te {
      }
      class su extends tu {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new xc(await super._call(T));
        }
      }
      class Co extends te {
      }
      class ru extends Co {
      }
      class nu extends Co {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class ko extends te {
      }
      class iu extends ko {
      }
      class Hc extends ko {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class So extends te {
      }
      class ou extends So {
      }
      class au extends So {
      }
      class $o extends te {
      }
      class lu extends $o {
      }
      class uu extends $o {
      }
      class du extends te {
      }
      class sn extends du {
      }
      class rn extends du {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Or extends te {
      }
      class Ao extends Or {
      }
      class nn extends Or {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Io(await super._call(T));
        }
      }
      class Ks extends Or {
        /**
         * Runs the model with the provided inputs
         * @param {Object} model_inputs Model inputs
         * @returns {Promise<DetrSegmentationOutput>} Object containing segmentation outputs
         */
        async _call(T) {
          return new Fo(await super._call(T));
        }
      }
      class Io extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification logits (including no-object) for all queries.
         * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).
         * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).
         */
        constructor({ logits: T, pred_boxes: N }) {
          super(), this.logits = T, this.pred_boxes = N;
        }
      }
      class Fo extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits The output logits of the model.
         * @param {Tensor} output.pred_boxes Predicted boxes.
         * @param {Tensor} output.pred_masks Predicted masks.
         */
        constructor({ logits: T, pred_boxes: N, pred_masks: ge }) {
          super(), this.logits = T, this.pred_boxes = N, this.pred_masks = ge;
        }
      }
      class Oo extends te {
      }
      class qc extends Oo {
      }
      class Nn extends Oo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Do(await super._call(T));
        }
      }
      class Do extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification logits (including no-object) for all queries.
         * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).
         * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).
         */
        constructor({ logits: T, pred_boxes: N }) {
          super(), this.logits = T, this.pred_boxes = N;
        }
      }
      class pi extends te {
      }
      class cu extends pi {
      }
      class pu extends pi {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Lo(await super._call(T));
        }
      }
      class Lo extends Io {
      }
      class hi extends te {
      }
      class hu extends hi {
      }
      class zo extends hi {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Bo extends te {
      }
      class mi extends Bo {
      }
      class Ro extends Bo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class No extends te {
      }
      class mu extends No {
      }
      class Qc extends No {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class jo extends te {
      }
      class Uo extends jo {
      }
      class jn extends jo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Vo extends te {
      }
      class Wo extends Vo {
      }
      class fu extends Vo {
      }
      class Go extends te {
      }
      class _u extends Go {
      }
      class Xc extends Go {
      }
      class gu extends te {
      }
      class wu extends gu {
      }
      class Ko extends te {
      }
      class yu extends Ko {
      }
      class fi extends Ko {
      }
      class Mu extends Ko {
      }
      class _i extends te {
      }
      class Ho extends _i {
      }
      class gi extends te {
      }
      class bu extends gi {
      }
      class vu extends gi {
      }
      class wi extends te {
      }
      class Yc extends wi {
      }
      class xu extends wi {
      }
      class Jc extends te {
      }
      class Tu extends Jc {
      }
      class qo extends te {
      }
      class Pu extends qo {
      }
      class Qo extends qo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Xo extends te {
      }
      class Eu extends Xo {
      }
      class Yo extends Xo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Jo extends te {
      }
      class Cu extends Jo {
      }
      class Zc extends Jo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Zo extends te {
      }
      class ku extends Zo {
      }
      class Su extends Zo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class ep extends te {
      }
      class $u extends ep {
      }
      class ea extends te {
      }
      class Au extends ea {
      }
      class Iu extends ea {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Fu(await super._call(T));
        }
      }
      class Fu extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification logits (including no-object) for all queries.
         * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).
         * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).
         */
        constructor({ logits: T, pred_boxes: N }) {
          super(), this.logits = T, this.pred_boxes = N;
        }
      }
      class tp extends te {
      }
      class yi extends tp {
        /**
         * Compute image embeddings and positional image embeddings, given the pixel values of an image.
         * @param {Object} model_inputs Object containing the model inputs.
         * @param {Tensor} model_inputs.pixel_values Pixel values obtained using a `SamProcessor`.
         * @returns {Promise<{ image_embeddings: Tensor, image_positional_embeddings: Tensor }>} The image embeddings and positional image embeddings.
         */
        async get_image_embeddings({ pixel_values: T }) {
          return await Fe(this, { pixel_values: T });
        }
        /**
         * @typedef {Object} SamModelInputs Object containing the model inputs.
         * @property {Tensor} pixel_values Pixel values as a Tensor with shape `(batch_size, num_channels, height, width)`.
         * These can be obtained using a `SamProcessor`.
         * @property {Tensor} [input_points] Input 2D spatial points with shape `(batch_size, num_points, 2)`.
         * This is used by the prompt encoder to encode the prompt.
         * @property {Tensor} [input_labels] Input labels for the points, as a Tensor of shape `(batch_size, point_batch_size, num_points)`.
         * This is used by the prompt encoder to encode the prompt. There are 4 types of labels:
         *  - `1`: the point is a point that contains the object of interest
         *  - `0`: the point is a point that does not contain the object of interest
         *  - `-1`: the point corresponds to the background
         *  - `-10`: the point is a padding point, thus should be ignored by the prompt encoder
         * @property {Tensor} [input_boxes] Input bounding boxes with shape `(batch_size, num_boxes, 4)`.
         * @property {Tensor} [image_embeddings] Image embeddings used by the mask decoder.
         * @property {Tensor} [image_positional_embeddings] Image positional embeddings used by the mask decoder.
         */
        /**
         * @param {SamModelInputs} model_inputs Object containing the model inputs.
         * @returns {Promise<Object>} The output of the model.
         */
        async forward(T) {
          if ((!T.image_embeddings || !T.image_positional_embeddings) && (T = {
            ...T,
            ...await this.get_image_embeddings(T)
          }), !T.input_labels && T.input_points) {
            const ge = T.input_points.dims.slice(0, -1), De = ge.reduce((Ie, et) => Ie * et, 1);
            T.input_labels = new b.Tensor(
              "int64",
              new BigInt64Array(De).fill(1n),
              ge
            );
          }
          const N = {
            image_embeddings: T.image_embeddings,
            image_positional_embeddings: T.image_positional_embeddings
          };
          return T.input_points && (N.input_points = T.input_points), T.input_labels && (N.input_labels = T.input_labels), T.input_boxes && (N.input_boxes = T.input_boxes), await _e(this.sessions.prompt_encoder_mask_decoder, N);
        }
        /**
         * Runs the model with the provided inputs
         * @param {Object} model_inputs Model inputs
         * @returns {Promise<SamImageSegmentationOutput>} Object containing segmentation outputs
         */
        async _call(T) {
          return new Un(await super._call(T));
        }
      }
      class Un extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.iou_scores The output logits of the model.
         * @param {Tensor} output.pred_masks Predicted boxes.
         */
        constructor({ iou_scores: T, pred_masks: N }) {
          super(), this.iou_scores = T, this.pred_masks = N;
        }
      }
      class Mi extends te {
      }
      class Ou extends Mi {
      }
      class Du extends Mi {
      }
      class ta extends te {
      }
      class Lu extends ta {
      }
      class sa extends ta {
      }
      class Kr extends te {
      }
      class zu extends Kr {
      }
      class Bu extends Kr {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new un(await super._call(T));
        }
      }
      class sp extends Kr {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Ru extends Kr {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class bi extends te {
      }
      class Nu extends bi {
      }
      class ju extends bi {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class Uu extends te {
      }
      class rp extends Uu {
      }
      class vi extends te {
      }
      class Vu extends vi {
      }
      class np extends vi {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new un(await super._call(T));
        }
      }
      class Wu extends vi {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Vn extends te {
      }
      class Gu extends Vn {
      }
      class Ku extends Vn {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new un(await super._call(T));
        }
      }
      class ip extends Vn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Hu extends Vn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class xi extends te {
      }
      class qu extends xi {
      }
      class op extends xi {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_features Float values of input mel-spectrogram.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new un(await super._call(T));
        }
      }
      class Qu extends xi {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class ap extends te {
      }
      class Xu extends Kr {
      }
      class Yu extends Kr {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new un(await super._call(T));
        }
      }
      class lp extends Kr {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class xn extends te {
      }
      class Ju extends xn {
      }
      class Zu extends xn {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new un(await super._call(T));
        }
      }
      class ed extends xn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class up extends xn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<XVectorOutput>} An object containing the model's output logits and speaker embeddings.
         */
        async _call(T) {
          return new Gn(await super._call(T));
        }
      }
      class td extends xn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Hs(await super._call(T));
        }
      }
      class sd extends te {
      }
      class rd extends sd {
      }
      class Ti extends te {
      }
      class qp extends Ti {
      }
      class Er extends Ti {
      }
      class Dr extends Ti {
        /**
         * @typedef {Object} SpeechOutput
         * @property {Tensor} [spectrogram] The predicted log-mel spectrogram of shape
         * `(output_sequence_length, config.num_mel_bins)`. Returned when no `vocoder` is provided
         * @property {Tensor} [waveform] The predicted waveform of shape `(num_frames,)`. Returned when a `vocoder` is provided.
         * @property {Tensor} [cross_attentions] The outputs of the decoder's cross-attention layers of shape
         * `(config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)`. returned when `output_cross_attentions` is `true`.
         */
        /**
         * Converts a sequence of input tokens into a sequence of mel spectrograms, which are subsequently turned into a speech waveform using a vocoder.
         * @param {Tensor} input_values Indices of input sequence tokens in the vocabulary.
         * @param {Tensor} speaker_embeddings Tensor containing the speaker embeddings.
         * @param {Object} options Optional parameters for generating speech.
         * @param {number} [options.threshold=0.5] The generated sequence ends when the predicted stop token probability exceeds this value.
         * @param {number} [options.minlenratio=0.0] Used to calculate the minimum required length for the output sequence.
         * @param {number} [options.maxlenratio=20.0] Used to calculate the maximum allowed length for the output sequence.
         * @param {Object} [options.vocoder=null] The vocoder that converts the mel spectrogram into a speech waveform. If `null`, the output is the mel spectrogram.
         * @param {boolean} [options.output_cross_attentions=false] Whether or not to return the attentions tensors of the decoder's cross-attention layers.
         * @returns {Promise<SpeechOutput>} A promise which resolves to an object containing the spectrogram, waveform, and cross-attention tensors.
         */
        async generate_speech(T, N, {
          threshold: ge = 0.5,
          minlenratio: De = 0,
          maxlenratio: Ie = 20,
          vocoder: et = null
          // output_cross_attentions = false, // TODO add
        } = {}) {
          const rt = {
            input_ids: T
          }, { encoder_outputs: _t, encoder_attention_mask: Mt } = await Fe(this, rt), jt = _t.dims[1] / this.config.reduction_factor, Vt = Math.floor(jt * Ie), Lt = Math.floor(jt * De), Gt = this.config.num_mel_bins;
          let ts = [], ns = null, Jt = null, os = 0;
          for (; ; ) {
            ++os;
            const cs = ce(!!Jt);
            let Es;
            Jt ? Es = Jt.output_sequence_out : Es = new b.Tensor(
              "float32",
              new Float32Array(Gt),
              [1, 1, Gt]
            );
            let Is = {
              use_cache_branch: cs,
              output_sequence: Es,
              encoder_attention_mask: Mt,
              speaker_embeddings: N,
              encoder_hidden_states: _t
            };
            this.addPastKeyValues(Is, ns), Jt = await _e(this.sessions.decoder_model_merged, Is), ns = this.getPastKeyValues(Jt, ns);
            const { prob: Zs, spectrum: Ys } = Jt;
            if (ts.push(Ys), os >= Lt && // Finished when stop token or maximum length is reached.
            (Array.from(Zs.data).filter((Mr) => Mr >= ge).length > 0 || os >= Vt))
              break;
          }
          const As = (0, b.cat)(ts), { waveform: xs } = await _e(et.sessions.model, { spectrogram: As });
          return {
            spectrogram: As,
            waveform: xs
            // cross_attentions: null, // TODO add
          };
        }
      }
      class on extends te {
        constructor() {
          super(...arguments);
          fe(this, "main_input_name", "spectrogram");
        }
      }
      class an extends te {
      }
      class nd extends an {
      }
      class ra extends te {
      }
      class id extends ra {
      }
      class od extends ra {
      }
      class na extends te {
      }
      class ad extends na {
      }
      class ld extends na {
      }
      class ia extends te {
      }
      class ud extends ia {
      }
      class dd extends ia {
      }
      class oa extends te {
      }
      class nr extends oa {
      }
      class cd extends oa {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "text_model"
          });
        }
      }
      class pd extends oa {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, N = {}) {
          return super.from_pretrained(T, {
            ...N,
            // Update default model file name if not provided
            model_file_name: N.model_file_name ?? "audio_model"
          });
        }
      }
      class aa extends te {
      }
      class la extends aa {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<VitsModelOutput>} The outputs for the VITS model.
         */
        async _call(T) {
          return new kp(await super._call(T));
        }
      }
      class ln extends te {
      }
      class dp extends ln {
      }
      class hd extends ln {
      }
      class md extends ln {
      }
      class ua extends te {
      }
      class fd extends ua {
      }
      class _d extends ua {
      }
      class Pi extends te {
      }
      class gd extends Pi {
      }
      class wd extends Pi {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class da extends te {
      }
      class cp extends da {
      }
      class pp extends da {
      }
      class Ei extends te {
        constructor() {
          super(...arguments);
          // NOTE: not MusicgenPreTrainedModel
          fe(this, "forward_params", [
            "input_ids",
            "attention_mask",
            "encoder_outputs",
            "decoder_input_ids",
            "decoder_attention_mask",
            "past_key_values"
          ]);
        }
        /**
         * Apply the pattern mask to the final ids,
         * then revert the pattern delay mask by filtering the pad token id in a single step.
         * @param {Tensor} outputs The output tensor from the model.
         * @returns {Tensor} The filtered output tensor.
         */
        _apply_and_filter_by_delay_pattern_mask(N) {
          const [ge, De] = N.dims, Ie = this.config.decoder.num_codebooks, et = De - Ie;
          let rt = 0;
          for (let jt = 0; jt < N.size; ++jt) {
            if (N.data[jt] === this.config.decoder.pad_token_id)
              continue;
            const Vt = jt % De, Lt = Math.floor(jt / De) % Ie, Gt = Vt - Lt;
            Gt > 0 && Gt <= et && (N.data[rt++] = N.data[jt]);
          }
          const _t = Math.floor(ge / Ie), Mt = rt / (_t * Ie);
          return new b.Tensor(
            N.type,
            N.data.slice(0, rt),
            [_t, Ie, Mt]
          );
        }
        prepare_inputs_for_generation(N, ge, De) {
          let Ie = structuredClone(N);
          for (let rt = 0; rt < Ie.length; ++rt)
            for (let _t = 0; _t < Ie[rt].length; ++_t)
              rt % this.config.decoder.num_codebooks >= _t && (Ie[rt][_t] = BigInt(this.config.decoder.pad_token_id));
          return De.guidance_scale !== null && De.guidance_scale > 1 && (Ie = Ie.concat(Ie)), super.prepare_inputs_for_generation(Ie, ge, De);
        }
        /**
         * Generates sequences of token ids for models with a language modeling head.
         * @param {import('./generation/parameters.js').GenerationFunctionParameters} options
         * @returns {Promise<ModelOutput|Tensor>} The output of the model, which can contain the generated token ids, attentions, and scores.
         */
        async generate(N) {
          const ge = await super.generate(N), De = this._apply_and_filter_by_delay_pattern_mask(
            /** @type {Tensor} */
            ge
          ).unsqueeze_(0), { audio_values: Ie } = await _e(this.sessions.encodec_decode, { audio_codes: De });
          return Ie;
        }
      }
      class ca extends te {
      }
      class hp extends ca {
      }
      class pa extends ca {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class ha extends te {
      }
      class yd extends ha {
      }
      class Md extends ha {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class bd extends te {
      }
      class vd extends bd {
      }
      class xd extends bd {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class ma extends te {
      }
      class mp extends ma {
      }
      class Td extends ma {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Qt(await super._call(T));
        }
      }
      class Pd extends te {
      }
      class fp extends Pd {
      }
      class Ed extends te {
      }
      class Cd extends Ed {
        /**
         * @param {ConstructorParameters<typeof MultiModalityPreTrainedModel>} args
         */
        constructor(...N) {
          super(...N);
          fe(this, "forward_params", [
            // prepare_inputs_embeds
            "input_ids",
            "pixel_values",
            "images_seq_mask",
            "images_emb_mask",
            // language_model
            "attention_mask",
            "position_ids",
            "past_key_values"
          ]);
          this._generation_mode = "text";
        }
        async forward(N) {
          const ge = this._generation_mode ?? "text";
          let De;
          if (ge === "text" || !N.past_key_values) {
            const Mt = this.sessions.prepare_inputs_embeds, jt = (0, R.pick)(N, Mt.inputNames);
            De = await _e(Mt, jt);
          } else {
            const Mt = this.sessions.gen_img_embeds, jt = (0, R.pick)({
              image_ids: N.input_ids
            }, Mt.inputNames);
            De = await _e(Mt, jt);
          }
          const Ie = { ...N, ...De }, et = await Ee(this, Ie), rt = this.sessions[ge === "text" ? "lm_head" : "gen_head"];
          if (!rt)
            throw new Error(`Unable to find "${rt}" generation head`);
          const _t = await _e(rt, (0, R.pick)(et, rt.inputNames));
          return {
            ...De,
            ...et,
            ..._t
          };
        }
        /**
         * @param {import('./generation/parameters.js').GenerationFunctionParameters} options
         */
        async generate(N) {
          return this._generation_mode = "text", super.generate(N);
        }
        /**
         * @param {import('./generation/parameters.js').GenerationFunctionParameters} options
         */
        async generate_images(N) {
          this._generation_mode = "image";
          const ge = (N.inputs ?? N[this.main_input_name]).dims[1], Ie = (
            /** @type {Tensor} */
            (await super.generate(N)).slice(null, [ge, null])
          ), et = this.sessions.image_decode, { decoded_image: rt } = await _e(et, {
            generated_tokens: Ie
          }), _t = rt.add_(1).mul_(255 / 2).clamp_(0, 255).to("uint8"), Mt = [];
          for (const jt of _t) {
            const Vt = I.RawImage.fromTensor(jt);
            Mt.push(Vt);
          }
          return Mt;
        }
      }
      class kd extends Ke {
        constructor({ char_logits: T, bpe_logits: N, wp_logits: ge }) {
          super(), this.char_logits = T, this.bpe_logits = N, this.wp_logits = ge;
        }
        get logits() {
          return [this.char_logits, this.bpe_logits, this.wp_logits];
        }
      }
      class Sd extends te {
      }
      class $d extends Sd {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new kd(await super._call(T));
        }
      }
      class Ad extends te {
      }
      class Id extends Ad {
      }
      class Fd extends Ad {
      }
      class fa extends te {
      }
      class Od extends fa {
      }
      class Dd extends fa {
      }
      class gs {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, {
          progress_callback: N = null,
          config: ge = null,
          cache_dir: De = null,
          local_files_only: Ie = !1,
          revision: et = "main",
          model_file_name: rt = null,
          subfolder: _t = "onnx",
          device: Mt = null,
          dtype: jt = null,
          use_external_data_format: Vt = null,
          session_options: Lt = {}
        } = {}) {
          const Gt = {
            progress_callback: N,
            config: ge,
            cache_dir: De,
            local_files_only: Ie,
            revision: et,
            model_file_name: rt,
            subfolder: _t,
            device: Mt,
            dtype: jt,
            use_external_data_format: Vt,
            session_options: Lt
          };
          if (Gt.config = await _.AutoConfig.from_pretrained(T, Gt), !this.MODEL_CLASS_MAPPINGS)
            throw new Error("`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: " + this.name);
          for (const ts of this.MODEL_CLASS_MAPPINGS) {
            const ns = ts.get(Gt.config.model_type);
            if (ns)
              return await ns[1].from_pretrained(T, Gt);
          }
          if (this.BASE_IF_FAIL)
            return console.warn(`Unknown model class "${Gt.config.model_type}", attempting to construct from base class.`), await te.from_pretrained(T, Gt);
          throw Error(`Unsupported model type: ${Gt.config.model_type}`);
        }
      }
      /**
       * Mapping from model type to model class.
       * @type {Map<string, Object>[]}
       */
      fe(gs, "MODEL_CLASS_MAPPINGS", null), /**
       * Whether to attempt to instantiate the base class (`PretrainedModel`) if 
       * the model type is not found in the mapping.
       */
      fe(gs, "BASE_IF_FAIL", !1);
      const _p = /* @__PURE__ */ new Map([
        ["bert", ["BertModel", Te]],
        ["modernbert", ["ModernBertModel", dt]],
        ["nomic_bert", ["NomicBertModel", oe]],
        ["roformer", ["RoFormerModel", me]],
        ["electra", ["ElectraModel", Ds]],
        ["esm", ["EsmModel", si]],
        ["convbert", ["ConvBertModel", vt]],
        ["camembert", ["CamembertModel", Nt]],
        ["deberta", ["DebertaModel", Jr]],
        ["deberta-v2", ["DebertaV2Model", Tt]],
        ["mpnet", ["MPNetModel", gn]],
        ["albert", ["AlbertModel", Ln]],
        ["distilbert", ["DistilBertModel", lr]],
        ["roberta", ["RobertaModel", ir]],
        ["xlm", ["XLMModel", Ls]],
        ["xlm-roberta", ["XLMRobertaModel", It]],
        ["clap", ["ClapModel", nr]],
        ["clip", ["CLIPModel", Qa]],
        ["clipseg", ["CLIPSegModel", tl]],
        ["chinese_clip", ["ChineseCLIPModel", _r]],
        ["siglip", ["SiglipModel", Ja]],
        ["jina_clip", ["JinaCLIPModel", oi]],
        ["mobilebert", ["MobileBertModel", In]],
        ["squeezebert", ["SqueezeBertModel", bn]],
        ["wav2vec2", ["Wav2Vec2Model", zu]],
        ["wav2vec2-bert", ["Wav2Vec2BertModel", qu]],
        ["unispeech", ["UniSpeechModel", Vu]],
        ["unispeech-sat", ["UniSpeechSatModel", Gu]],
        ["hubert", ["HubertModel", Xu]],
        ["wavlm", ["WavLMModel", Ju]],
        ["audio-spectrogram-transformer", ["ASTModel", za]],
        ["vits", ["VitsModel", la]],
        ["pyannote", ["PyAnnoteModel", Nu]],
        ["wespeaker-resnet", ["WeSpeakerResNetModel", rp]],
        ["detr", ["DetrModel", Ao]],
        ["rt_detr", ["RTDetrModel", qc]],
        ["table-transformer", ["TableTransformerModel", cu]],
        ["vit", ["ViTModel", Vl]],
        ["ijepa", ["IJepaModel", Wl]],
        ["pvt", ["PvtModel", ql]],
        ["vit_msn", ["ViTMSNModel", Kc]],
        ["vit_mae", ["ViTMAEModel", Xl]],
        ["groupvit", ["GroupViTModel", yr]],
        ["fastvit", ["FastViTModel", Zl]],
        ["mobilevit", ["MobileViTModel", ru]],
        ["mobilevitv2", ["MobileViTV2Model", iu]],
        ["owlvit", ["OwlViTModel", ou]],
        ["owlv2", ["Owlv2Model", lu]],
        ["beit", ["BeitModel", sn]],
        ["deit", ["DeiTModel", hu]],
        ["hiera", ["HieraModel", mi]],
        ["convnext", ["ConvNextModel", Pu]],
        ["convnextv2", ["ConvNextV2Model", Eu]],
        ["dinov2", ["Dinov2Model", Cu]],
        ["dinov2_with_registers", ["Dinov2WithRegistersModel", ku]],
        ["resnet", ["ResNetModel", mu]],
        ["swin", ["SwinModel", Uo]],
        ["swin2sr", ["Swin2SRModel", Wo]],
        ["donut-swin", ["DonutSwinModel", Tu]],
        ["yolos", ["YolosModel", Au]],
        ["dpt", ["DPTModel", _u]],
        ["glpn", ["GLPNModel", Yc]],
        ["hifigan", ["SpeechT5HifiGan", on]],
        ["efficientnet", ["EfficientNetModel", gd]],
        ["decision_transformer", ["DecisionTransformerModel", fp]],
        ["patchtst", ["PatchTSTForPrediction", Id]],
        ["patchtsmixer", ["PatchTSMixerForPrediction", Od]],
        ["mobilenet_v1", ["MobileNetV1Model", hp]],
        ["mobilenet_v2", ["MobileNetV2Model", yd]],
        ["mobilenet_v3", ["MobileNetV3Model", vd]],
        ["mobilenet_v4", ["MobileNetV4Model", mp]],
        ["maskformer", ["MaskFormerModel", bu]],
        ["mgp-str", ["MgpstrForSceneTextRecognition", $d]],
        ["style_text_to_speech_2", ["StyleTextToSpeech2Model", rd]]
      ]), gp = /* @__PURE__ */ new Map([
        ["t5", ["T5Model", P]],
        ["longt5", ["LongT5Model", ve]],
        ["mt5", ["MT5Model", pt]],
        ["bart", ["BartModel", xt]],
        ["mbart", ["MBartModel", Os]],
        ["marian", ["MarianModel", Ou]],
        ["whisper", ["WhisperModel", Ba]],
        ["m2m_100", ["M2M100Model", Lu]],
        ["blenderbot", ["BlenderbotModel", ze]],
        ["blenderbot-small", ["BlenderbotSmallModel", ks]]
      ]), wp = /* @__PURE__ */ new Map([
        ["bloom", ["BloomModel", zl]],
        ["jais", ["JAISModel", il]],
        ["gpt2", ["GPT2Model", rl]],
        ["gptj", ["GPTJModel", dl]],
        ["gpt_bigcode", ["GPTBigCodeModel", pl]],
        ["gpt_neo", ["GPTNeoModel", wr]],
        ["gpt_neox", ["GPTNeoXModel", ll]],
        ["codegen", ["CodeGenModel", eo]],
        ["llama", ["LlamaModel", so]],
        ["exaone", ["ExaoneModel", gl]],
        ["olmo", ["OlmoModel", Wc]],
        ["olmo2", ["Olmo2Model", bl]],
        ["mobilellm", ["MobileLLMModel", wl]],
        ["granite", ["GraniteModel", ds]],
        ["cohere", ["CohereModel", xl]],
        ["gemma", ["GemmaModel", Pl]],
        ["gemma2", ["Gemma2Model", Cl]],
        ["helium", ["HeliumModel", li]],
        ["glm", ["GlmModel", _l]],
        ["openelm", ["OpenELMModel", Sl]],
        ["qwen2", ["Qwen2Model", Rn]],
        ["phi", ["PhiModel", Fl]],
        ["phi3", ["Phi3Model", Dl]],
        ["mpt", ["MptModel", Rl]],
        ["opt", ["OPTModel", jl]],
        ["mistral", ["MistralModel", id]],
        ["starcoder2", ["Starcoder2Model", ad]],
        ["falcon", ["FalconModel", ud]],
        ["stablelm", ["StableLmModel", fd]]
      ]), Ld = /* @__PURE__ */ new Map([
        ["speecht5", ["SpeechT5ForSpeechToText", Er]],
        ["whisper", ["WhisperForConditionalGeneration", Ra]],
        ["moonshine", ["MoonshineForConditionalGeneration", Na]]
      ]), Wn = /* @__PURE__ */ new Map([
        ["speecht5", ["SpeechT5ForTextToSpeech", Dr]]
      ]), _a = /* @__PURE__ */ new Map([
        ["vits", ["VitsModel", la]],
        ["musicgen", ["MusicgenForConditionalGeneration", Ei]]
      ]), ga = /* @__PURE__ */ new Map([
        ["bert", ["BertForSequenceClassification", Ve]],
        ["modernbert", ["ModernBertForSequenceClassification", lt]],
        ["roformer", ["RoFormerForSequenceClassification", We]],
        ["electra", ["ElectraForSequenceClassification", kr]],
        ["esm", ["EsmForSequenceClassification", $n]],
        ["convbert", ["ConvBertForSequenceClassification", At]],
        ["camembert", ["CamembertForSequenceClassification", Sr]],
        ["deberta", ["DebertaForSequenceClassification", Br]],
        ["deberta-v2", ["DebertaV2ForSequenceClassification", Vs]],
        ["mpnet", ["MPNetForSequenceClassification", wn]],
        ["albert", ["AlbertForSequenceClassification", vn]],
        ["distilbert", ["DistilBertForSequenceClassification", Fs]],
        ["roberta", ["RobertaForSequenceClassification", fs]],
        ["xlm", ["XLMForSequenceClassification", Gs]],
        ["xlm-roberta", ["XLMRobertaForSequenceClassification", Ri]],
        ["bart", ["BartForSequenceClassification", ms]],
        ["mbart", ["MBartForSequenceClassification", rs]],
        ["mobilebert", ["MobileBertForSequenceClassification", Vr]],
        ["squeezebert", ["SqueezeBertForSequenceClassification", On]]
      ]), wa = /* @__PURE__ */ new Map([
        ["bert", ["BertForTokenClassification", Ne]],
        ["modernbert", ["ModernBertForTokenClassification", ht]],
        ["roformer", ["RoFormerForTokenClassification", Je]],
        ["electra", ["ElectraForTokenClassification", Qr]],
        ["esm", ["EsmForTokenClassification", An]],
        ["convbert", ["ConvBertForTokenClassification", is]],
        ["camembert", ["CamembertForTokenClassification", $r]],
        ["deberta", ["DebertaForTokenClassification", Rr]],
        ["deberta-v2", ["DebertaV2ForTokenClassification", Nr]],
        ["mpnet", ["MPNetForTokenClassification", yn]],
        ["distilbert", ["DistilBertForTokenClassification", Pr]],
        ["roberta", ["RobertaForTokenClassification", Ss]],
        ["xlm", ["XLMForTokenClassification", $t]],
        ["xlm-roberta", ["XLMRobertaForTokenClassification", Da]]
      ]), Ci = /* @__PURE__ */ new Map([
        ["t5", ["T5ForConditionalGeneration", Q]],
        ["longt5", ["LongT5ForConditionalGeneration", Se]],
        ["mt5", ["MT5ForConditionalGeneration", gt]],
        ["bart", ["BartForConditionalGeneration", Kt]],
        ["mbart", ["MBartForConditionalGeneration", Bt]],
        ["marian", ["MarianMTModel", Du]],
        ["m2m_100", ["M2M100ForConditionalGeneration", sa]],
        ["blenderbot", ["BlenderbotForConditionalGeneration", Js]],
        ["blenderbot-small", ["BlenderbotSmallForConditionalGeneration", Xs]]
      ]), ya = /* @__PURE__ */ new Map([
        ["bloom", ["BloomForCausalLM", Bl]],
        ["gpt2", ["GPT2LMHeadModel", nl]],
        ["jais", ["JAISLMHeadModel", ol]],
        ["gptj", ["GPTJForCausalLM", cl]],
        ["gpt_bigcode", ["GPTBigCodeForCausalLM", hl]],
        ["gpt_neo", ["GPTNeoForCausalLM", al]],
        ["gpt_neox", ["GPTNeoXForCausalLM", ul]],
        ["codegen", ["CodeGenForCausalLM", ml]],
        ["llama", ["LlamaForCausalLM", Vc]],
        ["exaone", ["ExaoneForCausalLM", oo]],
        ["olmo", ["OlmoForCausalLM", Ml]],
        ["olmo2", ["Olmo2ForCausalLM", Gc]],
        ["mobilellm", ["MobileLLMForCausalLM", yl]],
        ["granite", ["GraniteForCausalLM", vl]],
        ["cohere", ["CohereForCausalLM", Tl]],
        ["gemma", ["GemmaForCausalLM", El]],
        ["gemma2", ["Gemma2ForCausalLM", kl]],
        ["helium", ["HeliumForCausalLM", fl]],
        ["glm", ["GlmForCausalLM", Bn]],
        ["openelm", ["OpenELMForCausalLM", $l]],
        ["qwen2", ["Qwen2ForCausalLM", Al]],
        ["phi", ["PhiForCausalLM", Ol]],
        ["phi3", ["Phi3ForCausalLM", Ll]],
        ["mpt", ["MptForCausalLM", Nl]],
        ["opt", ["OPTForCausalLM", Ul]],
        ["mbart", ["MBartForCausalLM", rr]],
        ["mistral", ["MistralForCausalLM", od]],
        ["starcoder2", ["Starcoder2ForCausalLM", ld]],
        ["falcon", ["FalconForCausalLM", dd]],
        ["trocr", ["TrOCRForCausalLM", nd]],
        ["stablelm", ["StableLmForCausalLM", _d]],
        // Also image-text-to-text
        ["phi3_v", ["Phi3VForCausalLM", pr]]
      ]), yp = /* @__PURE__ */ new Map([
        ["multi_modality", ["MultiModalityCausalLM", Cd]]
      ]), Ma = /* @__PURE__ */ new Map([
        ["bert", ["BertForMaskedLM", Ue]],
        ["modernbert", ["ModernBertForMaskedLM", ct]],
        ["roformer", ["RoFormerForMaskedLM", Ae]],
        ["electra", ["ElectraForMaskedLM", sr]],
        ["esm", ["EsmForMaskedLM", Sn]],
        ["convbert", ["ConvBertForMaskedLM", kt]],
        ["camembert", ["CamembertForMaskedLM", Xr]],
        ["deberta", ["DebertaForMaskedLM", Ar]],
        ["deberta-v2", ["DebertaV2ForMaskedLM", Dt]],
        ["mpnet", ["MPNetForMaskedLM", Zr]],
        ["albert", ["AlbertForMaskedLM", as]],
        ["distilbert", ["DistilBertForMaskedLM", _n]],
        ["roberta", ["RobertaForMaskedLM", fr]],
        ["xlm", ["XLMWithLMHeadModel", $s]],
        ["xlm-roberta", ["XLMRobertaForMaskedLM", Oa]],
        ["mobilebert", ["MobileBertForMaskedLM", ri]],
        ["squeezebert", ["SqueezeBertForMaskedLM", Fn]]
      ]), ba = /* @__PURE__ */ new Map([
        ["bert", ["BertForQuestionAnswering", Re]],
        ["roformer", ["RoFormerForQuestionAnswering", ut]],
        ["electra", ["ElectraForQuestionAnswering", Us]],
        ["convbert", ["ConvBertForQuestionAnswering", ys]],
        ["camembert", ["CamembertForQuestionAnswering", Yr]],
        ["deberta", ["DebertaForQuestionAnswering", ar]],
        ["deberta-v2", ["DebertaV2ForQuestionAnswering", Ir]],
        ["mpnet", ["MPNetForQuestionAnswering", Mn]],
        ["albert", ["AlbertForQuestionAnswering", zn]],
        ["distilbert", ["DistilBertForQuestionAnswering", es]],
        ["roberta", ["RobertaForQuestionAnswering", yt]],
        ["xlm", ["XLMForQuestionAnswering", en]],
        ["xlm-roberta", ["XLMRobertaForQuestionAnswering", La]],
        ["mobilebert", ["MobileBertForQuestionAnswering", mr]],
        ["squeezebert", ["SqueezeBertForQuestionAnswering", Dn]]
      ]), va = /* @__PURE__ */ new Map([
        ["vision-encoder-decoder", ["VisionEncoderDecoderModel", Vi]],
        ["idefics3", ["Idefics3ForConditionalGeneration", Wi]]
      ]), Mp = /* @__PURE__ */ new Map([
        ["llava", ["LlavaForConditionalGeneration", ni]],
        ["llava_onevision", ["LlavaOnevisionForConditionalGeneration", ja]],
        ["moondream1", ["Moondream1ForConditionalGeneration", Ua]],
        ["florence2", ["Florence2ForConditionalGeneration", Wa]],
        ["qwen2-vl", ["Qwen2VLForConditionalGeneration", Il]],
        ["idefics3", ["Idefics3ForConditionalGeneration", Wi]],
        ["paligemma", ["PaliGemmaForConditionalGeneration", Ka]]
      ]), zd = /* @__PURE__ */ new Map([
        ["vision-encoder-decoder", ["VisionEncoderDecoderModel", Vi]]
      ]), Bd = /* @__PURE__ */ new Map([
        ["vit", ["ViTForImageClassification", vo]],
        ["ijepa", ["IJepaForImageClassification", Gl]],
        ["pvt", ["PvtForImageClassification", To]],
        ["vit_msn", ["ViTMSNForImageClassification", Yl]],
        ["fastvit", ["FastViTForImageClassification", eu]],
        ["mobilevit", ["MobileViTForImageClassification", nu]],
        ["mobilevitv2", ["MobileViTV2ForImageClassification", Hc]],
        ["beit", ["BeitForImageClassification", rn]],
        ["deit", ["DeiTForImageClassification", zo]],
        ["hiera", ["HieraForImageClassification", Ro]],
        ["convnext", ["ConvNextForImageClassification", Qo]],
        ["convnextv2", ["ConvNextV2ForImageClassification", Yo]],
        ["dinov2", ["Dinov2ForImageClassification", Zc]],
        ["dinov2_with_registers", ["Dinov2WithRegistersForImageClassification", Su]],
        ["resnet", ["ResNetForImageClassification", Qc]],
        ["swin", ["SwinForImageClassification", jn]],
        ["segformer", ["SegformerForImageClassification", hd]],
        ["efficientnet", ["EfficientNetForImageClassification", wd]],
        ["mobilenet_v1", ["MobileNetV1ForImageClassification", pa]],
        ["mobilenet_v2", ["MobileNetV2ForImageClassification", Md]],
        ["mobilenet_v3", ["MobileNetV3ForImageClassification", xd]],
        ["mobilenet_v4", ["MobileNetV4ForImageClassification", Td]]
      ]), Rd = /* @__PURE__ */ new Map([
        ["detr", ["DetrForObjectDetection", nn]],
        ["rt_detr", ["RTDetrForObjectDetection", Nn]],
        ["table-transformer", ["TableTransformerForObjectDetection", pu]],
        ["yolos", ["YolosForObjectDetection", Iu]]
      ]), xa = /* @__PURE__ */ new Map([
        ["owlvit", ["OwlViTForObjectDetection", au]],
        ["owlv2", ["Owlv2ForObjectDetection", uu]],
        ["grounding-dino", ["GroundingDinoForObjectDetection", $u]]
      ]), Nd = /* @__PURE__ */ new Map([
        // TODO: Do not add new models here
        ["detr", ["DetrForSegmentation", Ks]],
        ["clipseg", ["CLIPSegForImageSegmentation", sl]]
      ]), jd = /* @__PURE__ */ new Map([
        ["segformer", ["SegformerForSemanticSegmentation", md]],
        ["sapiens", ["SapiensForSemanticSegmentation", yu]]
      ]), Ud = /* @__PURE__ */ new Map([
        ["detr", ["DetrForSegmentation", Ks]],
        ["maskformer", ["MaskFormerForInstanceSegmentation", vu]]
      ]), Vd = /* @__PURE__ */ new Map([
        ["sam", ["SamModel", yi]]
      ]), bp = /* @__PURE__ */ new Map([
        ["wav2vec2", ["Wav2Vec2ForCTC", Bu]],
        ["wav2vec2-bert", ["Wav2Vec2BertForCTC", op]],
        ["unispeech", ["UniSpeechForCTC", np]],
        ["unispeech-sat", ["UniSpeechSatForCTC", Ku]],
        ["wavlm", ["WavLMForCTC", Zu]],
        ["hubert", ["HubertForCTC", Yu]]
      ]), Wd = /* @__PURE__ */ new Map([
        ["wav2vec2", ["Wav2Vec2ForSequenceClassification", sp]],
        ["wav2vec2-bert", ["Wav2Vec2BertForSequenceClassification", Qu]],
        ["unispeech", ["UniSpeechForSequenceClassification", Wu]],
        ["unispeech-sat", ["UniSpeechSatForSequenceClassification", ip]],
        ["wavlm", ["WavLMForSequenceClassification", ed]],
        ["hubert", ["HubertForSequenceClassification", lp]],
        ["audio-spectrogram-transformer", ["ASTForAudioClassification", Ni]]
      ]), Gd = /* @__PURE__ */ new Map([
        ["wavlm", ["WavLMForXVector", up]]
      ]), Kd = /* @__PURE__ */ new Map([
        ["unispeech-sat", ["UniSpeechSatForAudioFrameClassification", Hu]],
        ["wavlm", ["WavLMForAudioFrameClassification", td]],
        ["wav2vec2", ["Wav2Vec2ForAudioFrameClassification", Ru]],
        ["pyannote", ["PyAnnoteForAudioFrameClassification", ju]]
      ]), Hd = /* @__PURE__ */ new Map([
        ["vitmatte", ["VitMatteForImageMatting", su]]
      ]), Qp = /* @__PURE__ */ new Map([
        ["patchtst", ["PatchTSTForPrediction", Fd]],
        ["patchtsmixer", ["PatchTSMixerForPrediction", Dd]]
      ]), qd = /* @__PURE__ */ new Map([
        ["swin2sr", ["Swin2SRForImageSuperResolution", fu]]
      ]), Qd = /* @__PURE__ */ new Map([
        ["dpt", ["DPTForDepthEstimation", Xc]],
        ["depth_anything", ["DepthAnythingForDepthEstimation", wu]],
        ["glpn", ["GLPNForDepthEstimation", xu]],
        ["sapiens", ["SapiensForDepthEstimation", fi]],
        ["depth_pro", ["DepthProForDepthEstimation", Ho]]
      ]), Xd = /* @__PURE__ */ new Map([
        ["sapiens", ["SapiensForNormalEstimation", Mu]]
      ]), vp = /* @__PURE__ */ new Map([
        ["vitpose", ["VitPoseForPoseEstimation", Hl]]
      ]), Yd = /* @__PURE__ */ new Map([
        ["clip", ["CLIPVisionModelWithProjection", Ya]],
        ["siglip", ["SiglipVisionModel", el]],
        ["jina_clip", ["JinaCLIPVisionModel", gr]]
      ]), Jd = [
        [_p, A.EncoderOnly],
        [gp, A.EncoderDecoder],
        [wp, A.DecoderOnly],
        [ga, A.EncoderOnly],
        [wa, A.EncoderOnly],
        [Ci, A.Seq2Seq],
        [Ld, A.Seq2Seq],
        [ya, A.DecoderOnly],
        [yp, A.MultiModality],
        [Ma, A.EncoderOnly],
        [ba, A.EncoderOnly],
        [va, A.Vision2Seq],
        [Mp, A.ImageTextToText],
        [Bd, A.EncoderOnly],
        [Nd, A.EncoderOnly],
        [Ud, A.EncoderOnly],
        [jd, A.EncoderOnly],
        [Hd, A.EncoderOnly],
        [Qp, A.EncoderOnly],
        [qd, A.EncoderOnly],
        [Qd, A.EncoderOnly],
        [Xd, A.EncoderOnly],
        [vp, A.EncoderOnly],
        [Rd, A.EncoderOnly],
        [xa, A.EncoderOnly],
        [Vd, A.MaskGeneration],
        [bp, A.EncoderOnly],
        [Wd, A.EncoderOnly],
        [Wn, A.Seq2Seq],
        [_a, A.EncoderOnly],
        [Gd, A.EncoderOnly],
        [Kd, A.EncoderOnly],
        // Custom:
        [Yd, A.EncoderOnly]
      ];
      for (const [f, T] of Jd)
        for (const [N, ge] of f.values())
          S.set(N, T), x.set(ge, N), w.set(N, ge);
      const xp = [
        // OVERRIDE:
        // TODO: Refactor to allow class to specify model
        ["MusicgenForConditionalGeneration", Ei, A.Musicgen],
        ["Phi3VForCausalLM", pr, A.Phi3V],
        ["CLIPTextModelWithProjection", Xa, A.EncoderOnly],
        ["SiglipTextModel", Za, A.EncoderOnly],
        ["JinaCLIPTextModel", Ki, A.EncoderOnly],
        ["ClapTextModelWithProjection", cd, A.EncoderOnly],
        ["ClapAudioModelWithProjection", pd, A.EncoderOnly]
      ];
      for (const [f, T, N] of xp)
        S.set(f, N), x.set(T, f), w.set(f, T);
      class Ta extends gs {
      }
      /** @type {Map<string, Object>[]} */
      // @ts-ignore
      fe(Ta, "MODEL_CLASS_MAPPINGS", Jd.map((T) => T[0])), fe(Ta, "BASE_IF_FAIL", !0);
      class Tp extends gs {
      }
      fe(Tp, "MODEL_CLASS_MAPPINGS", [ga]);
      class Zd extends gs {
      }
      fe(Zd, "MODEL_CLASS_MAPPINGS", [wa]);
      class ec extends gs {
      }
      fe(ec, "MODEL_CLASS_MAPPINGS", [Ci]);
      class tc extends gs {
      }
      fe(tc, "MODEL_CLASS_MAPPINGS", [Ld]);
      class sc extends gs {
      }
      fe(sc, "MODEL_CLASS_MAPPINGS", [Wn]);
      class rc extends gs {
      }
      fe(rc, "MODEL_CLASS_MAPPINGS", [_a]);
      class nc extends gs {
      }
      fe(nc, "MODEL_CLASS_MAPPINGS", [ya]);
      class ic extends gs {
      }
      fe(ic, "MODEL_CLASS_MAPPINGS", [Ma]);
      class oc extends gs {
      }
      fe(oc, "MODEL_CLASS_MAPPINGS", [ba]);
      class ac extends gs {
      }
      fe(ac, "MODEL_CLASS_MAPPINGS", [va]);
      class lc extends gs {
      }
      fe(lc, "MODEL_CLASS_MAPPINGS", [Bd]);
      class uc extends gs {
      }
      fe(uc, "MODEL_CLASS_MAPPINGS", [Nd]);
      class Pa extends gs {
      }
      fe(Pa, "MODEL_CLASS_MAPPINGS", [jd]);
      class dc extends gs {
      }
      fe(dc, "MODEL_CLASS_MAPPINGS", [Ud]);
      class cc extends gs {
      }
      fe(cc, "MODEL_CLASS_MAPPINGS", [Rd]);
      class pc extends gs {
      }
      fe(pc, "MODEL_CLASS_MAPPINGS", [xa]);
      class hc extends gs {
      }
      fe(hc, "MODEL_CLASS_MAPPINGS", [Vd]);
      class mc extends gs {
      }
      fe(mc, "MODEL_CLASS_MAPPINGS", [bp]);
      class fc extends gs {
      }
      fe(fc, "MODEL_CLASS_MAPPINGS", [Wd]);
      class _c extends gs {
      }
      fe(_c, "MODEL_CLASS_MAPPINGS", [Gd]);
      class Pp extends gs {
      }
      fe(Pp, "MODEL_CLASS_MAPPINGS", [Kd]);
      class gc extends gs {
      }
      fe(gc, "MODEL_CLASS_MAPPINGS", [zd]);
      class wc extends gs {
      }
      fe(wc, "MODEL_CLASS_MAPPINGS", [Hd]);
      class yc extends gs {
      }
      fe(yc, "MODEL_CLASS_MAPPINGS", [qd]);
      class Ep extends gs {
      }
      fe(Ep, "MODEL_CLASS_MAPPINGS", [Qd]);
      class Mc extends gs {
      }
      fe(Mc, "MODEL_CLASS_MAPPINGS", [Xd]);
      class bc extends gs {
      }
      fe(bc, "MODEL_CLASS_MAPPINGS", [vp]);
      class vc extends gs {
      }
      fe(vc, "MODEL_CLASS_MAPPINGS", [Yd]);
      class Xp extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits The output logits of the model.
         * @param {Tensor} output.past_key_values An tensor of key/value pairs that represent the previous state of the model.
         * @param {Tensor} output.encoder_outputs The output of the encoder in a sequence-to-sequence model.
         * @param {Tensor} [output.decoder_attentions] Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the self-attention heads.
         * @param {Tensor} [output.cross_attentions] Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the weighted average in the cross-attention heads.
         */
        constructor({ logits: T, past_key_values: N, encoder_outputs: ge, decoder_attentions: De = null, cross_attentions: Ie = null }) {
          super(), this.logits = T, this.past_key_values = N, this.encoder_outputs = ge, this.decoder_attentions = De, this.cross_attentions = Ie;
        }
      }
      class Qt extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits classification (or regression if config.num_labels==1) scores (before SoftMax).
         * @param {Record<string, Tensor>} [output.attentions] Object of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
         * Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.
         */
        constructor({ logits: T, ...N }) {
          super(), this.logits = T;
          const ge = Object.values(N);
          ge.length > 0 && (this.attentions = ge);
        }
      }
      class Gn extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification hidden states before AMSoftmax, of shape `(batch_size, config.xvector_output_dim)`.
         * @param {Tensor} output.embeddings Utterance embeddings used for vector similarity-based retrieval, of shape `(batch_size, config.xvector_output_dim)`.
         */
        constructor({ logits: T, embeddings: N }) {
          super(), this.logits = T, this.embeddings = N;
        }
      }
      class Hs extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification scores (before SoftMax).
         */
        constructor({ logits: T }) {
          super(), this.logits = T;
        }
      }
      class qs extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
         */
        constructor({ logits: T }) {
          super(), this.logits = T;
        }
      }
      class tr extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.start_logits Span-start scores (before SoftMax).
         * @param {Tensor} output.end_logits Span-end scores (before SoftMax).
         */
        constructor({ start_logits: T, end_logits: N }) {
          super(), this.start_logits = T, this.end_logits = N;
        }
      }
      class un extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).
         */
        constructor({ logits: T }) {
          super(), this.logits = T;
        }
      }
      class Cp extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).
         * @param {Tensor} output.past_key_values Contains pre-computed hidden-states (key and values in the self-attention blocks)
         * that can be used (see `past_key_values` input) to speed up sequential decoding.
         */
        constructor({ logits: T, past_key_values: N }) {
          super(), this.logits = T, this.past_key_values = N;
        }
      }
      class xc extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.alphas Estimated alpha values, of shape `(batch_size, num_channels, height, width)`.
         */
        constructor({ alphas: T }) {
          super(), this.alphas = T;
        }
      }
      class kp extends Ke {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.waveform The final audio waveform predicted by the model, of shape `(batch_size, sequence_length)`.
         * @param {Tensor} output.spectrogram The log-mel spectrogram predicted at the output of the flow model.
         * This spectrogram is passed to the Hi-Fi GAN decoder model to obtain the final audio waveform.
         */
        constructor({ waveform: T, spectrogram: N }) {
          super(), this.waveform = T, this.spectrogram = N;
        }
      }
    }
  ),
  /***/
  "./src/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js": (
    /*!******************************************************************************************************!*\
      !*** ./src/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js ***!
      \******************************************************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ASTFeatureExtractor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var D = r(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      );
      class U extends _.FeatureExtractor {
        constructor(R) {
          super(R);
          const g = this.config.sampling_rate, v = (0, D.mel_filter_bank)(
            256,
            // num_frequency_bins
            this.config.num_mel_bins,
            // num_mel_filters
            20,
            // min_frequency
            Math.floor(g / 2),
            // max_frequency
            g,
            // sampling_rate
            null,
            // norm
            "kaldi",
            // mel_scale
            !0
            // triangularize_in_mel_space
          );
          for (let M = 0; M < v.length; ++M)
            v[M].push(0);
          this.mel_filters = v, this.window = (0, D.window_function)(400, "hann", {
            periodic: !1
          }), this.mean = this.config.mean, this.std = this.config.std;
        }
        /**
         * Computes the log-Mel spectrogram of the provided audio waveform.
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @param {number} max_length The maximum number of frames to return.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(R, g) {
          return (0, D.spectrogram)(
            R,
            this.window,
            // window
            400,
            // frame_length
            160,
            // hop_length
            {
              fft_length: 512,
              power: 2,
              center: !1,
              preemphasis: 0.97,
              mel_filters: this.mel_filters,
              log_mel: "log",
              mel_floor: 1192092955078125e-22,
              remove_dc_offset: !0,
              // Custom
              max_num_frames: g,
              transpose: !0
            }
          );
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_values: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.
         */
        async _call(R) {
          (0, _.validate_audio_inputs)(R, "ASTFeatureExtractor");
          const g = await this._extract_fbank_features(R, this.config.max_length);
          if (this.config.do_normalize) {
            const v = this.std * 2, M = g.data;
            for (let y = 0; y < M.length; ++y)
              M[y] = (M[y] - this.mean) / v;
          }
          return {
            input_values: g.unsqueeze_(0)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/auto/feature_extraction_auto.js": (
    /*!****************************************************!*\
      !*** ./src/models/auto/feature_extraction_auto.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        AutoFeatureExtractor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../utils/constants.js */
        "./src/utils/constants.js"
      ), D = r(
        /*! ../../utils/hub.js */
        "./src/utils/hub.js"
      );
      r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      var U = r(
        /*! ../feature_extractors.js */
        "./src/models/feature_extractors.js"
      );
      class Y {
        /** @type {typeof FeatureExtractor.from_pretrained} */
        static async from_pretrained(g, v = {}) {
          const M = await (0, D.getModelJSON)(g, _.FEATURE_EXTRACTOR_NAME, !0, v), y = M.feature_extractor_type, b = U[y];
          if (!b)
            throw new Error(`Unknown feature_extractor_type: '${y}'. Please report this at ${_.GITHUB_ISSUE_URL}.`);
          return new b(M);
        }
      }
    }
  ),
  /***/
  "./src/models/auto/image_processing_auto.js": (
    /*!**************************************************!*\
      !*** ./src/models/auto/image_processing_auto.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        AutoImageProcessor: () => (
          /* binding */
          R
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../utils/constants.js */
        "./src/utils/constants.js"
      ), D = r(
        /*! ../../utils/hub.js */
        "./src/utils/hub.js"
      ), U = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), Y = r(
        /*! ../image_processors.js */
        "./src/models/image_processors.js"
      );
      class R {
        /** @type {typeof ImageProcessor.from_pretrained} */
        static async from_pretrained(v, M = {}) {
          const y = await (0, D.getModelJSON)(v, _.IMAGE_PROCESSOR_NAME, !0, M), b = y.image_processor_type ?? y.feature_extractor_type;
          let I = Y[b];
          return I || (b !== void 0 && console.warn(`Image processor type '${b}' not found, assuming base ImageProcessor. Please report this at ${_.GITHUB_ISSUE_URL}.`), I = U.ImageProcessor), new I(y);
        }
      }
    }
  ),
  /***/
  "./src/models/auto/processing_auto.js": (
    /*!********************************************!*\
      !*** ./src/models/auto/processing_auto.js ***!
      \********************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        AutoProcessor: () => (
          /* binding */
          v
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../utils/constants.js */
        "./src/utils/constants.js"
      ), D = r(
        /*! ../../utils/hub.js */
        "./src/utils/hub.js"
      ), U = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), Y = r(
        /*! ../processors.js */
        "./src/models/processors.js"
      ), R = r(
        /*! ../image_processors.js */
        "./src/models/image_processors.js"
      ), g = r(
        /*! ../feature_extractors.js */
        "./src/models/feature_extractors.js"
      );
      class v {
        /** @type {typeof Processor.from_pretrained} */
        static async from_pretrained(y, b = {}) {
          const I = await (0, D.getModelJSON)(y, _.IMAGE_PROCESSOR_NAME, !0, b), { image_processor_type: K, feature_extractor_type: se, processor_class: ie } = I;
          if (ie && Y[ie])
            return Y[ie].from_pretrained(y, b);
          if (!K && !se)
            throw new Error("No `image_processor_type` or `feature_extractor_type` found in the config.");
          const W = {};
          if (K) {
            const q = R[K];
            if (!q)
              throw new Error(`Unknown image_processor_type: '${K}'.`);
            W.image_processor = new q(I);
          }
          if (se) {
            const q = R[se];
            if (q)
              W.image_processor = new q(I);
            else {
              const A = g[se];
              if (!A)
                throw new Error(`Unknown feature_extractor_type: '${se}'.`);
              W.feature_extractor = new A(I);
            }
          }
          const j = {};
          return new U.Processor(j, W);
        }
      }
    }
  ),
  /***/
  "./src/models/beit/image_processing_beit.js": (
    /*!**************************************************!*\
      !*** ./src/models/beit/image_processing_beit.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        BeitFeatureExtractor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/bit/image_processing_bit.js": (
    /*!************************************************!*\
      !*** ./src/models/bit/image_processing_bit.js ***!
      \************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        BitImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/chinese_clip/image_processing_chinese_clip.js": (
    /*!******************************************************************!*\
      !*** ./src/models/chinese_clip/image_processing_chinese_clip.js ***!
      \******************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ChineseCLIPFeatureExtractor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/clap/feature_extraction_clap.js": (
    /*!****************************************************!*\
      !*** ./src/models/clap/feature_extraction_clap.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ClapFeatureExtractor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var D = r(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      );
      class U extends _.FeatureExtractor {
        constructor(R) {
          super(R), this.mel_filters = (0, D.mel_filter_bank)(
            this.config.nb_frequency_bins,
            // num_frequency_bins
            this.config.feature_size,
            // num_mel_filters
            this.config.frequency_min,
            // min_frequency
            this.config.frequency_max,
            // max_frequency
            this.config.sampling_rate,
            // sampling_rate
            null,
            // norm
            "htk"
            // mel_scale
          ), this.mel_filters_slaney = (0, D.mel_filter_bank)(
            this.config.nb_frequency_bins,
            // num_frequency_bins
            this.config.feature_size,
            // num_mel_filters
            this.config.frequency_min,
            // min_frequency
            this.config.frequency_max,
            // max_frequency
            this.config.sampling_rate,
            // sampling_rate
            "slaney",
            // norm
            "slaney"
            // mel_scale
          ), this.window = (0, D.window_function)(this.config.fft_window_size, "hann");
        }
        /**
         * Extracts the mel spectrogram and prepares it for the mode based on the `truncation` and `padding` arguments.
         * 
         * Four different path are possible:
         *   - `truncation="fusion"` and the length of the waveform is greater than the max length: the mel spectrogram
         *     will be computed on the entire audio. 3 random crops and a dowsampled version of the full mel spectrogram
         *     are then stacked together. They will later be used for `feature_fusion`.
         *   - `truncation="rand_trunc"` and the length of the waveform is smaller than the max length: the audio is
         *     padded based on `padding`.
         *   - `truncation="fusion"` and the length of the waveform is smaller than the max length: the audio is padded
         *     based on `padding`, and is repeated `4` times.
         *   - `truncation="rand_trunc"` and the length of the waveform is greater than the max length: the mel
         *     spectrogram will be computed on a random crop of the waveform.
         * 
         * @param {Float32Array|Float64Array} waveform The input waveform.
         * @param {number} max_length The maximum length of the waveform.
         * @param {string} truncation The truncation strategy to use.
         * @param {string} padding The padding strategy to use.
         * @returns {Promise<Tensor>} An object containing the mel spectrogram data as a Float32Array, its dimensions as an array of numbers, and a boolean indicating whether the waveform was longer than the max length.
         * @private
         */
        async _get_input_mel(R, g, v, M) {
          let y;
          const b = R.length - g;
          if (b > 0)
            if (v === "rand_trunc") {
              const I = Math.floor(Math.random() * (b + 1));
              R = R.subarray(I, I + g), y = await this._extract_fbank_features(R, this.mel_filters_slaney, this.config.nb_max_samples);
            } else
              throw new Error(`Truncation strategy "${v}" not implemented`);
          else {
            if (b < 0) {
              let I = new Float64Array(g);
              if (I.set(R), M === "repeat")
                for (let K = R.length; K < g; K += R.length)
                  I.set(R.subarray(0, Math.min(R.length, g - K)), K);
              else if (M === "repeatpad")
                for (let K = R.length; K < -b; K += R.length)
                  I.set(R, K);
              R = I;
            }
            if (v === "fusion")
              throw new Error(`Truncation strategy "${v}" not implemented`);
            y = await this._extract_fbank_features(R, this.mel_filters_slaney, this.config.nb_max_samples);
          }
          return y.unsqueeze_(0);
        }
        /**
         * Compute the log-mel spectrogram of the provided `waveform` using the Hann window.
         * In CLAP, two different filter banks are used depending on the truncation pattern:
         *  - `self.mel_filters`: they correspond to the default parameters of `torchaudio` which can be obtained from
         *    calling `torchaudio.transforms.MelSpectrogram().mel_scale.fb`. These filters are used when `truncation`
         *    is set to `"fusion"`.
         *  - `self.mel_filteres_slaney` : they correspond to the default parameters of `librosa` which used
         *    `librosa.filters.mel` when computing the mel spectrogram. These filters were only used in the original
         *    implementation when the truncation mode is not `"fusion"`.
         * 
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @param {number[][]} mel_filters The mel filters to use.
         * @param {number} [max_length=null] The maximum number of frames to return.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(R, g, v = null) {
          return (0, D.spectrogram)(
            R,
            this.window,
            // window
            this.config.fft_window_size,
            // frame_length
            this.config.hop_length,
            // hop_length
            {
              power: 2,
              mel_filters: g,
              log_mel: "dB",
              // Custom
              max_num_frames: v,
              do_pad: !1,
              transpose: !0
            }
          );
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.
         */
        async _call(R, {
          max_length: g = null
        } = {}) {
          return (0, _.validate_audio_inputs)(R, "ClapFeatureExtractor"), {
            input_features: (await this._get_input_mel(
              R,
              g ?? this.config.nb_max_samples,
              this.config.truncation,
              this.config.padding
            )).unsqueeze_(0)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/clip/image_processing_clip.js": (
    /*!**************************************************!*\
      !*** ./src/models/clip/image_processing_clip.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        CLIPFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        CLIPImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/convnext/image_processing_convnext.js": (
    /*!**********************************************************!*\
      !*** ./src/models/convnext/image_processing_convnext.js ***!
      \**********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ConvNextFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        ConvNextImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        constructor(R) {
          super(R), this.crop_pct = this.config.crop_pct ?? 0.875;
        }
        async resize(R) {
          var v;
          const g = (v = this.size) == null ? void 0 : v.shortest_edge;
          if (g === void 0)
            throw new Error("Size dictionary must contain 'shortest_edge' key.");
          if (g < 384) {
            const M = Math.floor(g / this.crop_pct), [y, b] = this.get_resize_output_image_size(R, {
              shortest_edge: M
            });
            R = await R.resize(y, b, {
              resample: this.resample
            }), R = await R.center_crop(g, g);
          } else
            R = await R.resize(g, g, {
              resample: this.resample
            });
          return R;
        }
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/deit/image_processing_deit.js": (
    /*!**************************************************!*\
      !*** ./src/models/deit/image_processing_deit.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        DeiTFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        DeiTImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/detr/image_processing_detr.js": (
    /*!**************************************************!*\
      !*** ./src/models/detr/image_processing_detr.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        DetrFeatureExtractor: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        DetrImageProcessor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class U extends _.ImageProcessor {
        /**
         * Calls the feature extraction process on an array of images, preprocesses
         * each image, and concatenates the resulting features into a single Tensor.
         * @param {import('../../utils/image.js').RawImage[]} images The image(s) to extract features from.
         * @returns {Promise<DetrFeatureExtractorResult>} An object containing the concatenated pixel values of the preprocessed images.
         */
        async _call(g) {
          const v = await super._call(g), M = [v.pixel_values.dims[0], 64, 64], y = (0, D.full)(M, 1n);
          return { ...v, pixel_mask: y };
        }
        /** @type {typeof post_process_object_detection} */
        post_process_object_detection(...g) {
          return (0, _.post_process_object_detection)(...g);
        }
        /** @type {typeof post_process_panoptic_segmentation} */
        post_process_panoptic_segmentation(...g) {
          return (0, _.post_process_panoptic_segmentation)(...g);
        }
        /** @type {typeof post_process_instance_segmentation} */
        post_process_instance_segmentation(...g) {
          return (0, _.post_process_instance_segmentation)(...g);
        }
      }
      class Y extends U {
      }
    }
  ),
  /***/
  "./src/models/donut/image_processing_donut.js": (
    /*!****************************************************!*\
      !*** ./src/models/donut/image_processing_donut.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        DonutFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        DonutImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        pad_image(R, g, v, M = {}) {
          const [y, b, I] = g;
          let K = this.image_mean;
          Array.isArray(this.image_mean) || (K = new Array(I).fill(K));
          let se = this.image_std;
          Array.isArray(se) || (se = new Array(I).fill(K));
          const ie = K.map((W, j) => -W / se[j]);
          return super.pad_image(R, g, v, {
            center: !0,
            // Since normalization is done after padding, we need to use certain constant values to ensure the same behaviour is observed.
            // For more information, see https://github.com/huggingface/transformers/blob/main/src/transformers/models/donut/image_processing_donut.py#L433-L451
            constant_values: ie,
            ...M
          });
        }
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/dpt/image_processing_dpt.js": (
    /*!************************************************!*\
      !*** ./src/models/dpt/image_processing_dpt.js ***!
      \************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        DPTFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        DPTImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/efficientnet/image_processing_efficientnet.js": (
    /*!******************************************************************!*\
      !*** ./src/models/efficientnet/image_processing_efficientnet.js ***!
      \******************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        EfficientNetImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        constructor(Y) {
          super(Y), this.include_top = this.config.include_top ?? !0, this.include_top && (this.image_std = this.image_std.map((R) => R * R));
        }
      }
    }
  ),
  /***/
  "./src/models/feature_extractors.js": (
    /*!******************************************!*\
      !*** ./src/models/feature_extractors.js ***!
      \******************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ASTFeatureExtractor: () => (
          /* reexport safe */
          _.ASTFeatureExtractor
        ),
        /* harmony export */
        ClapFeatureExtractor: () => (
          /* reexport safe */
          D.ClapFeatureExtractor
        ),
        /* harmony export */
        ImageFeatureExtractor: () => (
          /* reexport safe */
          b.ImageProcessor
        ),
        /* harmony export */
        MoonshineFeatureExtractor: () => (
          /* reexport safe */
          U.MoonshineFeatureExtractor
        ),
        /* harmony export */
        PyAnnoteFeatureExtractor: () => (
          /* reexport safe */
          Y.PyAnnoteFeatureExtractor
        ),
        /* harmony export */
        SeamlessM4TFeatureExtractor: () => (
          /* reexport safe */
          R.SeamlessM4TFeatureExtractor
        ),
        /* harmony export */
        SpeechT5FeatureExtractor: () => (
          /* reexport safe */
          g.SpeechT5FeatureExtractor
        ),
        /* harmony export */
        Wav2Vec2FeatureExtractor: () => (
          /* reexport safe */
          v.Wav2Vec2FeatureExtractor
        ),
        /* harmony export */
        WeSpeakerFeatureExtractor: () => (
          /* reexport safe */
          M.WeSpeakerFeatureExtractor
        ),
        /* harmony export */
        WhisperFeatureExtractor: () => (
          /* reexport safe */
          y.WhisperFeatureExtractor
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js */
        "./src/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js"
      ), D = r(
        /*! ./clap/feature_extraction_clap.js */
        "./src/models/clap/feature_extraction_clap.js"
      ), U = r(
        /*! ./moonshine/feature_extraction_moonshine.js */
        "./src/models/moonshine/feature_extraction_moonshine.js"
      ), Y = r(
        /*! ./pyannote/feature_extraction_pyannote.js */
        "./src/models/pyannote/feature_extraction_pyannote.js"
      ), R = r(
        /*! ./seamless_m4t/feature_extraction_seamless_m4t.js */
        "./src/models/seamless_m4t/feature_extraction_seamless_m4t.js"
      ), g = r(
        /*! ./speecht5/feature_extraction_speecht5.js */
        "./src/models/speecht5/feature_extraction_speecht5.js"
      ), v = r(
        /*! ./wav2vec2/feature_extraction_wav2vec2.js */
        "./src/models/wav2vec2/feature_extraction_wav2vec2.js"
      ), M = r(
        /*! ./wespeaker/feature_extraction_wespeaker.js */
        "./src/models/wespeaker/feature_extraction_wespeaker.js"
      ), y = r(
        /*! ./whisper/feature_extraction_whisper.js */
        "./src/models/whisper/feature_extraction_whisper.js"
      ), b = r(
        /*! ../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
    }
  ),
  /***/
  "./src/models/florence2/processing_florence2.js": (
    /*!******************************************************!*\
      !*** ./src/models/florence2/processing_florence2.js ***!
      \******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Florence2Processor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      class Y extends _.Processor {
        constructor(g, v) {
          super(g, v);
          const {
            // @ts-expect-error TS2339
            tasks_answer_post_processing_type: M,
            // @ts-expect-error TS2339
            task_prompts_without_inputs: y,
            // @ts-expect-error TS2339
            task_prompts_with_input: b
          } = this.image_processor.config;
          this.tasks_answer_post_processing_type = new Map(Object.entries(M ?? {})), this.task_prompts_without_inputs = new Map(Object.entries(y ?? {})), this.task_prompts_with_input = new Map(Object.entries(b ?? {})), this.regexes = {
            quad_boxes: /(.+?)<loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)>/gm,
            bboxes: /([^<]+)?<loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)>/gm
          }, this.size_per_bin = 1e3;
        }
        /**
         * Helper function to construct prompts from input texts
         * @param {string|string[]} text
         * @returns {string[]}
         */
        construct_prompts(g) {
          typeof g == "string" && (g = [g]);
          const v = [];
          for (const M of g)
            if (this.task_prompts_without_inputs.has(M))
              v.push(this.task_prompts_without_inputs.get(M));
            else {
              for (const [y, b] of this.task_prompts_with_input)
                if (M.includes(y)) {
                  v.push(b.replaceAll("{input}", M).replaceAll(y, ""));
                  break;
                }
              v.length !== g.length && v.push(M);
            }
          return v;
        }
        /**
         * Post-process the output of the model to each of the task outputs.
         * @param {string} text The text to post-process.
         * @param {string} task The task to post-process the text for.
         * @param {[number, number]} image_size The size of the image. height x width.
         */
        post_process_generation(g, v, M) {
          const y = this.tasks_answer_post_processing_type.get(v) ?? "pure_text";
          g = g.replaceAll("<s>", "").replaceAll("</s>", "");
          let b;
          switch (y) {
            case "pure_text":
              b = g;
              break;
            case "description_with_bboxes":
            case "bboxes":
            case "phrase_grounding":
            case "ocr":
              const I = y === "ocr" ? "quad_boxes" : "bboxes", K = g.matchAll(this.regexes[I]), se = [], ie = [];
              for (const [W, j, ...q] of K)
                se.push(j ? j.trim() : se.at(-1) ?? ""), ie.push(
                  q.map((A, S) => (
                    // NOTE: Add 0.5 to use the center position of the bin as the coordinate.
                    (Number(A) + 0.5) / this.size_per_bin * M[S % 2]
                  ))
                );
              b = { labels: se, [I]: ie };
              break;
            default:
              throw new Error(`Task "${v}" (of type "${y}") not yet implemented.`);
          }
          return { [v]: b };
        }
        // NOTE: images and text are switched from the python version
        // `images` is required, `text` is optional
        async _call(g, v = null, M = {}) {
          if (!g && !v)
            throw new Error("Either text or images must be provided");
          const y = await this.image_processor(g, M), b = v ? this.tokenizer(v, M) : {};
          return {
            ...y,
            ...b
          };
        }
      }
      fe(Y, "tokenizer_class", U.AutoTokenizer), fe(Y, "image_processor_class", D.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/glpn/image_processing_glpn.js": (
    /*!**************************************************!*\
      !*** ./src/models/glpn/image_processing_glpn.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        GLPNFeatureExtractor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/grounding_dino/image_processing_grounding_dino.js": (
    /*!**********************************************************************!*\
      !*** ./src/models/grounding_dino/image_processing_grounding_dino.js ***!
      \**********************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        GroundingDinoImageProcessor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class U extends _.ImageProcessor {
        /**
         * Calls the feature extraction process on an array of images, preprocesses
         * each image, and concatenates the resulting features into a single Tensor.
         * @param {import('../../utils/image.js').RawImage[]} images The image(s) to extract features from.
         * @returns {Promise<GroundingDinoFeatureExtractorResult>} An object containing the concatenated pixel values of the preprocessed images.
         */
        async _call(R) {
          const g = await super._call(R), v = g.pixel_values.dims, M = (0, D.ones)([v[0], v[2], v[3]]);
          return { ...g, pixel_mask: M };
        }
      }
    }
  ),
  /***/
  "./src/models/grounding_dino/processing_grounding_dino.js": (
    /*!****************************************************************!*\
      !*** ./src/models/grounding_dino/processing_grounding_dino.js ***!
      \****************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        GroundingDinoProcessor: () => (
          /* binding */
          g
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), Y = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      function R(v, M) {
        const b = v.dims.at(-1) - 1, I = v.tolist();
        I.fill(!1, 0, 1), I.fill(!1, b);
        const K = M.tolist();
        return I.map((se, ie) => se ? ie : null).filter((se) => se !== null).map((se) => K[se]);
      }
      class g extends _.Processor {
        /**
         * @typedef {import('../../utils/image.js').RawImage} RawImage
         */
        /**
         * 
         * @param {RawImage|RawImage[]|RawImage[][]} images  
         * @param {string|string[]} text 
         * @returns {Promise<any>}
         */
        async _call(M, y, b = {}) {
          const I = M ? await this.image_processor(M, b) : {};
          return {
            ...y ? this.tokenizer(y, b) : {},
            ...I
          };
        }
        post_process_grounded_object_detection(M, y, {
          box_threshold: b = 0.25,
          text_threshold: I = 0.25,
          target_sizes: K = null
        } = {}) {
          const { logits: se, pred_boxes: ie } = M, W = se.dims[0];
          if (K !== null && K.length !== W)
            throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");
          const j = se.dims.at(1), q = se.sigmoid(), A = q.max(-1).tolist(), S = ie.tolist().map((x) => x.map((F) => (0, Y.center_to_corners_format)(F))), w = [];
          for (let x = 0; x < W; ++x) {
            const F = K !== null ? K[x] : null;
            F !== null && (S[x] = S[x].map((re) => re.map((xe, ce) => xe * F[(ce + 1) % 2])));
            const le = A[x], ne = [], be = [], _e = [];
            for (let re = 0; re < j; ++re) {
              const xe = le[re];
              if (xe <= b)
                continue;
              const ce = S[x][re], ke = q[x][re];
              ne.push(xe), _e.push(ce);
              const Fe = R(ke.gt(I), y[x]);
              be.push(Fe);
            }
            w.push({ scores: ne, boxes: _e, labels: this.batch_decode(be) });
          }
          return w;
        }
      }
      fe(g, "tokenizer_class", U.AutoTokenizer), fe(g, "image_processor_class", D.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/idefics3/image_processing_idefics3.js": (
    /*!**********************************************************!*\
      !*** ./src/models/idefics3/image_processing_idefics3.js ***!
      \**********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Idefics3ImageProcessor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class U extends _.ImageProcessor {
        constructor(R) {
          super(R), this.do_image_splitting = R.do_image_splitting ?? !0, this.max_image_size = R.max_image_size;
        }
        /**
         * @typedef {import('../../utils/image.js').RawImage} RawImage
         * @typedef {import('../../utils/tensor.js').Tensor} Tensor
         */
        /**
         * Calculate size to resize images to, to be multiples of `vision_encoder_max_size` while preserving the aspect ratio.
         * @param {Tensor} pixel_values Tensor of the image to resize.
         * @param {number} vision_encoder_max_size Maximum size of the output image. If the image is larger than this size,
         * it will be split into patches of this size, and the original image will be concatenated with the patches, resized to max_size.
         */
        get_resize_for_vision_encoder(R, g) {
          let [v, M] = R.dims.slice(-2);
          const y = M / v;
          return M >= v ? (M = Math.ceil(M / g) * g, v = Math.floor(M / y), v = Math.ceil(v / g) * g) : (v = Math.ceil(v / g) * g, M = Math.floor(v * y), M = Math.ceil(M / g) * g), { height: v, width: M };
        }
        /** @param {RawImage|RawImage[]|RawImage[][]} images */
        async _call(R, {
          do_image_splitting: g = null,
          return_row_col_info: v = !1
        } = {}) {
          let M;
          if (!Array.isArray(R))
            M = [[R]];
          else {
            if (R.length === 0 || !R[0])
              throw new Error("No images provided.");
            Array.isArray(R[0]) ? M = /** @type {RawImage[][]} */
            R : M = [
              /** @type {RawImage[]} */
              R
            ];
          }
          let y = [], b = [], I = [];
          const K = [], se = [];
          for (const x of M) {
            let F = await Promise.all(x.map((be) => this.preprocess(be)));
            K.push(...F.map((be) => be.original_size)), se.push(...F.map((be) => be.reshaped_input_size)), F.forEach((be) => be.pixel_values.unsqueeze_(0));
            const { longest_edge: le } = this.max_image_size;
            let ne;
            if (g ?? this.do_image_splitting) {
              let be = new Array(F.length), _e = new Array(F.length);
              ne = await Promise.all(F.map(async (re, xe) => {
                const ce = this.get_resize_for_vision_encoder(re.pixel_values, le), ke = await (0, D.interpolate_4d)(re.pixel_values, {
                  size: [ce.height, ce.width]
                }), { frames: Fe, num_splits_h: Ee, num_splits_w: tt } = await this.split_image(ke, this.max_image_size);
                return be[xe] = Ee, _e[xe] = tt, (0, D.cat)(Fe, 0);
              })), b.push(be), I.push(_e);
            } else {
              const be = [le, le];
              ne = await Promise.all(
                F.map((_e) => (0, D.interpolate_4d)(_e.pixel_values, { size: be }))
              ), b.push(new Array(F.length).fill(0)), I.push(new Array(F.length).fill(0));
            }
            y.push((0, D.cat)(ne, 0));
          }
          const ie = y.length, [W, j, q, A] = y[0].dims;
          let S, w;
          if (ie === 1)
            S = y[0].unsqueeze_(0), w = (0, D.full)([ie, W, q, A], !0);
          else {
            const x = Math.max(...y.map((ne) => ne.dims.at(0)));
            w = (0, D.full)([ie, x, q, A], !0);
            const F = w.data, le = x * q * A;
            for (let ne = 0; ne < ie; ++ne) {
              const be = y[ne].dims[0];
              if (be < x) {
                y[ne] = (0, D.cat)([
                  y[ne],
                  (0, D.full)([x - be, j, q, A], 0)
                ], 0);
                const _e = ne * le + be * q * A, re = (ne + 1) * le;
                F.fill(!1, _e, re);
              }
            }
            S = (0, D.stack)(y, 0);
          }
          return {
            pixel_values: S,
            pixel_attention_mask: w,
            original_sizes: K,
            reshaped_input_sizes: se,
            ...v ? { rows: b, cols: I } : {}
          };
        }
        async split_image(R, { longest_edge: g }) {
          const v = g, M = g, y = [], [b, I] = R.dims.slice(-2);
          let K = 0, se = 0;
          if (b > v || I > M) {
            K = Math.ceil(b / v), se = Math.ceil(I / M);
            const ie = Math.ceil(b / K), W = Math.ceil(I / se);
            for (let A = 0; A < K; ++A)
              for (let S = 0; S < se; ++S) {
                let w, x, F, le;
                A === K - 1 ? (x = b - ie, le = b) : (x = A * ie, le = (A + 1) * ie), S === se - 1 ? (w = I - W, F = I) : (w = S * W, F = (S + 1) * W);
                const ne = [x, w], be = [le, F], _e = await (0, D.slice)(R, ne, be, [2, 3]);
                y.push(_e);
              }
            const j = v, q = M;
            (b !== j || I !== q) && (R = await (0, D.interpolate_4d)(R, {
              size: [j, q]
            }));
          }
          return y.push(R), { frames: y, num_splits_h: K, num_splits_w: se };
        }
      }
    }
  ),
  /***/
  "./src/models/idefics3/processing_idefics3.js": (
    /*!****************************************************!*\
      !*** ./src/models/idefics3/processing_idefics3.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Idefics3Processor: () => (
          /* binding */
          M
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      r(
        /*! ../../utils/image.js */
        "./src/utils/image.js"
      );
      var Y = r(
        /*! ../../utils/core.js */
        "./src/utils/core.js"
      );
      function R(y, b, I, K, se, ie) {
        let W = "";
        for (let j = 0; j < b; ++j) {
          for (let q = 0; q < I; ++q)
            W += K + `<row_${j + 1}_col_${q + 1}>` + se.repeat(y);
          W += `
`;
        }
        return W += `
${K}${ie}` + se.repeat(y) + `${K}`, W;
      }
      function g(y, b, I, K) {
        return `${b}${K}` + I.repeat(y) + `${b}`;
      }
      function v(y, b, I, K, se, ie) {
        return y === 0 && b === 0 ? g(
          I,
          K,
          se,
          ie
        ) : R(
          I,
          y,
          b,
          K,
          se,
          ie
        );
      }
      class M extends _.Processor {
        constructor() {
          super(...arguments);
          fe(this, "fake_image_token", "<fake_token_around_image>");
          fe(this, "image_token", "<image>");
          fe(this, "global_img_token", "<global-img>");
        }
        /**
         * 
         * @param {string|string[]} text 
         * @param {RawImage|RawImage[]|RawImage[][]} images  
         * @returns {Promise<any>}
         */
        async _call(I, K = null, se = {}) {
          se.return_row_col_info ?? (se.return_row_col_info = !0);
          let ie;
          K && (ie = await this.image_processor(K, se)), Array.isArray(I) || (I = [I]);
          const W = ie.rows ?? [new Array(I.length).fill(0)], j = ie.cols ?? [new Array(I.length).fill(0)], q = this.config.image_seq_len, A = [], S = [];
          for (let x = 0; x < I.length; ++x) {
            const F = I[x], le = W[x], ne = j[x];
            A.push((0, Y.count)(F, this.image_token));
            const be = le.map(
              (xe, ce) => v(
                xe,
                ne[ce],
                q,
                this.fake_image_token,
                this.image_token,
                this.global_img_token
              )
            ), _e = F.split(this.image_token);
            if (_e.length === 0)
              throw new Error("The image token should be present in the text.");
            let re = _e[0];
            for (let xe = 0; xe < be.length; ++xe)
              re += be[xe] + _e[xe + 1];
            S.push(re);
          }
          return {
            ...this.tokenizer(S),
            ...ie
          };
        }
      }
      fe(M, "image_processor_class", D.AutoImageProcessor), fe(M, "tokenizer_class", U.AutoTokenizer), fe(M, "uses_processor_config", !0);
    }
  ),
  /***/
  "./src/models/image_processors.js": (
    /*!****************************************!*\
      !*** ./src/models/image_processors.js ***!
      \****************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        BeitFeatureExtractor: () => (
          /* reexport safe */
          _.BeitFeatureExtractor
        ),
        /* harmony export */
        BitImageProcessor: () => (
          /* reexport safe */
          D.BitImageProcessor
        ),
        /* harmony export */
        CLIPFeatureExtractor: () => (
          /* reexport safe */
          Y.CLIPFeatureExtractor
        ),
        /* harmony export */
        CLIPImageProcessor: () => (
          /* reexport safe */
          Y.CLIPImageProcessor
        ),
        /* harmony export */
        ChineseCLIPFeatureExtractor: () => (
          /* reexport safe */
          U.ChineseCLIPFeatureExtractor
        ),
        /* harmony export */
        ConvNextFeatureExtractor: () => (
          /* reexport safe */
          R.ConvNextFeatureExtractor
        ),
        /* harmony export */
        ConvNextImageProcessor: () => (
          /* reexport safe */
          R.ConvNextImageProcessor
        ),
        /* harmony export */
        DPTFeatureExtractor: () => (
          /* reexport safe */
          y.DPTFeatureExtractor
        ),
        /* harmony export */
        DPTImageProcessor: () => (
          /* reexport safe */
          y.DPTImageProcessor
        ),
        /* harmony export */
        DeiTFeatureExtractor: () => (
          /* reexport safe */
          g.DeiTFeatureExtractor
        ),
        /* harmony export */
        DeiTImageProcessor: () => (
          /* reexport safe */
          g.DeiTImageProcessor
        ),
        /* harmony export */
        DetrFeatureExtractor: () => (
          /* reexport safe */
          v.DetrFeatureExtractor
        ),
        /* harmony export */
        DetrImageProcessor: () => (
          /* reexport safe */
          v.DetrImageProcessor
        ),
        /* harmony export */
        DonutFeatureExtractor: () => (
          /* reexport safe */
          M.DonutFeatureExtractor
        ),
        /* harmony export */
        DonutImageProcessor: () => (
          /* reexport safe */
          M.DonutImageProcessor
        ),
        /* harmony export */
        EfficientNetImageProcessor: () => (
          /* reexport safe */
          b.EfficientNetImageProcessor
        ),
        /* harmony export */
        GLPNFeatureExtractor: () => (
          /* reexport safe */
          I.GLPNFeatureExtractor
        ),
        /* harmony export */
        GroundingDinoImageProcessor: () => (
          /* reexport safe */
          K.GroundingDinoImageProcessor
        ),
        /* harmony export */
        Idefics3ImageProcessor: () => (
          /* reexport safe */
          se.Idefics3ImageProcessor
        ),
        /* harmony export */
        JinaCLIPImageProcessor: () => (
          /* reexport safe */
          W.JinaCLIPImageProcessor
        ),
        /* harmony export */
        LlavaOnevisionImageProcessor: () => (
          /* reexport safe */
          j.LlavaOnevisionImageProcessor
        ),
        /* harmony export */
        Mask2FormerImageProcessor: () => (
          /* reexport safe */
          q.Mask2FormerImageProcessor
        ),
        /* harmony export */
        MaskFormerFeatureExtractor: () => (
          /* reexport safe */
          A.MaskFormerFeatureExtractor
        ),
        /* harmony export */
        MaskFormerImageProcessor: () => (
          /* reexport safe */
          A.MaskFormerImageProcessor
        ),
        /* harmony export */
        MobileNetV1FeatureExtractor: () => (
          /* reexport safe */
          S.MobileNetV1FeatureExtractor
        ),
        /* harmony export */
        MobileNetV1ImageProcessor: () => (
          /* reexport safe */
          S.MobileNetV1ImageProcessor
        ),
        /* harmony export */
        MobileNetV2FeatureExtractor: () => (
          /* reexport safe */
          w.MobileNetV2FeatureExtractor
        ),
        /* harmony export */
        MobileNetV2ImageProcessor: () => (
          /* reexport safe */
          w.MobileNetV2ImageProcessor
        ),
        /* harmony export */
        MobileNetV3FeatureExtractor: () => (
          /* reexport safe */
          x.MobileNetV3FeatureExtractor
        ),
        /* harmony export */
        MobileNetV3ImageProcessor: () => (
          /* reexport safe */
          x.MobileNetV3ImageProcessor
        ),
        /* harmony export */
        MobileNetV4FeatureExtractor: () => (
          /* reexport safe */
          F.MobileNetV4FeatureExtractor
        ),
        /* harmony export */
        MobileNetV4ImageProcessor: () => (
          /* reexport safe */
          F.MobileNetV4ImageProcessor
        ),
        /* harmony export */
        MobileViTFeatureExtractor: () => (
          /* reexport safe */
          le.MobileViTFeatureExtractor
        ),
        /* harmony export */
        MobileViTImageProcessor: () => (
          /* reexport safe */
          le.MobileViTImageProcessor
        ),
        /* harmony export */
        NougatImageProcessor: () => (
          /* reexport safe */
          ne.NougatImageProcessor
        ),
        /* harmony export */
        OwlViTFeatureExtractor: () => (
          /* reexport safe */
          _e.OwlViTFeatureExtractor
        ),
        /* harmony export */
        OwlViTImageProcessor: () => (
          /* reexport safe */
          _e.OwlViTImageProcessor
        ),
        /* harmony export */
        Owlv2ImageProcessor: () => (
          /* reexport safe */
          be.Owlv2ImageProcessor
        ),
        /* harmony export */
        Phi3VImageProcessor: () => (
          /* reexport safe */
          re.Phi3VImageProcessor
        ),
        /* harmony export */
        PvtImageProcessor: () => (
          /* reexport safe */
          xe.PvtImageProcessor
        ),
        /* harmony export */
        Qwen2VLImageProcessor: () => (
          /* reexport safe */
          ce.Qwen2VLImageProcessor
        ),
        /* harmony export */
        RTDetrImageProcessor: () => (
          /* reexport safe */
          ke.RTDetrImageProcessor
        ),
        /* harmony export */
        SamImageProcessor: () => (
          /* reexport safe */
          Fe.SamImageProcessor
        ),
        /* harmony export */
        SegformerFeatureExtractor: () => (
          /* reexport safe */
          Ee.SegformerFeatureExtractor
        ),
        /* harmony export */
        SegformerImageProcessor: () => (
          /* reexport safe */
          Ee.SegformerImageProcessor
        ),
        /* harmony export */
        SiglipImageProcessor: () => (
          /* reexport safe */
          tt.SiglipImageProcessor
        ),
        /* harmony export */
        Swin2SRImageProcessor: () => (
          /* reexport safe */
          Ge.Swin2SRImageProcessor
        ),
        /* harmony export */
        VLMImageProcessor: () => (
          /* reexport safe */
          ie.VLMImageProcessor
        ),
        /* harmony export */
        ViTFeatureExtractor: () => (
          /* reexport safe */
          ye.ViTFeatureExtractor
        ),
        /* harmony export */
        ViTImageProcessor: () => (
          /* reexport safe */
          ye.ViTImageProcessor
        ),
        /* harmony export */
        VitMatteImageProcessor: () => (
          /* reexport safe */
          J.VitMatteImageProcessor
        ),
        /* harmony export */
        VitPoseImageProcessor: () => (
          /* reexport safe */
          de.VitPoseImageProcessor
        ),
        /* harmony export */
        YolosFeatureExtractor: () => (
          /* reexport safe */
          Ce.YolosFeatureExtractor
        ),
        /* harmony export */
        YolosImageProcessor: () => (
          /* reexport safe */
          Ce.YolosImageProcessor
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./beit/image_processing_beit.js */
        "./src/models/beit/image_processing_beit.js"
      ), D = r(
        /*! ./bit/image_processing_bit.js */
        "./src/models/bit/image_processing_bit.js"
      ), U = r(
        /*! ./chinese_clip/image_processing_chinese_clip.js */
        "./src/models/chinese_clip/image_processing_chinese_clip.js"
      ), Y = r(
        /*! ./clip/image_processing_clip.js */
        "./src/models/clip/image_processing_clip.js"
      ), R = r(
        /*! ./convnext/image_processing_convnext.js */
        "./src/models/convnext/image_processing_convnext.js"
      ), g = r(
        /*! ./deit/image_processing_deit.js */
        "./src/models/deit/image_processing_deit.js"
      ), v = r(
        /*! ./detr/image_processing_detr.js */
        "./src/models/detr/image_processing_detr.js"
      ), M = r(
        /*! ./donut/image_processing_donut.js */
        "./src/models/donut/image_processing_donut.js"
      ), y = r(
        /*! ./dpt/image_processing_dpt.js */
        "./src/models/dpt/image_processing_dpt.js"
      ), b = r(
        /*! ./efficientnet/image_processing_efficientnet.js */
        "./src/models/efficientnet/image_processing_efficientnet.js"
      ), I = r(
        /*! ./glpn/image_processing_glpn.js */
        "./src/models/glpn/image_processing_glpn.js"
      ), K = r(
        /*! ./grounding_dino/image_processing_grounding_dino.js */
        "./src/models/grounding_dino/image_processing_grounding_dino.js"
      ), se = r(
        /*! ./idefics3/image_processing_idefics3.js */
        "./src/models/idefics3/image_processing_idefics3.js"
      ), ie = r(
        /*! ./janus/image_processing_janus.js */
        "./src/models/janus/image_processing_janus.js"
      ), W = r(
        /*! ./jina_clip/image_processing_jina_clip.js */
        "./src/models/jina_clip/image_processing_jina_clip.js"
      ), j = r(
        /*! ./llava_onevision/image_processing_llava_onevision.js */
        "./src/models/llava_onevision/image_processing_llava_onevision.js"
      ), q = r(
        /*! ./mask2former/image_processing_mask2former.js */
        "./src/models/mask2former/image_processing_mask2former.js"
      ), A = r(
        /*! ./maskformer/image_processing_maskformer.js */
        "./src/models/maskformer/image_processing_maskformer.js"
      ), S = r(
        /*! ./mobilenet_v1/image_processing_mobilenet_v1.js */
        "./src/models/mobilenet_v1/image_processing_mobilenet_v1.js"
      ), w = r(
        /*! ./mobilenet_v2/image_processing_mobilenet_v2.js */
        "./src/models/mobilenet_v2/image_processing_mobilenet_v2.js"
      ), x = r(
        /*! ./mobilenet_v3/image_processing_mobilenet_v3.js */
        "./src/models/mobilenet_v3/image_processing_mobilenet_v3.js"
      ), F = r(
        /*! ./mobilenet_v4/image_processing_mobilenet_v4.js */
        "./src/models/mobilenet_v4/image_processing_mobilenet_v4.js"
      ), le = r(
        /*! ./mobilevit/image_processing_mobilevit.js */
        "./src/models/mobilevit/image_processing_mobilevit.js"
      ), ne = r(
        /*! ./nougat/image_processing_nougat.js */
        "./src/models/nougat/image_processing_nougat.js"
      ), be = r(
        /*! ./owlv2/image_processing_owlv2.js */
        "./src/models/owlv2/image_processing_owlv2.js"
      ), _e = r(
        /*! ./owlvit/image_processing_owlvit.js */
        "./src/models/owlvit/image_processing_owlvit.js"
      ), re = r(
        /*! ./phi3_v/image_processing_phi3_v.js */
        "./src/models/phi3_v/image_processing_phi3_v.js"
      ), xe = r(
        /*! ./pvt/image_processing_pvt.js */
        "./src/models/pvt/image_processing_pvt.js"
      ), ce = r(
        /*! ./qwen2_vl/image_processing_qwen2_vl.js */
        "./src/models/qwen2_vl/image_processing_qwen2_vl.js"
      ), ke = r(
        /*! ./rt_detr/image_processing_rt_detr.js */
        "./src/models/rt_detr/image_processing_rt_detr.js"
      ), Fe = r(
        /*! ./sam/image_processing_sam.js */
        "./src/models/sam/image_processing_sam.js"
      ), Ee = r(
        /*! ./segformer/image_processing_segformer.js */
        "./src/models/segformer/image_processing_segformer.js"
      ), tt = r(
        /*! ./siglip/image_processing_siglip.js */
        "./src/models/siglip/image_processing_siglip.js"
      ), Ge = r(
        /*! ./swin2sr/image_processing_swin2sr.js */
        "./src/models/swin2sr/image_processing_swin2sr.js"
      ), ye = r(
        /*! ./vit/image_processing_vit.js */
        "./src/models/vit/image_processing_vit.js"
      ), J = r(
        /*! ./vitmatte/image_processing_vitmatte.js */
        "./src/models/vitmatte/image_processing_vitmatte.js"
      ), de = r(
        /*! ./vitpose/image_processing_vitpose.js */
        "./src/models/vitpose/image_processing_vitpose.js"
      ), Ce = r(
        /*! ./yolos/image_processing_yolos.js */
        "./src/models/yolos/image_processing_yolos.js"
      );
    }
  ),
  /***/
  "./src/models/janus/image_processing_janus.js": (
    /*!****************************************************!*\
      !*** ./src/models/janus/image_processing_janus.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        VLMImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        constructor(Y) {
          super({
            do_pad: !0,
            pad_size: {
              width: Y.image_size,
              height: Y.image_size
            },
            ...Y
          }), this.constant_values = this.config.background_color.map((R) => R * this.rescale_factor);
        }
        pad_image(Y, R, g, v) {
          return super.pad_image(Y, R, g, {
            constant_values: this.constant_values,
            center: !0,
            ...v
          });
        }
      }
    }
  ),
  /***/
  "./src/models/janus/processing_janus.js": (
    /*!**********************************************!*\
      !*** ./src/models/janus/processing_janus.js ***!
      \**********************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        VLChatProcessor: () => (
          /* binding */
          v
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), Y = r(
        /*! ../../utils/core.js */
        "./src/utils/core.js"
      ), R = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      ), g = r(
        /*! ../../utils/image.js */
        "./src/utils/image.js"
      );
      class v extends _.Processor {
        constructor(y, b) {
          super(y, b), this.image_tag = this.config.image_tag, this.image_start_tag = this.config.image_start_tag, this.image_end_tag = this.config.image_end_tag, this.num_image_tokens = this.config.num_image_tokens;
        }
        /**
         * @typedef {Object} MultimodalMessageProperties Additional properties for multimodal messages.
         * @property {(RawImage | string | URL)[]} [images] The images in the message.
         * @typedef {(import('../../tokenizers.js').Message & MultimodalMessageProperties)[]} MultimodalConversation The conversation possibly containing multimodal inputs.
         */
        /**
         * @typedef {Object} VLCChatProcessorResult The processed input.
         * @property {Tensor} input_ids The input IDs.
         * @property {Tensor} attention_mask The attention mask.
         * @property {Tensor} images_seq_mask The image sequence mask.
         * @property {Tensor} images_emb_mask The image embedding mask.
         */
        /**
         * @param {MultimodalConversation} conversation The chat messages to process.
         * @param {Object} options Additional options for processing.
         * @param {RawImage|RawImage[]} [options.images] The images to process, if not set in the conversation.
         * @param {string} [options.chat_template="default"] The chat template to use.
         * @returns {Promise<VLCChatProcessorResult | VLCChatProcessorResult & import('../../base/image_processors_utils.js').ImageProcessorResult>} The processed input.
         */
        async _call(y, {
          images: b = null,
          chat_template: I = "default"
        } = {}) {
          b ? Array.isArray(b) || (b = [b]) : b = await Promise.all(
            y.filter((ne) => ne.images).flatMap((ne) => ne.images).map((ne) => g.RawImage.read(ne))
          );
          const K = this.tokenizer, se = K.apply_chat_template(y, {
            tokenize: !1,
            add_generation_prompt: !0,
            chat_template: I
          }), ie = (ne) => K.encode(ne, { add_special_tokens: !1 }), W = (
            /** @type {string} */
            se.split(this.image_tag)
          ), j = W.length - 1;
          if (b.length !== j)
            throw new Error(`Number of images provided (${b.length}) does not match number of "${this.image_tag}" image tags (${j})`);
          const [
            q,
            A,
            S
          ] = K.model.convert_tokens_to_ids([
            this.image_tag,
            this.image_start_tag,
            this.image_end_tag
          ]);
          let w = ie(W[0]), x = new Array(w.length).fill(!1);
          for (let ne = 1; ne < W.length; ++ne) {
            const be = new Array(this.num_image_tokens).fill(q), _e = ie(W[ne]);
            w = (0, Y.mergeArrays)(
              w,
              [A],
              be,
              [S],
              _e
            );
            const re = new Array(this.num_image_tokens).fill(!0);
            x = (0, Y.mergeArrays)(
              x,
              [!1],
              re,
              [!1],
              new Array(_e.length).fill(!1)
            );
          }
          const F = [1, w.length], le = {
            input_ids: new R.Tensor("int64", w, F),
            attention_mask: new R.Tensor("int64", new Array(w.length).fill(1), F),
            images_seq_mask: new R.Tensor("bool", x, F),
            images_emb_mask: new R.Tensor(
              "bool",
              new Array(j * this.num_image_tokens).fill(!0),
              [1, j, this.num_image_tokens]
            )
          };
          if (b && b.length > 0) {
            const ne = await this.image_processor(b);
            return ne.pixel_values.unsqueeze_(0), { ...le, ...ne };
          }
          return le;
        }
      }
      fe(v, "image_processor_class", D.AutoImageProcessor), fe(v, "tokenizer_class", U.AutoTokenizer), fe(v, "uses_processor_config", !0);
    }
  ),
  /***/
  "./src/models/jina_clip/image_processing_jina_clip.js": (
    /*!************************************************************!*\
      !*** ./src/models/jina_clip/image_processing_jina_clip.js ***!
      \************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        JinaCLIPImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        constructor(Y) {
          const { resize_mode: R, fill_color: g, interpolation: v, size: M, ...y } = Y, b = R === "squash" ? { width: M, height: M } : R === "shortest" ? { shortest_edge: M } : { longest_edge: M }, I = v === "bicubic" ? 3 : 2;
          super({
            ...y,
            size: b,
            resample: I,
            do_center_crop: !0,
            crop_size: M,
            do_normalize: !0
          });
        }
      }
    }
  ),
  /***/
  "./src/models/jina_clip/processing_jina_clip.js": (
    /*!******************************************************!*\
      !*** ./src/models/jina_clip/processing_jina_clip.js ***!
      \******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        JinaCLIPProcessor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      class Y extends _.Processor {
        async _call(g = null, v = null, M = {}) {
          if (!g && !v)
            throw new Error("Either text or images must be provided");
          const y = g ? this.tokenizer(g, M) : {}, b = v ? await this.image_processor(v, M) : {};
          return {
            ...y,
            ...b
          };
        }
      }
      fe(Y, "tokenizer_class", U.AutoTokenizer), fe(Y, "image_processor_class", D.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/llava_onevision/image_processing_llava_onevision.js": (
    /*!************************************************************************!*\
      !*** ./src/models/llava_onevision/image_processing_llava_onevision.js ***!
      \************************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        LlavaOnevisionImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/mask2former/image_processing_mask2former.js": (
    /*!****************************************************************!*\
      !*** ./src/models/mask2former/image_processing_mask2former.js ***!
      \****************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Mask2FormerImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../maskformer/image_processing_maskformer.js */
        "./src/models/maskformer/image_processing_maskformer.js"
      );
      class D extends _.MaskFormerImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/maskformer/image_processing_maskformer.js": (
    /*!**************************************************************!*\
      !*** ./src/models/maskformer/image_processing_maskformer.js ***!
      \**************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MaskFormerFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        MaskFormerImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        /** @type {typeof post_process_panoptic_segmentation} */
        post_process_panoptic_segmentation(...R) {
          return (0, _.post_process_panoptic_segmentation)(...R);
        }
        /** @type {typeof post_process_instance_segmentation} */
        post_process_instance_segmentation(...R) {
          return (0, _.post_process_instance_segmentation)(...R);
        }
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/mgp_str/processing_mgp_str.js": (
    /*!**************************************************!*\
      !*** ./src/models/mgp_str/processing_mgp_str.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MgpstrProcessor: () => (
          /* binding */
          g
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), Y = r(
        /*! ../../utils/maths.js */
        "./src/utils/maths.js"
      );
      const R = {
        char: ["char_decode", 1],
        bpe: ["bpe_decode", 2],
        wp: ["wp_decode", 102]
      };
      class g extends _.Processor {
        /**
         * @returns {import('../../tokenizers.js').MgpstrTokenizer} The character tokenizer.
         */
        get char_tokenizer() {
          return this.components.char_tokenizer;
        }
        /**
         * @returns {import('../../tokenizers.js').GPT2Tokenizer} The BPE tokenizer.
         */
        get bpe_tokenizer() {
          return this.components.bpe_tokenizer;
        }
        /**
         * @returns {import('../../tokenizers.js').BertTokenizer} The WordPiece tokenizer.
         */
        get wp_tokenizer() {
          return this.components.wp_tokenizer;
        }
        /**
         * Helper function to decode the model prediction logits.
         * @param {import('../../utils/tensor.js').Tensor} pred_logits Model prediction logits.
         * @param {string} format Type of model prediction. Must be one of ['char', 'bpe', 'wp'].
         * @returns {[string[], number[]]} The decoded sentences and their confidence scores.
         */
        _decode_helper(M, y) {
          if (!R.hasOwnProperty(y))
            throw new Error(`Format ${y} is not supported.`);
          const [b, I] = R[y], K = this[b].bind(this), [se, ie] = M.dims, W = [], j = [], q = M.tolist();
          for (let S = 0; S < se; ++S) {
            const w = q[S], x = [], F = [];
            for (let ne = 1; ne < ie; ++ne) {
              const [be, _e] = (0, Y.max)((0, Y.softmax)(w[ne]));
              if (F.push(be), _e == I)
                break;
              x.push(_e);
            }
            const le = F.length > 0 ? F.reduce((ne, be) => ne * be, 1) : 0;
            j.push(x), W.push(le);
          }
          return [K(j), W];
        }
        /**
         * Convert a list of lists of char token ids into a list of strings by calling char tokenizer.
         * @param {number[][]} sequences List of tokenized input ids.
         * @returns {string[]} The list of char decoded sentences.
         */
        char_decode(M) {
          return this.char_tokenizer.batch_decode(M).map((y) => y.replaceAll(" ", ""));
        }
        /**
         * Convert a list of lists of BPE token ids into a list of strings by calling BPE tokenizer.
         * @param {number[][]} sequences List of tokenized input ids.
         * @returns {string[]} The list of BPE decoded sentences.
         */
        bpe_decode(M) {
          return this.bpe_tokenizer.batch_decode(M);
        }
        /**
         * Convert a list of lists of word piece token ids into a list of strings by calling word piece tokenizer.
         * @param {number[][]} sequences List of tokenized input ids.
         * @returns {string[]} The list of wp decoded sentences.
         */
        wp_decode(M) {
          return this.wp_tokenizer.batch_decode(M).map((y) => y.replaceAll(" ", ""));
        }
        /**
         * Convert a list of lists of token ids into a list of strings by calling decode.
         * @param {import('../../utils/tensor.js').Tensor[]} sequences List of tokenized input ids.
         * @returns {{generated_text: string[], scores: number[], char_preds: string[], bpe_preds: string[], wp_preds: string[]}}
         * Dictionary of all the outputs of the decoded results.
         * - generated_text: The final results after fusion of char, bpe, and wp.
         * - scores: The final scores after fusion of char, bpe, and wp.
         * - char_preds: The list of character decoded sentences.
         * - bpe_preds: The list of BPE decoded sentences.
         * - wp_preds: The list of wp decoded sentences.
         */
        // @ts-expect-error The type of this method is not compatible with the one
        // in the base class. It might be a good idea to fix this.
        batch_decode([M, y, b]) {
          const [I, K] = this._decode_helper(M, "char"), [se, ie] = this._decode_helper(y, "bpe"), [W, j] = this._decode_helper(b, "wp"), q = [], A = [];
          for (let S = 0; S < I.length; ++S) {
            const [w, x] = (0, Y.max)([K[S], ie[S], j[S]]);
            q.push([I[S], se[S], W[S]][x]), A.push(w);
          }
          return {
            generated_text: q,
            scores: A,
            char_preds: I,
            bpe_preds: se,
            wp_preds: W
          };
        }
        /** @type {typeof Processor.from_pretrained} */
        static async from_pretrained(...M) {
          const y = await super.from_pretrained(...M), b = await U.AutoTokenizer.from_pretrained("Xenova/gpt2"), I = await U.AutoTokenizer.from_pretrained("Xenova/bert-base-uncased");
          return y.components = {
            image_processor: y.image_processor,
            char_tokenizer: y.tokenizer,
            bpe_tokenizer: b,
            wp_tokenizer: I
          }, y;
        }
        async _call(M, y = null) {
          const b = await this.image_processor(M);
          return y && (b.labels = this.tokenizer(y).input_ids), b;
        }
      }
      fe(g, "tokenizer_class", U.AutoTokenizer), fe(g, "image_processor_class", D.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/mobilenet_v1/image_processing_mobilenet_v1.js": (
    /*!******************************************************************!*\
      !*** ./src/models/mobilenet_v1/image_processing_mobilenet_v1.js ***!
      \******************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MobileNetV1FeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        MobileNetV1ImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/mobilenet_v2/image_processing_mobilenet_v2.js": (
    /*!******************************************************************!*\
      !*** ./src/models/mobilenet_v2/image_processing_mobilenet_v2.js ***!
      \******************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MobileNetV2FeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        MobileNetV2ImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/mobilenet_v3/image_processing_mobilenet_v3.js": (
    /*!******************************************************************!*\
      !*** ./src/models/mobilenet_v3/image_processing_mobilenet_v3.js ***!
      \******************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MobileNetV3FeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        MobileNetV3ImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/mobilenet_v4/image_processing_mobilenet_v4.js": (
    /*!******************************************************************!*\
      !*** ./src/models/mobilenet_v4/image_processing_mobilenet_v4.js ***!
      \******************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MobileNetV4FeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        MobileNetV4ImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/mobilevit/image_processing_mobilevit.js": (
    /*!************************************************************!*\
      !*** ./src/models/mobilevit/image_processing_mobilevit.js ***!
      \************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MobileViTFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        MobileViTImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/moonshine/feature_extraction_moonshine.js": (
    /*!**************************************************************!*\
      !*** ./src/models/moonshine/feature_extraction_moonshine.js ***!
      \**************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MoonshineFeatureExtractor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class U extends _.FeatureExtractor {
        /**
         * Asynchronously extracts input values from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_values: Tensor; }>} The extracted input values.
         */
        async _call(R) {
          (0, _.validate_audio_inputs)(R, "MoonshineFeatureExtractor"), R instanceof Float64Array && (R = new Float32Array(R));
          const g = [
            1,
            /* batch_size */
            R.length
            /* num_samples */
          ];
          return {
            input_values: new D.Tensor("float32", R, g)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/moonshine/processing_moonshine.js": (
    /*!******************************************************!*\
      !*** ./src/models/moonshine/processing_moonshine.js ***!
      \******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        MoonshineProcessor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      ), D = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), U = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      );
      class Y extends U.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(g) {
          return await this.feature_extractor(g);
        }
      }
      fe(Y, "tokenizer_class", D.AutoTokenizer), fe(Y, "feature_extractor_class", _.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/nougat/image_processing_nougat.js": (
    /*!******************************************************!*\
      !*** ./src/models/nougat/image_processing_nougat.js ***!
      \******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        NougatImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../donut/image_processing_donut.js */
        "./src/models/donut/image_processing_donut.js"
      );
      class D extends _.DonutImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/owlv2/image_processing_owlv2.js": (
    /*!****************************************************!*\
      !*** ./src/models/owlv2/image_processing_owlv2.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Owlv2ImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../owlvit/image_processing_owlvit.js */
        "./src/models/owlvit/image_processing_owlvit.js"
      );
      class D extends _.OwlViTImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/owlvit/image_processing_owlvit.js": (
    /*!******************************************************!*\
      !*** ./src/models/owlvit/image_processing_owlvit.js ***!
      \******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        OwlViTFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        OwlViTImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        /** @type {typeof post_process_object_detection} */
        post_process_object_detection(...R) {
          return (0, _.post_process_object_detection)(...R);
        }
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/owlvit/processing_owlvit.js": (
    /*!************************************************!*\
      !*** ./src/models/owlvit/processing_owlvit.js ***!
      \************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        OwlViTProcessor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      class Y extends _.Processor {
      }
      fe(Y, "tokenizer_class", U.AutoTokenizer), fe(Y, "image_processor_class", D.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/paligemma/processing_paligemma.js": (
    /*!******************************************************!*\
      !*** ./src/models/paligemma/processing_paligemma.js ***!
      \******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        PaliGemmaProcessor: () => (
          /* binding */
          g
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      const Y = "<image>";
      function R(v, M, y, b, I) {
        return `${b.repeat(y * I)}${M}${v}
`;
      }
      class g extends _.Processor {
        /**
         * @typedef {import('../../utils/image.js').RawImage} RawImage
         */
        // `images` is required, `text` is optional
        async _call(M, y = null, b = {}) {
          y || (console.warn(
            "You are using PaliGemma without a text prefix. It will perform as a picture-captioning model."
          ), y = ""), Array.isArray(M) || (M = [M]), Array.isArray(y) || (y = [y]);
          const I = this.tokenizer.bos_token, K = this.image_processor.config.image_seq_length;
          let se;
          y.some((j) => j.includes(Y)) ? se = y.map(
            (j) => {
              const q = j.replaceAll(Y, Y.repeat(K)), A = q.lastIndexOf(Y), S = A === -1 ? 0 : A + Y.length;
              return q.slice(0, S) + I + q.slice(S) + `
`;
            }
          ) : (console.warn(
            "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens."
          ), se = y.map(
            (j) => R(
              j,
              I,
              K,
              Y,
              M.length
            )
          ));
          const ie = this.tokenizer(se, b);
          return {
            ...await this.image_processor(M, b),
            ...ie
          };
        }
      }
      fe(g, "tokenizer_class", U.AutoTokenizer), fe(g, "image_processor_class", D.AutoImageProcessor), fe(g, "uses_processor_config", !1);
    }
  ),
  /***/
  "./src/models/phi3_v/image_processing_phi3_v.js": (
    /*!******************************************************!*\
      !*** ./src/models/phi3_v/image_processing_phi3_v.js ***!
      \******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Phi3VImageProcessor: () => (
          /* binding */
          M
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      const U = 336, Y = [2, 3], { ceil: R, floor: g, sqrt: v } = Math;
      class M extends _.ImageProcessor {
        constructor(b) {
          super({
            ...b,
            do_normalize: !0,
            do_pad: !0,
            pad_size: "custom",
            do_convert_rgb: !0,
            do_resize: !0
            // Smart resizing "hd_transform"
          }), this._num_crops = b.num_crops;
        }
        calc_num_image_tokens_from_image_size(b, I) {
          const { num_img_tokens: K } = this.config;
          return g((g(I / U) * g(b / U) + 1) * K + 1 + (g(I / U) + 1) * v(K));
        }
        /** @type {ImageProcessor['get_resize_output_image_size']} */
        get_resize_output_image_size(b, I) {
          const K = this._num_crops, [se, ie] = b.size;
          let W = se / ie, j = 1;
          for (; j * Math.ceil(j / W) <= K; )
            j += 1;
          j -= 1;
          const q = Math.floor(j * 336), A = Math.floor(q / W);
          return [q, A];
        }
        /** @type {ImageProcessor['pad_image']} */
        pad_image(b, I, K, se = {}) {
          const [ie, W] = I, j = U * R(ie / U), q = U * R(W / U), A = [1, 1, 1].map((S, w) => (S - this.image_mean[w]) / this.image_std[w]);
          return super.pad_image(b, I, { width: q, height: j }, {
            center: !0,
            constant_values: A,
            ...se
          });
        }
        async _call(b, {
          num_crops: I = null
        } = {}) {
          if (this._num_crops = I ?? (I = this.config.num_crops), I < 4 || v(I) % 1 !== 0)
            throw new Error("num_crops must be a square number >= 4");
          Array.isArray(b) || (b = [b]);
          const K = b.length, se = await Promise.all(b.map((x) => this.preprocess(x))), ie = se.map((x) => x.original_size), W = se.map((x) => x.reshaped_input_size), j = [];
          for (const { pixel_values: x } of se) {
            x.unsqueeze_(0);
            const [F, le] = x.dims.slice(-2), ne = await (0, D.interpolate_4d)(x, {
              size: [U, U],
              mode: "bicubic"
            });
            if (I > 0) {
              const be = [], _e = v(I), re = g(le / _e), xe = g(F / _e);
              for (let ke = 0; ke < _e; ++ke)
                for (let Fe = 0; Fe < _e; ++Fe) {
                  let Ee, tt, Ge, ye;
                  ke === _e - 1 ? (tt = F - xe, ye = F) : (tt = ke * xe, ye = (ke + 1) * xe), Fe === _e - 1 ? (Ee = le - re, Ge = le) : (Ee = Fe * re, Ge = (Fe + 1) * re);
                  const J = [tt, Ee], de = [ye, Ge], Ce = await (0, D.slice)(x, J, de, Y);
                  be.push(Ce);
                }
              const ce = await (0, D.interpolate_4d)((0, D.cat)(be, 0), {
                size: [U, U],
                mode: "bicubic"
              });
              j.push((0, D.cat)([ne, ce], 0));
            } else
              j.push(ne);
          }
          const q = (0, D.stack)(j, 0), A = W.map((x) => x.map((F) => U * R(F / U))), S = new D.Tensor(
            "int64",
            A.flat(),
            [K, 2]
          ), w = A.map(
            ([x, F]) => this.calc_num_image_tokens_from_image_size(F, x)
          );
          return { pixel_values: q, original_sizes: ie, reshaped_input_sizes: W, image_sizes: S, num_img_tokens: w };
        }
      }
    }
  ),
  /***/
  "./src/models/phi3_v/processing_phi3_v.js": (
    /*!************************************************!*\
      !*** ./src/models/phi3_v/processing_phi3_v.js ***!
      \************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Phi3VProcessor: () => (
          /* binding */
          g
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      r(
        /*! ../../utils/image.js */
        "./src/utils/image.js"
      );
      const Y = "<|image|>", R = /<\|image_\d+\|>/g;
      class g extends _.Processor {
        /**
         * 
         * @param {string|string[]} text 
         * @param {RawImage|RawImage[]} images 
         * @param  { { padding?: boolean, truncation?: boolean, num_crops?: number } | undefined } options
         * @returns {Promise<any>}
         */
        async _call(M, y = null, {
          padding: b = !0,
          truncation: I = !0,
          num_crops: K = null
        } = {}) {
          Array.isArray(M) || (M = [M]);
          let se, ie;
          if (y) {
            ie = await this.image_processor(y, { num_crops: K });
            const { num_img_tokens: W } = ie, j = M.map((A, S) => A.split(R).join(Y.repeat(W[S])));
            se = this.tokenizer(j, { padding: b, truncation: I });
            const q = this.tokenizer.model.convert_tokens_to_ids([Y])[0];
            se.input_ids.map_((A) => A == q ? -A : A);
          } else
            se = this.tokenizer(M);
          return {
            ...se,
            ...ie
          };
        }
      }
      fe(g, "image_processor_class", D.AutoImageProcessor), fe(g, "tokenizer_class", U.AutoTokenizer);
    }
  ),
  /***/
  "./src/models/processors.js": (
    /*!**********************************!*\
      !*** ./src/models/processors.js ***!
      \**********************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Florence2Processor: () => (
          /* reexport safe */
          _.Florence2Processor
        ),
        /* harmony export */
        GroundingDinoProcessor: () => (
          /* reexport safe */
          D.GroundingDinoProcessor
        ),
        /* harmony export */
        Idefics3Processor: () => (
          /* reexport safe */
          U.Idefics3Processor
        ),
        /* harmony export */
        JinaCLIPProcessor: () => (
          /* reexport safe */
          R.JinaCLIPProcessor
        ),
        /* harmony export */
        MgpstrProcessor: () => (
          /* reexport safe */
          g.MgpstrProcessor
        ),
        /* harmony export */
        MoonshineProcessor: () => (
          /* reexport safe */
          v.MoonshineProcessor
        ),
        /* harmony export */
        OwlViTProcessor: () => (
          /* reexport safe */
          M.OwlViTProcessor
        ),
        /* harmony export */
        PaliGemmaProcessor: () => (
          /* reexport safe */
          b.PaliGemmaProcessor
        ),
        /* harmony export */
        Phi3VProcessor: () => (
          /* reexport safe */
          y.Phi3VProcessor
        ),
        /* harmony export */
        PyAnnoteProcessor: () => (
          /* reexport safe */
          I.PyAnnoteProcessor
        ),
        /* harmony export */
        Qwen2VLProcessor: () => (
          /* reexport safe */
          K.Qwen2VLProcessor
        ),
        /* harmony export */
        SamProcessor: () => (
          /* reexport safe */
          se.SamProcessor
        ),
        /* harmony export */
        SpeechT5Processor: () => (
          /* reexport safe */
          ie.SpeechT5Processor
        ),
        /* harmony export */
        VLChatProcessor: () => (
          /* reexport safe */
          Y.VLChatProcessor
        ),
        /* harmony export */
        Wav2Vec2Processor: () => (
          /* reexport safe */
          W.Wav2Vec2Processor
        ),
        /* harmony export */
        Wav2Vec2ProcessorWithLM: () => (
          /* reexport safe */
          j.Wav2Vec2ProcessorWithLM
        ),
        /* harmony export */
        WhisperProcessor: () => (
          /* reexport safe */
          q.WhisperProcessor
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./florence2/processing_florence2.js */
        "./src/models/florence2/processing_florence2.js"
      ), D = r(
        /*! ./grounding_dino/processing_grounding_dino.js */
        "./src/models/grounding_dino/processing_grounding_dino.js"
      ), U = r(
        /*! ./idefics3/processing_idefics3.js */
        "./src/models/idefics3/processing_idefics3.js"
      ), Y = r(
        /*! ./janus/processing_janus.js */
        "./src/models/janus/processing_janus.js"
      ), R = r(
        /*! ./jina_clip/processing_jina_clip.js */
        "./src/models/jina_clip/processing_jina_clip.js"
      ), g = r(
        /*! ./mgp_str/processing_mgp_str.js */
        "./src/models/mgp_str/processing_mgp_str.js"
      ), v = r(
        /*! ./moonshine/processing_moonshine.js */
        "./src/models/moonshine/processing_moonshine.js"
      ), M = r(
        /*! ./owlvit/processing_owlvit.js */
        "./src/models/owlvit/processing_owlvit.js"
      ), y = r(
        /*! ./phi3_v/processing_phi3_v.js */
        "./src/models/phi3_v/processing_phi3_v.js"
      ), b = r(
        /*! ./paligemma/processing_paligemma.js */
        "./src/models/paligemma/processing_paligemma.js"
      ), I = r(
        /*! ./pyannote/processing_pyannote.js */
        "./src/models/pyannote/processing_pyannote.js"
      ), K = r(
        /*! ./qwen2_vl/processing_qwen2_vl.js */
        "./src/models/qwen2_vl/processing_qwen2_vl.js"
      ), se = r(
        /*! ./sam/processing_sam.js */
        "./src/models/sam/processing_sam.js"
      ), ie = r(
        /*! ./speecht5/processing_speecht5.js */
        "./src/models/speecht5/processing_speecht5.js"
      ), W = r(
        /*! ./wav2vec2/processing_wav2vec2.js */
        "./src/models/wav2vec2/processing_wav2vec2.js"
      ), j = r(
        /*! ./wav2vec2_with_lm/processing_wav2vec2_with_lm.js */
        "./src/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.js"
      ), q = r(
        /*! ./whisper/processing_whisper.js */
        "./src/models/whisper/processing_whisper.js"
      );
    }
  ),
  /***/
  "./src/models/pvt/image_processing_pvt.js": (
    /*!************************************************!*\
      !*** ./src/models/pvt/image_processing_pvt.js ***!
      \************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        PvtImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/pyannote/feature_extraction_pyannote.js": (
    /*!************************************************************!*\
      !*** ./src/models/pyannote/feature_extraction_pyannote.js ***!
      \************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        PyAnnoteFeatureExtractor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      ), U = r(
        /*! ../../utils/maths.js */
        "./src/utils/maths.js"
      );
      class Y extends _.FeatureExtractor {
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_values: Tensor; }>} The extracted input features.
         */
        async _call(g) {
          (0, _.validate_audio_inputs)(g, "PyAnnoteFeatureExtractor"), g instanceof Float64Array && (g = new Float32Array(g));
          const v = [
            1,
            /* batch_size */
            1,
            /* num_channels */
            g.length
            /* num_samples */
          ];
          return {
            input_values: new D.Tensor("float32", g, v)
          };
        }
        /**
         * NOTE: Can return fractional values. `Math.ceil` will ensure correct value.
         * @param {number} samples The number of frames in the audio.
         * @returns {number} The number of frames in the audio.
         */
        samples_to_frames(g) {
          return (g - this.config.offset) / this.config.step;
        }
        /**
         * Post-processes the speaker diarization logits output by the model.
         * @param {import('../../utils/tensor.js').Tensor} logits The speaker diarization logits output by the model.
         * @param {number} num_samples Number of samples in the input audio.
         * @returns {Array<Array<{ id: number, start: number, end: number, confidence: number }>>} The post-processed speaker diarization results.
         */
        post_process_speaker_diarization(g, v) {
          const M = v / this.samples_to_frames(v) / this.config.sampling_rate, y = [];
          for (const b of g.tolist()) {
            const I = [];
            let K = -1;
            for (let se = 0; se < b.length; ++se) {
              const ie = (0, U.softmax)(b[se]), [W, j] = (0, U.max)(ie), [q, A] = [se, se + 1];
              j !== K ? (K = j, I.push({ id: j, start: q, end: A, score: W })) : (I.at(-1).end = A, I.at(-1).score += W);
            }
            y.push(I.map(
              // Convert frame-space to time-space
              // and compute the confidence
              ({ id: se, start: ie, end: W, score: j }) => ({
                id: se,
                start: ie * M,
                end: W * M,
                confidence: j / (W - ie)
              })
            ));
          }
          return y;
        }
      }
    }
  ),
  /***/
  "./src/models/pyannote/processing_pyannote.js": (
    /*!****************************************************!*\
      !*** ./src/models/pyannote/processing_pyannote.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        PyAnnoteProcessor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ./feature_extraction_pyannote.js */
        "./src/models/pyannote/feature_extraction_pyannote.js"
      );
      class U extends _.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(R) {
          return await this.feature_extractor(R);
        }
        /** @type {PyAnnoteFeatureExtractor['post_process_speaker_diarization']} */
        post_process_speaker_diarization(...R) {
          return (
            /** @type {PyAnnoteFeatureExtractor} */
            this.feature_extractor.post_process_speaker_diarization(...R)
          );
        }
        get sampling_rate() {
          return this.feature_extractor.config.sampling_rate;
        }
      }
      fe(U, "feature_extractor_class", D.PyAnnoteFeatureExtractor);
    }
  ),
  /***/
  "./src/models/qwen2_vl/image_processing_qwen2_vl.js": (
    /*!**********************************************************!*\
      !*** ./src/models/qwen2_vl/image_processing_qwen2_vl.js ***!
      \**********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Qwen2VLImageProcessor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class U extends _.ImageProcessor {
        async _call(R, ...g) {
          const { pixel_values: v, original_sizes: M, reshaped_input_sizes: y } = await super._call(R, ...g);
          let b = v;
          const { temporal_patch_size: I, merge_size: K, patch_size: se } = this.config;
          b.dims[0] === 1 && (b = (0, D.cat)(Array.from({ length: I }, () => b), 0));
          const ie = b.dims[0] / I, W = b.dims[1], j = Math.floor(b.dims[2] / se), q = Math.floor(b.dims[3] / se), A = b.view(
            ie,
            I,
            W,
            Math.floor(j / K),
            K,
            se,
            Math.floor(q / K),
            K,
            se
          ).permute(0, 3, 6, 4, 7, 2, 1, 5, 8).view(
            ie * j * q,
            W * I * se * se
          ), S = new D.Tensor("int64", [ie, j, q], [1, 3]);
          return {
            pixel_values: A,
            image_grid_thw: S,
            original_sizes: M,
            reshaped_input_sizes: y
          };
        }
      }
    }
  ),
  /***/
  "./src/models/qwen2_vl/processing_qwen2_vl.js": (
    /*!****************************************************!*\
      !*** ./src/models/qwen2_vl/processing_qwen2_vl.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Qwen2VLProcessor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), U = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      r(
        /*! ../../utils/image.js */
        "./src/utils/image.js"
      );
      class Y extends _.Processor {
        /**
         * 
         * @param {string|string[]} text 
         * @param {RawImage|RawImage[]} images 
         * @param  {...any} args 
         * @returns {Promise<any>}
         */
        async _call(g, v = null, ...M) {
          Array.isArray(g) || (g = [g]);
          let y, b;
          if (v && (y = await this.image_processor(v), b = y.image_grid_thw), b) {
            let K = this.image_processor.config.merge_size ** 2, se = 0;
            const ie = b.tolist();
            g = g.map((W) => {
              for (; W.includes("<|image_pad|>"); ) {
                const j = Number(ie[se++].reduce((q, A) => q * A, 1n));
                W = W.replace("<|image_pad|>", "<|placeholder|>".repeat(Math.floor(j / K)));
              }
              return W.replaceAll("<|placeholder|>", "<|image_pad|>");
            });
          }
          return {
            ...this.tokenizer(g),
            ...y
            // TODO: ...videos_inputs,
          };
        }
      }
      fe(Y, "image_processor_class", D.AutoImageProcessor), fe(Y, "tokenizer_class", U.AutoTokenizer);
    }
  ),
  /***/
  "./src/models/rt_detr/image_processing_rt_detr.js": (
    /*!********************************************************!*\
      !*** ./src/models/rt_detr/image_processing_rt_detr.js ***!
      \********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        RTDetrImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        /** @type {typeof post_process_object_detection} */
        post_process_object_detection(...Y) {
          return (0, _.post_process_object_detection)(...Y);
        }
      }
    }
  ),
  /***/
  "./src/models/sam/image_processing_sam.js": (
    /*!************************************************!*\
      !*** ./src/models/sam/image_processing_sam.js ***!
      \************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        SamImageProcessor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), D = r(
        /*! ../../utils/core.js */
        "./src/utils/core.js"
      ), U = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class Y extends _.ImageProcessor {
        /**
         * 
         * @param {any} input_points 
         * @param {import("../../base/image_processors_utils.js").HeightWidth[]} original_sizes 
         * @param {import("../../base/image_processors_utils.js").HeightWidth[]} reshaped_input_sizes 
         * @returns {Tensor}
         */
        reshape_input_points(g, v, M, y = !1) {
          g = structuredClone(g);
          let b = (0, D.calculateDimensions)(g);
          if (b.length === 3)
            y || (b = [1, ...b]), g = [g];
          else if (b.length !== 4)
            throw Error("The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.");
          for (let I = 0; I < g.length; ++I) {
            let K = v[I], se = M[I], ie = [
              se[0] / K[0],
              se[1] / K[1]
            ];
            for (let W = 0; W < g[I].length; ++W)
              for (let j = 0; j < g[I][W].length; ++j)
                for (let q = 0; q < g[I][W][j].length; ++q)
                  g[I][W][j][q] *= ie[q % 2];
          }
          return new U.Tensor(
            "float32",
            Float32Array.from(g.flat(1 / 0)),
            b
          );
        }
        /**
         * 
         * @param {any} input_labels 
         * @param {Tensor} input_points 
         * @returns {Tensor}
         */
        add_input_labels(g, v) {
          let M = (0, D.calculateDimensions)(g);
          if (M.length === 2)
            M = [1, ...M], g = [g];
          else if (M.length !== 3)
            throw Error("The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.");
          if (M.some((y, b) => y !== v.dims[b]))
            throw Error(`The first ${M.length} dimensions of 'input_points' and 'input_labels' must be the same.`);
          return new U.Tensor(
            "int64",
            g.flat(1 / 0).map(BigInt),
            M
          );
        }
        /**
         * @param {any[]} images The URL(s) of the image(s) to extract features from.
         * @param {Object} [options] Additional options for the processor.
         * @param {any} [options.input_points=null] A 3D or 4D array, representing the input points provided by the user.
         * - 3D: `[point_batch_size, nb_points_per_image, 2]`. In this case, `batch_size` is assumed to be 1.
         * - 4D: `[batch_size, point_batch_size, nb_points_per_image, 2]`.
         * @param {any} [options.input_labels=null] A 2D or 3D array, representing the input labels for the points, used by the prompt encoder to encode the prompt.
         * - 2D: `[point_batch_size, nb_points_per_image]`. In this case, `batch_size` is assumed to be 1.
         * - 3D: `[batch_size, point_batch_size, nb_points_per_image]`.
         * @param {number[][][]} [options.input_boxes=null] A 3D array of shape `(batch_size, num_boxes, 4)`, representing the input boxes provided by the user.
         * This is used by the prompt encoder to encode the prompt. Generally yields to much better generated masks.
         * The processor will generate a tensor, with each dimension corresponding respectively to the image batch size,
         * the number of boxes per image and the coordinates of the top left and botton right point of the box.
         * In the order (`x1`, `y1`, `x2`, `y2`):
         * - `x1`: the x coordinate of the top left point of the input box
         * - `y1`: the y coordinate of the top left point of the input box
         * - `x2`: the x coordinate of the bottom right point of the input box
         * - `y2`: the y coordinate of the bottom right point of the input box
         * @returns {Promise<SamImageProcessorResult>}
         */
        async _call(g, {
          input_points: v = null,
          input_labels: M = null,
          input_boxes: y = null
        } = {}) {
          const b = await super._call(g);
          if (v && (b.input_points = this.reshape_input_points(
            v,
            b.original_sizes,
            b.reshaped_input_sizes
          )), M) {
            if (!b.input_points)
              throw Error("`input_points` must be provided if `input_labels` are provided.");
            b.input_labels = this.add_input_labels(M, b.input_points);
          }
          return y && (b.input_boxes = this.reshape_input_points(
            y,
            b.original_sizes,
            b.reshaped_input_sizes,
            !0
          )), b;
        }
        /**
         * Remove padding and upscale masks to the original image size.
         * @param {Tensor} masks Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.
         * @param {[number, number][]} original_sizes The original sizes of each image before it was resized to the model's expected input shape, in (height, width) format.
         * @param {[number, number][]} reshaped_input_sizes The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.
         * @param {Object} options Optional parameters for post-processing.
         * @param {number} [options.mask_threshold] The threshold to use for binarizing the masks.
         * @param {boolean} [options.binarize] Whether to binarize the masks.
         * @param {Object} [options.pad_size] The target size the images were padded to before being passed to the model. If `null`, the target size is assumed to be the processor's `pad_size`.
         * @param {number} [options.pad_size.height] The height the images were padded to.
         * @param {number} [options.pad_size.width] The width the images were padded to.
         * @returns {Promise<Tensor[]>} Batched masks in batch_size, num_channels, height, width) format, where (height, width) is given by original_size.
         */
        async post_process_masks(g, v, M, {
          mask_threshold: y = 0,
          binarize: b = !0,
          pad_size: I = null
        } = {}) {
          const K = [];
          I = I ?? this.pad_size;
          const se = [I.height, I.width];
          for (let ie = 0; ie < v.length; ++ie) {
            const W = v[ie], j = M[ie];
            let q = await (0, U.interpolate_4d)(
              g[ie],
              { mode: "bilinear", size: se }
            );
            if (q = q.slice(null, null, [0, j[0]], [0, j[1]]), q = await (0, U.interpolate_4d)(
              q,
              { mode: "bilinear", size: W }
            ), b) {
              const A = q.data, S = new Uint8Array(A.length);
              for (let w = 0; w < A.length; ++w)
                A[w] > y && (S[w] = 1);
              q = new U.Tensor(
                "bool",
                S,
                q.dims
              );
            }
            K.push(q);
          }
          return K;
        }
        /**
         * Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.
         * @param {import("../../utils/image.js").RawImage} image Input original image
         * @param {number} target_size Target size of the resized image
         * @param {Object} options Options for generating crop boxes 
         * @param {number} [options.crop_n_layers] If >0, mask prediction will be run again on crops of the image.
         * Sets the number of layers to run, where each layer has 2**i_layer number of image crops.
         * @param {number} [options.overlap_ratio] Sets the degree to which crops overlap. In the first crop layer,
         * crops will overlap by this fraction of the image length. Later layers with more crops scale down this overlap.
         * @param {number} [options.points_per_crop] Number of points to sample from each crop.
         * @param {number} [options.crop_n_points_downscale_factor] The number of points-per-side sampled in layer n is
         * scaled down by crop_n_points_downscale_factor**n.
         * @returns {Object} An object containing the crop boxes, number of points per crop, cropped images, and input labels.
         */
        generate_crop_boxes(g, v, {
          crop_n_layers: M = 0,
          overlap_ratio: y = 0.3413333333333333,
          points_per_crop: b = 32,
          crop_n_points_downscale_factor: I = 1
        } = {}) {
        }
      }
    }
  ),
  /***/
  "./src/models/sam/processing_sam.js": (
    /*!******************************************!*\
      !*** ./src/models/sam/processing_sam.js ***!
      \******************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        SamProcessor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      );
      class U extends _.Processor {
        async _call(...R) {
          return await this.image_processor(...R);
        }
        post_process_masks(...R) {
          return this.image_processor.post_process_masks(...R);
        }
        reshape_input_points(...R) {
          return this.image_processor.reshape_input_points(...R);
        }
      }
      fe(U, "image_processor_class", D.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/seamless_m4t/feature_extraction_seamless_m4t.js": (
    /*!********************************************************************!*\
      !*** ./src/models/seamless_m4t/feature_extraction_seamless_m4t.js ***!
      \********************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        SeamlessM4TFeatureExtractor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      ), U = r(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      );
      class Y extends _.FeatureExtractor {
        constructor(g) {
          super(g);
          const v = this.config.sampling_rate, M = (0, U.mel_filter_bank)(
            256,
            // num_frequency_bins
            this.config.num_mel_bins,
            // num_mel_filters
            20,
            // min_frequency
            Math.floor(v / 2),
            // max_frequency
            v,
            // sampling_rate
            null,
            // norm
            "kaldi",
            // mel_scale
            !0
            // triangularize_in_mel_space
          );
          for (let y = 0; y < M.length; ++y)
            M[y].push(0);
          this.mel_filters = M, this.window = (0, U.window_function)(400, "povey", {
            periodic: !1
          });
        }
        /**
         * Computes the log-Mel spectrogram of the provided audio waveform.
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @param {number} max_length The maximum number of frames to return.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(g, v) {
          return g = g.map((M) => M * 32768), (0, U.spectrogram)(
            g,
            this.window,
            // window
            400,
            // frame_length
            160,
            // hop_length
            {
              fft_length: 512,
              power: 2,
              center: !1,
              preemphasis: 0.97,
              mel_filters: this.mel_filters,
              log_mel: "log",
              mel_floor: 1192092955078125e-22,
              remove_dc_offset: !0,
              // Custom
              max_num_frames: v,
              transpose: !0
            }
          );
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @param {Object} options Optional parameters for feature extraction.
         * @param {boolean} [options.padding=true] Whether to pad the sequence to a multiple of `pad_to_multiple_of`.
         * @param {number} [options.pad_to_multiple_of=2] The number to pad the sequence to a multiple of.
         * @param {boolean} [options.do_normalize_per_mel_bins=true] Whether or not to zero-mean unit-variance normalize the input per mel-channel.
         * @param {boolean} [options.return_attention_mask=true] Whether to return the attention mask.
         * @returns {Promise<{ input_features: Tensor, attention_mask?: Tensor }>} A Promise resolving to an object containing the extracted input features and attention masks as Tensors.
         */
        async _call(g, {
          padding: v = !0,
          pad_to_multiple_of: M = 2,
          do_normalize_per_mel_bins: y = !0,
          return_attention_mask: b = !0
        } = {}) {
          (0, _.validate_audio_inputs)(g, "SeamlessM4TFeatureExtractor");
          let I = await this._extract_fbank_features(g, this.config.max_length);
          if (y) {
            const [S, w] = I.dims, x = I.data;
            for (let F = 0; F < w; ++F) {
              let le = 0;
              for (let re = 0; re < S; ++re)
                le += x[re * w + F];
              const ne = le / S;
              let be = 0;
              for (let re = 0; re < S; ++re)
                be += (x[re * w + F] - ne) ** 2;
              be /= S - 1;
              const _e = Math.sqrt(be + 1e-7);
              for (let re = 0; re < S; ++re) {
                const xe = re * w + F;
                x[xe] = (x[xe] - ne) / _e;
              }
            }
          }
          let K;
          if (v) {
            const [S, w] = I.dims, x = (
              /** @type {Float32Array} */
              I.data
            ), F = S % M;
            if (F > 0) {
              const le = new Float32Array(w * (S + F));
              le.set(x), le.fill(this.config.padding_value, x.length);
              const ne = S + F;
              I = new D.Tensor(
                I.type,
                le,
                [ne, w]
              ), b && (K = new D.Tensor(
                "int64",
                new BigInt64Array(ne),
                [1, ne]
              ), K.data.fill(1n, 0, S));
            }
          }
          const [se, ie] = I.dims, W = this.config.stride;
          if (se % W !== 0)
            throw new Error(`The number of frames (${se}) must be a multiple of the stride (${W}).`);
          const q = I.view(
            1,
            Math.floor(se / W),
            ie * W
          ), A = { input_features: q };
          if (b) {
            const S = q.dims[1], w = new BigInt64Array(S);
            if (K) {
              const x = K.data;
              for (let F = 1, le = 0; F < se; F += W, ++le)
                w[le] = x[F];
            } else
              w.fill(1n);
            A.attention_mask = new D.Tensor(
              "int64",
              w,
              [1, S]
            );
          }
          return A;
        }
      }
    }
  ),
  /***/
  "./src/models/segformer/image_processing_segformer.js": (
    /*!************************************************************!*\
      !*** ./src/models/segformer/image_processing_segformer.js ***!
      \************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        SegformerFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        SegformerImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        /** @type {typeof post_process_semantic_segmentation} */
        post_process_semantic_segmentation(...R) {
          return (0, _.post_process_semantic_segmentation)(...R);
        }
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/siglip/image_processing_siglip.js": (
    /*!******************************************************!*\
      !*** ./src/models/siglip/image_processing_siglip.js ***!
      \******************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        SiglipImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/speecht5/feature_extraction_speecht5.js": (
    /*!************************************************************!*\
      !*** ./src/models/speecht5/feature_extraction_speecht5.js ***!
      \************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        SpeechT5FeatureExtractor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      class D extends _.FeatureExtractor {
      }
    }
  ),
  /***/
  "./src/models/speecht5/processing_speecht5.js": (
    /*!****************************************************!*\
      !*** ./src/models/speecht5/processing_speecht5.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        SpeechT5Processor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), D = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), U = r(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      );
      class Y extends _.Processor {
        /**
         * Calls the feature_extractor function with the given input.
         * @param {any} input The input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(g) {
          return await this.feature_extractor(g);
        }
      }
      fe(Y, "tokenizer_class", D.AutoTokenizer), fe(Y, "feature_extractor_class", U.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/swin2sr/image_processing_swin2sr.js": (
    /*!********************************************************!*\
      !*** ./src/models/swin2sr/image_processing_swin2sr.js ***!
      \********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Swin2SRImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        pad_image(Y, R, g, v = {}) {
          const [M, y, b] = R;
          return super.pad_image(Y, R, {
            // NOTE: For Swin2SR models, the original python implementation adds padding even when the image's width/height is already
            // a multiple of `pad_size`. However, this is most likely a bug (PR: https://github.com/mv-lab/swin2sr/pull/19).
            // For this reason, we only add padding when the image's width/height is not a multiple of `pad_size`.
            width: y + (g - y % g) % g,
            height: M + (g - M % g) % g
          }, {
            mode: "symmetric",
            center: !1,
            constant_values: -1,
            ...v
          });
        }
      }
    }
  ),
  /***/
  "./src/models/vit/image_processing_vit.js": (
    /*!************************************************!*\
      !*** ./src/models/vit/image_processing_vit.js ***!
      \************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        ViTFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        ViTImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/models/vitmatte/image_processing_vitmatte.js": (
    /*!**********************************************************!*\
      !*** ./src/models/vitmatte/image_processing_vitmatte.js ***!
      \**********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        VitMatteImageProcessor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class U extends _.ImageProcessor {
        /**
         * Calls the feature extraction process on an array of images, preprocesses
         * each image, and concatenates the resulting features into a single Tensor.
         * @param {import("../../utils/image.js").RawImage[]} images The image(s) to extract features from.
         * @param {import("../../utils/image.js").RawImage[]} trimaps The trimaps(s) to extract features from.
         * @returns {Promise<import("../../base/image_processors_utils.js").ImageProcessorResult>} An object containing the concatenated pixel values of the preprocessed images.
         */
        async _call(R, g) {
          Array.isArray(R) || (R = [R]), Array.isArray(g) || (g = [g]);
          const v = await Promise.all(R.map((b) => this.preprocess(b))), M = await Promise.all(g.map((b) => this.preprocess(b, {
            do_normalize: !1,
            do_convert_rgb: !1,
            do_convert_grayscale: !0
          })));
          return {
            pixel_values: (0, D.stack)(v.map(
              // Concatenate images and trimaps
              (b, I) => (0, D.cat)([b.pixel_values, M[I].pixel_values], 0)
            ), 0),
            // Original sizes of images
            original_sizes: v.map((b) => b.original_size),
            // Reshaped sizes of images, before padding or cropping
            reshaped_input_sizes: v.map((b) => b.reshaped_input_size)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/vitpose/image_processing_vitpose.js": (
    /*!********************************************************!*\
      !*** ./src/models/vitpose/image_processing_vitpose.js ***!
      \********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        VitPoseImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        /**
         * Transform the heatmaps into keypoint predictions and transform them back to the image.
         * NOTE: This is a naive implementation and does not include advanced post-processing techniques,
         * so the results may not be as accurate as the original implementation.
         * @param {import('../../utils/tensor.js').Tensor} outputs The model outputs.
         * @param {[number, number, number, number][][]} boxes List or array of bounding boxes for each image.
         * Each box should be a list of 4 floats representing the bounding box coordinates in COCO format (top_left_x, top_left_y, width, height).
         * @returns {{
         *   bbox: [number, number, number, number],
         *   scores: number[],
         *   labels: number[],
         *   keypoints: [number, number][]
         * }[][]} List of keypoints predictions for each image.
         */
        post_process_pose_estimation(Y, R, {
          threshold: g = null
          // TODO:
          // kernel_size = 11,
          // target_sizes = null,
        } = {}) {
          const v = Y.tolist(), [M, y, b, I] = Y.dims, K = [];
          for (let se = 0; se < M; ++se) {
            const ie = v[se], W = R[se], j = [];
            for (let q = 0; q < W.length; ++q) {
              const A = W[q], S = [], w = [], x = [], F = A.at(-2) / I, le = A.at(-1) / b;
              for (let ne = 0; ne < ie.length; ++ne) {
                let [be, _e] = [0, 0], re = 0, xe = -1 / 0;
                const ce = ie[ne];
                for (let Fe = 0; Fe < ce.length; ++Fe) {
                  const Ee = ce[Fe];
                  for (let tt = 0; tt < Ee.length; ++tt) {
                    const Ge = Ee[tt];
                    re += Ge, xe = Math.max(xe, Ge), be += (tt + 0.5) * Ge, _e += Fe * Ge;
                  }
                }
                if (g != null && xe < g) continue;
                const ke = [
                  F * be / re,
                  le * _e / re
                ];
                S.push(ke), x.push(ne), w.push(xe);
              }
              j.push({
                bbox: A,
                scores: w,
                labels: x,
                keypoints: S
              });
            }
            K.push(j);
          }
          return K;
        }
      }
    }
  ),
  /***/
  "./src/models/wav2vec2/feature_extraction_wav2vec2.js": (
    /*!************************************************************!*\
      !*** ./src/models/wav2vec2/feature_extraction_wav2vec2.js ***!
      \************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Wav2Vec2FeatureExtractor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      ), D = r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class U extends _.FeatureExtractor {
        /**
         * @param {Float32Array} input_values 
         * @returns {Float32Array} 
         */
        _zero_mean_unit_var_norm(R) {
          const v = R.reduce((y, b) => y + b, 0) / R.length, M = R.reduce((y, b) => y + (b - v) ** 2, 0) / R.length;
          return R.map((y) => (y - v) / Math.sqrt(M + 1e-7));
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_values: Tensor; attention_mask: Tensor }>} A Promise resolving to an object containing the extracted input features and attention mask as Tensors.
         */
        async _call(R) {
          (0, _.validate_audio_inputs)(R, "Wav2Vec2FeatureExtractor"), R instanceof Float64Array && (R = new Float32Array(R));
          let g = R;
          this.config.do_normalize && (g = this._zero_mean_unit_var_norm(g));
          const v = [1, g.length];
          return {
            input_values: new D.Tensor("float32", g, v),
            attention_mask: new D.Tensor("int64", new BigInt64Array(g.length).fill(1n), v)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/wav2vec2/processing_wav2vec2.js": (
    /*!****************************************************!*\
      !*** ./src/models/wav2vec2/processing_wav2vec2.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Wav2Vec2Processor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), D = r(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      ), U = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      );
      class Y extends U.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(g) {
          return await this.feature_extractor(g);
        }
      }
      fe(Y, "tokenizer_class", _.AutoTokenizer), fe(Y, "feature_extractor_class", D.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.js": (
    /*!********************************************************************!*\
      !*** ./src/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.js ***!
      \********************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Wav2Vec2ProcessorWithLM: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), D = r(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      ), U = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      );
      class Y extends U.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(g) {
          return await this.feature_extractor(g);
        }
      }
      fe(Y, "tokenizer_class", _.AutoTokenizer), fe(Y, "feature_extractor_class", D.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/wespeaker/feature_extraction_wespeaker.js": (
    /*!**************************************************************!*\
      !*** ./src/models/wespeaker/feature_extraction_wespeaker.js ***!
      \**************************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        WeSpeakerFeatureExtractor: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var D = r(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      );
      class U extends _.FeatureExtractor {
        constructor(R) {
          super(R);
          const g = this.config.sampling_rate, v = (0, D.mel_filter_bank)(
            256,
            // num_frequency_bins
            this.config.num_mel_bins,
            // num_mel_filters
            20,
            // min_frequency
            Math.floor(g / 2),
            // max_frequency
            g,
            // sampling_rate
            null,
            // norm
            "kaldi",
            // mel_scale
            !0
            // triangularize_in_mel_space
          );
          for (let M = 0; M < v.length; ++M)
            v[M].push(0);
          this.mel_filters = v, this.window = (0, D.window_function)(400, "hamming", {
            periodic: !1
          }), this.min_num_frames = this.config.min_num_frames;
        }
        /**
         * Computes the log-Mel spectrogram of the provided audio waveform.
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(R) {
          return R = R.map((g) => g * 32768), (0, D.spectrogram)(
            R,
            this.window,
            // window
            400,
            // frame_length
            160,
            // hop_length
            {
              fft_length: 512,
              power: 2,
              center: !1,
              preemphasis: 0.97,
              mel_filters: this.mel_filters,
              log_mel: "log",
              mel_floor: 1192092955078125e-22,
              remove_dc_offset: !0,
              // Custom
              transpose: !0,
              min_num_frames: this.min_num_frames
            }
          );
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.
         */
        async _call(R) {
          (0, _.validate_audio_inputs)(R, "WeSpeakerFeatureExtractor");
          const g = (await this._extract_fbank_features(R)).unsqueeze_(0);
          if (this.config.fbank_centering_span === null) {
            const v = (
              /** @type {Float32Array} */
              g.mean(1).data
            ), M = (
              /** @type {Float32Array} */
              g.data
            ), [y, b, I] = g.dims;
            for (let K = 0; K < y; ++K) {
              const se = K * b * I, ie = K * I;
              for (let W = 0; W < b; ++W) {
                const j = se + W * I;
                for (let q = 0; q < I; ++q)
                  M[j + q] -= v[ie + q];
              }
            }
          }
          return {
            input_features: g
          };
        }
      }
    }
  ),
  /***/
  "./src/models/whisper/common_whisper.js": (
    /*!**********************************************!*\
      !*** ./src/models/whisper/common_whisper.js ***!
      \**********************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        WHISPER_LANGUAGE_MAPPING: () => (
          /* binding */
          D
        ),
        /* harmony export */
        WHISPER_TO_LANGUAGE_CODE_MAPPING: () => (
          /* binding */
          U
        ),
        /* harmony export */
        whisper_language_to_code: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      const _ = [
        ["en", "english"],
        ["zh", "chinese"],
        ["de", "german"],
        ["es", "spanish"],
        ["ru", "russian"],
        ["ko", "korean"],
        ["fr", "french"],
        ["ja", "japanese"],
        ["pt", "portuguese"],
        ["tr", "turkish"],
        ["pl", "polish"],
        ["ca", "catalan"],
        ["nl", "dutch"],
        ["ar", "arabic"],
        ["sv", "swedish"],
        ["it", "italian"],
        ["id", "indonesian"],
        ["hi", "hindi"],
        ["fi", "finnish"],
        ["vi", "vietnamese"],
        ["he", "hebrew"],
        ["uk", "ukrainian"],
        ["el", "greek"],
        ["ms", "malay"],
        ["cs", "czech"],
        ["ro", "romanian"],
        ["da", "danish"],
        ["hu", "hungarian"],
        ["ta", "tamil"],
        ["no", "norwegian"],
        ["th", "thai"],
        ["ur", "urdu"],
        ["hr", "croatian"],
        ["bg", "bulgarian"],
        ["lt", "lithuanian"],
        ["la", "latin"],
        ["mi", "maori"],
        ["ml", "malayalam"],
        ["cy", "welsh"],
        ["sk", "slovak"],
        ["te", "telugu"],
        ["fa", "persian"],
        ["lv", "latvian"],
        ["bn", "bengali"],
        ["sr", "serbian"],
        ["az", "azerbaijani"],
        ["sl", "slovenian"],
        ["kn", "kannada"],
        ["et", "estonian"],
        ["mk", "macedonian"],
        ["br", "breton"],
        ["eu", "basque"],
        ["is", "icelandic"],
        ["hy", "armenian"],
        ["ne", "nepali"],
        ["mn", "mongolian"],
        ["bs", "bosnian"],
        ["kk", "kazakh"],
        ["sq", "albanian"],
        ["sw", "swahili"],
        ["gl", "galician"],
        ["mr", "marathi"],
        ["pa", "punjabi"],
        ["si", "sinhala"],
        ["km", "khmer"],
        ["sn", "shona"],
        ["yo", "yoruba"],
        ["so", "somali"],
        ["af", "afrikaans"],
        ["oc", "occitan"],
        ["ka", "georgian"],
        ["be", "belarusian"],
        ["tg", "tajik"],
        ["sd", "sindhi"],
        ["gu", "gujarati"],
        ["am", "amharic"],
        ["yi", "yiddish"],
        ["lo", "lao"],
        ["uz", "uzbek"],
        ["fo", "faroese"],
        ["ht", "haitian creole"],
        ["ps", "pashto"],
        ["tk", "turkmen"],
        ["nn", "nynorsk"],
        ["mt", "maltese"],
        ["sa", "sanskrit"],
        ["lb", "luxembourgish"],
        ["my", "myanmar"],
        ["bo", "tibetan"],
        ["tl", "tagalog"],
        ["mg", "malagasy"],
        ["as", "assamese"],
        ["tt", "tatar"],
        ["haw", "hawaiian"],
        ["ln", "lingala"],
        ["ha", "hausa"],
        ["ba", "bashkir"],
        ["jw", "javanese"],
        ["su", "sundanese"]
      ], D = new Map(_), U = new Map([
        ..._.map(([R, g]) => [g, R]),
        ["burmese", "my"],
        ["valencian", "ca"],
        ["flemish", "nl"],
        ["haitian", "ht"],
        ["letzeburgesch", "lb"],
        ["pushto", "ps"],
        ["panjabi", "pa"],
        ["moldavian", "ro"],
        ["moldovan", "ro"],
        ["sinhalese", "si"],
        ["castilian", "es"]
      ]);
      function Y(R) {
        R = R.toLowerCase();
        let g = U.get(R);
        if (g === void 0)
          if (D.has(R))
            g = R;
          else {
            const M = R.length === 2 ? D.keys() : D.values();
            throw new Error(`Language "${R}" is not supported. Must be one of: ${JSON.stringify(M)}`);
          }
        return g;
      }
    }
  ),
  /***/
  "./src/models/whisper/feature_extraction_whisper.js": (
    /*!**********************************************************!*\
      !*** ./src/models/whisper/feature_extraction_whisper.js ***!
      \**********************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        WhisperFeatureExtractor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      r(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var D = r(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      ), U = r(
        /*! ../../utils/maths.js */
        "./src/utils/maths.js"
      );
      class Y extends _.FeatureExtractor {
        constructor(g) {
          var v;
          super(g), (v = this.config).mel_filters ?? (v.mel_filters = (0, D.mel_filter_bank)(
            Math.floor(1 + this.config.n_fft / 2),
            // num_frequency_bins
            this.config.feature_size,
            // num_mel_filters
            0,
            // min_frequency
            8e3,
            // max_frequency
            this.config.sampling_rate,
            // sampling_rate
            "slaney",
            // norm
            "slaney"
            // mel_scale
          )), this.window = (0, D.window_function)(this.config.n_fft, "hann");
        }
        /**
         * Computes the log-Mel spectrogram of the provided audio waveform.
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(g) {
          const v = await (0, D.spectrogram)(
            g,
            this.window,
            // window
            this.config.n_fft,
            // frame_length
            this.config.hop_length,
            // hop_length
            {
              power: 2,
              mel_filters: this.config.mel_filters,
              log_mel: "log10",
              // Custom
              max_num_frames: this.config.nb_max_frames
              // 3000
            }
          ), M = v.data, y = (0, U.max)(
            /** @type {Float32Array} */
            M
          )[0];
          for (let b = 0; b < M.length; ++b)
            M[b] = (Math.max(M[b], y - 8) + 4) / 4;
          return v;
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.
         */
        async _call(g) {
          (0, _.validate_audio_inputs)(g, "WhisperFeatureExtractor");
          let v;
          return g.length > this.config.n_samples ? (console.warn(
            "Attempting to extract features for audio longer than 30 seconds. If using a pipeline to extract transcript from a long audio clip, remember to specify `chunk_length_s` and/or `stride_length_s`."
          ), v = g.slice(0, this.config.n_samples)) : (v = new Float32Array(this.config.n_samples), v.set(g)), {
            input_features: (await this._extract_fbank_features(v)).unsqueeze_(0)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/whisper/generation_whisper.js": (
    /*!**************************************************!*\
      !*** ./src/models/whisper/generation_whisper.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        WhisperGenerationConfig: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../generation/configuration_utils.js */
        "./src/generation/configuration_utils.js"
      );
      class D extends _.GenerationConfig {
        constructor() {
          super(...arguments);
          /**
           * Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.
           * @type {boolean}
           */
          fe(this, "return_timestamps", null);
          /**
           * Whether to return token-level timestamps
           * with the text. This can be used with or without the `return_timestamps` option. To get word-level
           * timestamps, use the tokenizer to group the tokens into words.
           * @type {boolean}
           */
          fe(this, "return_token_timestamps", null);
          /**
           * The number of audio frames available in this chunk. This is only used generating word-level timestamps.
           * @type {number}
           */
          fe(this, "num_frames", null);
          /**
           * Alignment heads to predict word-level timestamps. This is a list of [layer, head] pairs that
           * select the cross-attention heads that are highly correlated to word-level timing.
           * @type {[number, number][]}
           */
          fe(this, "alignment_heads", null);
          /**
           * Task to use for generation, either "translate" or "transcribe".
           * @type {string}
           */
          fe(this, "task", null);
          /**
           * Language token to use for generation, can be either in the form of `<|en|>`, `en` or `english`.
           * You can find all the possible language tokens in the `model.generation_config.lang_to_id` dictionary.
           * @type {string}
           */
          fe(this, "language", null);
          /**
           * The id of the `"<|notimestamps|>"` token.
           * @type {number}
           */
          fe(this, "no_timestamps_token_id", null);
          /**
           * Rank-1 list of token IDs created by passing text to [`~WhisperProcessor.get_prompt_ids`] that is
           * provided as a prompt to each chunk. This can be used to provide or "prompt-engineer" a context for
           * transcription, e.g. custom vocabularies or proper nouns to make it more likely to predict those words
           * correctly. It cannot be used in conjunction with `decoder_start_token_id` as it overwrites this value.
           * @type {number[]}
           */
          fe(this, "prompt_ids", null);
          /**
           * Whether the model is multilingual or not.
           * @type {boolean}
           */
          fe(this, "is_multilingual", null);
          /**
           * (Optional) A mapping from language tokens to their corresponding IDs.
           * Only required if the model is multilingual.
           * @type {Record<string, number>|null}
           */
          fe(this, "lang_to_id", null);
          /**
           * (Optional) A mapping from task tokens to their corresponding IDs.
           * @type {Record<string, number>|null}
           */
          fe(this, "task_to_id", null);
          /**
           * Used to set the maximum value of the initial timestamp. This is used to prevent the model from
           * predicting timestamps that are too far in the future.
           * @type {number}
           */
          fe(this, "max_initial_timestamp_index", 1);
        }
      }
    }
  ),
  /***/
  "./src/models/whisper/processing_whisper.js": (
    /*!**************************************************!*\
      !*** ./src/models/whisper/processing_whisper.js ***!
      \**************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        WhisperProcessor: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      ), D = r(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), U = r(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      );
      class Y extends U.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(g) {
          return await this.feature_extractor(g);
        }
      }
      fe(Y, "tokenizer_class", D.AutoTokenizer), fe(Y, "feature_extractor_class", _.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/yolos/image_processing_yolos.js": (
    /*!****************************************************!*\
      !*** ./src/models/yolos/image_processing_yolos.js ***!
      \****************************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        YolosFeatureExtractor: () => (
          /* binding */
          U
        ),
        /* harmony export */
        YolosImageProcessor: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class D extends _.ImageProcessor {
        /** @type {typeof post_process_object_detection} */
        post_process_object_detection(...R) {
          return (0, _.post_process_object_detection)(...R);
        }
      }
      class U extends D {
      }
    }
  ),
  /***/
  "./src/ops/registry.js": (
    /*!*****************************!*\
      !*** ./src/ops/registry.js ***!
      \*****************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        TensorOpRegistry: () => (
          /* binding */
          g
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../backends/onnx.js */
        "./src/backends/onnx.js"
      ), D = r(
        /*! ../utils/tensor.js */
        "./src/utils/tensor.js"
      ), U = r(
        /*! ../env.js */
        "./src/env.js"
      );
      const Y = U.apis.IS_BROWSER_ENV || U.apis.IS_WEBWORKER_ENV, R = async (v, M, y) => {
        const b = await (0, _.createInferenceSession)(
          new Uint8Array(v),
          M
        );
        let I = Promise.resolve();
        return (
          /** @type {any} */
          async (K) => {
            const se = (0, _.isONNXProxy)(), ie = Object.fromEntries(Object.entries(K).map(([j, q]) => [j, (se ? q.clone() : q).ort_tensor])), W = await (I = Y ? I.then(() => b.run(ie)) : b.run(ie));
            return Array.isArray(y) ? y.map((j) => new D.Tensor(W[j])) : new D.Tensor(W[
              /** @type {string} */
              y
            ]);
          }
        );
      };
      class g {
        static get nearest_interpolate_4d() {
          return this._nearest_interpolate_4d || (this._nearest_interpolate_4d = R(
            [8, 10, 18, 0, 58, 129, 1, 10, 41, 10, 1, 120, 10, 0, 10, 0, 10, 1, 115, 18, 1, 121, 34, 6, 82, 101, 115, 105, 122, 101, 42, 18, 10, 4, 109, 111, 100, 101, 34, 7, 110, 101, 97, 114, 101, 115, 116, 160, 1, 3, 18, 1, 114, 90, 31, 10, 1, 120, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 90, 15, 10, 1, 115, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 4, 98, 31, 10, 1, 121, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 66, 2, 16, 21],
            this.session_options,
            "y"
          )), this._nearest_interpolate_4d;
        }
        static get bilinear_interpolate_4d() {
          return this._bilinear_interpolate_4d || (this._bilinear_interpolate_4d = R(
            [8, 9, 18, 0, 58, 128, 1, 10, 40, 10, 1, 120, 10, 0, 10, 0, 10, 1, 115, 18, 1, 121, 34, 6, 82, 101, 115, 105, 122, 101, 42, 17, 10, 4, 109, 111, 100, 101, 34, 6, 108, 105, 110, 101, 97, 114, 160, 1, 3, 18, 1, 114, 90, 31, 10, 1, 120, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 90, 15, 10, 1, 115, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 4, 98, 31, 10, 1, 121, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 66, 2, 16, 20],
            this.session_options,
            "y"
          )), this._bilinear_interpolate_4d;
        }
        static get bicubic_interpolate_4d() {
          return this._bicubic_interpolate_4d || (this._bicubic_interpolate_4d = R(
            [8, 9, 18, 0, 58, 127, 10, 39, 10, 1, 120, 10, 0, 10, 0, 10, 1, 115, 18, 1, 121, 34, 6, 82, 101, 115, 105, 122, 101, 42, 16, 10, 4, 109, 111, 100, 101, 34, 5, 99, 117, 98, 105, 99, 160, 1, 3, 18, 1, 114, 90, 31, 10, 1, 120, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 90, 15, 10, 1, 115, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 4, 98, 31, 10, 1, 121, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 66, 2, 16, 20],
            this.session_options,
            "y"
          )), this._bicubic_interpolate_4d;
        }
        static get matmul() {
          return this._matmul || (this._matmul = R(
            [8, 9, 18, 0, 58, 55, 10, 17, 10, 1, 97, 10, 1, 98, 18, 1, 99, 34, 6, 77, 97, 116, 77, 117, 108, 18, 1, 114, 90, 9, 10, 1, 97, 18, 4, 10, 2, 8, 1, 90, 9, 10, 1, 98, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 99, 18, 4, 10, 2, 8, 1, 66, 2, 16, 20],
            this.session_options,
            "c"
          )), this._matmul;
        }
        static get stft() {
          return this._stft || (this._stft = R(
            [8, 7, 18, 0, 58, 148, 1, 10, 38, 10, 1, 115, 10, 1, 106, 10, 1, 119, 10, 1, 108, 18, 1, 111, 34, 4, 83, 84, 70, 84, 42, 15, 10, 8, 111, 110, 101, 115, 105, 100, 101, 100, 24, 1, 160, 1, 2, 18, 1, 115, 90, 26, 10, 1, 115, 18, 21, 10, 19, 8, 1, 18, 15, 10, 3, 18, 1, 98, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 90, 11, 10, 1, 106, 18, 6, 10, 4, 8, 7, 18, 0, 90, 16, 10, 1, 119, 18, 11, 10, 9, 8, 1, 18, 5, 10, 3, 18, 1, 119, 90, 11, 10, 1, 108, 18, 6, 10, 4, 8, 7, 18, 0, 98, 31, 10, 1, 111, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 102, 10, 3, 18, 1, 100, 10, 3, 18, 1, 99, 66, 2, 16, 17],
            this.session_options,
            "o"
          )), this._stft;
        }
        static get rfft() {
          return this._rfft || (this._rfft = R(
            [8, 9, 18, 0, 58, 97, 10, 33, 10, 1, 120, 10, 0, 10, 1, 97, 18, 1, 121, 34, 3, 68, 70, 84, 42, 15, 10, 8, 111, 110, 101, 115, 105, 100, 101, 100, 24, 1, 160, 1, 2, 18, 1, 100, 90, 21, 10, 1, 120, 18, 16, 10, 14, 8, 1, 18, 10, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 90, 11, 10, 1, 97, 18, 6, 10, 4, 8, 7, 18, 0, 98, 21, 10, 1, 121, 18, 16, 10, 14, 8, 1, 18, 10, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 66, 2, 16, 20],
            this.session_options,
            "y"
          )), this._rfft;
        }
        static get top_k() {
          return this._top_k || (this._top_k = R(
            [8, 10, 18, 0, 58, 73, 10, 18, 10, 1, 120, 10, 1, 107, 18, 1, 118, 18, 1, 105, 34, 4, 84, 111, 112, 75, 18, 1, 116, 90, 9, 10, 1, 120, 18, 4, 10, 2, 8, 1, 90, 15, 10, 1, 107, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 118, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 105, 18, 4, 10, 2, 8, 7, 66, 2, 16, 21],
            this.session_options,
            [
              /* Values */
              "v",
              /* Indices */
              "i"
            ]
          )), this._top_k;
        }
        static get slice() {
          return this._slice || (this._slice = R(
            [8, 7, 18, 0, 58, 96, 10, 25, 10, 1, 120, 10, 1, 115, 10, 1, 101, 10, 1, 97, 10, 1, 116, 18, 1, 121, 34, 5, 83, 108, 105, 99, 101, 18, 1, 114, 90, 9, 10, 1, 120, 18, 4, 10, 2, 8, 1, 90, 9, 10, 1, 115, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 101, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 97, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 116, 18, 4, 10, 2, 8, 7, 98, 9, 10, 1, 121, 18, 4, 10, 2, 8, 1, 66, 2, 16, 13],
            this.session_options,
            "y"
          )), this._slice;
        }
      }
      fe(g, "session_options", {
        // TODO: Allow for multiple execution providers
        // executionProviders: ['webgpu'],
      });
    }
  ),
  /***/
  "./src/pipelines.js": (
    /*!**************************!*\
      !*** ./src/pipelines.js ***!
      \**************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        AudioClassificationPipeline: () => (
          /* binding */
          _e
        ),
        /* harmony export */
        AutomaticSpeechRecognitionPipeline: () => (
          /* binding */
          xe
        ),
        /* harmony export */
        DepthEstimationPipeline: () => (
          /* binding */
          Ce
        ),
        /* harmony export */
        DocumentQuestionAnsweringPipeline: () => (
          /* binding */
          ye
        ),
        /* harmony export */
        FeatureExtractionPipeline: () => (
          /* binding */
          ne
        ),
        /* harmony export */
        FillMaskPipeline: () => (
          /* binding */
          q
        ),
        /* harmony export */
        ImageClassificationPipeline: () => (
          /* binding */
          ke
        ),
        /* harmony export */
        ImageFeatureExtractionPipeline: () => (
          /* binding */
          be
        ),
        /* harmony export */
        ImageSegmentationPipeline: () => (
          /* binding */
          Fe
        ),
        /* harmony export */
        ImageToImagePipeline: () => (
          /* binding */
          de
        ),
        /* harmony export */
        ImageToTextPipeline: () => (
          /* binding */
          ce
        ),
        /* harmony export */
        ObjectDetectionPipeline: () => (
          /* binding */
          tt
        ),
        /* harmony export */
        Pipeline: () => (
          /* binding */
          se
        ),
        /* harmony export */
        QuestionAnsweringPipeline: () => (
          /* binding */
          j
        ),
        /* harmony export */
        SummarizationPipeline: () => (
          /* binding */
          S
        ),
        /* harmony export */
        Text2TextGenerationPipeline: () => (
          /* binding */
          A
        ),
        /* harmony export */
        TextClassificationPipeline: () => (
          /* binding */
          ie
        ),
        /* harmony export */
        TextGenerationPipeline: () => (
          /* binding */
          F
        ),
        /* harmony export */
        TextToAudioPipeline: () => (
          /* binding */
          J
        ),
        /* harmony export */
        TokenClassificationPipeline: () => (
          /* binding */
          W
        ),
        /* harmony export */
        TranslationPipeline: () => (
          /* binding */
          w
        ),
        /* harmony export */
        ZeroShotAudioClassificationPipeline: () => (
          /* binding */
          re
        ),
        /* harmony export */
        ZeroShotClassificationPipeline: () => (
          /* binding */
          le
        ),
        /* harmony export */
        ZeroShotImageClassificationPipeline: () => (
          /* binding */
          Ee
        ),
        /* harmony export */
        ZeroShotObjectDetectionPipeline: () => (
          /* binding */
          Ge
        ),
        /* harmony export */
        pipeline: () => (
          /* binding */
          te
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./tokenizers.js */
        "./src/tokenizers.js"
      ), D = r(
        /*! ./models.js */
        "./src/models.js"
      ), U = r(
        /*! ./models/auto/processing_auto.js */
        "./src/models/auto/processing_auto.js"
      );
      r(
        /*! ./base/processing_utils.js */
        "./src/base/processing_utils.js"
      );
      var Y = r(
        /*! ./utils/generic.js */
        "./src/utils/generic.js"
      ), R = r(
        /*! ./utils/core.js */
        "./src/utils/core.js"
      ), g = r(
        /*! ./utils/maths.js */
        "./src/utils/maths.js"
      ), v = r(
        /*! ./utils/audio.js */
        "./src/utils/audio.js"
      ), M = r(
        /*! ./utils/tensor.js */
        "./src/utils/tensor.js"
      ), y = r(
        /*! ./utils/image.js */
        "./src/utils/image.js"
      );
      async function b(je) {
        return Array.isArray(je) || (je = [je]), await Promise.all(je.map((ae) => y.RawImage.read(ae)));
      }
      async function I(je, ae) {
        return Array.isArray(je) || (je = [je]), await Promise.all(je.map((Te) => typeof Te == "string" || Te instanceof URL ? (0, v.read_audio)(Te, ae) : Te instanceof Float64Array ? new Float32Array(Te) : Te));
      }
      function K(je, ae) {
        ae && (je = je.map((Re) => Re | 0));
        const [Te, Ue, Ve, Ne] = je;
        return { xmin: Te, ymin: Ue, xmax: Ve, ymax: Ne };
      }
      class se extends Y.Callable {
        /**
         * Create a new Pipeline.
         * @param {Object} options An object containing the following properties:
         * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.
         * @param {PreTrainedModel} [options.model] The model used by the pipeline.
         * @param {PreTrainedTokenizer} [options.tokenizer=null] The tokenizer used by the pipeline (if any).
         * @param {Processor} [options.processor=null] The processor used by the pipeline (if any).
         */
        constructor({ task: ae, model: Te, tokenizer: Ue = null, processor: Ve = null }) {
          super(), this.task = ae, this.model = Te, this.tokenizer = Ue, this.processor = Ve;
        }
        /** @type {DisposeType} */
        async dispose() {
          await this.model.dispose();
        }
      }
      class ie extends /** @type {new (options: TextPipelineConstructorArgs) => TextClassificationPipelineType} */
      se {
        /**
         * Create a new TextClassificationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {TextClassificationPipelineCallback} */
        async _call(ae, {
          top_k: Te = 1
        } = {}) {
          const Ue = this.tokenizer(ae, {
            padding: !0,
            truncation: !0
          }), Ve = await this.model(Ue), Ne = (
            // @ts-expect-error TS2339
            this.model.config.problem_type === "multi_label_classification" ? (dt) => dt.sigmoid() : (dt) => new M.Tensor(
              "float32",
              (0, g.softmax)(dt.data),
              dt.dims
            )
          ), Re = this.model.config.id2label, st = [];
          for (const dt of Ve.logits) {
            const ct = Ne(dt), lt = await (0, M.topk)(ct, Te), ht = lt[0].tolist(), oe = lt[1].tolist().map((H, me) => ({
              label: Re ? Re[H] : `LABEL_${H}`,
              score: ht[me]
            }));
            Te === 1 ? st.push(...oe) : st.push(oe);
          }
          return Array.isArray(ae) || Te === 1 ? (
            /** @type {TextClassificationOutput} */
            st
          ) : (
            /** @type {TextClassificationOutput[]} */
            st[0]
          );
        }
      }
      class W extends /** @type {new (options: TextPipelineConstructorArgs) => TokenClassificationPipelineType} */
      se {
        /**
         * Create a new TokenClassificationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {TokenClassificationPipelineCallback} */
        async _call(ae, {
          ignore_labels: Te = ["O"]
        } = {}) {
          const Ue = Array.isArray(ae), Ve = this.tokenizer(Ue ? ae : [ae], {
            padding: !0,
            truncation: !0
          }), Re = (await this.model(Ve)).logits, st = this.model.config.id2label, dt = [];
          for (let ct = 0; ct < Re.dims[0]; ++ct) {
            const lt = Ve.input_ids[ct], ht = Re[ct], L = [];
            for (let oe = 0; oe < ht.dims[0]; ++oe) {
              const H = ht[oe], me = (0, g.max)(H.data)[1], Ae = st ? st[me] : `LABEL_${me}`;
              if (Te.includes(Ae))
                continue;
              const We = this.tokenizer.decode([lt[oe].item()], { skip_special_tokens: !0 });
              if (We === "")
                continue;
              const Je = (0, g.softmax)(H.data);
              L.push({
                entity: Ae,
                score: Je[me],
                index: oe,
                word: We
                // TODO: Add support for start and end
                // start: null,
                // end: null,
              });
            }
            dt.push(L);
          }
          return Ue ? dt : dt[0];
        }
      }
      class j extends /** @type {new (options: TextPipelineConstructorArgs) => QuestionAnsweringPipelineType} */
      se {
        /**
         * Create a new QuestionAnsweringPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {QuestionAnsweringPipelineCallback} */
        async _call(ae, Te, {
          top_k: Ue = 1
        } = {}) {
          const Ve = this.tokenizer(ae, {
            text_pair: Te,
            padding: !0,
            truncation: !0
          }), { start_logits: Ne, end_logits: Re } = await this.model(Ve), st = Ve.input_ids.tolist(), dt = Ve.attention_mask.tolist(), ct = this.tokenizer.all_special_ids, lt = [];
          for (let ht = 0; ht < Ne.dims[0]; ++ht) {
            const L = st[ht], oe = L.findIndex(
              (ut) => (
                // We use == to match bigint with number
                // @ts-ignore
                ut == this.tokenizer.sep_token_id
              )
            );
            dt[ht].map((ut, mt) => ut == 1 && (mt === 0 || mt > oe && ct.findIndex((vt) => vt == L[mt]) === -1));
            const H = Ne[ht].tolist(), me = Re[ht].tolist();
            for (let ut = 1; ut < H.length; ++ut)
              (dt[ht] == 0 || ut <= oe || ct.findIndex((mt) => mt == L[ut]) !== -1) && (H[ut] = -1 / 0, me[ut] = -1 / 0);
            const Ae = (0, g.softmax)(H).map((ut, mt) => [ut, mt]), We = (0, g.softmax)(me).map((ut, mt) => [ut, mt]);
            Ae[0][0] = 0, We[0][0] = 0;
            const Je = (0, R.product)(Ae, We).filter((ut) => ut[0][1] <= ut[1][1]).map((ut) => [ut[0][1], ut[1][1], ut[0][0] * ut[1][0]]).sort((ut, mt) => mt[2] - ut[2]);
            for (let ut = 0; ut < Math.min(Je.length, Ue); ++ut) {
              const [mt, vt, kt] = Je[ut], At = L.slice(mt, vt + 1), is = this.tokenizer.decode(At, {
                skip_special_tokens: !0
              });
              lt.push({
                answer: is,
                score: kt
              });
            }
          }
          return Ue === 1 ? lt[0] : lt;
        }
      }
      class q extends /** @type {new (options: TextPipelineConstructorArgs) => FillMaskPipelineType} */
      se {
        /**
         * Create a new FillMaskPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {FillMaskPipelineCallback} */
        async _call(ae, {
          top_k: Te = 5
        } = {}) {
          const Ue = this.tokenizer(ae, {
            padding: !0,
            truncation: !0
          }), { logits: Ve } = await this.model(Ue), Ne = [], Re = Ue.input_ids.tolist();
          for (let st = 0; st < Re.length; ++st) {
            const dt = Re[st], ct = dt.findIndex(
              (H) => (
                // We use == to match bigint with number
                // @ts-ignore
                H == this.tokenizer.mask_token_id
              )
            );
            if (ct === -1)
              throw Error(`Mask token (${this.tokenizer.mask_token}) not found in text.`);
            const lt = Ve[st][ct], ht = await (0, M.topk)(new M.Tensor(
              "float32",
              (0, g.softmax)(lt.data),
              lt.dims
            ), Te), L = ht[0].tolist(), oe = ht[1].tolist();
            Ne.push(oe.map((H, me) => {
              const Ae = dt.slice();
              return Ae[ct] = H, {
                score: L[me],
                token: Number(H),
                token_str: this.tokenizer.decode([H]),
                sequence: this.tokenizer.decode(Ae, { skip_special_tokens: !0 })
              };
            }));
          }
          return Array.isArray(ae) ? Ne : Ne[0];
        }
      }
      class A extends /** @type {new (options: TextPipelineConstructorArgs) => Text2TextGenerationPipelineType} */
      se {
        /**
         * Create a new Text2TextGenerationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(Te) {
          super(Te);
          /** @type {'generated_text'} */
          fe(this, "_key", "generated_text");
        }
        /** @type {Text2TextGenerationPipelineCallback} */
        async _call(Te, Ue = {}) {
          Array.isArray(Te) || (Te = [Te]), this.model.config.prefix && (Te = Te.map((ct) => this.model.config.prefix + ct));
          const Ve = this.model.config.task_specific_params;
          Ve && Ve[this.task] && Ve[this.task].prefix && (Te = Te.map((ct) => Ve[this.task].prefix + ct));
          const Ne = this.tokenizer, Re = {
            padding: !0,
            truncation: !0
          };
          let st;
          this instanceof w && "_build_translation_inputs" in Ne ? st = Ne._build_translation_inputs(Te, Re, Ue) : st = Ne(Te, Re);
          const dt = await this.model.generate({ ...st, ...Ue });
          return Ne.batch_decode(
            /** @type {Tensor} */
            dt,
            {
              skip_special_tokens: !0
            }
          ).map((ct) => ({ [this._key]: ct }));
        }
      }
      class S extends /** @type {new (options: TextPipelineConstructorArgs) => SummarizationPipelineType} */
      /** @type {any} */
      A {
        /**
         * Create a new SummarizationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(Te) {
          super(Te);
          /** @type {'summary_text'} */
          fe(this, "_key", "summary_text");
        }
      }
      class w extends /** @type {new (options: TextPipelineConstructorArgs) => TranslationPipelineType} */
      /** @type {any} */
      A {
        /**
         * Create a new TranslationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(Te) {
          super(Te);
          /** @type {'translation_text'} */
          fe(this, "_key", "translation_text");
        }
      }
      function x(je) {
        return Array.isArray(je) && je.every((ae) => "role" in ae && "content" in ae);
      }
      class F extends /** @type {new (options: TextPipelineConstructorArgs) => TextGenerationPipelineType} */
      se {
        /**
         * Create a new TextGenerationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {TextGenerationPipelineCallback} */
        async _call(ae, Te = {}) {
          let Ue = !1, Ve = !1, Ne;
          if (typeof ae == "string")
            Ne = ae = [ae];
          else if (Array.isArray(ae) && ae.every((oe) => typeof oe == "string"))
            Ue = !0, Ne = /** @type {string[]} */
            ae;
          else {
            if (x(ae))
              ae = [
                /** @type {Chat} */
                ae
              ];
            else if (Array.isArray(ae) && ae.every(x))
              Ue = !0;
            else
              throw new Error("Input must be a string, an array of strings, a Chat, or an array of Chats");
            Ve = !0, Ne = /** @type {string[]} */
            /** @type {Chat[]} */
            ae.map(
              (oe) => this.tokenizer.apply_chat_template(oe, {
                tokenize: !1,
                add_generation_prompt: !0
              })
            );
          }
          const Re = Te.add_special_tokens ?? !1, st = Ve ? !1 : Te.return_full_text ?? !0;
          this.tokenizer.padding_side = "left";
          const dt = this.tokenizer(Ne, {
            add_special_tokens: Re,
            padding: !0,
            truncation: !0
          }), ct = (
            /** @type {Tensor} */
            await this.model.generate({
              ...dt,
              ...Te
            })
          ), lt = this.tokenizer.batch_decode(ct, {
            skip_special_tokens: !0
          });
          let ht;
          !st && dt.input_ids.dims.at(-1) > 0 && (ht = this.tokenizer.batch_decode(dt.input_ids, {
            skip_special_tokens: !0
          }).map((oe) => oe.length));
          const L = Array.from({ length: ae.length }, (oe) => []);
          for (let oe = 0; oe < lt.length; ++oe) {
            const H = Math.floor(oe / ct.dims[0] * ae.length);
            ht && (lt[oe] = lt[oe].slice(ht[H])), L[H].push({
              generated_text: Ve ? [
                .../** @type {Chat[]} */
                ae[H],
                { role: "assistant", content: lt[oe] }
              ] : lt[oe]
            });
          }
          return !Ue && L.length === 1 ? L[0] : L;
        }
      }
      class le extends /** @type {new (options: TextPipelineConstructorArgs) => ZeroShotClassificationPipelineType} */
      se {
        /**
         * Create a new ZeroShotClassificationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae), this.label2id = Object.fromEntries(
            Object.entries(
              /** @type {any} */
              this.model.config.label2id
            ).map(
              ([Te, Ue]) => [Te.toLowerCase(), Ue]
            )
          ), this.entailment_id = this.label2id.entailment, this.entailment_id === void 0 && (console.warn("Could not find 'entailment' in label2id mapping. Using 2 as entailment_id."), this.entailment_id = 2), this.contradiction_id = this.label2id.contradiction ?? this.label2id.not_entailment, this.contradiction_id === void 0 && (console.warn("Could not find 'contradiction' in label2id mapping. Using 0 as contradiction_id."), this.contradiction_id = 0);
        }
        /** @type {ZeroShotClassificationPipelineCallback} */
        async _call(ae, Te, {
          hypothesis_template: Ue = "This example is {}.",
          multi_label: Ve = !1
        } = {}) {
          const Ne = Array.isArray(ae);
          Ne || (ae = [
            /** @type {string} */
            ae
          ]), Array.isArray(Te) || (Te = [Te]);
          const Re = Te.map(
            (ct) => Ue.replace("{}", ct)
          ), st = Ve || Te.length === 1, dt = [];
          for (const ct of ae) {
            const lt = [];
            for (const oe of Re) {
              const H = this.tokenizer(ct, {
                text_pair: oe,
                padding: !0,
                truncation: !0
              }), me = await this.model(H);
              st ? lt.push([
                me.logits.data[this.contradiction_id],
                me.logits.data[this.entailment_id]
              ]) : lt.push(me.logits.data[this.entailment_id]);
            }
            const L = (st ? lt.map((oe) => (0, g.softmax)(oe)[1]) : (0, g.softmax)(lt)).map((oe, H) => [oe, H]).sort((oe, H) => H[0] - oe[0]);
            dt.push({
              sequence: ct,
              labels: L.map((oe) => Te[oe[1]]),
              scores: L.map((oe) => oe[0])
            });
          }
          return Ne ? dt : dt[0];
        }
      }
      class ne extends /** @type {new (options: TextPipelineConstructorArgs) => FeatureExtractionPipelineType} */
      se {
        /**
         * Create a new FeatureExtractionPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {FeatureExtractionPipelineCallback} */
        async _call(ae, {
          pooling: Te = (
            /** @type {'none'} */
            "none"
          ),
          normalize: Ue = !1,
          quantize: Ve = !1,
          precision: Ne = (
            /** @type {'binary'} */
            "binary"
          )
        } = {}) {
          const Re = this.tokenizer(ae, {
            padding: !0,
            truncation: !0
          }), st = await this.model(Re);
          let dt = st.last_hidden_state ?? st.logits ?? st.token_embeddings;
          if (Te !== "none") if (Te === "mean")
            dt = (0, M.mean_pooling)(dt, Re.attention_mask);
          else if (Te === "cls")
            dt = dt.slice(null, 0);
          else
            throw Error(`Pooling method '${Te}' not supported.`);
          return Ue && (dt = dt.normalize(2, -1)), Ve && (dt = (0, M.quantize_embeddings)(dt, Ne)), dt;
        }
      }
      class be extends /** @type {new (options: ImagePipelineConstructorArgs) => ImageFeatureExtractionPipelineType} */
      se {
        /**
         * Create a new ImageFeatureExtractionPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {ImageFeatureExtractionPipelineCallback} */
        async _call(ae, {
          pool: Te = null
        } = {}) {
          const Ue = await b(ae), { pixel_values: Ve } = await this.processor(Ue), Ne = await this.model({ pixel_values: Ve });
          let Re;
          if (Te) {
            if (!("pooler_output" in Ne))
              throw Error("No pooled output was returned. Make sure the model has a 'pooler' layer when using the 'pool' option.");
            Re = Ne.pooler_output;
          } else
            Re = Ne.last_hidden_state ?? Ne.logits ?? Ne.image_embeds;
          return Re;
        }
      }
      class _e extends /** @type {new (options: AudioPipelineConstructorArgs) => AudioClassificationPipelineType} */
      se {
        /**
         * Create a new AudioClassificationPipeline.
         * @param {AudioPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {AudioClassificationPipelineCallback} */
        async _call(ae, {
          top_k: Te = 5
        } = {}) {
          const Ue = this.processor.feature_extractor.config.sampling_rate, Ve = await I(ae, Ue), Ne = this.model.config.id2label, Re = [];
          for (const st of Ve) {
            const dt = await this.processor(st), lt = (await this.model(dt)).logits[0], ht = await (0, M.topk)(new M.Tensor(
              "float32",
              (0, g.softmax)(lt.data),
              lt.dims
            ), Te), L = ht[0].tolist(), H = ht[1].tolist().map((me, Ae) => ({
              label: (
                /** @type {string} */
                Ne ? Ne[me] : `LABEL_${me}`
              ),
              score: (
                /** @type {number} */
                L[Ae]
              )
            }));
            Re.push(H);
          }
          return Array.isArray(ae) ? Re : Re[0];
        }
      }
      class re extends /** @type {new (options: TextAudioPipelineConstructorArgs) => ZeroShotAudioClassificationPipelineType} */
      se {
        /**
         * Create a new ZeroShotAudioClassificationPipeline.
         * @param {TextAudioPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {ZeroShotAudioClassificationPipelineCallback} */
        async _call(ae, Te, {
          hypothesis_template: Ue = "This is a sound of {}."
        } = {}) {
          const Ve = !Array.isArray(ae);
          Ve && (ae = [
            /** @type {AudioInput} */
            ae
          ]);
          const Ne = Te.map(
            (lt) => Ue.replace("{}", lt)
          ), Re = this.tokenizer(Ne, {
            padding: !0,
            truncation: !0
          }), st = this.processor.feature_extractor.config.sampling_rate, dt = await I(ae, st), ct = [];
          for (const lt of dt) {
            const ht = await this.processor(lt), L = await this.model({ ...Re, ...ht }), oe = (0, g.softmax)(L.logits_per_audio.data);
            ct.push([...oe].map((H, me) => ({
              score: H,
              label: Te[me]
            })));
          }
          return Ve ? ct[0] : ct;
        }
      }
      class xe extends /** @type {new (options: TextAudioPipelineConstructorArgs) => AutomaticSpeechRecognitionPipelineType} */
      se {
        /**
         * Create a new AutomaticSpeechRecognitionPipeline.
         * @param {TextAudioPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {AutomaticSpeechRecognitionPipelineCallback} */
        async _call(ae, Te = {}) {
          switch (this.model.config.model_type) {
            case "whisper":
              return this._call_whisper(ae, Te);
            case "wav2vec2":
            case "wav2vec2-bert":
            case "unispeech":
            case "unispeech-sat":
            case "hubert":
              return this._call_wav2vec2(ae, Te);
            case "moonshine":
              return this._call_moonshine(ae, Te);
            default:
              throw new Error(`AutomaticSpeechRecognitionPipeline does not support model type '${this.model.config.model_type}'.`);
          }
        }
        /**
         * @type {AutomaticSpeechRecognitionPipelineCallback}
         * @private
         */
        async _call_wav2vec2(ae, Te) {
          Te.language && console.warn('`language` parameter is not yet supported for `wav2vec2` models, defaulting to "English".'), Te.task && console.warn('`task` parameter is not yet supported for `wav2vec2` models, defaulting to "transcribe".');
          const Ue = !Array.isArray(ae);
          Ue && (ae = [
            /** @type {AudioInput} */
            ae
          ]);
          const Ve = this.processor.feature_extractor.config.sampling_rate, Ne = await I(ae, Ve), Re = [];
          for (const st of Ne) {
            const dt = await this.processor(st), lt = (await this.model(dt)).logits[0], ht = [];
            for (const oe of lt)
              ht.push((0, g.max)(oe.data)[1]);
            const L = this.tokenizer.decode(ht);
            Re.push({ text: L });
          }
          return Ue ? Re[0] : Re;
        }
        /**
         * @type {AutomaticSpeechRecognitionPipelineCallback}
         * @private
         */
        async _call_whisper(ae, Te) {
          const Ue = Te.return_timestamps ?? !1, Ve = Te.chunk_length_s ?? 0, Ne = Te.force_full_sequences ?? !1;
          let Re = Te.stride_length_s ?? null;
          const st = { ...Te };
          Ue === "word" && (st.return_token_timestamps = !0, st.return_timestamps = !1);
          const dt = !Array.isArray(ae);
          dt && (ae = [
            /** @type {AudioInput} */
            ae
          ]);
          const ct = this.processor.feature_extractor.config.chunk_length / this.model.config.max_source_positions, lt = this.processor.feature_extractor.config.hop_length, ht = this.processor.feature_extractor.config.sampling_rate, L = await I(ae, ht), oe = [];
          for (const H of L) {
            let me = [];
            if (Ve > 0) {
              if (Re === null)
                Re = Ve / 6;
              else if (Ve <= Re)
                throw Error("`chunk_length_s` must be larger than `stride_length_s`.");
              const Je = ht * Ve, ut = ht * Re, mt = Je - 2 * ut;
              let vt = 0;
              for (; ; ) {
                const kt = vt + Je, At = H.subarray(vt, kt), is = await this.processor(At), ys = vt === 0, Cs = kt >= H.length;
                if (me.push({
                  stride: [
                    At.length,
                    ys ? 0 : ut,
                    Cs ? 0 : ut
                  ],
                  input_features: is.input_features,
                  is_last: Cs
                }), Cs) break;
                vt += mt;
              }
            } else
              me = [{
                stride: [H.length, 0, 0],
                input_features: (await this.processor(H)).input_features,
                is_last: !0
              }];
            for (const Je of me) {
              st.num_frames = Math.floor(Je.stride[0] / lt);
              const ut = await this.model.generate({
                inputs: Je.input_features,
                ...st
              });
              Ue === "word" ? (Je.tokens = ut.sequences.tolist()[0], Je.token_timestamps = ut.token_timestamps.tolist()[0].map(
                (mt) => (0, g.round)(mt, 2)
              )) : Je.tokens = /** @type {Tensor} */
              ut[0].tolist(), Je.stride = Je.stride.map((mt) => mt / ht);
            }
            const [Ae, We] = this.tokenizer._decode_asr(me, {
              time_precision: ct,
              return_timestamps: Ue,
              force_full_sequences: Ne
            });
            oe.push({ text: Ae, ...We });
          }
          return dt ? oe[0] : oe;
        }
        /**
         * @type {AutomaticSpeechRecognitionPipelineCallback}
         * @private
         */
        async _call_moonshine(ae, Te) {
          const Ue = !Array.isArray(ae);
          Ue && (ae = [
            /** @type {AudioInput} */
            ae
          ]);
          const Ve = this.processor.feature_extractor.config.sampling_rate, Ne = await I(ae, Ve), Re = [];
          for (const st of Ne) {
            const dt = await this.processor(st), ct = Math.floor(st.length / Ve) * 6, lt = await this.model.generate({ max_new_tokens: ct, ...Te, ...dt }), ht = this.processor.batch_decode(
              /** @type {Tensor} */
              lt,
              { skip_special_tokens: !0 }
            )[0];
            Re.push({ text: ht });
          }
          return Ue ? Re[0] : Re;
        }
      }
      class ce extends /** @type {new (options: TextImagePipelineConstructorArgs) => ImageToTextPipelineType} */
      se {
        /**
         * Create a new ImageToTextPipeline.
         * @param {TextImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {ImageToTextPipelineCallback} */
        async _call(ae, Te = {}) {
          const Ue = Array.isArray(ae), Ve = await b(ae), { pixel_values: Ne } = await this.processor(Ve), Re = [];
          for (const st of Ne) {
            st.dims = [1, ...st.dims];
            const dt = await this.model.generate({ inputs: st, ...Te }), ct = this.tokenizer.batch_decode(
              /** @type {Tensor} */
              dt,
              {
                skip_special_tokens: !0
              }
            ).map((lt) => ({ generated_text: lt.trim() }));
            Re.push(ct);
          }
          return Ue ? Re : Re[0];
        }
      }
      class ke extends /** @type {new (options: ImagePipelineConstructorArgs) => ImageClassificationPipelineType} */
      se {
        /**
         * Create a new ImageClassificationPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {ImageClassificationPipelineCallback} */
        async _call(ae, {
          top_k: Te = 5
        } = {}) {
          const Ue = await b(ae), { pixel_values: Ve } = await this.processor(Ue), Ne = await this.model({ pixel_values: Ve }), Re = this.model.config.id2label, st = [];
          for (const dt of Ne.logits) {
            const ct = await (0, M.topk)(new M.Tensor(
              "float32",
              (0, g.softmax)(dt.data),
              dt.dims
            ), Te), lt = ct[0].tolist(), L = ct[1].tolist().map((oe, H) => ({
              label: (
                /** @type {string} */
                Re ? Re[oe] : `LABEL_${oe}`
              ),
              score: (
                /** @type {number} */
                lt[H]
              )
            }));
            st.push(L);
          }
          return Array.isArray(ae) ? st : st[0];
        }
      }
      class Fe extends /** @type {new (options: ImagePipelineConstructorArgs) => ImageSegmentationPipelineType} */
      se {
        /**
         * Create a new ImageSegmentationPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae), this.subtasks_mapping = {
            // Mapping of subtasks to their corresponding post-processing function names.
            panoptic: "post_process_panoptic_segmentation",
            instance: "post_process_instance_segmentation",
            semantic: "post_process_semantic_segmentation"
          };
        }
        /** @type {ImageSegmentationPipelineCallback} */
        async _call(ae, {
          threshold: Te = 0.5,
          mask_threshold: Ue = 0.5,
          overlap_mask_area_threshold: Ve = 0.8,
          label_ids_to_fuse: Ne = null,
          target_sizes: Re = null,
          subtask: st = null
        } = {}) {
          if (Array.isArray(ae) && ae.length !== 1)
            throw Error("Image segmentation pipeline currently only supports a batch size of 1.");
          const ct = await b(ae), lt = ct.map((We) => [We.height, We.width]), { pixel_values: ht, pixel_mask: L } = await this.processor(ct), oe = await this.model({ pixel_values: ht, pixel_mask: L });
          let H = null;
          if (st !== null)
            H = this.subtasks_mapping[st];
          else
            for (let [We, Je] of Object.entries(this.subtasks_mapping))
              if (Je in this.processor.image_processor) {
                H = this.processor.image_processor[Je].bind(this.processor.image_processor), st = We;
                break;
              }
          const me = this.model.config.id2label, Ae = [];
          if (st === "panoptic" || st === "instance") {
            const We = H(
              oe,
              Te,
              Ue,
              Ve,
              Ne,
              Re ?? lt
              // TODO FIX?
            )[0], Je = We.segmentation;
            for (const ut of We.segments_info) {
              const mt = new Uint8ClampedArray(Je.data.length);
              for (let kt = 0; kt < Je.data.length; ++kt)
                Je.data[kt] === ut.id && (mt[kt] = 255);
              const vt = new y.RawImage(mt, Je.dims[1], Je.dims[0], 1);
              Ae.push({
                score: ut.score,
                label: me[ut.label_id],
                mask: vt
              });
            }
          } else if (st === "semantic") {
            const { segmentation: We, labels: Je } = H(oe, Re ?? lt)[0];
            for (const ut of Je) {
              const mt = new Uint8ClampedArray(We.data.length);
              for (let kt = 0; kt < We.data.length; ++kt)
                We.data[kt] === ut && (mt[kt] = 255);
              const vt = new y.RawImage(mt, We.dims[1], We.dims[0], 1);
              Ae.push({
                score: null,
                label: me[ut],
                mask: vt
              });
            }
          } else
            throw Error(`Subtask ${st} not supported.`);
          return Ae;
        }
      }
      class Ee extends /** @type {new (options: TextImagePipelineConstructorArgs) => ZeroShotImageClassificationPipelineType} */
      se {
        /**
         * Create a new ZeroShotImageClassificationPipeline.
         * @param {TextImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {ZeroShotImageClassificationPipelineCallback} */
        async _call(ae, Te, {
          hypothesis_template: Ue = "This is a photo of {}"
        } = {}) {
          const Ve = Array.isArray(ae), Ne = await b(ae), Re = Te.map(
            (L) => Ue.replace("{}", L)
          ), st = this.tokenizer(Re, {
            padding: this.model.config.model_type === "siglip" ? "max_length" : !0,
            truncation: !0
          }), { pixel_values: dt } = await this.processor(Ne), ct = await this.model({ ...st, pixel_values: dt }), lt = this.model.config.model_type === "siglip" ? (L) => L.sigmoid().data : (L) => (0, g.softmax)(L.data), ht = [];
          for (const L of ct.logits_per_image) {
            const H = [...lt(L)].map((me, Ae) => ({
              score: me,
              label: Te[Ae]
            }));
            H.sort((me, Ae) => Ae.score - me.score), ht.push(H);
          }
          return Ve ? ht : ht[0];
        }
      }
      class tt extends /** @type {new (options: ImagePipelineConstructorArgs) => ObjectDetectionPipelineType} */
      se {
        /**
         * Create a new ObjectDetectionPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {ObjectDetectionPipelineCallback} */
        async _call(ae, {
          threshold: Te = 0.9,
          percentage: Ue = !1
        } = {}) {
          const Ve = Array.isArray(ae);
          if (Ve && ae.length !== 1)
            throw Error("Object detection pipeline currently only supports a batch size of 1.");
          const Ne = await b(ae), Re = Ue ? null : Ne.map((oe) => [oe.height, oe.width]), { pixel_values: st, pixel_mask: dt } = await this.processor(Ne), ct = await this.model({ pixel_values: st, pixel_mask: dt }), lt = this.processor.image_processor.post_process_object_detection(ct, Te, Re), ht = this.model.config.id2label, L = lt.map((oe) => oe.boxes.map((H, me) => ({
            score: oe.scores[me],
            label: ht[oe.classes[me]],
            box: K(H, !Ue)
          })));
          return Ve ? L : L[0];
        }
      }
      class Ge extends /** @type {new (options: TextImagePipelineConstructorArgs) => ZeroShotObjectDetectionPipelineType} */
      se {
        /**
         * Create a new ZeroShotObjectDetectionPipeline.
         * @param {TextImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {ZeroShotObjectDetectionPipelineCallback} */
        async _call(ae, Te, {
          threshold: Ue = 0.1,
          top_k: Ve = null,
          percentage: Ne = !1
        } = {}) {
          const Re = Array.isArray(ae), st = await b(ae), dt = this.tokenizer(Te, {
            padding: !0,
            truncation: !0
          }), ct = await this.processor(st), lt = [];
          for (let ht = 0; ht < st.length; ++ht) {
            const L = st[ht], oe = Ne ? null : [[L.height, L.width]], H = ct.pixel_values[ht].unsqueeze_(0), me = await this.model({ ...dt, pixel_values: H });
            let Ae;
            if ("post_process_grounded_object_detection" in this.processor) {
              const We = this.processor.post_process_grounded_object_detection(
                me,
                dt.input_ids,
                {
                  // TODO: support separate threshold values
                  box_threshold: Ue,
                  text_threshold: Ue,
                  target_sizes: oe
                }
              )[0];
              Ae = We.boxes.map((Je, ut) => ({
                score: We.scores[ut],
                label: We.labels[ut],
                box: K(Je, !Ne)
              }));
            } else {
              const We = this.processor.image_processor.post_process_object_detection(me, Ue, oe, !0)[0];
              Ae = We.boxes.map((Je, ut) => ({
                score: We.scores[ut],
                label: Te[We.classes[ut]],
                box: K(Je, !Ne)
              }));
            }
            Ae.sort((We, Je) => Je.score - We.score), Ve !== null && (Ae = Ae.slice(0, Ve)), lt.push(Ae);
          }
          return Re ? lt : lt[0];
        }
      }
      class ye extends /** @type {new (options: TextImagePipelineConstructorArgs) => DocumentQuestionAnsweringPipelineType} */
      se {
        /**
         * Create a new DocumentQuestionAnsweringPipeline.
         * @param {TextImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {DocumentQuestionAnsweringPipelineCallback} */
        async _call(ae, Te, Ue = {}) {
          const Ve = (await b(ae))[0], { pixel_values: Ne } = await this.processor(Ve), Re = `<s_docvqa><s_question>${Te}</s_question><s_answer>`, st = this.tokenizer(Re, {
            add_special_tokens: !1,
            padding: !0,
            truncation: !0
          }).input_ids, dt = await this.model.generate({
            inputs: Ne,
            // @ts-expect-error TS2339
            max_length: this.model.config.decoder.max_position_embeddings,
            decoder_input_ids: st,
            ...Ue
          }), lt = this.tokenizer.batch_decode(
            /** @type {Tensor} */
            dt
          )[0].match(/<s_answer>(.*?)<\/s_answer>/);
          let ht = null;
          return lt && lt.length >= 2 && (ht = lt[1].trim()), [{ answer: ht }];
        }
      }
      class J extends /** @type {new (options: TextToAudioPipelineConstructorArgs) => TextToAudioPipelineType} */
      se {
        /**
         * Create a new TextToAudioPipeline.
         * @param {TextToAudioPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(Te) {
          super(Te);
          fe(this, "DEFAULT_VOCODER_ID", "Xenova/speecht5_hifigan");
          this.vocoder = Te.vocoder ?? null;
        }
        /** @type {TextToAudioPipelineCallback} */
        async _call(Te, {
          speaker_embeddings: Ue = null
        } = {}) {
          return this.processor ? this._call_text_to_spectrogram(Te, { speaker_embeddings: Ue }) : this._call_text_to_waveform(Te);
        }
        async _call_text_to_waveform(Te) {
          const Ue = this.tokenizer(Te, {
            padding: !0,
            truncation: !0
          }), { waveform: Ve } = await this.model(Ue), Ne = this.model.config.sampling_rate;
          return new v.RawAudio(
            Ve.data,
            Ne
          );
        }
        async _call_text_to_spectrogram(Te, { speaker_embeddings: Ue }) {
          if (this.vocoder || (console.log("No vocoder specified, using default HifiGan vocoder."), this.vocoder = await D.AutoModel.from_pretrained(this.DEFAULT_VOCODER_ID, { dtype: "fp32" })), (typeof Ue == "string" || Ue instanceof URL) && (Ue = new Float32Array(
            await (await fetch(Ue)).arrayBuffer()
          )), Ue instanceof Float32Array)
            Ue = new M.Tensor(
              "float32",
              Ue,
              [1, Ue.length]
            );
          else if (!(Ue instanceof M.Tensor))
            throw new Error("Speaker embeddings must be a `Tensor`, `Float32Array`, `string`, or `URL`.");
          const { input_ids: Ve } = this.tokenizer(Te, {
            padding: !0,
            truncation: !0
          }), { waveform: Ne } = await this.model.generate_speech(Ve, Ue, { vocoder: this.vocoder }), Re = this.processor.feature_extractor.config.sampling_rate;
          return new v.RawAudio(
            Ne.data,
            Re
          );
        }
      }
      class de extends /** @type {new (options: ImagePipelineConstructorArgs) => ImageToImagePipelineType} */
      se {
        /**
         * Create a new ImageToImagePipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {ImageToImagePipelineCallback} */
        async _call(ae) {
          const Te = await b(ae), Ue = await this.processor(Te), Ve = await this.model(Ue), Ne = [];
          for (const Re of Ve.reconstruction) {
            const st = Re.squeeze().clamp_(0, 1).mul_(255).round_().to("uint8");
            Ne.push(y.RawImage.fromTensor(st));
          }
          return Ne.length > 1 ? Ne : Ne[0];
        }
      }
      class Ce extends /** @type {new (options: ImagePipelineConstructorArgs) => DepthEstimationPipelineType} */
      se {
        /**
         * Create a new DepthEstimationPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ae) {
          super(ae);
        }
        /** @type {DepthEstimationPipelineCallback} */
        async _call(ae) {
          const Te = await b(ae), Ue = await this.processor(Te), { predicted_depth: Ve } = await this.model(Ue), Ne = [];
          for (let Re = 0; Re < Te.length; ++Re) {
            const st = Ve[Re], [dt, ct] = st.dims.slice(-2), [lt, ht] = Te[Re].size, L = (await (0, M.interpolate_4d)(st.view(1, 1, dt, ct), {
              size: [ht, lt],
              mode: "bilinear"
            })).view(ht, lt), oe = (
              /** @type {number} */
              L.min().item()
            ), H = (
              /** @type {number} */
              L.max().item()
            ), me = L.sub(oe).div_(H - oe).mul_(255).to("uint8").unsqueeze(0), Ae = y.RawImage.fromTensor(me);
            Ne.push({
              predicted_depth: L,
              depth: Ae
            });
          }
          return Ne.length > 1 ? Ne : Ne[0];
        }
      }
      const Be = Object.freeze({
        "text-classification": {
          tokenizer: _.AutoTokenizer,
          pipeline: ie,
          model: D.AutoModelForSequenceClassification,
          default: {
            // TODO: replace with original
            // "model": "distilbert-base-uncased-finetuned-sst-2-english",
            model: "Xenova/distilbert-base-uncased-finetuned-sst-2-english"
          },
          type: "text"
        },
        "token-classification": {
          tokenizer: _.AutoTokenizer,
          pipeline: W,
          model: D.AutoModelForTokenClassification,
          default: {
            // TODO: replace with original
            // "model": "Davlan/bert-base-multilingual-cased-ner-hrl",
            model: "Xenova/bert-base-multilingual-cased-ner-hrl"
          },
          type: "text"
        },
        "question-answering": {
          tokenizer: _.AutoTokenizer,
          pipeline: j,
          model: D.AutoModelForQuestionAnswering,
          default: {
            // TODO: replace with original
            // "model": "distilbert-base-cased-distilled-squad",
            model: "Xenova/distilbert-base-cased-distilled-squad"
          },
          type: "text"
        },
        "fill-mask": {
          tokenizer: _.AutoTokenizer,
          pipeline: q,
          model: D.AutoModelForMaskedLM,
          default: {
            // TODO: replace with original
            // "model": "bert-base-uncased",
            model: "Xenova/bert-base-uncased"
          },
          type: "text"
        },
        summarization: {
          tokenizer: _.AutoTokenizer,
          pipeline: S,
          model: D.AutoModelForSeq2SeqLM,
          default: {
            // TODO: replace with original
            // "model": "sshleifer/distilbart-cnn-6-6",
            model: "Xenova/distilbart-cnn-6-6"
          },
          type: "text"
        },
        translation: {
          tokenizer: _.AutoTokenizer,
          pipeline: w,
          model: D.AutoModelForSeq2SeqLM,
          default: {
            // TODO: replace with original
            // "model": "t5-small",
            model: "Xenova/t5-small"
          },
          type: "text"
        },
        "text2text-generation": {
          tokenizer: _.AutoTokenizer,
          pipeline: A,
          model: D.AutoModelForSeq2SeqLM,
          default: {
            // TODO: replace with original
            // "model": "google/flan-t5-small",
            model: "Xenova/flan-t5-small"
          },
          type: "text"
        },
        "text-generation": {
          tokenizer: _.AutoTokenizer,
          pipeline: F,
          model: D.AutoModelForCausalLM,
          default: {
            // TODO: replace with original
            // "model": "gpt2",
            model: "Xenova/gpt2"
          },
          type: "text"
        },
        "zero-shot-classification": {
          tokenizer: _.AutoTokenizer,
          pipeline: le,
          model: D.AutoModelForSequenceClassification,
          default: {
            // TODO: replace with original
            // "model": "typeform/distilbert-base-uncased-mnli",
            model: "Xenova/distilbert-base-uncased-mnli"
          },
          type: "text"
        },
        "audio-classification": {
          pipeline: _e,
          model: D.AutoModelForAudioClassification,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "superb/wav2vec2-base-superb-ks",
            model: "Xenova/wav2vec2-base-superb-ks"
          },
          type: "audio"
        },
        "zero-shot-audio-classification": {
          tokenizer: _.AutoTokenizer,
          pipeline: re,
          model: D.AutoModel,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "laion/clap-htsat-fused",
            model: "Xenova/clap-htsat-unfused"
          },
          type: "multimodal"
        },
        "automatic-speech-recognition": {
          tokenizer: _.AutoTokenizer,
          pipeline: xe,
          model: [D.AutoModelForSpeechSeq2Seq, D.AutoModelForCTC],
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "openai/whisper-tiny.en",
            model: "Xenova/whisper-tiny.en"
          },
          type: "multimodal"
        },
        "text-to-audio": {
          tokenizer: _.AutoTokenizer,
          pipeline: J,
          model: [D.AutoModelForTextToWaveform, D.AutoModelForTextToSpectrogram],
          processor: [
            U.AutoProcessor,
            /* Some don't use a processor */
            null
          ],
          default: {
            // TODO: replace with original
            // "model": "microsoft/speecht5_tts",
            model: "Xenova/speecht5_tts"
          },
          type: "text"
        },
        "image-to-text": {
          tokenizer: _.AutoTokenizer,
          pipeline: ce,
          model: D.AutoModelForVision2Seq,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "nlpconnect/vit-gpt2-image-captioning",
            model: "Xenova/vit-gpt2-image-captioning"
          },
          type: "multimodal"
        },
        "image-classification": {
          // no tokenizer
          pipeline: ke,
          model: D.AutoModelForImageClassification,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "google/vit-base-patch16-224",
            model: "Xenova/vit-base-patch16-224"
          },
          type: "multimodal"
        },
        "image-segmentation": {
          // no tokenizer
          pipeline: Fe,
          model: [D.AutoModelForImageSegmentation, D.AutoModelForSemanticSegmentation, D.AutoModelForUniversalSegmentation],
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "facebook/detr-resnet-50-panoptic",
            model: "Xenova/detr-resnet-50-panoptic"
          },
          type: "multimodal"
        },
        "zero-shot-image-classification": {
          tokenizer: _.AutoTokenizer,
          pipeline: Ee,
          model: D.AutoModel,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "openai/clip-vit-base-patch32",
            model: "Xenova/clip-vit-base-patch32"
          },
          type: "multimodal"
        },
        "object-detection": {
          // no tokenizer
          pipeline: tt,
          model: D.AutoModelForObjectDetection,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "facebook/detr-resnet-50",
            model: "Xenova/detr-resnet-50"
          },
          type: "multimodal"
        },
        "zero-shot-object-detection": {
          tokenizer: _.AutoTokenizer,
          pipeline: Ge,
          model: D.AutoModelForZeroShotObjectDetection,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "google/owlvit-base-patch32",
            model: "Xenova/owlvit-base-patch32"
          },
          type: "multimodal"
        },
        "document-question-answering": {
          tokenizer: _.AutoTokenizer,
          pipeline: ye,
          model: D.AutoModelForDocumentQuestionAnswering,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "naver-clova-ix/donut-base-finetuned-docvqa",
            model: "Xenova/donut-base-finetuned-docvqa"
          },
          type: "multimodal"
        },
        "image-to-image": {
          // no tokenizer
          pipeline: de,
          model: D.AutoModelForImageToImage,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "caidas/swin2SR-classical-sr-x2-64",
            model: "Xenova/swin2SR-classical-sr-x2-64"
          },
          type: "image"
        },
        "depth-estimation": {
          // no tokenizer
          pipeline: Ce,
          model: D.AutoModelForDepthEstimation,
          processor: U.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "Intel/dpt-large",
            model: "Xenova/dpt-large"
          },
          type: "image"
        },
        // This task serves as a useful interface for dealing with sentence-transformers (https://huggingface.co/sentence-transformers).
        "feature-extraction": {
          tokenizer: _.AutoTokenizer,
          pipeline: ne,
          model: D.AutoModel,
          default: {
            // TODO: replace with original
            // "model": "sentence-transformers/all-MiniLM-L6-v2",
            model: "Xenova/all-MiniLM-L6-v2"
          },
          type: "text"
        },
        "image-feature-extraction": {
          processor: U.AutoProcessor,
          pipeline: be,
          model: [D.AutoModelForImageFeatureExtraction, D.AutoModel],
          default: {
            // TODO: replace with original
            // "model": "google/vit-base-patch16-224",
            model: "Xenova/vit-base-patch16-224-in21k"
          },
          type: "image"
        }
      }), Ze = Object.freeze({
        "sentiment-analysis": "text-classification",
        ner: "token-classification",
        // "vqa": "visual-question-answering", // TODO: Add
        asr: "automatic-speech-recognition",
        "text-to-speech": "text-to-audio",
        // Add for backwards compatibility
        embeddings: "feature-extraction"
      });
      async function te(je, ae = null, {
        progress_callback: Te = null,
        config: Ue = null,
        cache_dir: Ve = null,
        local_files_only: Ne = !1,
        revision: Re = "main",
        device: st = null,
        dtype: dt = null,
        model_file_name: ct = null,
        session_options: lt = {}
      } = {}) {
        je = Ze[je] ?? je;
        const ht = Be[je.split("_", 1)[0]];
        if (!ht)
          throw Error(`Unsupported pipeline: ${je}. Must be one of [${Object.keys(Be)}]`);
        ae || (ae = ht.default.model, console.log(`No model specified. Using default model: "${ae}".`));
        const L = {
          progress_callback: Te,
          config: Ue,
          cache_dir: Ve,
          local_files_only: Ne,
          revision: Re,
          device: st,
          dtype: dt,
          model_file_name: ct,
          session_options: lt
        }, oe = /* @__PURE__ */ new Map([
          ["tokenizer", ht.tokenizer],
          ["model", ht.model],
          ["processor", ht.processor]
        ]), H = await Ke(oe, ae, L);
        H.task = je, (0, R.dispatchCallback)(Te, {
          status: "ready",
          task: je,
          model: ae
        });
        const me = ht.pipeline;
        return new me(H);
      }
      async function Ke(je, ae, Te) {
        const Ue = /* @__PURE__ */ Object.create(null), Ve = [];
        for (const [Ne, Re] of je.entries()) {
          if (!Re) continue;
          let st;
          Array.isArray(Re) ? st = new Promise(async (dt, ct) => {
            var ht, L;
            let lt;
            for (const oe of Re) {
              if (oe === null) {
                dt(null);
                return;
              }
              try {
                dt(await oe.from_pretrained(ae, Te));
                return;
              } catch (H) {
                if ((ht = H.message) != null && ht.includes("Unsupported model type"))
                  lt = H;
                else if ((L = H.message) != null && L.includes("Could not locate file"))
                  lt = H;
                else {
                  ct(H);
                  return;
                }
              }
            }
            ct(lt);
          }) : st = Re.from_pretrained(ae, Te), Ue[Ne] = st, Ve.push(st);
        }
        await Promise.all(Ve);
        for (const [Ne, Re] of Object.entries(Ue))
          Ue[Ne] = await Re;
        return Ue;
      }
    }
  ),
  /***/
  "./src/tokenizers.js": (
    /*!***************************!*\
      !*** ./src/tokenizers.js ***!
      \***************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        AlbertTokenizer: () => (
          /* binding */
          Sr
        ),
        /* harmony export */
        AutoTokenizer: () => (
          /* binding */
          as
        ),
        /* harmony export */
        BartTokenizer: () => (
          /* binding */
          Ir
        ),
        /* harmony export */
        BertTokenizer: () => (
          /* binding */
          Xr
        ),
        /* harmony export */
        BlenderbotSmallTokenizer: () => (
          /* binding */
          On
        ),
        /* harmony export */
        BlenderbotTokenizer: () => (
          /* binding */
          Fn
        ),
        /* harmony export */
        BloomTokenizer: () => (
          /* binding */
          Pr
        ),
        /* harmony export */
        CLIPTokenizer: () => (
          /* binding */
          yn
        ),
        /* harmony export */
        CamembertTokenizer: () => (
          /* binding */
          it
        ),
        /* harmony export */
        CodeGenTokenizer: () => (
          /* binding */
          wn
        ),
        /* harmony export */
        CodeLlamaTokenizer: () => (
          /* binding */
          jr
        ),
        /* harmony export */
        CohereTokenizer: () => (
          /* binding */
          vn
        ),
        /* harmony export */
        ConvBertTokenizer: () => (
          /* binding */
          Br
        ),
        /* harmony export */
        DebertaTokenizer: () => (
          /* binding */
          dr
        ),
        /* harmony export */
        DebertaV2Tokenizer: () => (
          /* binding */
          Jr
        ),
        /* harmony export */
        DistilBertTokenizer: () => (
          /* binding */
          ar
        ),
        /* harmony export */
        ElectraTokenizer: () => (
          /* binding */
          Dt
        ),
        /* harmony export */
        EsmTokenizer: () => (
          /* binding */
          Ur
        ),
        /* harmony export */
        FalconTokenizer: () => (
          /* binding */
          $n
        ),
        /* harmony export */
        GPT2Tokenizer: () => (
          /* binding */
          Nr
        ),
        /* harmony export */
        GPTNeoXTokenizer: () => (
          /* binding */
          An
        ),
        /* harmony export */
        GemmaTokenizer: () => (
          /* binding */
          ri
        ),
        /* harmony export */
        Grok1Tokenizer: () => (
          /* binding */
          Vr
        ),
        /* harmony export */
        HerbertTokenizer: () => (
          /* binding */
          Ar
        ),
        /* harmony export */
        LlamaTokenizer: () => (
          /* binding */
          _n
        ),
        /* harmony export */
        M2M100Tokenizer: () => (
          /* binding */
          gn
        ),
        /* harmony export */
        MBart50Tokenizer: () => (
          /* binding */
          lr
        ),
        /* harmony export */
        MBartTokenizer: () => (
          /* binding */
          Ms
        ),
        /* harmony export */
        MPNetTokenizer: () => (
          /* binding */
          Sn
        ),
        /* harmony export */
        MarianTokenizer: () => (
          /* binding */
          zt
        ),
        /* harmony export */
        MgpstrTokenizer: () => (
          /* binding */
          zn
        ),
        /* harmony export */
        MobileBertTokenizer: () => (
          /* binding */
          $r
        ),
        /* harmony export */
        NllbTokenizer: () => (
          /* binding */
          ur
        ),
        /* harmony export */
        NougatTokenizer: () => (
          /* binding */
          Wr
        ),
        /* harmony export */
        PreTrainedTokenizer: () => (
          /* binding */
          Nt
        ),
        /* harmony export */
        Qwen2Tokenizer: () => (
          /* binding */
          In
        ),
        /* harmony export */
        RoFormerTokenizer: () => (
          /* binding */
          Rr
        ),
        /* harmony export */
        RobertaTokenizer: () => (
          /* binding */
          Fs
        ),
        /* harmony export */
        SiglipTokenizer: () => (
          /* binding */
          Mn
        ),
        /* harmony export */
        SpeechT5Tokenizer: () => (
          /* binding */
          Dn
        ),
        /* harmony export */
        SqueezeBertTokenizer: () => (
          /* binding */
          Yr
        ),
        /* harmony export */
        T5Tokenizer: () => (
          /* binding */
          Vs
        ),
        /* harmony export */
        TokenizerModel: () => (
          /* binding */
          be
        ),
        /* harmony export */
        VitsTokenizer: () => (
          /* binding */
          Ln
        ),
        /* harmony export */
        Wav2Vec2CTCTokenizer: () => (
          /* binding */
          bn
        ),
        /* harmony export */
        WhisperTokenizer: () => (
          /* binding */
          Zr
        ),
        /* harmony export */
        XLMRobertaTokenizer: () => (
          /* binding */
          si
        ),
        /* harmony export */
        XLMTokenizer: () => (
          /* binding */
          Tt
        ),
        /* harmony export */
        is_chinese_char: () => (
          /* binding */
          q
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./utils/generic.js */
        "./src/utils/generic.js"
      ), D = r(
        /*! ./utils/core.js */
        "./src/utils/core.js"
      ), U = r(
        /*! ./utils/hub.js */
        "./src/utils/hub.js"
      ), Y = r(
        /*! ./utils/maths.js */
        "./src/utils/maths.js"
      ), R = r(
        /*! ./utils/tensor.js */
        "./src/utils/tensor.js"
      ), g = r(
        /*! ./utils/data-structures.js */
        "./src/utils/data-structures.js"
      ), v = r(
        /*! @huggingface/jinja */
        "./node_modules/@huggingface/jinja/dist/index.js"
      ), M = r(
        /*! ./models/whisper/common_whisper.js */
        "./src/models/whisper/common_whisper.js"
      );
      async function y(Pe, P) {
        const Q = await Promise.all([
          (0, U.getModelJSON)(Pe, "tokenizer.json", !0, P),
          (0, U.getModelJSON)(Pe, "tokenizer_config.json", !0, P)
        ]);
        return P.legacy !== null && (Q[1].legacy = P.legacy), Q;
      }
      function b(Pe, P) {
        const Q = [];
        let ue = 0;
        for (const ve of Pe.matchAll(P)) {
          const Se = ve[0];
          ue < ve.index && Q.push(Pe.slice(ue, ve.index)), Se.length > 0 && Q.push(Se), ue = ve.index + Se.length;
        }
        return ue < Pe.length && Q.push(Pe.slice(ue)), Q;
      }
      function I(Pe, P = !0) {
        if (Pe.Regex !== void 0) {
          let Q = Pe.Regex.replace(/\\([#&~])/g, "$1");
          for (const [ue, ve] of le)
            Q = Q.replaceAll(ue, ve);
          return new RegExp(Q, "gu");
        } else if (Pe.String !== void 0) {
          const Q = (0, D.escapeRegExp)(Pe.String);
          return new RegExp(P ? Q : `(${Q})`, "gu");
        } else
          return console.warn("Unknown pattern type:", Pe), null;
      }
      function K(Pe) {
        return new Map(Object.entries(Pe));
      }
      function se(Pe) {
        const P = Pe.dims;
        switch (P.length) {
          case 1:
            return Pe.tolist();
          case 2:
            if (P[0] !== 1)
              throw new Error("Unable to decode tensor with `batch size !== 1`. Use `tokenizer.batch_decode(...)` for batched inputs.");
            return Pe.tolist()[0];
          default:
            throw new Error(`Expected tensor to have 1-2 dimensions, got ${P.length}.`);
        }
      }
      function ie(Pe) {
        return Pe.replace(/ \./g, ".").replace(/ \?/g, "?").replace(/ \!/g, "!").replace(/ ,/g, ",").replace(/ \' /g, "'").replace(/ n\'t/g, "n't").replace(/ \'m/g, "'m").replace(/ \'s/g, "'s").replace(/ \'ve/g, "'ve").replace(/ \'re/g, "'re");
      }
      function W(Pe) {
        return Pe.replace(new RegExp("\\p{M}", "gu"), "");
      }
      function j(Pe) {
        return W(Pe.toLowerCase());
      }
      function q(Pe) {
        return Pe >= 19968 && Pe <= 40959 || Pe >= 13312 && Pe <= 19903 || Pe >= 131072 && Pe <= 173791 || Pe >= 173824 && Pe <= 177983 || Pe >= 177984 && Pe <= 178207 || Pe >= 178208 && Pe <= 183983 || Pe >= 63744 && Pe <= 64255 || Pe >= 194560 && Pe <= 195103;
      }
      function A(Pe, P, Q) {
        const ue = [];
        let ve = 0;
        for (; ve < Pe.length; ) {
          if (ue.push(Pe[ve]), (P.get(Pe[ve]) ?? Q) !== Q) {
            ++ve;
            continue;
          }
          for (; ++ve < Pe.length && (P.get(Pe[ve]) ?? Q) === Q; )
            P.get(ue.at(-1)) !== Q && (ue[ue.length - 1] += Pe[ve]);
        }
        return ue;
      }
      function S(Pe) {
        return Pe.match(/\S+/g) || [];
      }
      const w = "\\p{P}\\u0021-\\u002F\\u003A-\\u0040\\u005B-\\u0060\\u007B-\\u007E", x = new RegExp(`^[${w}]+$`, "gu"), F = ".,!?…。，、।۔،", le = /* @__PURE__ */ new Map([
        // This uses the case insensitive group modifier, which is not supported in JavaScript.
        // When parsing the regex, an "Invalid group" error is thrown.
        ["(?i:'s|'t|'re|'ve|'m|'ll|'d)", "(?:'([sS]|[tT]|[rR][eE]|[vV][eE]|[mM]|[lL][lL]|[dD]))"],
        // Used to override the default (invalid) regex of the bloom pretokenizer.
        // For more information, see https://github.com/huggingface/transformers.js/issues/94
        [` ?[^(\\s|[${F}])]+`, ` ?[^\\s${F}]+`]
      ]);
      class ne {
        /**
         * Creates a new instance of AddedToken.
         * @param {Object} config Added token configuration object.
         * @param {string} config.content The content of the added token.
         * @param {number} config.id The id of the added token.
         * @param {boolean} [config.single_word=false] Whether this token must be a single word or can break words.
         * @param {boolean} [config.lstrip=false] Whether this token should strip whitespaces on its left.
         * @param {boolean} [config.rstrip=false] Whether this token should strip whitespaces on its right.
         * @param {boolean} [config.normalized=false] Whether this token should be normalized.
         * @param {boolean} [config.special=false] Whether this token is special.
         */
        constructor(P) {
          this.content = P.content, this.id = P.id, this.single_word = P.single_word ?? !1, this.lstrip = P.lstrip ?? !1, this.rstrip = P.rstrip ?? !1, this.special = P.special ?? !1, this.normalized = P.normalized ?? null;
        }
      }
      class be extends _.Callable {
        /**
         * Creates a new instance of TokenizerModel.
         * @param {Object} config The configuration object for the TokenizerModel.
         */
        constructor(P) {
          super(), this.config = P, this.vocab = [], this.tokens_to_ids = /* @__PURE__ */ new Map(), this.unk_token_id = void 0, this.unk_token = void 0, this.end_of_word_suffix = void 0, this.fuse_unk = this.config.fuse_unk ?? !1;
        }
        /**
         * Instantiates a new TokenizerModel instance based on the configuration object provided.
         * @param {Object} config The configuration object for the TokenizerModel.
         * @param {...*} args Optional arguments to pass to the specific TokenizerModel constructor.
         * @returns {TokenizerModel} A new instance of a TokenizerModel.
         * @throws Will throw an error if the TokenizerModel type in the config is not recognized.
         */
        static fromConfig(P, ...Q) {
          switch (P.type) {
            case "WordPiece":
              return new _e(P);
            case "Unigram":
              return new re(P, ...Q);
            case "BPE":
              return new ke(P);
            default:
              if (P.vocab)
                return Array.isArray(P.vocab) ? new re(P, ...Q) : typeof P.vocab == "object" && P.continuing_subword_prefix && P.unk_token ? new _e(P) : new Fe(P, ...Q);
              throw new Error(`Unknown TokenizerModel type: ${P.type}`);
          }
        }
        /**
         * Internal function to call the TokenizerModel instance.
         * @param {string[]} tokens The tokens to encode.
         * @returns {string[]} The encoded tokens.
         */
        _call(P) {
          return P = this.encode(P), this.fuse_unk && (P = A(P, this.tokens_to_ids, this.unk_token_id)), P;
        }
        /**
         * Encodes a list of tokens into a list of token IDs.
         * @param {string[]} tokens The tokens to encode.
         * @returns {string[]} The encoded tokens.
         * @throws Will throw an error if not implemented in a subclass.
         */
        encode(P) {
          throw Error("encode should be implemented in subclass.");
        }
        /**
         * Converts a list of tokens into a list of token IDs.
         * @param {string[]} tokens The tokens to convert.
         * @returns {number[]} The converted token IDs.
         */
        convert_tokens_to_ids(P) {
          return P.map((Q) => this.tokens_to_ids.get(Q) ?? this.unk_token_id);
        }
        /**
         * Converts a list of token IDs into a list of tokens.
         * @param {number[]|bigint[]} ids The token IDs to convert.
         * @returns {string[]} The converted tokens.
         */
        convert_ids_to_tokens(P) {
          return P.map((Q) => this.vocab[Q] ?? this.unk_token);
        }
      }
      class _e extends be {
        /**
         * @param {Object} config The configuration object.
         * @param {Object} config.vocab A mapping of tokens to ids.
         * @param {string} config.unk_token The unknown token string.
         * @param {string} config.continuing_subword_prefix The prefix to use for continuing subwords.
         * @param {number} [config.max_input_chars_per_word=100] The maximum number of characters per word.
         */
        constructor(P) {
          super(P), this.tokens_to_ids = K(P.vocab), this.unk_token_id = this.tokens_to_ids.get(P.unk_token), this.unk_token = P.unk_token, this.max_input_chars_per_word = P.max_input_chars_per_word ?? 100, this.vocab = new Array(this.tokens_to_ids.size);
          for (const [Q, ue] of this.tokens_to_ids)
            this.vocab[ue] = Q;
        }
        /**
         * Encodes an array of tokens using WordPiece encoding.
         * @param {string[]} tokens The tokens to encode.
         * @returns {string[]} An array of encoded tokens.
         */
        encode(P) {
          const Q = [];
          for (const ue of P) {
            const ve = [...ue];
            if (ve.length > this.max_input_chars_per_word) {
              Q.push(this.unk_token);
              continue;
            }
            let Se = !1, Qe = 0;
            const pt = [];
            for (; Qe < ve.length; ) {
              let gt = ve.length, ft = null;
              for (; Qe < gt; ) {
                let xt = ve.slice(Qe, gt).join("");
                if (Qe > 0 && (xt = this.config.continuing_subword_prefix + xt), this.tokens_to_ids.has(xt)) {
                  ft = xt;
                  break;
                }
                --gt;
              }
              if (ft === null) {
                Se = !0;
                break;
              }
              pt.push(ft), Qe = gt;
            }
            Se ? Q.push(this.unk_token) : Q.push(...pt);
          }
          return Q;
        }
      }
      class re extends be {
        /**
         * Create a new Unigram tokenizer model.
         * @param {Object} config The configuration object for the Unigram model.
         * @param {number} config.unk_id The ID of the unknown token
         * @param {[string, number][]} config.vocab A 2D array representing a mapping of tokens to scores.
         * @param {Object} moreConfig Additional configuration object for the Unigram model.
         */
        constructor(P, Q) {
          super(P);
          const ue = P.vocab.length;
          this.vocab = new Array(ue), this.scores = new Array(ue);
          for (let ve = 0; ve < ue; ++ve)
            [this.vocab[ve], this.scores[ve]] = P.vocab[ve];
          this.unk_token_id = P.unk_id, this.unk_token = this.vocab[P.unk_id], this.tokens_to_ids = new Map(this.vocab.map((ve, Se) => [ve, Se])), this.bos_token = " ", this.bos_token_id = this.tokens_to_ids.get(this.bos_token), this.eos_token = Q.eos_token, this.eos_token_id = this.tokens_to_ids.get(this.eos_token), this.unk_token = this.vocab[this.unk_token_id], this.minScore = (0, Y.min)(this.scores)[0], this.unk_score = this.minScore - 10, this.scores[this.unk_token_id] = this.unk_score, this.trie = new g.CharTrie(), this.trie.extend(this.vocab), this.fuse_unk = !0;
        }
        /**
         * Populates lattice nodes.
         * @param {TokenLattice} lattice The token lattice to populate with nodes.
         */
        populateNodes(P) {
          const Q = P.chars, ue = 1;
          let ve = 0;
          for (; ve < Q.length; ) {
            let Se = !1;
            const Qe = Q.slice(ve).join(""), pt = this.trie.commonPrefixSearch(Qe);
            for (const gt of pt) {
              const ft = this.tokens_to_ids.get(gt), xt = this.scores[ft], Kt = (0, D.len)(gt);
              P.insert(ve, Kt, xt, ft), !Se && Kt === ue && (Se = !0);
            }
            Se || P.insert(ve, ue, this.unk_score, this.unk_token_id), ve += ue;
          }
        }
        /**
         * Encodes an array of tokens into an array of subtokens using the unigram model.
         *
         * @param {string} normalized The normalized string.
         * @returns {string[]} An array of subtokens obtained by encoding the input tokens using the unigram model.
         */
        tokenize(P) {
          const Q = new g.TokenLattice(P, this.bos_token_id, this.eos_token_id);
          return this.populateNodes(Q), Q.tokens();
        }
        /**
         * Encodes an array of tokens using Unigram encoding.
         * @param {string[]} tokens The tokens to encode.
         * @returns {string[]} An array of encoded tokens.
         */
        encode(P) {
          const Q = [];
          for (const ue of P) {
            const ve = this.tokenize(ue);
            Q.push(...ve);
          }
          return Q;
        }
      }
      const xe = (() => {
        const Pe = [
          ...Array.from({ length: 94 }, (ve, Se) => Se + 33),
          ...Array.from({ length: 12 }, (ve, Se) => Se + 161),
          ...Array.from({ length: 82 }, (ve, Se) => Se + 174)
        ], P = Pe.slice();
        let Q = 0;
        for (let ve = 0; ve < 256; ++ve)
          Pe.includes(ve) || (Pe.push(ve), P.push(256 + Q), Q += 1);
        const ue = P.map((ve) => String.fromCharCode(ve));
        return Object.fromEntries(Pe.map((ve, Se) => [ve, ue[Se]]));
      })(), ce = (0, D.reverseDictionary)(xe);
      class ke extends be {
        /**
         * Create a BPE instance.
         * @param {Object} config The configuration object for BPE.
         * @param {Object} config.vocab A mapping of tokens to ids.
         * @param {string[]|[string, string][]} config.merges An array of BPE merges as strings.
         * @param {string} config.unk_token The unknown token used for out of vocabulary words.
         * @param {string} config.end_of_word_suffix The suffix to place at the end of each word.
         * @param {string} [config.continuing_subword_suffix] The suffix to insert between words.
         * @param {boolean} [config.byte_fallback=false] Whether to use spm byte-fallback trick (defaults to False)
         * @param {boolean} [config.ignore_merges=false] Whether or not to match tokens with the vocab before using merges.
         */
        constructor(P) {
          super(P), this.tokens_to_ids = K(P.vocab), this.unk_token_id = this.tokens_to_ids.get(P.unk_token), this.unk_token = P.unk_token, this.vocab = new Array(this.tokens_to_ids.size);
          for (const [ue, ve] of this.tokens_to_ids)
            this.vocab[ve] = ue;
          const Q = Array.isArray(P.merges[0]);
          this.merges = Q ? (
            /** @type {[string, string][]} */
            P.merges
          ) : (
            /** @type {string[]} */
            P.merges.map((ue) => (
              /** @type {[string, string]} */
              ue.split(" ", 2)
            ))
          ), this.bpe_ranks = new Map(this.merges.map((ue, ve) => [JSON.stringify(ue), ve])), this.end_of_word_suffix = P.end_of_word_suffix, this.continuing_subword_suffix = P.continuing_subword_suffix ?? null, this.byte_fallback = this.config.byte_fallback ?? !1, this.byte_fallback && (this.text_encoder = new TextEncoder()), this.ignore_merges = this.config.ignore_merges ?? !1, this.cache = /* @__PURE__ */ new Map();
        }
        /**
         * Apply Byte-Pair-Encoding (BPE) to a given token. Efficient heap-based priority
         * queue implementation adapted from https://github.com/belladoreai/llama-tokenizer-js.
         * @param {string} token The token to encode.
         * @returns {string[]} The BPE encoded tokens.
         */
        bpe(P) {
          if (P.length === 0)
            return [];
          const Q = this.cache.get(P);
          if (Q !== void 0)
            return Q;
          const ue = Array.from(P);
          this.end_of_word_suffix && (ue[ue.length - 1] += this.end_of_word_suffix);
          let ve = [];
          if (ue.length > 1) {
            const Se = new g.PriorityQueue((gt, ft) => gt.score < ft.score);
            let Qe = {
              token: ue[0],
              bias: 0,
              prev: null,
              next: null
            }, pt = Qe;
            for (let gt = 1; gt < ue.length; ++gt) {
              const ft = {
                bias: gt / ue.length,
                // Add fractional component to break ties
                token: ue[gt],
                prev: pt,
                next: null
              };
              pt.next = ft, this._add_node(Se, pt), pt = ft;
            }
            for (; !Se.isEmpty(); ) {
              const gt = Se.pop();
              if (gt.deleted || !gt.next || gt.next.deleted) continue;
              if (gt.deleted = !0, gt.next.deleted = !0, gt.prev) {
                const xt = { ...gt.prev };
                gt.prev.deleted = !0, gt.prev = xt, xt.prev ? xt.prev.next = xt : Qe = xt;
              }
              const ft = {
                token: gt.token + gt.next.token,
                bias: gt.bias,
                prev: gt.prev,
                next: gt.next.next
              };
              ft.prev ? (ft.prev.next = ft, this._add_node(Se, ft.prev)) : Qe = ft, ft.next && (ft.next.prev = ft, this._add_node(Se, ft));
            }
            for (let gt = Qe; gt !== null; gt = gt.next)
              ve.push(gt.token);
          } else
            ve = ue;
          if (this.continuing_subword_suffix)
            for (let Se = 0; Se < ve.length - 1; ++Se)
              ve[Se] += this.continuing_subword_suffix;
          return this.cache.set(P, ve), ve;
        }
        /**
         * Helper function to add a node to the priority queue.
         * @param {PriorityQueue} queue 
         * @param {BPENode} node
         * @private
         */
        _add_node(P, Q) {
          const ue = this.bpe_ranks.get(JSON.stringify([Q.token, Q.next.token]));
          ue !== void 0 && (Q.score = ue + Q.bias, P.push(Q));
        }
        /**
         * Encodes the input sequence of tokens using the BPE algorithm and returns the resulting subword tokens.
         * @param {string[]} tokens The input sequence of tokens to encode.
         * @returns {string[]} The resulting subword tokens after applying the BPE algorithm to the input sequence of tokens.
         */
        encode(P) {
          const Q = [];
          for (const ue of P) {
            if (this.ignore_merges && this.tokens_to_ids.has(ue)) {
              Q.push(ue);
              continue;
            }
            const ve = this.bpe(ue);
            for (const Se of ve)
              if (this.tokens_to_ids.has(Se))
                Q.push(Se);
              else if (this.byte_fallback) {
                const Qe = Array.from(this.text_encoder.encode(Se)).map((pt) => `<0x${pt.toString(16).toUpperCase().padStart(2, "0")}>`);
                Qe.every((pt) => this.tokens_to_ids.has(pt)) ? Q.push(...Qe) : Q.push(this.unk_token);
              } else
                Q.push(this.unk_token);
          }
          return Q;
        }
      }
      class Fe extends be {
        /**
         * Create a LegacyTokenizerModel instance.
         * @param {Object} config The configuration object for LegacyTokenizerModel.
         * @param {Object} config.vocab A (possibly nested) mapping of tokens to ids.
         * @param {Object} moreConfig Additional configuration object for the LegacyTokenizerModel model.
         */
        constructor(P, Q) {
          super(P), this.tokens_to_ids = K(
            Q.target_lang ? P.vocab[Q.target_lang] : P.vocab
          ), this.bos_token = Q.bos_token, this.bos_token_id = this.tokens_to_ids.get(this.bos_token), this.eos_token = Q.eos_token, this.eos_token_id = this.tokens_to_ids.get(this.eos_token), this.pad_token = Q.pad_token, this.pad_token_id = this.tokens_to_ids.get(this.pad_token), this.unk_token = Q.unk_token, this.unk_token_id = this.tokens_to_ids.get(this.unk_token), this.vocab = new Array(this.tokens_to_ids.size);
          for (const [ue, ve] of this.tokens_to_ids)
            this.vocab[ve] = ue;
        }
        encode(P) {
          return P;
        }
      }
      class Ee extends _.Callable {
        /**
         * @param {Object} config The configuration object for the normalizer.
         */
        constructor(P) {
          super(), this.config = P;
        }
        /**
         * Factory method for creating normalizers from config objects.
         * @static
         * @param {Object} config The configuration object for the normalizer.
         * @returns {Normalizer} A Normalizer object.
         * @throws {Error} If an unknown Normalizer type is specified in the config.
         */
        static fromConfig(P) {
          if (P === null) return null;
          switch (P.type) {
            case "BertNormalizer":
              return new Ke(P);
            case "Precompiled":
              return new ys(P);
            case "Sequence":
              return new te(P);
            case "Replace":
              return new tt(P);
            case "NFC":
              return new Ge(P);
            case "NFKC":
              return new ye(P);
            case "NFKD":
              return new J(P);
            case "Strip":
              return new de(P);
            case "StripAccents":
              return new Ce(P);
            case "Lowercase":
              return new Be(P);
            case "Prepend":
              return new Ze(P);
            default:
              throw new Error(`Unknown Normalizer type: ${P.type}`);
          }
        }
        /**
         * Normalize the input text.
         * @abstract
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         * @throws {Error} If this method is not implemented in a subclass.
         */
        normalize(P) {
          throw Error("normalize should be implemented in subclass.");
        }
        /**
         * Alias for {@link Normalizer#normalize}.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        _call(P) {
          return this.normalize(P);
        }
      }
      class tt extends Ee {
        /**
         * Normalize the input text by replacing the pattern with the content.
         * @param {string} text The input text to be normalized.
         * @returns {string} The normalized text after replacing the pattern with the content.
         */
        normalize(P) {
          const Q = I(this.config.pattern);
          return Q === null ? P : P.replaceAll(Q, this.config.content);
        }
      }
      class Ge extends Ee {
        /**
         * Normalize the input text by applying Unicode normalization form C (NFC).
         * @param {string} text The input text to be normalized.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.normalize("NFC"), P;
        }
      }
      class ye extends Ee {
        /**
         * Normalize text using NFKC normalization.
         * @param {string} text The text to be normalized.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.normalize("NFKC"), P;
        }
      }
      class J extends Ee {
        /**
         * Normalize text using NFKD normalization.
         * @param {string} text The text to be normalized.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.normalize("NFKD"), P;
        }
      }
      class de extends Ee {
        /**
         * Strip leading and/or trailing whitespace from the input text.
         * @param {string} text The input text.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return this.config.strip_left && this.config.strip_right ? P = P.trim() : (this.config.strip_left && (P = P.trimStart()), this.config.strip_right && (P = P.trimEnd())), P;
        }
      }
      class Ce extends Ee {
        /**
         * Remove all accents from the text.
         * @param {string} text The input text.
         * @returns {string} The normalized text without accents.
         */
        normalize(P) {
          return P = W(P), P;
        }
      }
      class Be extends Ee {
        /**
         * Lowercases the input string.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.toLowerCase(), P;
        }
      }
      class Ze extends Ee {
        /**
         * Prepends the input string.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = this.config.prepend + P, P;
        }
      }
      class te extends Ee {
        /**
        * Create a new instance of NormalizerSequence.
        * @param {Object} config The configuration object.
        * @param {Object[]} config.normalizers An array of Normalizer configuration objects.
        */
        constructor(P) {
          super(P), this.normalizers = P.normalizers.map((Q) => Ee.fromConfig(Q));
        }
        /**
        * Apply a sequence of Normalizers to the input text.
        * @param {string} text The text to normalize.
        * @returns {string} The normalized text.
        */
        normalize(P) {
          return this.normalizers.reduce((Q, ue) => ue.normalize(Q), P);
        }
      }
      class Ke extends Ee {
        /**
         * Adds whitespace around any CJK (Chinese, Japanese, or Korean) character in the input text.
         *
         * @param {string} text The input text to tokenize.
         * @returns {string} The tokenized text with whitespace added around CJK characters.
         */
        _tokenize_chinese_chars(P) {
          const Q = [];
          for (let ue = 0; ue < P.length; ++ue) {
            const ve = P[ue], Se = ve.charCodeAt(0);
            q(Se) ? (Q.push(" "), Q.push(ve), Q.push(" ")) : Q.push(ve);
          }
          return Q.join("");
        }
        /**
         * Strips accents from the given text.
         * @param {string} text The text to strip accents from.
         * @returns {string} The text with accents removed.
         */
        stripAccents(P) {
          return P.normalize("NFD").replace(new RegExp("\\p{Mn}", "gu"), "");
        }
        /**
         * Checks whether `char` is a control character.
         * @param {string} char The character to check.
         * @returns {boolean} Whether `char` is a control character.
         * @private
         */
        _is_control(P) {
          switch (P) {
            case "	":
            case `
`:
            case "\r":
              return !1;
            default:
              return new RegExp("^\\p{Cc}|\\p{Cf}|\\p{Co}|\\p{Cs}$", "u").test(P);
          }
        }
        /**
         * Performs invalid character removal and whitespace cleanup on text.
         * @param {string} text The text to clean.
         * @returns {string} The cleaned text.
         * @private
         */
        _clean_text(P) {
          const Q = [];
          for (const ue of P) {
            const ve = ue.charCodeAt(0);
            ve === 0 || ve === 65533 || this._is_control(ue) || (/^\s$/.test(ue) ? Q.push(" ") : Q.push(ue));
          }
          return Q.join("");
        }
        /**
         * Normalizes the given text based on the configuration.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return this.config.clean_text && (P = this._clean_text(P)), this.config.handle_chinese_chars && (P = this._tokenize_chinese_chars(P)), this.config.lowercase ? (P = P.toLowerCase(), this.config.strip_accents !== !1 && (P = this.stripAccents(P))) : this.config.strip_accents && (P = this.stripAccents(P)), P;
        }
      }
      class je extends _.Callable {
        /**
        * Factory method that returns an instance of a subclass of `PreTokenizer` based on the provided configuration.
        *
        * @static
        * @param {Object} config A configuration object for the pre-tokenizer.
        * @returns {PreTokenizer} An instance of a subclass of `PreTokenizer`.
        * @throws {Error} If the provided configuration object does not correspond to any known pre-tokenizer.
        */
        static fromConfig(P) {
          if (P === null) return null;
          switch (P.type) {
            case "BertPreTokenizer":
              return new ae(P);
            case "Sequence":
              return new Cs(P);
            case "Whitespace":
              return new Ds(P);
            case "WhitespaceSplit":
              return new sr(P);
            case "Metaspace":
              return new At(P);
            case "ByteLevel":
              return new Te(P);
            case "Split":
              return new Ue(P);
            case "Punctuation":
              return new Ve(P);
            case "Digits":
              return new Ne(P);
            case "Replace":
              return new kr(P);
            default:
              throw new Error(`Unknown PreTokenizer type: ${P.type}`);
          }
        }
        /**
         * Method that should be implemented by subclasses to define the specific pre-tokenization logic.
         *
         * @abstract
         * @param {string} text The text to pre-tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} The pre-tokenized text.
         * @throws {Error} If the method is not implemented in the subclass.
         */
        pre_tokenize_text(P, Q) {
          throw Error("pre_tokenize_text should be implemented in subclass.");
        }
        /**
         * Tokenizes the given text into pre-tokens.
         * @param {string|string[]} text The text or array of texts to pre-tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of pre-tokens.
         */
        pre_tokenize(P, Q) {
          return (Array.isArray(P) ? P.map((ue) => this.pre_tokenize_text(ue, Q)) : this.pre_tokenize_text(P, Q)).flat();
        }
        /**
         * Alias for {@link PreTokenizer#pre_tokenize}.
         * @param {string|string[]} text The text or array of texts to pre-tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of pre-tokens.
         */
        _call(P, Q) {
          return this.pre_tokenize(P, Q);
        }
      }
      class ae extends je {
        /**
         * A PreTokenizer that splits text into wordpieces using a basic tokenization scheme
         * similar to that used in the original implementation of BERT.
         * 
         * @param {Object} config The configuration object.
         */
        constructor(P) {
          super(), this.pattern = new RegExp(`[^\\s${w}]+|[${w}]`, "gu");
        }
        /**
         * Tokenizes a single text using the BERT pre-tokenization scheme.
         * 
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, Q) {
          return P.trim().match(this.pattern) || [];
        }
      }
      class Te extends je {
        /**
         * Creates a new instance of the `ByteLevelPreTokenizer` class.
         * @param {Object} config The configuration object.
         */
        constructor(P) {
          super(), this.config = P, this.add_prefix_space = this.config.add_prefix_space, this.trim_offsets = this.config.trim_offsets, this.use_regex = this.config.use_regex ?? !0, this.pattern = new RegExp("'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+", "gu"), this.byte_encoder = xe, this.text_encoder = new TextEncoder();
        }
        /**
         * Tokenizes a single piece of text using byte-level tokenization.
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, Q) {
          return this.add_prefix_space && !P.startsWith(" ") && (P = " " + P), (this.use_regex ? P.match(this.pattern) || [] : [P]).map(
            (ve) => Array.from(this.text_encoder.encode(ve), (Se) => this.byte_encoder[Se]).join("")
          );
        }
      }
      class Ue extends je {
        /**
         * @param {Object} config The configuration options for the pre-tokenizer.
         * @param {Object} config.pattern The pattern used to split the text. Can be a string or a regex object.
         * @param {string|undefined} config.pattern.String The string to use for splitting. Only defined if the pattern is a string.
         * @param {string|undefined} config.pattern.Regex The regex to use for splitting. Only defined if the pattern is a regex.
         * @param {SplitDelimiterBehavior} config.behavior The behavior to use when splitting.
         * @param {boolean} config.invert Whether to split (invert=false) or match (invert=true) the pattern.
         */
        constructor(P) {
          super(), this.config = P, this.pattern = I(this.config.pattern, this.config.invert);
        }
        /**
         * Tokenizes text by splitting it using the given pattern.
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, Q) {
          var ue;
          return this.pattern === null ? [] : this.config.invert ? P.match(this.pattern) || [] : ((ue = this.config.behavior) == null ? void 0 : ue.toLowerCase()) === "removed" ? P.split(this.pattern).filter((ve) => ve) : b(P, this.pattern);
        }
      }
      class Ve extends je {
        /**
         * @param {Object} config The configuration options for the pre-tokenizer.
         * @param {SplitDelimiterBehavior} config.behavior The behavior to use when splitting.
         */
        constructor(P) {
          super(), this.config = P, this.pattern = new RegExp(`[^${w}]+|[${w}]+`, "gu");
        }
        /**
         * Tokenizes text by splitting it using the given pattern.
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, Q) {
          return P.match(this.pattern) || [];
        }
      }
      class Ne extends je {
        /**
         * @param {Object} config The configuration options for the pre-tokenizer.
         * @param {boolean} config.individual_digits Whether to split on individual digits.
         */
        constructor(P) {
          super(), this.config = P;
          const Q = `[^\\d]+|\\d${this.config.individual_digits ? "" : "+"}`;
          this.pattern = new RegExp(Q, "gu");
        }
        /**
         * Tokenizes text by splitting it using the given pattern.
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, Q) {
          return P.match(this.pattern) || [];
        }
      }
      class Re extends _.Callable {
        /**
         * @param {Object} config The configuration for the post-processor.
         */
        constructor(P) {
          super(), this.config = P;
        }
        /**
         * Factory method to create a PostProcessor object from a configuration object.
         *
         * @param {Object} config Configuration object representing a PostProcessor.
         * @returns {PostProcessor} A PostProcessor object created from the given configuration.
         * @throws {Error} If an unknown PostProcessor type is encountered.
         */
        static fromConfig(P) {
          if (P === null) return null;
          switch (P.type) {
            case "TemplateProcessing":
              return new ct(P);
            case "ByteLevel":
              return new lt(P);
            case "RobertaProcessing":
              return new dt(P);
            case "BertProcessing":
              return new st(P);
            case "Sequence":
              return new ht(P);
            default:
              throw new Error(`Unknown PostProcessor type: ${P.type}`);
          }
        }
        /**
         * Method to be implemented in subclass to apply post-processing on the given tokens.
         *
         * @param {Array} tokens The input tokens to be post-processed.
         * @param {...*} args Additional arguments required by the post-processing logic.
         * @returns {PostProcessedOutput} The post-processed tokens.
         * @throws {Error} If the method is not implemented in subclass.
         */
        post_process(P, ...Q) {
          throw Error("post_process should be implemented in subclass.");
        }
        /**
         * Alias for {@link PostProcessor#post_process}.
         * @param {Array} tokens The text or array of texts to post-process.
         * @param {...*} args Additional arguments required by the post-processing logic.
         * @returns {PostProcessedOutput} The post-processed tokens.
         */
        _call(P, ...Q) {
          return this.post_process(P, ...Q);
        }
      }
      class st extends Re {
        /**
         * @param {Object} config The configuration for the post-processor.
         * @param {string[]} config.cls The special tokens to add to the beginning of the input.
         * @param {string[]} config.sep The special tokens to add to the end of the input.
         */
        constructor(P) {
          super(P), this.cls = P.cls[0], this.sep = P.sep[0];
        }
        /**
         * Adds the special tokens to the beginning and end of the input.
         * @param {string[]} tokens The input tokens.
         * @param {string[]} [tokens_pair=null] An optional second set of input tokens.
         * @returns {PostProcessedOutput} The post-processed tokens with the special tokens added to the beginning and end.
         */
        post_process(P, Q = null, {
          add_special_tokens: ue = !0
        } = {}) {
          ue && (P = (0, D.mergeArrays)([this.cls], P, [this.sep]));
          let ve = new Array(P.length).fill(0);
          if (Q !== null) {
            const Se = ue && this instanceof dt ? [this.sep] : [], Qe = ue ? [this.sep] : [];
            P = (0, D.mergeArrays)(P, Se, Q, Qe), ve = (0, D.mergeArrays)(ve, new Array(Q.length + Se.length + Qe.length).fill(1));
          }
          return { tokens: P, token_type_ids: ve };
        }
      }
      class dt extends st {
      }
      class ct extends Re {
        /**
         * Creates a new instance of `TemplateProcessing`.
         * @param {Object} config The configuration options for the post processor.
         * @param {Array} config.single The template for a single sequence of tokens.
         * @param {Array} config.pair The template for a pair of sequences of tokens.
         */
        constructor(P) {
          super(P), this.single = P.single, this.pair = P.pair;
        }
        /**
         * Replaces special tokens in the template with actual tokens.
         * @param {string[]} tokens The list of tokens for the first sequence.
         * @param {string[]} [tokens_pair=null] The list of tokens for the second sequence (optional).
         * @returns {PostProcessedOutput} An object containing the list of tokens with the special tokens replaced with actual tokens.
         */
        post_process(P, Q = null, {
          add_special_tokens: ue = !0
        } = {}) {
          const ve = Q === null ? this.single : this.pair;
          let Se = [], Qe = [];
          for (const pt of ve)
            "SpecialToken" in pt ? ue && (Se.push(pt.SpecialToken.id), Qe.push(pt.SpecialToken.type_id)) : "Sequence" in pt && (pt.Sequence.id === "A" ? (Se = (0, D.mergeArrays)(Se, P), Qe = (0, D.mergeArrays)(Qe, new Array(P.length).fill(pt.Sequence.type_id))) : pt.Sequence.id === "B" && (Se = (0, D.mergeArrays)(Se, Q), Qe = (0, D.mergeArrays)(Qe, new Array(Q.length).fill(pt.Sequence.type_id))));
          return { tokens: Se, token_type_ids: Qe };
        }
      }
      class lt extends Re {
        /**
         * Post process the given tokens.
         * @param {string[]} tokens The list of tokens for the first sequence.
         * @param {string[]} [tokens_pair=null] The list of tokens for the second sequence (optional).
         * @returns {PostProcessedOutput} An object containing the post-processed tokens.
         */
        post_process(P, Q = null) {
          return Q && (P = (0, D.mergeArrays)(P, Q)), { tokens: P };
        }
      }
      class ht extends Re {
        /**
         * Creates a new instance of PostProcessorSequence.
         * @param {Object} config The configuration object.
         * @param {Object[]} config.processors The list of post-processors to apply.
         */
        constructor(P) {
          super(P), this.processors = P.processors.map((Q) => Re.fromConfig(Q));
        }
        /**
         * Post process the given tokens.
         * @param {string[]} tokens The list of tokens for the first sequence.
         * @param {string[]} [tokens_pair=null] The list of tokens for the second sequence (optional).
         * @returns {PostProcessedOutput} An object containing the post-processed tokens.
         */
        post_process(P, Q = null, ue = {}) {
          let ve;
          for (const Se of this.processors)
            if (Se instanceof lt)
              P = Se.post_process(P).tokens, Q && (Q = Se.post_process(Q).tokens);
            else {
              const Qe = Se.post_process(P, Q, ue);
              P = Qe.tokens, ve = Qe.token_type_ids;
            }
          return { tokens: P, token_type_ids: ve };
        }
      }
      class L extends _.Callable {
        /**
        * Creates an instance of `Decoder`.
        *
        * @param {Object} config The configuration object.
        */
        constructor(P) {
          super(), this.config = P, this.added_tokens = [], this.end_of_word_suffix = null, this.trim_offsets = P.trim_offsets;
        }
        /**
        * Creates a decoder instance based on the provided configuration.
        *
        * @param {Object} config The configuration object.
        * @returns {Decoder} A decoder instance.
        * @throws {Error} If an unknown decoder type is provided.
        */
        static fromConfig(P) {
          if (P === null) return null;
          switch (P.type) {
            case "WordPiece":
              return new We(P);
            case "Metaspace":
              return new is(P);
            case "ByteLevel":
              return new Je(P);
            case "Replace":
              return new oe(P);
            case "ByteFallback":
              return new H(P);
            case "Fuse":
              return new me(P);
            case "Strip":
              return new Ae(P);
            case "Sequence":
              return new mt(P);
            case "CTC":
              return new ut(P);
            case "BPEDecoder":
              return new vt(P);
            default:
              throw new Error(`Unknown Decoder type: ${P.type}`);
          }
        }
        /**
        * Calls the `decode` method.
        *
        * @param {string[]} tokens The list of tokens.
        * @returns {string} The decoded string.
        */
        _call(P) {
          return this.decode(P);
        }
        /**
        * Decodes a list of tokens.
        * @param {string[]} tokens The list of tokens.
        * @returns {string} The decoded string.
        */
        decode(P) {
          return this.decode_chain(P).join("");
        }
        /**
         * Apply the decoder to a list of tokens.
         * 
         * @param {string[]} tokens The list of tokens.
         * @returns {string[]} The decoded list of tokens.
         * @throws {Error} If the `decode_chain` method is not implemented in the subclass.
         */
        decode_chain(P) {
          throw Error("`decode_chain` should be implemented in subclass.");
        }
      }
      class oe extends L {
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          const Q = I(this.config.pattern);
          return Q === null ? P : P.map((ue) => ue.replaceAll(Q, this.config.content));
        }
      }
      class H extends L {
        constructor(P) {
          super(P), this.text_decoder = new TextDecoder();
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          const Q = [];
          let ue = [];
          for (const ve of P) {
            let Se = null;
            if (ve.length === 6 && ve.startsWith("<0x") && ve.endsWith(">")) {
              const Qe = parseInt(ve.slice(3, 5), 16);
              isNaN(Qe) || (Se = Qe);
            }
            if (Se !== null)
              ue.push(Se);
            else {
              if (ue.length > 0) {
                const Qe = this.text_decoder.decode(Uint8Array.from(ue));
                Q.push(Qe), ue = [];
              }
              Q.push(ve);
            }
          }
          if (ue.length > 0) {
            const ve = this.text_decoder.decode(Uint8Array.from(ue));
            Q.push(ve), ue = [];
          }
          return Q;
        }
      }
      class me extends L {
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return [P.join("")];
        }
      }
      class Ae extends L {
        constructor(P) {
          super(P), this.content = this.config.content, this.start = this.config.start, this.stop = this.config.stop;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return P.map((Q) => {
            let ue = 0;
            for (let Se = 0; Se < this.start && Q[Se] === this.content; ++Se) {
              ue = Se + 1;
              continue;
            }
            let ve = Q.length;
            for (let Se = 0; Se < this.stop; ++Se) {
              const Qe = Q.length - Se - 1;
              if (Q[Qe] === this.content) {
                ve = Qe;
                continue;
              } else
                break;
            }
            return Q.slice(ue, ve);
          });
        }
      }
      class We extends L {
        /**
         * Creates a new instance of WordPieceDecoder.
         * @param {Object} config The configuration object.
         * @param {string} config.prefix The prefix used for WordPiece encoding.
         * @param {boolean} config.cleanup Whether to cleanup the decoded string.
         */
        constructor(P) {
          super(P), this.cleanup = P.cleanup;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return P.map((Q, ue) => (ue !== 0 && (Q.startsWith(this.config.prefix) ? Q = Q.replace(this.config.prefix, "") : Q = " " + Q), this.cleanup && (Q = ie(Q)), Q));
        }
      }
      class Je extends L {
        /**
         * Create a `ByteLevelDecoder` object.
         * @param {Object} config Configuration object.
         */
        constructor(P) {
          super(P), this.byte_decoder = ce, this.text_decoder = new TextDecoder("utf-8", {
            fatal: !1,
            ignoreBOM: !0
          }), this.end_of_word_suffix = null;
        }
        /**
         * Convert an array of tokens to string by decoding each byte.
         * @param {string[]} tokens Array of tokens to be decoded.
         * @returns {string} The decoded string.
         */
        convert_tokens_to_string(P) {
          const Q = P.join(""), ue = new Uint8Array([...Q].map((Se) => this.byte_decoder[Se]));
          return this.text_decoder.decode(ue);
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          const Q = [];
          let ue = [];
          for (const ve of P)
            this.added_tokens.find((Se) => Se.content === ve) !== void 0 ? (ue.length > 0 && (Q.push(this.convert_tokens_to_string(ue)), ue = []), Q.push(ve)) : ue.push(ve);
          return ue.length > 0 && Q.push(this.convert_tokens_to_string(ue)), Q;
        }
      }
      class ut extends L {
        constructor(P) {
          super(P), this.pad_token = this.config.pad_token, this.word_delimiter_token = this.config.word_delimiter_token, this.cleanup = this.config.cleanup;
        }
        /**
         * Converts a connectionist-temporal-classification (CTC) output tokens into a single string.
         * @param {string[]} tokens Array of tokens to be decoded.
         * @returns {string} The decoded string.
         */
        convert_tokens_to_string(P) {
          if (P.length === 0) return "";
          const Q = [P[0]];
          for (let Se = 1; Se < P.length; ++Se)
            P[Se] !== Q.at(-1) && Q.push(P[Se]);
          let ve = Q.filter((Se) => Se !== this.pad_token).join("");
          return this.cleanup && (ve = ie(ve).replaceAll(this.word_delimiter_token, " ").trim()), ve;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return [this.convert_tokens_to_string(P)];
        }
      }
      class mt extends L {
        /**
         * Creates a new instance of DecoderSequence.
         * @param {Object} config The configuration object.
         * @param {Object[]} config.decoders The list of decoders to apply.
         */
        constructor(P) {
          super(P), this.decoders = P.decoders.map((Q) => L.fromConfig(Q));
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return this.decoders.reduce((Q, ue) => ue.decode_chain(Q), P);
        }
      }
      class vt extends L {
        constructor(P) {
          super(P), this.suffix = this.config.suffix;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return P.map((Q, ue) => Q.replaceAll(this.suffix, ue === P.length - 1 ? "" : " "));
        }
      }
      class kt extends L {
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          let Q = "";
          for (let ue = 1; ue < P.length; ue += 2)
            Q += P[ue];
          return [Q];
        }
      }
      class At extends je {
        /**
         * @param {Object} config The configuration object for the MetaspacePreTokenizer.
         * @param {boolean} config.add_prefix_space Whether to add a prefix space to the first token.
         * @param {string} config.replacement The character to replace spaces with.
         * @param {string} [config.str_rep=config.replacement] An optional string representation of the replacement character.
         * @param {'first'|'never'|'always'} [config.prepend_scheme='always'] The metaspace prepending scheme.
         */
        constructor(P) {
          super(), this.addPrefixSpace = P.add_prefix_space, this.replacement = P.replacement, this.strRep = P.str_rep || this.replacement, this.prepend_scheme = P.prepend_scheme ?? "always";
        }
        /**
         * This method takes a string, replaces spaces with the replacement character,
         * adds a prefix space if requested, and returns a new list of tokens.
         * @param {string} text The text to pre-tokenize.
         * @param {Object} [options] The options for the pre-tokenization.
         * @param {number} [options.section_index] The index of the section to pre-tokenize.
         * @returns {string[]} A new list of pre-tokenized tokens.
         */
        pre_tokenize_text(P, {
          section_index: Q = void 0
        } = {}) {
          let ue = P.replaceAll(" ", this.strRep);
          return (
            // We add a prefix space if:
            //  (1) The addPrefixSpace option is enabled and the normalized
            //      token does not already start with the replacement character.
            this.addPrefixSpace && !ue.startsWith(this.replacement) && (this.prepend_scheme === "always" || this.prepend_scheme === "first" && Q === 0) && (ue = this.strRep + ue), [ue]
          );
        }
      }
      class is extends L {
        /**
         * Constructs a new MetaspaceDecoder object.
         * @param {Object} config The configuration object for the MetaspaceDecoder.
         * @param {boolean} config.add_prefix_space Whether to add a prefix space to the decoded string.
         * @param {string} config.replacement The string to replace spaces with.
         */
        constructor(P) {
          super(P), this.addPrefixSpace = P.add_prefix_space, this.replacement = P.replacement;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          const Q = [];
          for (let ue = 0; ue < P.length; ++ue) {
            let ve = P[ue].replaceAll(this.replacement, " ");
            this.addPrefixSpace && ue == 0 && ve.startsWith(" ") && (ve = ve.substring(1)), Q.push(ve);
          }
          return Q;
        }
      }
      class ys extends Ee {
        /**
         * Create a new instance of Precompiled normalizer.
         * @param {Object} config The configuration object.
         * @param {any} config.precompiled_charsmap Precompiled chars mapping.
         */
        constructor(P) {
          super(P), this.charsmap = P.precompiled_charsmap;
        }
        /**
         * Normalizes the given text by applying the precompiled charsmap.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.replace(/[\u0001-\u0008\u000B\u000E-\u001F\u007F\u008F\u009F]/gm, ""), P = P.replace(/[\u0009\u000A\u000C\u000D\u00A0\u1680\u2000-\u200F\u2028\u2029\u202F\u205F\u2581\u3000\uFEFF\uFFFD]/gm, " "), P.includes("～") ? P = P.split("～").map((ue) => ue.normalize("NFKC")).join("～") : P = P.normalize("NFKC"), P;
        }
      }
      class Cs extends je {
        /**
         * Creates an instance of PreTokenizerSequence.
         * @param {Object} config The configuration object for the pre-tokenizer sequence.
         * @param {Object[]} config.pretokenizers An array of pre-tokenizer configurations.
         */
        constructor(P) {
          super(), this.tokenizers = P.pretokenizers.map((Q) => je.fromConfig(Q));
        }
        /**
         * Applies each pre-tokenizer in the sequence to the input text in turn.
         * @param {string} text The text to pre-tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} The pre-tokenized text.
         */
        pre_tokenize_text(P, Q) {
          return this.tokenizers.reduce((ue, ve) => ve.pre_tokenize(ue, Q), [P]);
        }
      }
      class Ds extends je {
        /**
         * Creates an instance of WhitespacePreTokenizer.
         * @param {Object} config The configuration object for the pre-tokenizer.
         */
        constructor(P) {
          super();
        }
        /**
         * Pre-tokenizes the input text by splitting it on word boundaries.
         * @param {string} text The text to be pre-tokenized.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens produced by splitting the input text on whitespace.
         */
        pre_tokenize_text(P, Q) {
          return P.match(/\w+|[^\w\s]+/g) || [];
        }
      }
      class sr extends je {
        /**
         * Creates an instance of WhitespaceSplit.
         * @param {Object} config The configuration object for the pre-tokenizer.
         */
        constructor(P) {
          super();
        }
        /**
         * Pre-tokenizes the input text by splitting it on whitespace characters.
         * @param {string} text The text to be pre-tokenized.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens produced by splitting the input text on whitespace.
         */
        pre_tokenize_text(P, Q) {
          return S(P);
        }
      }
      class kr extends je {
        /**
         * @param {Object} config The configuration options for the pre-tokenizer.
         * @param {Object} config.pattern The pattern used to split the text. Can be a string or a regex object.
         * @param {string} config.content What to replace the pattern with.
         */
        constructor(P) {
          super(), this.config = P, this.pattern = I(this.config.pattern), this.content = this.config.content;
        }
        /**
         * Pre-tokenizes the input text by replacing certain characters.
         * @param {string} text The text to be pre-tokenized.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens produced by replacing certain characters.
         */
        pre_tokenize_text(P, Q) {
          return this.pattern === null ? [P] : [P.replaceAll(this.pattern, this.config.content)];
        }
      }
      const Qr = [
        "bos_token",
        "eos_token",
        "unk_token",
        "sep_token",
        "pad_token",
        "cls_token",
        "mask_token"
        // additional_special_tokens (TODO)
      ];
      function Us(Pe, P, Q, ue) {
        for (const ve of Object.keys(Pe)) {
          const Se = P - Pe[ve].length, Qe = Q(ve), pt = new Array(Se).fill(Qe);
          Pe[ve] = ue === "right" ? (0, D.mergeArrays)(Pe[ve], pt) : (0, D.mergeArrays)(pt, Pe[ve]);
        }
      }
      function Tr(Pe, P) {
        for (const Q of Object.keys(Pe))
          Pe[Q].length = P;
      }
      class Nt extends _.Callable {
        /**
         * Create a new PreTrainedTokenizer instance.
         * @param {Object} tokenizerJSON The JSON of the tokenizer.
         * @param {Object} tokenizerConfig The config of the tokenizer.
         */
        constructor(Q, ue) {
          super();
          fe(this, "return_token_type_ids", !1);
          fe(this, "padding_side", "right");
          this._tokenizer_config = ue, this.normalizer = Ee.fromConfig(Q.normalizer), this.pre_tokenizer = je.fromConfig(Q.pre_tokenizer), this.model = be.fromConfig(Q.model, ue), this.post_processor = Re.fromConfig(Q.post_processor), this.decoder = L.fromConfig(Q.decoder), this.special_tokens = [], this.all_special_ids = [], this.added_tokens = [];
          for (const ve of Q.added_tokens) {
            const Se = new ne(ve);
            this.added_tokens.push(Se), this.model.tokens_to_ids.set(Se.content, Se.id), this.model.vocab[Se.id] = Se.content, Se.special && (this.special_tokens.push(Se.content), this.all_special_ids.push(Se.id));
          }
          if (this.additional_special_tokens = ue.additional_special_tokens ?? [], this.special_tokens.push(...this.additional_special_tokens), this.special_tokens = [...new Set(this.special_tokens)], this.decoder && (this.decoder.added_tokens = this.added_tokens, this.decoder.end_of_word_suffix = this.model.end_of_word_suffix), this.added_tokens_regex = this.added_tokens.length > 0 ? new RegExp(
            this.added_tokens.slice().sort((ve, Se) => Se.content.length - ve.content.length).map((ve) => `${ve.lstrip ? "\\s*" : ""}(${(0, D.escapeRegExp)(ve.content)})${ve.rstrip ? "\\s*" : ""}`).join("|")
          ) : null, this.mask_token = this.getToken("mask_token"), this.mask_token_id = this.model.tokens_to_ids.get(this.mask_token), this.pad_token = this.getToken("pad_token", "eos_token"), this.pad_token_id = this.model.tokens_to_ids.get(this.pad_token), this.sep_token = this.getToken("sep_token"), this.sep_token_id = this.model.tokens_to_ids.get(this.sep_token), this.unk_token = this.getToken("unk_token"), this.unk_token_id = this.model.tokens_to_ids.get(this.unk_token), this.bos_token = this.getToken("bos_token"), this.bos_token_id = this.model.tokens_to_ids.get(this.bos_token), this.eos_token = this.getToken("eos_token"), this.eos_token_id = this.model.tokens_to_ids.get(this.eos_token), this.model_max_length = ue.model_max_length, this.remove_space = ue.remove_space, this.clean_up_tokenization_spaces = ue.clean_up_tokenization_spaces ?? !0, this.do_lowercase_and_remove_accent = ue.do_lowercase_and_remove_accent ?? !1, ue.padding_side && (this.padding_side = ue.padding_side), this.legacy = !1, this.chat_template = ue.chat_template ?? null, Array.isArray(this.chat_template)) {
            const ve = /* @__PURE__ */ Object.create(null);
            for (const { name: Se, template: Qe } of this.chat_template) {
              if (typeof Se != "string" || typeof Qe != "string")
                throw new Error('Chat template must be a list of objects with "name" and "template" properties');
              ve[Se] = Qe;
            }
            this.chat_template = ve;
          }
          this._compiled_template_cache = /* @__PURE__ */ new Map();
        }
        /**
         * Returns the value of the first matching key in the tokenizer config object.
         * @param {...string} keys One or more keys to search for in the tokenizer config object.
         * @returns {string|null} The value associated with the first matching key, or null if no match is found.
         * @throws {Error} If an object is found for a matching key and its __type property is not "AddedToken".
         * @private
         */
        getToken(...Q) {
          for (const ue of Q) {
            const ve = this._tokenizer_config[ue];
            if (ve)
              if (typeof ve == "object") {
                if (ve.__type === "AddedToken")
                  return ve.content;
                throw Error(`Unknown token: ${ve}`);
              } else
                return ve;
          }
          return null;
        }
        /**
         * Loads a pre-trained tokenizer from the given `pretrained_model_name_or_path`. 
         * 
         * @param {string} pretrained_model_name_or_path The path to the pre-trained tokenizer.
         * @param {PretrainedTokenizerOptions} options Additional options for loading the tokenizer.
         * 
         * @throws {Error} Throws an error if the tokenizer.json or tokenizer_config.json files are not found in the `pretrained_model_name_or_path`.
         * @returns {Promise<PreTrainedTokenizer>} A new instance of the `PreTrainedTokenizer` class.
         */
        static async from_pretrained(Q, {
          progress_callback: ue = null,
          config: ve = null,
          cache_dir: Se = null,
          local_files_only: Qe = !1,
          revision: pt = "main",
          legacy: gt = null
        } = {}) {
          const ft = await y(Q, {
            progress_callback: ue,
            config: ve,
            cache_dir: Se,
            local_files_only: Qe,
            revision: pt,
            legacy: gt
          });
          return new this(...ft);
        }
        /**
         * @typedef {number[]|number[][]|Tensor} BatchEncodingItem
         * 
         * @typedef {Object} BatchEncoding Holds the output of the tokenizer's call function.
         * @property {BatchEncodingItem} input_ids List of token ids to be fed to a model.
         * @property {BatchEncodingItem} attention_mask List of indices specifying which tokens should be attended to by the model.
         * @property {BatchEncodingItem} [token_type_ids] List of token type ids to be fed to a model.
         */
        /**
         * Encode/tokenize the given text(s).
         * @param {string|string[]} text The text to tokenize.
         * @param {Object} options An optional object containing the following properties:
         * @param {string|string[]} [options.text_pair=null] Optional second sequence to be encoded. If set, must be the same type as text.
         * @param {boolean|'max_length'} [options.padding=false] Whether to pad the input sequences.
         * @param {boolean} [options.add_special_tokens=true] Whether or not to add the special tokens associated with the corresponding model.
         * @param {boolean} [options.truncation=null] Whether to truncate the input sequences.
         * @param {number} [options.max_length=null] Maximum length of the returned list and optionally padding length.
         * @param {boolean} [options.return_tensor=true] Whether to return the results as Tensors or arrays.
         * @param {boolean} [options.return_token_type_ids=null] Whether to return the token type ids.
         * @returns {BatchEncoding} Object to be passed to the model.
         */
        _call(Q, {
          text_pair: ue = null,
          add_special_tokens: ve = !0,
          padding: Se = !1,
          truncation: Qe = null,
          max_length: pt = null,
          return_tensor: gt = !0,
          // Different to HF
          return_token_type_ids: ft = null
        } = {}) {
          const xt = Array.isArray(Q);
          let Kt;
          if (xt) {
            if (Q.length === 0)
              throw Error("text array must be non-empty");
            if (ue !== null) {
              if (Array.isArray(ue)) {
                if (Q.length !== ue.length)
                  throw Error("text and text_pair must have the same length");
              } else throw Error("text_pair must also be an array");
              Kt = Q.map(
                (us, Os) => this._encode_plus(us, { text_pair: ue[Os], add_special_tokens: ve, return_token_type_ids: ft })
              );
            } else
              Kt = Q.map((us) => this._encode_plus(us, { add_special_tokens: ve, return_token_type_ids: ft }));
          } else {
            if (Q == null)
              throw Error("text may not be null or undefined");
            if (Array.isArray(ue))
              throw Error("When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).");
            Kt = [this._encode_plus(Q, { text_pair: ue, add_special_tokens: ve, return_token_type_ids: ft })];
          }
          if (pt === null ? Se === "max_length" ? pt = this.model_max_length : pt = (0, Y.max)(Kt.map((us) => us.input_ids.length))[0] : Qe || console.warn("Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=true` to explicitly truncate examples to max length."), pt = Math.min(pt, this.model_max_length ?? 1 / 0), Se || Qe)
            for (let us = 0; us < Kt.length; ++us)
              Kt[us].input_ids.length !== pt && (Kt[us].input_ids.length > pt ? Qe && Tr(Kt[us], pt) : Se && Us(
                Kt[us],
                pt,
                (Os) => Os === "input_ids" ? this.pad_token_id : 0,
                this.padding_side
              ));
          const ms = {};
          if (gt) {
            if (!(Se && Qe) && Kt.some((Os) => {
              var Bt;
              for (const rs of Object.keys(Os))
                if (Os[rs].length !== ((Bt = Kt[0][rs]) == null ? void 0 : Bt.length))
                  return !0;
              return !1;
            }))
              throw Error(
                "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=true' and 'truncation=true' to have batched tensors with the same length."
              );
            const us = [Kt.length, Kt[0].input_ids.length];
            for (const Os of Object.keys(Kt[0]))
              ms[Os] = new R.Tensor(
                "int64",
                BigInt64Array.from(Kt.flatMap((Bt) => Bt[Os]).map(BigInt)),
                us
              );
          } else {
            for (const us of Object.keys(Kt[0]))
              ms[us] = Kt.map((Os) => Os[us]);
            if (!xt)
              for (const us of Object.keys(ms))
                ms[us] = ms[us][0];
          }
          return (
            /** @type {BatchEncoding} */
            ms
          );
        }
        /**
         * Encodes a single text using the preprocessor pipeline of the tokenizer.
         *
         * @param {string|null} text The text to encode.
         * @returns {string[]|null} The encoded tokens.
         */
        _encode_text(Q) {
          return Q === null ? null : (this.added_tokens_regex ? Q.split(this.added_tokens_regex).filter((Se) => Se) : [Q]).map((Se, Qe) => {
            if (this.added_tokens.find((gt) => gt.content === Se) !== void 0)
              return Se;
            {
              if (this.remove_space === !0 && (Se = Se.trim().split(/\s+/).join(" ")), this.do_lowercase_and_remove_accent && (Se = j(Se)), this.normalizer !== null && (Se = this.normalizer(Se)), Se.length === 0)
                return [];
              const gt = this.pre_tokenizer !== null ? this.pre_tokenizer(Se, {
                section_index: Qe
              }) : [Se];
              return this.model(gt);
            }
          }).flat();
        }
        /**
         * Encodes a single text or a pair of texts using the model's tokenizer.
         *
         * @param {string} text The text to encode.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.text_pair=null] The optional second text to encode.
         * @param {boolean} [options.add_special_tokens=true] Whether or not to add the special tokens associated with the corresponding model.
         * @param {boolean} [options.return_token_type_ids=null] Whether to return token_type_ids.
         * @returns {EncodingSingle} An object containing the encoded text.
         * @private
         */
        _encode_plus(Q, {
          text_pair: ue = null,
          add_special_tokens: ve = !0,
          return_token_type_ids: Se = null
        } = {}) {
          const { tokens: Qe, token_type_ids: pt } = this._tokenize_helper(Q, { pair: ue, add_special_tokens: ve }), gt = this.model.convert_tokens_to_ids(Qe), ft = {
            input_ids: gt,
            attention_mask: new Array(gt.length).fill(1)
          };
          return (Se ?? this.return_token_type_ids) && pt && (ft.token_type_ids = pt), ft;
        }
        /**
         * Internal helper function to tokenize a text, and optionally a pair of texts.
         * @param {string} text The text to tokenize.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.pair=null] The optional second text to tokenize.
         * @param {boolean} [options.add_special_tokens=false] Whether or not to add the special tokens associated with the corresponding model.
         * @returns {{tokens: string[], token_type_ids?: number[]}} An object containing the tokens and optionally the token type IDs.
         */
        _tokenize_helper(Q, {
          pair: ue = null,
          add_special_tokens: ve = !1
        } = {}) {
          const Se = this._encode_text(Q), Qe = this._encode_text(ue);
          return this.post_processor ? this.post_processor(Se, Qe, { add_special_tokens: ve }) : { tokens: (0, D.mergeArrays)(Se ?? [], Qe ?? []) };
        }
        /**
         * Converts a string into a sequence of tokens.
         * @param {string} text The sequence to be encoded.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.pair] A second sequence to be encoded with the first.
         * @param {boolean} [options.add_special_tokens=false] Whether or not to add the special tokens associated with the corresponding model.
         * @returns {string[]} The list of tokens.
         */
        tokenize(Q, {
          pair: ue = null,
          add_special_tokens: ve = !1
        } = {}) {
          return this._tokenize_helper(Q, { pair: ue, add_special_tokens: ve }).tokens;
        }
        /**
         * Encodes a single text or a pair of texts using the model's tokenizer.
         *
         * @param {string} text The text to encode.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.text_pair=null] The optional second text to encode.
         * @param {boolean} [options.add_special_tokens=true] Whether or not to add the special tokens associated with the corresponding model.
         * @param {boolean} [options.return_token_type_ids=null] Whether to return token_type_ids.
         * @returns {number[]} An array of token IDs representing the encoded text(s).
         */
        encode(Q, {
          text_pair: ue = null,
          add_special_tokens: ve = !0,
          return_token_type_ids: Se = null
        } = {}) {
          return this._encode_plus(Q, {
            text_pair: ue,
            add_special_tokens: ve,
            return_token_type_ids: Se
          }).input_ids;
        }
        /**
         * Decode a batch of tokenized sequences.
         * @param {number[][]|Tensor} batch List/Tensor of tokenized input sequences.
         * @param {Object} decode_args (Optional) Object with decoding arguments.
         * @returns {string[]} List of decoded sequences.
         */
        batch_decode(Q, ue = {}) {
          return Q instanceof R.Tensor && (Q = Q.tolist()), Q.map((ve) => this.decode(ve, ue));
        }
        /**
         * Decodes a sequence of token IDs back to a string.
         *
         * @param {number[]|bigint[]|Tensor} token_ids List/Tensor of token IDs to decode.
         * @param {Object} [decode_args={}]
         * @param {boolean} [decode_args.skip_special_tokens=false] If true, special tokens are removed from the output string.
         * @param {boolean} [decode_args.clean_up_tokenization_spaces=true] If true, spaces before punctuations and abbreviated forms are removed.
         *
         * @returns {string} The decoded string.
         * @throws {Error} If `token_ids` is not a non-empty array of integers.
         */
        decode(Q, ue = {}) {
          if (Q instanceof R.Tensor && (Q = se(Q)), !Array.isArray(Q) || Q.length === 0 || !(0, D.isIntegralNumber)(Q[0]))
            throw Error("token_ids must be a non-empty array of integers.");
          return this.decode_single(Q, ue);
        }
        /**
         * Decode a single list of token ids to a string.
         * @param {number[]|bigint[]} token_ids List of token ids to decode
         * @param {Object} decode_args Optional arguments for decoding
         * @param {boolean} [decode_args.skip_special_tokens=false] Whether to skip special tokens during decoding
         * @param {boolean} [decode_args.clean_up_tokenization_spaces=null] Whether to clean up tokenization spaces during decoding.
         * If null, the value is set to `this.decoder.cleanup` if it exists, falling back to `this.clean_up_tokenization_spaces` if it exists, falling back to `true`.
         * @returns {string} The decoded string
         */
        decode_single(Q, {
          skip_special_tokens: ue = !1,
          clean_up_tokenization_spaces: ve = null
        }) {
          let Se = this.model.convert_ids_to_tokens(Q);
          ue && (Se = Se.filter((pt) => !this.special_tokens.includes(pt)));
          let Qe = this.decoder ? this.decoder(Se) : Se.join(" ");
          return this.decoder && this.decoder.end_of_word_suffix && (Qe = Qe.replaceAll(this.decoder.end_of_word_suffix, " "), ue && (Qe = Qe.trim())), (ve ?? this.clean_up_tokenization_spaces) && (Qe = ie(Qe)), Qe;
        }
        /**
         * Retrieve the chat template string used for tokenizing chat messages. This template is used
         * internally by the `apply_chat_template` method and can also be used externally to retrieve the model's chat
         * template for better generation tracking.
         * 
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.chat_template=null]
         * A Jinja template or the name of a template to use for this conversion.
         * It is usually not necessary to pass anything to this argument,
         * as the model's template will be used by default.
         * @param {Object[]} [options.tools=null]
         * A list of tools (callable functions) that will be accessible to the model. If the template does not
         * support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,
         * giving the name, description and argument types for the tool. See our
         * [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)
         * for more information.
         * @returns {string} The chat template string.
         */
        get_chat_template({
          chat_template: Q = null,
          tools: ue = null
        } = {}) {
          if (this.chat_template && typeof this.chat_template == "object") {
            const ve = this.chat_template;
            if (Q !== null && Object.hasOwn(ve, Q))
              Q = ve[Q];
            else if (Q === null)
              if (ue !== null && "tool_use" in ve)
                Q = ve.tool_use;
              else if ("default" in ve)
                Q = ve.default;
              else
                throw Error(
                  `This model has multiple chat templates with no default specified! Please either pass a chat template or the name of the template you wish to use to the 'chat_template' argument. Available template names are ${Object.keys(ve).sort()}.`
                );
          } else if (Q === null)
            if (this.chat_template)
              Q = this.chat_template;
            else
              throw Error(
                "Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
              );
          return Q;
        }
        /**
         * Converts a list of message objects with `"role"` and `"content"` keys to a list of token
         * ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to
         * determine the format and control tokens to use when converting.
         * 
         * See [here](https://huggingface.co/docs/transformers/chat_templating) for more information.
         * 
         * **Example:** Applying a chat template to a conversation.
         * 
         * ```javascript
         * import { AutoTokenizer } from "@huggingface/transformers";
         * 
         * const tokenizer = await AutoTokenizer.from_pretrained("Xenova/mistral-tokenizer-v1");
         * 
         * const chat = [
         *   { "role": "user", "content": "Hello, how are you?" },
         *   { "role": "assistant", "content": "I'm doing great. How can I help you today?" },
         *   { "role": "user", "content": "I'd like to show off how chat templating works!" },
         * ]
         * 
         * const text = tokenizer.apply_chat_template(chat, { tokenize: false });
         * // "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
         * 
         * const input_ids = tokenizer.apply_chat_template(chat, { tokenize: true, return_tensor: false });
         * // [1, 733, 16289, 28793, 22557, 28725, 910, 460, 368, 28804, 733, 28748, 16289, 28793, 28737, 28742, 28719, 2548, 1598, 28723, 1602, 541, 315, 1316, 368, 3154, 28804, 2, 28705, 733, 16289, 28793, 315, 28742, 28715, 737, 298, 1347, 805, 910, 10706, 5752, 1077, 3791, 28808, 733, 28748, 16289, 28793]
         * ```
         * 
         * @param {Message[]} conversation A list of message objects with `"role"` and `"content"` keys,
         * representing the chat history so far.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.chat_template=null] A Jinja template to use for this conversion. If
         * this is not passed, the model's chat template will be used instead.
         * @param {Object[]} [options.tools=null]
         * A list of tools (callable functions) that will be accessible to the model. If the template does not
         * support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,
         * giving the name, description and argument types for the tool. See our
         * [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)
         * for more information.
         * @param {Record<string, string>[]} [options.documents=null]
         * A list of dicts representing documents that will be accessible to the model if it is performing RAG
         * (retrieval-augmented generation). If the template does not support RAG, this argument will have no
         * effect. We recommend that each document should be a dict containing "title" and "text" keys. Please
         * see the RAG section of the [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#arguments-for-RAG)
         * for examples of passing documents with chat templates.
         * @param {boolean} [options.add_generation_prompt=false] Whether to end the prompt with the token(s) that indicate
         * the start of an assistant message. This is useful when you want to generate a response from the model.
         * Note that this argument will be passed to the chat template, and so it must be supported in the
         * template for this argument to have any effect.
         * @param {boolean} [options.tokenize=true] Whether to tokenize the output. If false, the output will be a string.
         * @param {boolean} [options.padding=false] Whether to pad sequences to the maximum length. Has no effect if tokenize is false.
         * @param {boolean} [options.truncation=false] Whether to truncate sequences to the maximum length. Has no effect if tokenize is false.
         * @param {number} [options.max_length=null] Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is false.
         * If not specified, the tokenizer's `max_length` attribute will be used as a default.
         * @param {boolean} [options.return_tensor=true] Whether to return the output as a Tensor or an Array. Has no effect if tokenize is false.
         * @param {boolean} [options.return_dict=true] Whether to return a dictionary with named outputs. Has no effect if tokenize is false.
         * @param {Object} [options.tokenizer_kwargs={}] Additional options to pass to the tokenizer.
         * @returns {string | Tensor | number[]| number[][]|BatchEncoding} The tokenized output.
         */
        apply_chat_template(Q, {
          tools: ue = null,
          documents: ve = null,
          chat_template: Se = null,
          add_generation_prompt: Qe = !1,
          tokenize: pt = !0,
          padding: gt = !1,
          truncation: ft = !1,
          max_length: xt = null,
          return_tensor: Kt = !0,
          return_dict: ms = !1,
          tokenizer_kwargs: us = {},
          ...Os
        } = {}) {
          if (Se = this.get_chat_template({ chat_template: Se, tools: ue }), typeof Se != "string")
            throw Error(`chat_template must be a string, but got ${typeof Se}`);
          let Bt = this._compiled_template_cache.get(Se);
          Bt === void 0 && (Bt = new v.Template(Se), this._compiled_template_cache.set(Se, Bt));
          const rs = /* @__PURE__ */ Object.create(null);
          for (const Ws of Qr) {
            const ze = this.getToken(Ws);
            ze && (rs[Ws] = ze);
          }
          const rr = Bt.render({
            messages: Q,
            add_generation_prompt: Qe,
            tools: ue,
            documents: ve,
            ...rs,
            ...Os
          });
          if (pt) {
            const Ws = this._call(rr, {
              add_special_tokens: !1,
              padding: gt,
              truncation: ft,
              max_length: xt,
              return_tensor: Kt,
              ...us
            });
            return ms ? Ws : Ws.input_ids;
          }
          return rr;
        }
      }
      class Xr extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class Sr extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class $r extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class Yr extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class dr extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class Jr extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class Ar extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class Br extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class Rr extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class ar extends Nt {
      }
      class it extends Nt {
      }
      class Tt extends Nt {
        constructor(Q, ue) {
          super(Q, ue);
          fe(this, "return_token_type_ids", !0);
          console.warn('WARNING: `XLMTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.');
        }
      }
      class Dt extends Nt {
        constructor() {
          super(...arguments);
          fe(this, "return_token_type_ids", !0);
        }
      }
      class Vs extends Nt {
      }
      class Nr extends Nt {
      }
      class Ir extends Nt {
      }
      class Ms extends Nt {
        constructor(P, Q) {
          super(P, Q), this.languageRegex = /^[a-z]{2}_[A-Z]{2}$/, this.language_codes = this.special_tokens.filter((ue) => this.languageRegex.test(ue)), this.lang_to_token = (ue) => ue;
        }
        /**
         * Helper function to build translation inputs for an `MBartTokenizer`.
         * @param {string|string[]} raw_inputs The text to tokenize.
         * @param {Object} tokenizer_options Options to be sent to the tokenizer
         * @param {Object} generate_kwargs Generation options.
         * @returns {Object} Object to be passed to the model.
         */
        _build_translation_inputs(P, Q, ue) {
          return mr(this, P, Q, ue);
        }
      }
      class lr extends Ms {
      }
      class Fs extends Nt {
      }
      class Pr extends Nt {
      }
      const es = "▁";
      class _n extends Nt {
        constructor(Q, ue) {
          super(Q, ue);
          fe(this, "padding_side", "left");
          this.legacy = ue.legacy ?? !0, this.legacy || (this.normalizer = null, this.pre_tokenizer = new At({
            replacement: es,
            add_prefix_space: !0,
            prepend_scheme: "first"
          }));
        }
        /**
         * Helper function to handle legacy encoding of SPM tokenizers.
         * Adapted from https://github.com/huggingface/transformers/blob/e6dcf8abd6f65bb4b6dfc1831b20d9ba49ce00e2/src/transformers/models/t5/tokenization_t5.py#L374-L387
         * @param {string} text The text to encode.
         * @returns {string[]} The encoded tokens.
         */
        _encode_text(Q) {
          if (Q === null) return null;
          if (this.legacy || Q.length === 0)
            return super._encode_text(Q);
          let ue = super._encode_text(es + Q.replaceAll(es, " "));
          return ue.length > 1 && ue[0] === es && this.special_tokens.includes(ue[1]) && (ue = ue.slice(1)), ue;
        }
      }
      class jr extends Nt {
      }
      class si extends Nt {
      }
      class Sn extends Nt {
      }
      class $n extends Nt {
      }
      class An extends Nt {
      }
      class Ur extends Nt {
      }
      class In extends Nt {
      }
      class ri extends Nt {
      }
      class Vr extends Nt {
      }
      function mr(Pe, P, Q, ue) {
        if (!("language_codes" in Pe) || !Array.isArray(Pe.language_codes))
          throw new Error("Tokenizer must have `language_codes` attribute set and it should be an array of language ids.");
        if (!("languageRegex" in Pe) || !(Pe.languageRegex instanceof RegExp))
          throw new Error("Tokenizer must have `languageRegex` attribute set and it should be a regular expression.");
        if (!("lang_to_token" in Pe) || typeof Pe.lang_to_token != "function")
          throw new Error("Tokenizer must have `lang_to_token` attribute set and it should be a function.");
        const ve = ue.src_lang, Se = ue.tgt_lang;
        if (!Pe.language_codes.includes(Se))
          throw new Error(`Target language code "${Se}" is not valid. Must be one of: {${Pe.language_codes.join(", ")}}`);
        if (ve !== void 0) {
          if (!Pe.language_codes.includes(ve))
            throw new Error(`Source language code "${ve}" is not valid. Must be one of: {${Pe.language_codes.join(", ")}}`);
          for (const Qe of Pe.post_processor.config.single)
            if ("SpecialToken" in Qe && Pe.languageRegex.test(Qe.SpecialToken.id)) {
              Qe.SpecialToken.id = Pe.lang_to_token(ve);
              break;
            }
        }
        return ue.forced_bos_token_id = Pe.model.convert_tokens_to_ids([Pe.lang_to_token(Se)])[0], Pe._call(P, Q);
      }
      class ur extends Nt {
        constructor(P, Q) {
          super(P, Q), this.languageRegex = /^[a-z]{3}_[A-Z][a-z]{3}$/, this.language_codes = this.special_tokens.filter((ue) => this.languageRegex.test(ue)), this.lang_to_token = (ue) => ue;
        }
        /**
         * Helper function to build translation inputs for an `NllbTokenizer`.
         * @param {string|string[]} raw_inputs The text to tokenize.
         * @param {Object} tokenizer_options Options to be sent to the tokenizer
         * @param {Object} generate_kwargs Generation options.
         * @returns {Object} Object to be passed to the model.
         */
        _build_translation_inputs(P, Q, ue) {
          return mr(this, P, Q, ue);
        }
      }
      class gn extends Nt {
        constructor(P, Q) {
          super(P, Q), this.languageRegex = /^__[a-z]{2,3}__$/, this.language_codes = this.special_tokens.filter((ue) => this.languageRegex.test(ue)).map((ue) => ue.slice(2, -2)), this.lang_to_token = (ue) => `__${ue}__`;
        }
        /**
         * Helper function to build translation inputs for an `M2M100Tokenizer`.
         * @param {string|string[]} raw_inputs The text to tokenize.
         * @param {Object} tokenizer_options Options to be sent to the tokenizer
         * @param {Object} generate_kwargs Generation options.
         * @returns {Object} Object to be passed to the model.
         */
        _build_translation_inputs(P, Q, ue) {
          return mr(this, P, Q, ue);
        }
      }
      class Zr extends Nt {
        get timestamp_begin() {
          return this.model.convert_tokens_to_ids(["<|notimestamps|>"])[0] + 1;
        }
        /**
         * Decodes automatic speech recognition (ASR) sequences.
         * @param {Array<{tokens: bigint[], token_timestamps?: number[], stride: number[]}>} sequences The sequences to decode.
         * @param {Object} options The options to use for decoding.
         * @returns {Array<string|{chunks?: undefined|Array<{language: string|null, timestamp: Array<number|null>, text: string}>}>} The decoded sequences.
         */
        _decode_asr(P, {
          return_timestamps: Q = !1,
          return_language: ue = !1,
          time_precision: ve = null,
          force_full_sequences: Se = !0
        } = {}) {
          if (ve === null)
            throw Error("Must specify time_precision");
          let Qe = null;
          const pt = Q === "word";
          function gt() {
            return { language: Qe, timestamp: [null, null], text: "" };
          }
          const ft = [];
          let xt = gt(), Kt = 0;
          const ms = this.timestamp_begin, Os = ms + 1500;
          let Bt = [], rs = [], rr = !1, Ws = null;
          const ze = new Set(this.all_special_ids);
          for (const ks of P) {
            const Xs = ks.tokens, Ft = pt ? ks.token_timestamps : null;
            let ir = null, fr = ms;
            if ("stride" in ks) {
              const [yt, qt, Ls] = ks.stride;
              if (Kt -= qt, Ws = yt - Ls, qt && (fr = qt / ve + ms), Ls)
                for (let $s = Xs.length - 1; $s >= 0; --$s) {
                  const Gs = Number(Xs[$s]);
                  if (Gs >= ms) {
                    if (ir !== null && (Gs - ms) * ve < Ws)
                      break;
                    ir = Gs;
                  }
                }
            }
            let fs = [], Ss = [];
            for (let yt = 0; yt < Xs.length; ++yt) {
              const qt = Number(Xs[yt]);
              if (ze.has(qt)) {
                const Ls = this.decode([qt]), $s = M.WHISPER_LANGUAGE_MAPPING.get(Ls.slice(2, -2));
                if ($s !== void 0) {
                  if (Qe !== null && $s !== Qe && !Q) {
                    Bt.push(fs);
                    const Gs = this.findLongestCommonSequence(Bt)[0], $t = this.decode(Gs);
                    xt.text = $t, ft.push(xt), Bt = [], fs = [], xt = gt();
                  }
                  Qe = xt.language = $s;
                }
              } else if (qt >= ms && qt <= Os) {
                const Ls = (qt - ms) * ve + Kt, $s = (0, Y.round)(Ls, 2);
                if (ir !== null && qt >= ir)
                  rr = !0;
                else if (rr || Bt.length > 0 && qt < fr)
                  rr = !1;
                else if (xt.timestamp[0] === null)
                  xt.timestamp[0] = $s;
                else if ($s !== xt.timestamp[0]) {
                  xt.timestamp[1] = $s, Bt.push(fs), pt && rs.push(Ss);
                  const [Gs, $t] = this.findLongestCommonSequence(
                    Bt,
                    rs
                  ), en = this.decode(Gs);
                  xt.text = en, pt && (xt.words = this.collateWordTimestamps(
                    Gs,
                    $t,
                    Qe
                  )), ft.push(xt), Bt = [], fs = [], rs = [], Ss = [], xt = gt();
                }
              } else if (fs.push(qt), pt) {
                let Ls = (0, Y.round)(Ft[yt] + Kt, 2), $s;
                if (yt + 1 < Ft.length) {
                  $s = (0, Y.round)(Ft[yt + 1] + Kt, 2);
                  const Gs = this.decode([qt]);
                  x.test(Gs) && ($s = (0, Y.round)(Math.min(Ls + ve, $s), 2));
                } else
                  $s = null;
                Ss.push([Ls, $s]);
              }
            }
            if ("stride" in ks) {
              const [yt, qt, Ls] = ks.stride;
              Kt += yt - Ls;
            }
            fs.length > 0 ? (Bt.push(fs), pt && rs.push(Ss)) : Bt.every((yt) => yt.length === 0) && (xt = gt(), Bt = [], fs = [], rs = [], Ss = []);
          }
          if (Bt.length > 0) {
            if (Se && Q)
              throw new Error(
                "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation."
              );
            const [ks, Xs] = this.findLongestCommonSequence(Bt, rs), Ft = this.decode(ks);
            xt.text = Ft, pt && (xt.words = this.collateWordTimestamps(
              ks,
              Xs,
              Qe
            )), ft.push(xt);
          }
          let Js = /* @__PURE__ */ Object.create(null);
          const Fr = ft.map((ks) => ks.text).join("");
          if (Q || ue) {
            for (let ks = 0; ks < ft.length; ++ks) {
              const Xs = ft[ks];
              Q || delete Xs.timestamp, ue || delete Xs.language;
            }
            if (pt) {
              const ks = [];
              for (const Xs of ft)
                for (const Ft of Xs.words)
                  ks.push(Ft);
              Js = { chunks: ks };
            } else
              Js = { chunks: ft };
          }
          return [Fr, Js];
        }
        /**
         * Finds the longest common sequence among the provided sequences.
         * @param {number[][]} sequences An array of sequences of token ids to compare.
         * @returns {number[][]} The longest common sequence found.
         * @throws {Error} If there is a bug within the function.
         * @private
         */
        findLongestCommonSequence(P, Q = null) {
          let ue = P[0], ve = ue.length, Se = [];
          const Qe = Array.isArray(Q) && Q.length > 0;
          let pt = Qe ? [] : null, gt = Qe ? Q[0] : null;
          for (let ft = 1; ft < P.length; ++ft) {
            const xt = P[ft];
            let Kt = 0, ms = [ve, ve, 0, 0];
            const us = xt.length;
            for (let Js = 1; Js < ve + us; ++Js) {
              const Fr = Math.max(0, ve - Js), ks = Math.min(ve, ve + us - Js), Xs = ue.slice(Fr, ks), Ft = Math.max(0, Js - ve), ir = Math.min(us, Js), fr = xt.slice(Ft, ir);
              if (Xs.length !== fr.length)
                throw new Error("There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.");
              let fs;
              Qe ? fs = Xs.filter((qt, Ls) => qt === fr[Ls] && gt[Fr + Ls] <= Q[ft][Ft + Ls]).length : fs = Xs.filter((qt, Ls) => qt === fr[Ls]).length;
              const Ss = Js / 1e4, yt = fs / Js + Ss;
              fs > 1 && yt > Kt && (Kt = yt, ms = [Fr, ks, Ft, ir]);
            }
            const [Os, Bt, rs, rr] = ms, Ws = Math.floor((Bt + Os) / 2), ze = Math.floor((rr + rs) / 2);
            Se.push(...ue.slice(0, Ws)), ue = xt.slice(ze), ve = ue.length, Qe && (pt.push(...gt.slice(0, Ws)), gt = Q[ft].slice(ze));
          }
          return Se.push(...ue), Qe ? (pt.push(...gt), [Se, pt]) : [Se, []];
        }
        /** @private */
        collateWordTimestamps(P, Q, ue) {
          const [ve, Se, Qe] = this.combineTokensIntoWords(P, ue), pt = [];
          for (let gt = 0; gt < ve.length; ++gt) {
            const ft = Qe[gt];
            pt.push({
              text: ve[gt],
              timestamp: [
                Q[ft.at(0)][0],
                Q[ft.at(-1)][1]
              ]
            });
          }
          return pt;
        }
        /**
         * Groups tokens by word. Returns a tuple containing a list of strings with the words,
         * and a list of `token_id` sequences with the tokens making up each word.
         * @param {number[]} tokens 
         * @param {string} [language] 
         * @param {string} prepend_punctionations 
         * @param {string} append_punctuations 
         * 
         * @private
         */
        combineTokensIntoWords(P, Q, ue = `"'“¡¿([{-`, ve = `"'.。,，!！?？:：”)]}、`) {
          Q = Q ?? "english";
          let Se, Qe, pt;
          return ["chinese", "japanese", "thai", "lao", "myanmar"].includes(Q) ? [Se, Qe, pt] = this.splitTokensOnUnicode(P) : [Se, Qe, pt] = this.splitTokensOnSpaces(P), this.mergePunctuations(Se, Qe, pt, ue, ve);
        }
        /** @type {PreTrainedTokenizer['decode']} */
        decode(P, Q) {
          let ue;
          return Q != null && Q.decode_with_timestamps ? (P instanceof R.Tensor && (P = se(P)), ue = this.decodeWithTimestamps(P, Q)) : ue = super.decode(P, Q), ue;
        }
        /**
         * @param {number[]|bigint[]} token_ids List of token IDs to decode.
         * @param {Object} decode_args Optional arguments for decoding
         * @private
         */
        decodeWithTimestamps(P, Q) {
          const ue = (Q == null ? void 0 : Q.time_precision) ?? 0.02, ve = Array.from(this.all_special_ids).at(-1) + 1;
          let Se = [[]];
          for (let Qe of P)
            if (Qe = Number(Qe), Qe >= ve) {
              const pt = ((Qe - ve) * ue).toFixed(2);
              Se.push(`<|${pt}|>`), Se.push([]);
            } else
              Se[Se.length - 1].push(Qe);
          return Se = Se.map(
            (Qe) => typeof Qe == "string" ? Qe : super.decode(Qe, Q)
          ), Se.join("");
        }
        /**
         * Combine tokens into words by splitting at any position where the tokens are decoded as valid unicode points.
         * @param {number[]} tokens 
         * @returns {*}
         * @private
         */
        splitTokensOnUnicode(P) {
          const Q = this.decode(P, {
            // @ts-ignore
            decode_with_timestamps: !0
          }), ue = "�", ve = [], Se = [], Qe = [];
          let pt = [], gt = [], ft = 0;
          for (let xt = 0; xt < P.length; ++xt) {
            const Kt = P[xt];
            pt.push(Kt), gt.push(xt);
            const ms = this.decode(pt, {
              // @ts-ignore
              decode_with_timestamps: !0
            });
            (!ms.includes(ue) || Q[ft + ms.indexOf(ue)] === ue) && (ve.push(ms), Se.push(pt), Qe.push(gt), pt = [], gt = [], ft += ms.length);
          }
          return [ve, Se, Qe];
        }
        /**
         * Combine tokens into words by splitting at whitespace and punctuation tokens.
         * @param {number[]} tokens 
         * @private
         */
        splitTokensOnSpaces(P) {
          const [Q, ue, ve] = this.splitTokensOnUnicode(P), Se = [], Qe = [], pt = [], gt = new RegExp(`^[${w}]$`, "gu");
          for (let ft = 0; ft < Q.length; ++ft) {
            const xt = Q[ft], Kt = ue[ft], ms = ve[ft], us = Kt[0] >= this.model.tokens_to_ids.get("<|endoftext|>"), Os = xt.startsWith(" "), Bt = xt.trim(), rs = gt.test(Bt);
            if (us || Os || rs || Se.length === 0)
              Se.push(xt), Qe.push(Kt), pt.push(ms);
            else {
              const rr = Se.length - 1;
              Se[rr] += xt, Qe[rr].push(...Kt), pt[rr].push(...ms);
            }
          }
          return [Se, Qe, pt];
        }
        /**
         * Merges punctuation tokens with neighboring words.
         * @param {string[]} words 
         * @param {number[][]} tokens 
         * @param {number[][]} indices 
         * @param {string} prepended 
         * @param {string} appended 
         * @private
         */
        mergePunctuations(P, Q, ue, ve, Se) {
          const Qe = structuredClone(P), pt = structuredClone(Q), gt = structuredClone(ue);
          let ft = Qe.length - 2, xt = Qe.length - 1;
          for (; ft >= 0; )
            Qe[ft].startsWith(" ") && ve.includes(Qe[ft].trim()) ? (Qe[xt] = Qe[ft] + Qe[xt], pt[xt] = (0, D.mergeArrays)(pt[ft], pt[xt]), gt[xt] = (0, D.mergeArrays)(gt[ft], gt[xt]), Qe[ft] = "", pt[ft] = [], gt[ft] = []) : xt = ft, --ft;
          for (ft = 0, xt = 1; xt < Qe.length; )
            !Qe[ft].endsWith(" ") && Se.includes(Qe[xt]) ? (Qe[ft] += Qe[xt], pt[ft] = (0, D.mergeArrays)(pt[ft], pt[xt]), gt[ft] = (0, D.mergeArrays)(gt[ft], gt[xt]), Qe[xt] = "", pt[xt] = [], gt[xt] = []) : ft = xt, ++xt;
          return [
            Qe.filter((Kt) => Kt),
            pt.filter((Kt) => Kt.length > 0),
            gt.filter((Kt) => Kt.length > 0)
          ];
        }
      }
      class wn extends Nt {
      }
      class yn extends Nt {
      }
      class Mn extends Nt {
      }
      class zt extends Nt {
        /**
         * Create a new MarianTokenizer instance.
         * @param {Object} tokenizerJSON The JSON of the tokenizer.
         * @param {Object} tokenizerConfig The config of the tokenizer.
         */
        constructor(P, Q) {
          super(P, Q), this.languageRegex = /^(>>\w+<<)\s*/g, this.supported_language_codes = this.model.vocab.filter(
            (ue) => this.languageRegex.test(ue)
          ), console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.');
        }
        /**
         * Encodes a single text. Overriding this method is necessary since the language codes
         * must be removed before encoding with sentencepiece model.
         * @see https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213
         *
         * @param {string|null} text The text to encode.
         * @returns {Array} The encoded tokens.
         */
        _encode_text(P) {
          if (P === null) return null;
          const [Q, ...ue] = P.trim().split(this.languageRegex);
          if (ue.length === 0)
            return super._encode_text(Q);
          if (ue.length === 2) {
            const [ve, Se] = ue;
            return this.supported_language_codes.includes(ve) || console.warn(`Unsupported language code "${ve}" detected, which may lead to unexpected behavior. Should be one of: ${JSON.stringify(this.supported_language_codes)}`), (0, D.mergeArrays)([ve], super._encode_text(Se));
          }
        }
      }
      class bn extends Nt {
      }
      class Fn extends Nt {
      }
      class On extends Nt {
      }
      class Dn extends Nt {
      }
      class Wr extends Nt {
      }
      class Ln extends Nt {
        constructor(P, Q) {
          super(P, Q), this.decoder = new kt({});
        }
      }
      class vn extends Nt {
      }
      class zn extends Nt {
      }
      class as {
        /**
         * Instantiate one of the tokenizer classes of the library from a pretrained model.
         * 
         * The tokenizer class to instantiate is selected based on the `tokenizer_class` property of the config object
         * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained tokenizer hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing tokenizer files, e.g., `./my_model_directory/`.
         * @param {PretrainedTokenizerOptions} options Additional options for loading the tokenizer.
         * 
         * @returns {Promise<PreTrainedTokenizer>} A new instance of the PreTrainedTokenizer class.
         */
        static async from_pretrained(P, {
          progress_callback: Q = null,
          config: ue = null,
          cache_dir: ve = null,
          local_files_only: Se = !1,
          revision: Qe = "main",
          legacy: pt = null
        } = {}) {
          var ms;
          const [gt, ft] = await y(P, {
            progress_callback: Q,
            config: ue,
            cache_dir: ve,
            local_files_only: Se,
            revision: Qe,
            legacy: pt
          }), xt = ((ms = ft.tokenizer_class) == null ? void 0 : ms.replace(/Fast$/, "")) ?? "PreTrainedTokenizer";
          let Kt = this.TOKENIZER_CLASS_MAPPING[xt];
          return Kt || (console.warn(`Unknown tokenizer class "${xt}", attempting to construct from base class.`), Kt = Nt), new Kt(gt, ft);
        }
      }
      fe(as, "TOKENIZER_CLASS_MAPPING", {
        T5Tokenizer: Vs,
        DistilBertTokenizer: ar,
        CamembertTokenizer: it,
        DebertaTokenizer: dr,
        DebertaV2Tokenizer: Jr,
        BertTokenizer: Xr,
        HerbertTokenizer: Ar,
        ConvBertTokenizer: Br,
        RoFormerTokenizer: Rr,
        XLMTokenizer: Tt,
        ElectraTokenizer: Dt,
        MobileBertTokenizer: $r,
        SqueezeBertTokenizer: Yr,
        AlbertTokenizer: Sr,
        GPT2Tokenizer: Nr,
        BartTokenizer: Ir,
        MBartTokenizer: Ms,
        MBart50Tokenizer: lr,
        RobertaTokenizer: Fs,
        WhisperTokenizer: Zr,
        CodeGenTokenizer: wn,
        CLIPTokenizer: yn,
        SiglipTokenizer: Mn,
        MarianTokenizer: zt,
        BloomTokenizer: Pr,
        NllbTokenizer: ur,
        M2M100Tokenizer: gn,
        LlamaTokenizer: _n,
        CodeLlamaTokenizer: jr,
        XLMRobertaTokenizer: si,
        MPNetTokenizer: Sn,
        FalconTokenizer: $n,
        GPTNeoXTokenizer: An,
        EsmTokenizer: Ur,
        Wav2Vec2CTCTokenizer: bn,
        BlenderbotTokenizer: Fn,
        BlenderbotSmallTokenizer: On,
        SpeechT5Tokenizer: Dn,
        NougatTokenizer: Wr,
        VitsTokenizer: Ln,
        Qwen2Tokenizer: In,
        GemmaTokenizer: ri,
        Grok1Tokenizer: Vr,
        CohereTokenizer: vn,
        MgpstrTokenizer: zn,
        // Base case:
        PreTrainedTokenizer: Nt
      });
    }
  ),
  /***/
  "./src/utils/audio.js": (
    /*!****************************!*\
      !*** ./src/utils/audio.js ***!
      \****************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        RawAudio: () => (
          /* binding */
          _e
        ),
        /* harmony export */
        hamming: () => (
          /* binding */
          b
        ),
        /* harmony export */
        hanning: () => (
          /* binding */
          y
        ),
        /* harmony export */
        mel_filter_bank: () => (
          /* binding */
          q
        ),
        /* harmony export */
        read_audio: () => (
          /* binding */
          v
        ),
        /* harmony export */
        spectrogram: () => (
          /* binding */
          F
        ),
        /* harmony export */
        window_function: () => (
          /* binding */
          le
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./hub.js */
        "./src/utils/hub.js"
      ), D = r(
        /*! ./maths.js */
        "./src/utils/maths.js"
      ), U = r(
        /*! ./core.js */
        "./src/utils/core.js"
      ), Y = r(
        /*! ../env.js */
        "./src/env.js"
      ), R = r(
        /*! fs */
        "?7a2c"
      ), g = r(
        /*! ./tensor.js */
        "./src/utils/tensor.js"
      );
      async function v(re, xe) {
        if (typeof AudioContext > "u")
          throw Error(
            "Unable to load audio from path/URL since `AudioContext` is not available in your environment. Instead, audio data should be passed directly to the pipeline/processor. For more information and some example code, see https://huggingface.co/docs/transformers.js/guides/node-audio-processing."
          );
        const ce = await (await (0, _.getFile)(re)).arrayBuffer(), ke = new AudioContext({ sampleRate: xe });
        typeof xe > "u" && console.warn(`No sampling rate provided, using default of ${ke.sampleRate}Hz.`);
        const Fe = await ke.decodeAudioData(ce);
        let Ee;
        if (Fe.numberOfChannels === 2) {
          const tt = Math.sqrt(2), Ge = Fe.getChannelData(0), ye = Fe.getChannelData(1);
          Ee = new Float32Array(Ge.length);
          for (let J = 0; J < Fe.length; ++J)
            Ee[J] = tt * (Ge[J] + ye[J]) / 2;
        } else
          Ee = Fe.getChannelData(0);
        return Ee;
      }
      function M(re, xe) {
        if (re < 1)
          return new Float64Array();
        if (re === 1)
          return new Float64Array([1]);
        const ce = 1 - xe, ke = 2 * Math.PI / (re - 1), Fe = new Float64Array(re);
        for (let Ee = 0; Ee < re; ++Ee)
          Fe[Ee] = xe - ce * Math.cos(Ee * ke);
        return Fe;
      }
      function y(re) {
        return M(re, 0.5);
      }
      function b(re) {
        return M(re, 0.54);
      }
      const I = {
        htk: (re) => 2595 * Math.log10(1 + re / 700),
        kaldi: (re) => 1127 * Math.log(1 + re / 700),
        slaney: (re, xe = 1e3, ce = 15, ke = 27 / Math.log(6.4)) => re >= xe ? ce + Math.log(re / xe) * ke : 3 * re / 200
      };
      function K(re, xe = "htk") {
        const ce = I[xe];
        if (!ce)
          throw new Error('mel_scale should be one of "htk", "slaney" or "kaldi".');
        return typeof re == "number" ? ce(re) : re.map((ke) => ce(ke));
      }
      const se = {
        htk: (re) => 700 * (10 ** (re / 2595) - 1),
        kaldi: (re) => 700 * (Math.exp(re / 1127) - 1),
        slaney: (re, xe = 1e3, ce = 15, ke = Math.log(6.4) / 27) => re >= ce ? xe * Math.exp(ke * (re - ce)) : 200 * re / 3
      };
      function ie(re, xe = "htk") {
        const ce = se[xe];
        if (!ce)
          throw new Error('mel_scale should be one of "htk", "slaney" or "kaldi".');
        return typeof re == "number" ? ce(re) : re.map((ke) => ce(ke));
      }
      function W(re, xe) {
        const ce = Float64Array.from(
          { length: xe.length - 1 },
          (tt, Ge) => xe[Ge + 1] - xe[Ge]
        ), ke = Array.from({
          length: re.length
        }, () => new Array(xe.length));
        for (let tt = 0; tt < re.length; ++tt) {
          const Ge = ke[tt];
          for (let ye = 0; ye < xe.length; ++ye)
            Ge[ye] = xe[ye] - re[tt];
        }
        const Fe = xe.length - 2, Ee = Array.from({ length: Fe }, () => new Array(re.length));
        for (let tt = 0; tt < re.length; ++tt) {
          const Ge = ke[tt];
          for (let ye = 0; ye < Fe; ++ye) {
            const J = -Ge[ye] / ce[ye], de = Ge[ye + 2] / ce[ye + 1];
            Ee[ye][tt] = Math.max(0, Math.min(J, de));
          }
        }
        return Ee;
      }
      function j(re, xe, ce) {
        const ke = (xe - re) / (ce - 1);
        return Float64Array.from({ length: ce }, (Fe, Ee) => re + ke * Ee);
      }
      function q(re, xe, ce, ke, Fe, Ee = null, tt = "htk", Ge = !1) {
        if (Ee !== null && Ee !== "slaney")
          throw new Error('norm must be one of null or "slaney"');
        const ye = K(ce, tt), J = K(ke, tt), de = j(ye, J, xe + 2);
        let Ce = ie(de, tt), Be;
        if (Ge) {
          const te = Fe / (re * 2);
          Be = K(Float64Array.from({ length: re }, (Ke, je) => je * te), tt), Ce = de;
        } else
          Be = j(0, Math.floor(Fe / 2), re);
        const Ze = W(Be, Ce);
        if (Ee !== null && Ee === "slaney")
          for (let te = 0; te < xe; ++te) {
            const Ke = Ze[te], je = 2 / (Ce[te + 2] - Ce[te]);
            for (let ae = 0; ae < re; ++ae)
              Ke[ae] *= je;
          }
        return Ze;
      }
      function A(re, xe, ce) {
        const ke = new re.constructor(re.length + xe + ce), Fe = re.length - 1;
        for (let Ee = 0; Ee < re.length; ++Ee)
          ke[xe + Ee] = re[Ee];
        for (let Ee = 1; Ee <= xe; ++Ee)
          ke[xe - Ee] = re[(0, U.calculateReflectOffset)(Ee, Fe)];
        for (let Ee = 1; Ee <= ce; ++Ee)
          ke[Fe + xe + Ee] = re[(0, U.calculateReflectOffset)(Fe - Ee, Fe)];
        return ke;
      }
      function S(re, xe, ce, ke, Fe) {
        if (ce <= 0)
          throw new Error("reference must be greater than zero");
        if (ke <= 0)
          throw new Error("min_value must be greater than zero");
        ce = Math.max(ke, ce);
        const Ee = Math.log10(ce);
        for (let tt = 0; tt < re.length; ++tt)
          re[tt] = xe * Math.log10(Math.max(ke, re[tt]) - Ee);
        if (Fe !== null) {
          if (Fe <= 0)
            throw new Error("db_range must be greater than zero");
          const tt = (0, D.max)(re)[0] - Fe;
          for (let Ge = 0; Ge < re.length; ++Ge)
            re[Ge] = Math.max(re[Ge], tt);
        }
        return re;
      }
      function w(re, xe = 1, ce = 1e-5, ke = null) {
        return S(re, 20, xe, ce, ke);
      }
      function x(re, xe = 1, ce = 1e-10, ke = null) {
        return S(re, 10, xe, ce, ke);
      }
      async function F(re, xe, ce, ke, {
        fft_length: Fe = null,
        power: Ee = 1,
        center: tt = !0,
        pad_mode: Ge = "reflect",
        onesided: ye = !0,
        preemphasis: J = null,
        mel_filters: de = null,
        mel_floor: Ce = 1e-10,
        log_mel: Be = null,
        reference: Ze = 1,
        min_value: te = 1e-10,
        db_range: Ke = null,
        remove_dc_offset: je = null,
        // Custom parameters for efficiency reasons
        min_num_frames: ae = null,
        max_num_frames: Te = null,
        do_pad: Ue = !0,
        transpose: Ve = !1
      } = {}) {
        const Ne = xe.length;
        if (Fe === null && (Fe = ce), ce > Fe)
          throw Error(`frame_length (${ce}) may not be larger than fft_length (${Fe})`);
        if (Ne !== ce)
          throw new Error(`Length of the window (${Ne}) must equal frame_length (${ce})`);
        if (ke <= 0)
          throw new Error("hop_length must be greater than zero");
        if (Ee === null && de !== null)
          throw new Error(
            "You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram. Specify `power` to fix this issue."
          );
        if (tt) {
          if (Ge !== "reflect")
            throw new Error(`pad_mode="${Ge}" not implemented yet.`);
          const We = Math.floor((Fe - 1) / 2) + 1;
          re = A(re, We, We);
        }
        let Re = Math.floor(1 + Math.floor((re.length - ce) / ke));
        ae !== null && Re < ae && (Re = ae);
        const st = ye ? Math.floor(Fe / 2) + 1 : Fe;
        let dt = Re, ct = Re;
        Te !== null && (Te > Re ? Ue && (ct = Te) : ct = dt = Te);
        const lt = new D.FFT(Fe), ht = new Float64Array(Fe), L = new Float64Array(lt.outputBufferSize), oe = new Float32Array(st * ct);
        for (let We = 0; We < dt; ++We) {
          const Je = We * ke, ut = Math.min(re.length - Je, ce);
          ut !== ce && ht.fill(0, 0, ce);
          for (let mt = 0; mt < ut; ++mt)
            ht[mt] = re[Je + mt];
          if (je) {
            let mt = 0;
            for (let kt = 0; kt < ut; ++kt)
              mt += ht[kt];
            const vt = mt / ut;
            for (let kt = 0; kt < ut; ++kt)
              ht[kt] -= vt;
          }
          if (J !== null) {
            for (let mt = ut - 1; mt >= 1; --mt)
              ht[mt] -= J * ht[mt - 1];
            ht[0] *= 1 - J;
          }
          for (let mt = 0; mt < xe.length; ++mt)
            ht[mt] *= xe[mt];
          lt.realTransform(L, ht);
          for (let mt = 0; mt < st; ++mt) {
            const vt = mt << 1;
            oe[mt * ct + We] = L[vt] ** 2 + L[vt + 1] ** 2;
          }
        }
        if (Ee !== null && Ee !== 2) {
          const We = 2 / Ee;
          for (let Je = 0; Je < oe.length; ++Je)
            oe[Je] **= We;
        }
        const H = de.length;
        let me = await (0, g.matmul)(
          // TODO: Make `mel_filters` a Tensor during initialization
          new g.Tensor("float32", de.flat(), [H, st]),
          new g.Tensor("float32", oe, [st, ct])
        );
        Ve && (me = me.transpose(1, 0));
        const Ae = (
          /** @type {Float32Array} */
          me.data
        );
        for (let We = 0; We < Ae.length; ++We)
          Ae[We] = Math.max(Ce, Ae[We]);
        if (Ee !== null && Be !== null) {
          const We = Math.min(Ae.length, dt * H);
          switch (Be) {
            case "log":
              for (let Je = 0; Je < We; ++Je)
                Ae[Je] = Math.log(Ae[Je]);
              break;
            case "log10":
              for (let Je = 0; Je < We; ++Je)
                Ae[Je] = Math.log10(Ae[Je]);
              break;
            case "dB":
              if (Ee === 1)
                w(Ae, Ze, te, Ke);
              else if (Ee === 2)
                x(Ae, Ze, te, Ke);
              else
                throw new Error(`Cannot use log_mel option '${Be}' with power ${Ee}`);
              break;
            default:
              throw new Error(`log_mel must be one of null, 'log', 'log10' or 'dB'. Got '${Be}'`);
          }
        }
        return me;
      }
      function le(re, xe, {
        periodic: ce = !0,
        frame_length: ke = null,
        center: Fe = !0
      } = {}) {
        const Ee = ce ? re + 1 : re;
        let tt;
        switch (xe) {
          case "boxcar":
            tt = new Float64Array(Ee).fill(1);
            break;
          case "hann":
          case "hann_window":
            tt = y(Ee);
            break;
          case "hamming":
            tt = b(Ee);
            break;
          case "povey":
            tt = y(Ee).map((Ge) => Math.pow(Ge, 0.85));
            break;
          default:
            throw new Error(`Unknown window type ${xe}.`);
        }
        if (ce && (tt = tt.subarray(0, re)), ke === null)
          return tt;
        if (re > ke)
          throw new Error(`Length of the window (${re}) may not be larger than frame_length (${ke})`);
        return tt;
      }
      function ne(re, xe) {
        let ce = 44;
        const ke = new ArrayBuffer(ce + re.length * 4), Fe = new DataView(ke);
        be(Fe, 0, "RIFF"), Fe.setUint32(4, 36 + re.length * 4, !0), be(Fe, 8, "WAVE"), be(Fe, 12, "fmt "), Fe.setUint32(16, 16, !0), Fe.setUint16(20, 3, !0), Fe.setUint16(22, 1, !0), Fe.setUint32(24, xe, !0), Fe.setUint32(28, xe * 4, !0), Fe.setUint16(32, 4, !0), Fe.setUint16(34, 32, !0), be(Fe, 36, "data"), Fe.setUint32(40, re.length * 4, !0);
        for (let Ee = 0; Ee < re.length; ++Ee, ce += 4)
          Fe.setFloat32(ce, re[Ee], !0);
        return ke;
      }
      function be(re, xe, ce) {
        for (let ke = 0; ke < ce.length; ++ke)
          re.setUint8(xe + ke, ce.charCodeAt(ke));
      }
      class _e {
        /**
         * Create a new `RawAudio` object.
         * @param {Float32Array} audio Audio data
         * @param {number} sampling_rate Sampling rate of the audio data
         */
        constructor(xe, ce) {
          this.audio = xe, this.sampling_rate = ce;
        }
        /**
         * Convert the audio to a wav file buffer.
         * @returns {ArrayBuffer} The WAV file.
         */
        toWav() {
          return ne(this.audio, this.sampling_rate);
        }
        /**
         * Convert the audio to a blob.
         * @returns {Blob}
         */
        toBlob() {
          const xe = this.toWav();
          return new Blob([xe], { type: "audio/wav" });
        }
        /**
         * Save the audio to a wav file.
         * @param {string} path
         */
        async save(xe) {
          let ce;
          if (Y.apis.IS_BROWSER_ENV) {
            if (Y.apis.IS_WEBWORKER_ENV)
              throw new Error("Unable to save a file from a Web Worker.");
            ce = U.saveBlob;
          } else if (Y.apis.IS_FS_AVAILABLE)
            ce = async (ke, Fe) => {
              let Ee = await Fe.arrayBuffer();
              R.writeFileSync(ke, Buffer.from(Ee));
            };
          else
            throw new Error("Unable to save because filesystem is disabled in this environment.");
          await ce(xe, this.toBlob());
        }
      }
    }
  ),
  /***/
  "./src/utils/constants.js": (
    /*!********************************!*\
      !*** ./src/utils/constants.js ***!
      \********************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        CHAT_TEMPLATE_NAME: () => (
          /* binding */
          g
        ),
        /* harmony export */
        CONFIG_NAME: () => (
          /* binding */
          D
        ),
        /* harmony export */
        FEATURE_EXTRACTOR_NAME: () => (
          /* binding */
          U
        ),
        /* harmony export */
        GENERATION_CONFIG_NAME: () => (
          /* binding */
          v
        ),
        /* harmony export */
        GITHUB_ISSUE_URL: () => (
          /* binding */
          _
        ),
        /* harmony export */
        IMAGE_PROCESSOR_NAME: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        PROCESSOR_NAME: () => (
          /* binding */
          R
        )
        /* harmony export */
      });
      const _ = "https://github.com/huggingface/transformers.js/issues/new/choose", D = "config.json", U = "preprocessor_config.json", Y = U, R = "processor_config.json", g = "chat_template.json", v = "generation_config.json";
    }
  ),
  /***/
  "./src/utils/core.js": (
    /*!***************************!*\
      !*** ./src/utils/core.js ***!
      \***************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        calculateDimensions: () => (
          /* binding */
          v
        ),
        /* harmony export */
        calculateReflectOffset: () => (
          /* binding */
          I
        ),
        /* harmony export */
        count: () => (
          /* binding */
          W
        ),
        /* harmony export */
        dispatchCallback: () => (
          /* binding */
          _
        ),
        /* harmony export */
        escapeRegExp: () => (
          /* binding */
          U
        ),
        /* harmony export */
        isIntegralNumber: () => (
          /* binding */
          R
        ),
        /* harmony export */
        isNullishDimension: () => (
          /* binding */
          g
        ),
        /* harmony export */
        isTypedArray: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        len: () => (
          /* binding */
          ie
        ),
        /* harmony export */
        mergeArrays: () => (
          /* binding */
          y
        ),
        /* harmony export */
        pick: () => (
          /* binding */
          se
        ),
        /* harmony export */
        pop: () => (
          /* binding */
          M
        ),
        /* harmony export */
        product: () => (
          /* binding */
          b
        ),
        /* harmony export */
        reverseDictionary: () => (
          /* binding */
          D
        ),
        /* harmony export */
        saveBlob: () => (
          /* binding */
          K
        )
        /* harmony export */
      });
      function _(j, q) {
        j && j(q);
      }
      function D(j) {
        return Object.fromEntries(Object.entries(j).map(([q, A]) => [A, q]));
      }
      function U(j) {
        return j.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
      }
      function Y(j) {
        var q, A, S;
        return ((S = (A = (q = j == null ? void 0 : j.prototype) == null ? void 0 : q.__proto__) == null ? void 0 : A.constructor) == null ? void 0 : S.name) === "TypedArray";
      }
      function R(j) {
        return Number.isInteger(j) || typeof j == "bigint";
      }
      function g(j) {
        return j == null || j === -1;
      }
      function v(j) {
        const q = [];
        let A = j;
        for (; Array.isArray(A); )
          q.push(A.length), A = A[0];
        return q;
      }
      function M(j, q, A = void 0) {
        const S = j[q];
        if (S !== void 0)
          return delete j[q], S;
        if (A === void 0)
          throw Error(`Key ${q} does not exist in object.`);
        return A;
      }
      function y(...j) {
        return Array.prototype.concat.apply([], j);
      }
      function b(...j) {
        return j.reduce((q, A) => q.flatMap((S) => A.map((w) => [S, w])));
      }
      function I(j, q) {
        return Math.abs((j + q) % (2 * q) - q);
      }
      function K(j, q) {
        const A = URL.createObjectURL(q), S = document.createElement("a");
        S.href = A, S.download = j, S.click(), S.remove(), URL.revokeObjectURL(A);
      }
      function se(j, q) {
        return Object.assign(
          {},
          ...q.map((A) => {
            if (j[A] !== void 0)
              return { [A]: j[A] };
          })
        );
      }
      function ie(j) {
        let q = 0;
        for (const A of j) ++q;
        return q;
      }
      function W(j, q) {
        let A = 0;
        for (const S of j)
          S === q && ++A;
        return A;
      }
    }
  ),
  /***/
  "./src/utils/data-structures.js": (
    /*!**************************************!*\
      !*** ./src/utils/data-structures.js ***!
      \**************************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        CharTrie: () => (
          /* binding */
          D
        ),
        /* harmony export */
        PriorityQueue: () => (
          /* binding */
          _
        ),
        /* harmony export */
        TokenLattice: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      class _ {
        /**
         * Create a new PriorityQueue.
         * @param {function(any, any): boolean} comparator Comparator function to determine priority. Defaults to a MaxHeap.
         */
        constructor(v = (y, b) => y > b, M = 1 / 0) {
          this._heap = [], this._comparator = v, this._maxSize = M;
        }
        /**
         * The size of the queue
         */
        get size() {
          return this._heap.length;
        }
        /**
         * Check if the queue is empty.
         * @returns {boolean} `true` if the queue is empty, `false` otherwise.
         */
        isEmpty() {
          return this.size === 0;
        }
        /**
         * Return the element with the highest priority in the queue.
         * @returns {any} The highest priority element in the queue.
         */
        peek() {
          return this._heap[0];
        }
        /**
         * Add one or more elements to the queue.
         * @param  {...any} values The values to push into the queue.
         * @returns {number} The new size of the queue.
         */
        push(...v) {
          return this.extend(v);
        }
        /**
         * Add multiple elements to the queue.
         * @param {any[]} values The values to push into the queue.
         * @returns {number} The new size of the queue.
         */
        extend(v) {
          for (const M of v)
            if (this.size < this._maxSize)
              this._heap.push(M), this._siftUp();
            else {
              const y = this._smallest();
              this._comparator(M, this._heap[y]) && (this._heap[y] = M, this._siftUpFrom(y));
            }
          return this.size;
        }
        /**
         * Remove and return the element with the highest priority in the queue.
         * @returns {any} The element with the highest priority in the queue.
         */
        pop() {
          const v = this.peek(), M = this.size - 1;
          return M > 0 && this._swap(0, M), this._heap.pop(), this._siftDown(), v;
        }
        /**
         * Replace the element with the highest priority in the queue with a new value.
         * @param {*} value The new value.
         * @returns {*} The replaced value.
         */
        replace(v) {
          const M = this.peek();
          return this._heap[0] = v, this._siftDown(), M;
        }
        /**
         * Compute the index for the parent of the node at index `i`.
         * @param {number} i The index of the node to get the parent of.
         * @returns {number} The index of the parent node.
         * @private
         */
        _parent(v) {
          return (v + 1 >>> 1) - 1;
        }
        /**
         * Compute the index for the left child of the node at index `i`.
         * @param {number} i The index of the node to get the left child of.
         * @returns {number} The index of the left child.
         * @private
         */
        _left(v) {
          return (v << 1) + 1;
        }
        /**
         * Compute the index for the right child of the node at index `i`.
         * @param {number} i The index of the node to get the right child of.
         * @returns {number} The index of the right child.
         * @private
         */
        _right(v) {
          return v + 1 << 1;
        }
        /**
         * Check if the element at index `i` is greater than the element at index `j`.
         * @param {number} i The index of the first element to compare.
         * @param {number} j The index of the second element to compare.
         * @returns {boolean} `true` if the element at index `i` is greater than the element at index `j`, `false` otherwise.
         * @private
         */
        _greater(v, M) {
          return this._comparator(this._heap[v], this._heap[M]);
        }
        /**
         * Swap the elements at indices `i` and `j`.
         * @param {number} i The index of the first element to swap.
         * @param {number} j The index of the second element to swap.
         * @private
         */
        _swap(v, M) {
          const y = this._heap[v];
          this._heap[v] = this._heap[M], this._heap[M] = y;
        }
        /**
         * Maintain the heap property by updating positions in the heap,
         * starting at the last element and moving up the heap.
         * @private
         */
        _siftUp() {
          this._siftUpFrom(this.size - 1);
        }
        /**
         * Helper function to sift up from a given node.
         * @param {number} node The index of the node to start sifting up from.
         */
        _siftUpFrom(v) {
          for (; v > 0 && this._greater(v, this._parent(v)); )
            this._swap(v, this._parent(v)), v = this._parent(v);
        }
        /**
         * Maintain the heap property by updating positions in the heap,
         * starting at the first element and moving down the heap.
         * @private
         */
        _siftDown() {
          let v = 0;
          for (; this._left(v) < this.size && this._greater(this._left(v), v) || this._right(v) < this.size && this._greater(this._right(v), v); ) {
            const M = this._right(v) < this.size && this._greater(this._right(v), this._left(v)) ? this._right(v) : this._left(v);
            this._swap(v, M), v = M;
          }
        }
        /**
         * Get the index of the smallest element in the heap. Since we use an array-based heap,
         * the index can be computed without needing to traverse the heap.
         * @private
         */
        _smallest() {
          return 2 ** Math.floor(Math.log2(this.size)) - 1;
        }
      }
      class D {
        constructor() {
          this.root = U.default();
        }
        /**
         * Adds one or more `texts` to the trie.
         * @param {string[]} texts The strings to add to the trie.
         */
        extend(v) {
          for (const M of v)
            this.push(M);
        }
        /**
         * Adds text to the trie.
         * @param {string} text The string to add to the trie.
         */
        push(v) {
          let M = this.root;
          for (const y of v) {
            let b = M.children.get(y);
            b === void 0 && (b = U.default(), M.children.set(y, b)), M = b;
          }
          M.isLeaf = !0;
        }
        /**
         * Searches the trie for all strings with a common prefix of `text`.
         * @param {string} text The common prefix to search for.
         * @yields {string} Each string in the trie that has `text` as a prefix.
         */
        *commonPrefixSearch(v) {
          let M = this.root;
          if (M === void 0) return;
          let y = "";
          for (const b of v) {
            if (y += b, M = M.children.get(b), M === void 0) return;
            M.isLeaf && (yield y);
          }
        }
      }
      class U {
        /**
         * Create a new CharTrieNode.
         * @param {boolean} isLeaf Whether the node is a leaf node or not.
         * @param {Map<string, CharTrieNode>} children A map containing the node's children, where the key is a character and the value is a `CharTrieNode`.
         */
        constructor(v, M) {
          this.isLeaf = v, this.children = M;
        }
        /**
         * Returns a new `CharTrieNode` instance with default values.
         * @returns {CharTrieNode} A new `CharTrieNode` instance with `isLeaf` set to `false` and an empty `children` map.
         */
        static default() {
          return new U(!1, /* @__PURE__ */ new Map());
        }
      }
      class Y {
        /**
         * Creates a new TokenLattice instance.
         *
         * @param {string} sentence The input sentence to be tokenized.
         * @param {number} bosTokenId The beginning-of-sequence token ID.
         * @param {number} eosTokenId The end-of-sequence token ID.
         */
        constructor(v, M, y) {
          this.chars = Array.from(v), this.len = this.chars.length, this.bosTokenId = M, this.eosTokenId = y, this.nodes = [], this.beginNodes = Array.from({ length: this.len + 1 }, () => []), this.endNodes = Array.from({ length: this.len + 1 }, () => []);
          const b = new R(this.bosTokenId, 0, 0, 0, 0), I = new R(this.eosTokenId, 1, this.len, 0, 0);
          this.nodes.push(b.clone()), this.nodes.push(I.clone()), this.beginNodes[this.len].push(I), this.endNodes[0].push(b);
        }
        /**
         * Inserts a new token node into the token lattice.
         *
         * @param {number} pos The starting position of the token.
         * @param {number} length The length of the token.
         * @param {number} score The score of the token.
         * @param {number} tokenId The token ID of the token.
         */
        insert(v, M, y, b) {
          const I = this.nodes.length, K = new R(b, I, v, M, y);
          this.beginNodes[v].push(K), this.endNodes[v + M].push(K), this.nodes.push(K);
        }
        /**
         * Implements the Viterbi algorithm to compute the most likely sequence of tokens.
         *
         * @returns {TokenLatticeNode[]} The most likely sequence of tokens.
         */
        viterbi() {
          const v = this.len;
          let M = 0;
          for (; M <= v; ) {
            if (this.beginNodes[M].length == 0)
              return [];
            for (let se of this.beginNodes[M]) {
              se.prev = null;
              let ie = 0, W = null;
              for (let j of this.endNodes[M]) {
                const q = j.backtraceScore + se.score;
                (W === null || q > ie) && (W = j.clone(), ie = q);
              }
              if (W !== null)
                se.prev = W, se.backtraceScore = ie;
              else
                return [];
            }
            ++M;
          }
          const y = [], I = this.beginNodes[v][0].prev;
          if (I === null)
            return [];
          let K = I.clone();
          for (; K.prev !== null; )
            y.push(K.clone()), K = K.clone().prev.clone();
          return y.reverse(), y;
        }
        /**
         * @param {TokenLatticeNode} node
         * @returns {string} The array of nodes representing the most likely sequence of tokens.
         */
        piece(v) {
          return this.chars.slice(v.pos, v.pos + v.length).join("");
        }
        /**
         * @returns {string[]} The most likely sequence of tokens.
         */
        tokens() {
          return this.viterbi().map((M) => this.piece(M));
        }
        /**
         * @returns {number[]} The most likely sequence of token ids.
         */
        tokenIds() {
          return this.viterbi().map((M) => M.tokenId);
        }
      }
      class R {
        /**
         * Represents a node in a token lattice for a given sentence.
         * @param {number} tokenId The ID of the token associated with this node.
         * @param {number} nodeId The ID of this node.
         * @param {number} pos The starting position of the token in the sentence.
         * @param {number} length The length of the token.
         * @param {number} score The score associated with the token.
         */
        constructor(v, M, y, b, I) {
          this.tokenId = v, this.nodeId = M, this.pos = y, this.length = b, this.score = I, this.prev = null, this.backtraceScore = 0;
        }
        /**
         * Returns a clone of this node.
         * @returns {TokenLatticeNode} A clone of this node.
         */
        clone() {
          const v = new R(this.tokenId, this.nodeId, this.pos, this.length, this.score);
          return v.prev = this.prev, v.backtraceScore = this.backtraceScore, v;
        }
      }
    }
  ),
  /***/
  "./src/utils/devices.js": (
    /*!******************************!*\
      !*** ./src/utils/devices.js ***!
      \******************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        DEVICE_TYPES: () => (
          /* binding */
          _
        )
        /* harmony export */
      });
      const _ = Object.freeze({
        auto: "auto",
        // Auto-detect based on device and environment
        gpu: "gpu",
        // Auto-detect GPU
        cpu: "cpu",
        // CPU
        wasm: "wasm",
        // WebAssembly
        webgpu: "webgpu",
        // WebGPU
        cuda: "cuda",
        // CUDA
        dml: "dml",
        // DirectML
        webnn: "webnn",
        // WebNN (default)
        "webnn-npu": "webnn-npu",
        // WebNN NPU
        "webnn-gpu": "webnn-gpu",
        // WebNN GPU
        "webnn-cpu": "webnn-cpu"
        // WebNN CPU
      });
    }
  ),
  /***/
  "./src/utils/dtypes.js": (
    /*!*****************************!*\
      !*** ./src/utils/dtypes.js ***!
      \*****************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        DATA_TYPES: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        DEFAULT_DEVICE_DTYPE_MAPPING: () => (
          /* binding */
          R
        ),
        /* harmony export */
        DEFAULT_DTYPE_SUFFIX_MAPPING: () => (
          /* binding */
          g
        ),
        /* harmony export */
        isWebGpuFp16Supported: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      var _ = r(
        /*! ../env.js */
        "./src/env.js"
      ), D = r(
        /*! ./devices.js */
        "./src/utils/devices.js"
      );
      const U = /* @__PURE__ */ function() {
        let v;
        return async function() {
          if (v === void 0)
            if (!_.apis.IS_WEBGPU_AVAILABLE)
              v = !1;
            else
              try {
                v = (await navigator.gpu.requestAdapter()).features.has("shader-f16");
              } catch {
                v = !1;
              }
          return v;
        };
      }(), Y = Object.freeze({
        auto: "auto",
        // Auto-detect based on environment
        fp32: "fp32",
        fp16: "fp16",
        q8: "q8",
        int8: "int8",
        uint8: "uint8",
        q4: "q4",
        bnb4: "bnb4",
        q4f16: "q4f16"
        // fp16 model with int4 block weight quantization
      }), R = Object.freeze({
        // NOTE: If not specified, will default to fp32
        [D.DEVICE_TYPES.wasm]: Y.q8
      }), g = Object.freeze({
        [Y.fp32]: "",
        [Y.fp16]: "_fp16",
        [Y.int8]: "_int8",
        [Y.uint8]: "_uint8",
        [Y.q8]: "_quantized",
        [Y.q4]: "_q4",
        [Y.q4f16]: "_q4f16",
        [Y.bnb4]: "_bnb4"
      });
    }
  ),
  /***/
  "./src/utils/generic.js": (
    /*!******************************!*\
      !*** ./src/utils/generic.js ***!
      \******************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Callable: () => (
          /* binding */
          _
        )
        /* harmony export */
      });
      const _ = (
        /** @type {any} */
        class {
          /**
          * Creates a new instance of the Callable class.
          */
          constructor() {
            let D = function(...U) {
              return D._call(...U);
            };
            return Object.setPrototypeOf(D, new.target.prototype);
          }
          /**
           * This method should be implemented in subclasses to provide the
           * functionality of the callable object.
           *
           * @param {any[]} args
           * @throws {Error} If the subclass does not implement the `_call` method.
           */
          _call(...D) {
            throw Error("Must implement _call method in subclass");
          }
        }
      );
    }
  ),
  /***/
  "./src/utils/hub.js": (
    /*!**************************!*\
      !*** ./src/utils/hub.js ***!
      \**************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        getFile: () => (
          /* binding */
          M
        ),
        /* harmony export */
        getModelFile: () => (
          /* binding */
          se
        ),
        /* harmony export */
        getModelJSON: () => (
          /* binding */
          ie
        )
        /* harmony export */
      });
      var _ = r(
        /*! fs */
        "?7a2c"
      ), D = r(
        /*! path */
        "?a42a"
      ), U = r(
        /*! ../env.js */
        "./src/env.js"
      ), Y = r(
        /*! ./core.js */
        "./src/utils/core.js"
      );
      const R = {
        txt: "text/plain",
        html: "text/html",
        css: "text/css",
        js: "text/javascript",
        json: "application/json",
        png: "image/png",
        jpg: "image/jpeg",
        jpeg: "image/jpeg",
        gif: "image/gif"
      };
      class g {
        /**
         * Creates a new `FileResponse` object.
         * @param {string|URL} filePath
         */
        constructor(A) {
          if (this.filePath = A, this.headers = new Headers(), this.exists = _.existsSync(A), this.exists) {
            this.status = 200, this.statusText = "OK";
            let S = _.statSync(A);
            this.headers.set("content-length", S.size.toString()), this.updateContentType();
            let w = this;
            this.body = new ReadableStream({
              start(x) {
                w.arrayBuffer().then((F) => {
                  x.enqueue(new Uint8Array(F)), x.close();
                });
              }
            });
          } else
            this.status = 404, this.statusText = "Not Found", this.body = null;
        }
        /**
         * Updates the 'content-type' header property of the response based on the extension of
         * the file specified by the filePath property of the current object.
         * @returns {void}
         */
        updateContentType() {
          const A = this.filePath.toString().split(".").pop().toLowerCase();
          this.headers.set("content-type", R[A] ?? "application/octet-stream");
        }
        /**
         * Clone the current FileResponse object.
         * @returns {FileResponse} A new FileResponse object with the same properties as the current object.
         */
        clone() {
          let A = new g(this.filePath);
          return A.exists = this.exists, A.status = this.status, A.statusText = this.statusText, A.headers = new Headers(this.headers), A;
        }
        /**
         * Reads the contents of the file specified by the filePath property and returns a Promise that
         * resolves with an ArrayBuffer containing the file's contents.
         * @returns {Promise<ArrayBuffer>} A Promise that resolves with an ArrayBuffer containing the file's contents.
         * @throws {Error} If the file cannot be read.
         */
        async arrayBuffer() {
          return (
            /** @type {ArrayBuffer} */
            (await _.promises.readFile(this.filePath)).buffer
          );
        }
        /**
         * Reads the contents of the file specified by the filePath property and returns a Promise that
         * resolves with a Blob containing the file's contents.
         * @returns {Promise<Blob>} A Promise that resolves with a Blob containing the file's contents.
         * @throws {Error} If the file cannot be read.
         */
        async blob() {
          const A = await _.promises.readFile(this.filePath);
          return new Blob([A], { type: this.headers.get("content-type") });
        }
        /**
         * Reads the contents of the file specified by the filePath property and returns a Promise that
         * resolves with a string containing the file's contents.
         * @returns {Promise<string>} A Promise that resolves with a string containing the file's contents.
         * @throws {Error} If the file cannot be read.
         */
        async text() {
          return await _.promises.readFile(this.filePath, "utf8");
        }
        /**
         * Reads the contents of the file specified by the filePath property and returns a Promise that
         * resolves with a parsed JavaScript object containing the file's contents.
         * 
         * @returns {Promise<Object>} A Promise that resolves with a parsed JavaScript object containing the file's contents.
         * @throws {Error} If the file cannot be read.
         */
        async json() {
          return JSON.parse(await this.text());
        }
      }
      function v(q, A = null, S = null) {
        let w;
        try {
          w = new URL(q);
        } catch {
          return !1;
        }
        return !(A && !A.includes(w.protocol) || S && !S.includes(w.hostname));
      }
      async function M(q) {
        var A, S, w, x;
        if (U.env.useFS && !v(q, ["http:", "https:", "blob:"]))
          return new g(q);
        if (typeof process < "u" && ((A = process == null ? void 0 : process.release) == null ? void 0 : A.name) === "node") {
          const F = !!((S = process.env) != null && S.TESTING_REMOTELY), le = U.env.version, ne = new Headers();
          if (ne.set("User-Agent", `transformers.js/${le}; is_ci/${F};`), v(q, ["http:", "https:"], ["huggingface.co", "hf.co"])) {
            const _e = ((w = process.env) == null ? void 0 : w.HF_TOKEN) ?? ((x = process.env) == null ? void 0 : x.HF_ACCESS_TOKEN);
            _e && ne.set("Authorization", `Bearer ${_e}`);
          }
          return fetch(q, { headers: ne });
        } else
          return fetch(q);
      }
      const y = {
        // 4xx errors (https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#client_error_responses)
        400: "Bad request error occurred while trying to load file",
        401: "Unauthorized access to file",
        403: "Forbidden access to file",
        404: "Could not locate file",
        408: "Request timeout error occurred while trying to load file",
        // 5xx errors (https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#server_error_responses)
        500: "Internal server error error occurred while trying to load file",
        502: "Bad gateway error occurred while trying to load file",
        503: "Service unavailable error occurred while trying to load file",
        504: "Gateway timeout error occurred while trying to load file"
      };
      function b(q, A, S) {
        if (!S)
          return null;
        const w = y[q] ?? `Error (${q}) occurred while trying to load file`;
        throw Error(`${w}: "${A}".`);
      }
      class I {
        /**
         * Instantiate a `FileCache` object.
         * @param {string} path 
         */
        constructor(A) {
          this.path = A;
        }
        /**
         * Checks whether the given request is in the cache.
         * @param {string} request 
         * @returns {Promise<FileResponse | undefined>}
         */
        async match(A) {
          let S = D.join(this.path, A), w = new g(S);
          if (w.exists)
            return w;
        }
        /**
         * Adds the given response to the cache.
         * @param {string} request 
         * @param {Response|FileResponse} response 
         * @returns {Promise<void>}
         */
        async put(A, S) {
          const w = Buffer.from(await S.arrayBuffer());
          let x = D.join(this.path, A);
          try {
            await _.promises.mkdir(D.dirname(x), { recursive: !0 }), await _.promises.writeFile(x, w);
          } catch (F) {
            console.warn("An error occurred while writing the file to cache:", F);
          }
        }
        // TODO add the rest?
        // addAll(requests: RequestInfo[]): Promise<void>;
        // delete(request: RequestInfo | URL, options?: CacheQueryOptions): Promise<boolean>;
        // keys(request?: RequestInfo | URL, options?: CacheQueryOptions): Promise<ReadonlyArray<Request>>;
        // match(request: RequestInfo | URL, options?: CacheQueryOptions): Promise<Response | undefined>;
        // matchAll(request?: RequestInfo | URL, options?: CacheQueryOptions): Promise<ReadonlyArray<Response>>;
      }
      async function K(q, ...A) {
        for (let S of A)
          try {
            let w = await q.match(S);
            if (w) return w;
          } catch {
            continue;
          }
      }
      async function se(q, A, S = !0, w = {}) {
        if (!U.env.allowLocalModels) {
          if (w.local_files_only)
            throw Error("Invalid configuration detected: local models are disabled (`env.allowLocalModels=false`) but you have requested to only use local models (`local_files_only=true`).");
          if (!U.env.allowRemoteModels)
            throw Error("Invalid configuration detected: both local and remote models are disabled. Fix by setting `env.allowLocalModels` or `env.allowRemoteModels` to `true`.");
        }
        (0, Y.dispatchCallback)(w.progress_callback, {
          status: "initiate",
          name: q,
          file: A
        });
        let x;
        if (!x && U.env.useBrowserCache) {
          if (typeof caches > "u")
            throw Error("Browser cache is not available in this environment.");
          try {
            x = await caches.open("transformers-cache");
          } catch (tt) {
            console.warn("An error occurred while opening the browser cache:", tt);
          }
        }
        if (!x && U.env.useFSCache && (x = new I(w.cache_dir ?? U.env.cacheDir)), !x && U.env.useCustomCache) {
          if (!U.env.customCache)
            throw Error("`env.useCustomCache=true`, but `env.customCache` is not defined.");
          if (!U.env.customCache.match || !U.env.customCache.put)
            throw new Error(
              "`env.customCache` must be an object which implements the `match` and `put` functions of the Web Cache API. For more information, see https://developer.mozilla.org/en-US/docs/Web/API/Cache"
            );
          x = U.env.customCache;
        }
        const F = w.revision ?? "main";
        let le = j(q, A), ne = j(U.env.localModelPath, le), be = j(
          U.env.remoteHost,
          U.env.remotePathTemplate.replaceAll("{model}", q).replaceAll("{revision}", encodeURIComponent(F)),
          A
        ), _e = F === "main" ? le : j(q, F, A), re, xe = x instanceof I ? _e : be, ce = !1, ke;
        x && (ke = await K(x, ne, xe));
        const Fe = ke !== void 0;
        if (ke === void 0) {
          if (U.env.allowLocalModels)
            if (v(le, ["http:", "https:"])) {
              if (w.local_files_only)
                throw new Error(`\`local_files_only=true\`, but attempted to load a remote file from: ${le}.`);
              if (!U.env.allowRemoteModels)
                throw new Error(`\`env.allowRemoteModels=false\`, but attempted to load a remote file from: ${le}.`);
            } else try {
              ke = await M(ne), re = ne;
            } catch (Ge) {
              console.warn(`Unable to load from local path "${ne}": "${Ge}"`);
            }
          if (ke === void 0 || ke.status === 404) {
            if (w.local_files_only || !U.env.allowRemoteModels) {
              if (S)
                throw Error(`\`local_files_only=true\` or \`env.allowRemoteModels=false\` and file was not found locally at "${ne}".`);
              return null;
            }
            if (ke = await M(be), ke.status !== 200)
              return b(ke.status, be, S);
            re = xe;
          }
          ce = x && typeof Response < "u" && ke instanceof Response && ke.status === 200;
        }
        (0, Y.dispatchCallback)(w.progress_callback, {
          status: "download",
          name: q,
          file: A
        });
        let Ee;
        return w.progress_callback ? Fe && typeof navigator < "u" && /firefox/i.test(navigator.userAgent) ? (Ee = new Uint8Array(await ke.arrayBuffer()), (0, Y.dispatchCallback)(w.progress_callback, {
          status: "progress",
          name: q,
          file: A,
          progress: 100,
          loaded: Ee.length,
          total: Ee.length
        })) : Ee = await W(ke, (tt) => {
          (0, Y.dispatchCallback)(w.progress_callback, {
            status: "progress",
            name: q,
            file: A,
            ...tt
          });
        }) : Ee = new Uint8Array(await ke.arrayBuffer()), // Only cache web responses
        // i.e., do not cache FileResponses (prevents duplication)
        ce && re && // Check again whether request is in cache. If not, we add the response to the cache
        await x.match(re) === void 0 && await x.put(re, new Response(Ee, {
          headers: ke.headers
        })).catch((tt) => {
          console.warn(`Unable to add response to browser cache: ${tt}.`);
        }), (0, Y.dispatchCallback)(w.progress_callback, {
          status: "done",
          name: q,
          file: A
        }), Ee;
      }
      async function ie(q, A, S = !0, w = {}) {
        let x = await se(q, A, S, w);
        if (x === null)
          return {};
        let le = new TextDecoder("utf-8").decode(x);
        return JSON.parse(le);
      }
      async function W(q, A) {
        const S = q.headers.get("Content-Length");
        S === null && console.warn("Unable to determine content-length from response headers. Will expand buffer when needed.");
        let w = parseInt(S ?? "0"), x = new Uint8Array(w), F = 0;
        const le = q.body.getReader();
        async function ne() {
          const { done: be, value: _e } = await le.read();
          if (be) return;
          let re = F + _e.length;
          if (re > w) {
            w = re;
            let ce = new Uint8Array(w);
            ce.set(x), x = ce;
          }
          x.set(_e, F), F = re;
          const xe = F / w * 100;
          return A({
            progress: xe,
            loaded: F,
            total: w
          }), ne();
        }
        return await ne(), x;
      }
      function j(...q) {
        return q = q.map((A, S) => (S && (A = A.replace(new RegExp("^/"), "")), S !== q.length - 1 && (A = A.replace(new RegExp("/$"), "")), A)), q.join("/");
      }
    }
  ),
  /***/
  "./src/utils/image.js": (
    /*!****************************!*\
      !*** ./src/utils/image.js ***!
      \****************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        RawImage: () => (
          /* binding */
          K
        ),
        /* harmony export */
        load_image: () => (
          /* binding */
          se
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./core.js */
        "./src/utils/core.js"
      ), D = r(
        /*! ./hub.js */
        "./src/utils/hub.js"
      ), U = r(
        /*! ../env.js */
        "./src/env.js"
      ), Y = r(
        /*! ./tensor.js */
        "./src/utils/tensor.js"
      ), R = r(
        /*! sharp */
        "?2b25"
      );
      let g, v, M;
      const y = U.apis.IS_BROWSER_ENV || U.apis.IS_WEBWORKER_ENV;
      if (y)
        g = (ie, W) => {
          if (!self.OffscreenCanvas)
            throw new Error("OffscreenCanvas not supported by this browser.");
          return new self.OffscreenCanvas(ie, W);
        }, M = self.createImageBitmap, v = self.ImageData;
      else if (R)
        M = async (ie) => {
          const j = (await ie.metadata()).channels, { data: q, info: A } = await ie.rotate().raw().toBuffer({ resolveWithObject: !0 }), S = new K(new Uint8ClampedArray(q), A.width, A.height, A.channels);
          return j !== void 0 && j !== A.channels && S.convert(j), S;
        };
      else
        throw new Error("Unable to load image processing library.");
      const b = {
        0: "nearest",
        1: "lanczos",
        2: "bilinear",
        3: "bicubic",
        4: "box",
        5: "hamming"
      }, I = /* @__PURE__ */ new Map([
        ["png", "image/png"],
        ["jpg", "image/jpeg"],
        ["jpeg", "image/jpeg"],
        ["gif", "image/gif"]
      ]);
      class K {
        /**
         * Create a new `RawImage` object.
         * @param {Uint8ClampedArray|Uint8Array} data The pixel data.
         * @param {number} width The width of the image.
         * @param {number} height The height of the image.
         * @param {1|2|3|4} channels The number of channels.
         */
        constructor(W, j, q, A) {
          this.data = W, this.width = j, this.height = q, this.channels = A;
        }
        /**
         * Returns the size of the image (width, height).
         * @returns {[number, number]} The size of the image (width, height).
         */
        get size() {
          return [this.width, this.height];
        }
        /**
         * Helper method for reading an image from a variety of input types.
         * @param {RawImage|string|URL} input
         * @returns The image object.
         *
         * **Example:** Read image from a URL.
         * ```javascript
         * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');
         * // RawImage {
         * //   "data": Uint8ClampedArray [ 25, 25, 25, 19, 19, 19, ... ],
         * //   "width": 800,
         * //   "height": 533,
         * //   "channels": 3
         * // }
         * ```
         */
        static async read(W) {
          if (W instanceof K)
            return W;
          if (typeof W == "string" || W instanceof URL)
            return await this.fromURL(W);
          throw new Error(`Unsupported input type: ${typeof W}`);
        }
        /**
         * Read an image from a canvas.
         * @param {HTMLCanvasElement|OffscreenCanvas} canvas The canvas to read the image from.
         * @returns {RawImage} The image object.
         */
        static fromCanvas(W) {
          if (!y)
            throw new Error("fromCanvas() is only supported in browser environments.");
          const q = W.getContext("2d").getImageData(0, 0, W.width, W.height).data;
          return new K(q, W.width, W.height, 4);
        }
        /**
         * Read an image from a URL or file path.
         * @param {string|URL} url The URL or file path to read the image from.
         * @returns {Promise<RawImage>} The image object.
         */
        static async fromURL(W) {
          const j = await (0, D.getFile)(W);
          if (j.status !== 200)
            throw new Error(`Unable to read image from "${W}" (${j.status} ${j.statusText})`);
          const q = await j.blob();
          return this.fromBlob(q);
        }
        /**
         * Helper method to create a new Image from a blob.
         * @param {Blob} blob The blob to read the image from.
         * @returns {Promise<RawImage>} The image object.
         */
        static async fromBlob(W) {
          if (y) {
            const j = await M(W), q = g(j.width, j.height).getContext("2d");
            return q.drawImage(j, 0, 0), new this(q.getImageData(0, 0, j.width, j.height).data, j.width, j.height, 4);
          } else {
            const j = R(await W.arrayBuffer());
            return await M(j);
          }
        }
        /**
         * Helper method to create a new Image from a tensor
         * @param {Tensor} tensor
         */
        static fromTensor(W, j = "CHW") {
          if (W.dims.length !== 3)
            throw new Error(`Tensor should have 3 dimensions, but has ${W.dims.length} dimensions.`);
          if (j === "CHW")
            W = W.transpose(1, 2, 0);
          else if (j !== "HWC") throw new Error(`Unsupported channel format: ${j}`);
          if (!(W.data instanceof Uint8ClampedArray || W.data instanceof Uint8Array))
            throw new Error(`Unsupported tensor type: ${W.type}`);
          switch (W.dims[2]) {
            case 1:
            case 2:
            case 3:
            case 4:
              return new K(W.data, W.dims[1], W.dims[0], W.dims[2]);
            default:
              throw new Error(`Unsupported number of channels: ${W.dims[2]}`);
          }
        }
        /**
         * Convert the image to grayscale format.
         * @returns {RawImage} `this` to support chaining.
         */
        grayscale() {
          if (this.channels === 1)
            return this;
          const W = new Uint8ClampedArray(this.width * this.height * 1);
          switch (this.channels) {
            case 3:
            // rgb to grayscale
            case 4:
              for (let j = 0, q = 0; j < this.data.length; j += this.channels) {
                const A = this.data[j], S = this.data[j + 1], w = this.data[j + 2];
                W[q++] = Math.round(0.2989 * A + 0.587 * S + 0.114 * w);
              }
              break;
            default:
              throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
          }
          return this._update(W, this.width, this.height, 1);
        }
        /**
         * Convert the image to RGB format.
         * @returns {RawImage} `this` to support chaining.
         */
        rgb() {
          if (this.channels === 3)
            return this;
          const W = new Uint8ClampedArray(this.width * this.height * 3);
          switch (this.channels) {
            case 1:
              for (let j = 0, q = 0; j < this.data.length; ++j)
                W[q++] = this.data[j], W[q++] = this.data[j], W[q++] = this.data[j];
              break;
            case 4:
              for (let j = 0, q = 0; j < this.data.length; j += 4)
                W[q++] = this.data[j], W[q++] = this.data[j + 1], W[q++] = this.data[j + 2];
              break;
            default:
              throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
          }
          return this._update(W, this.width, this.height, 3);
        }
        /**
         * Convert the image to RGBA format.
         * @returns {RawImage} `this` to support chaining.
         */
        rgba() {
          if (this.channels === 4)
            return this;
          const W = new Uint8ClampedArray(this.width * this.height * 4);
          switch (this.channels) {
            case 1:
              for (let j = 0, q = 0; j < this.data.length; ++j)
                W[q++] = this.data[j], W[q++] = this.data[j], W[q++] = this.data[j], W[q++] = 255;
              break;
            case 3:
              for (let j = 0, q = 0; j < this.data.length; j += 3)
                W[q++] = this.data[j], W[q++] = this.data[j + 1], W[q++] = this.data[j + 2], W[q++] = 255;
              break;
            default:
              throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
          }
          return this._update(W, this.width, this.height, 4);
        }
        /**
         * Apply an alpha mask to the image. Operates in place.
         * @param {RawImage} mask The mask to apply. It should have a single channel.
         * @returns {RawImage} The masked image.
         * @throws {Error} If the mask is not the same size as the image.
         * @throws {Error} If the image does not have 4 channels.
         * @throws {Error} If the mask is not a single channel.
         */
        putAlpha(W) {
          if (W.width !== this.width || W.height !== this.height)
            throw new Error(`Expected mask size to be ${this.width}x${this.height}, but got ${W.width}x${W.height}`);
          if (W.channels !== 1)
            throw new Error(`Expected mask to have 1 channel, but got ${W.channels}`);
          const j = this.data, q = W.data, A = this.width * this.height;
          if (this.channels === 3) {
            const S = new Uint8ClampedArray(A * 4);
            for (let w = 0, x = 0, F = 0; w < A; ++w)
              S[F++] = j[x++], S[F++] = j[x++], S[F++] = j[x++], S[F++] = q[w];
            return this._update(S, this.width, this.height, 4);
          } else if (this.channels === 4) {
            for (let S = 0; S < A; ++S)
              j[4 * S + 3] = q[S];
            return this;
          }
          throw new Error(`Expected image to have 3 or 4 channels, but got ${this.channels}`);
        }
        /**
         * Resize the image to the given dimensions. This method uses the canvas API to perform the resizing.
         * @param {number} width The width of the new image. `null` or `-1` will preserve the aspect ratio.
         * @param {number} height The height of the new image. `null` or `-1` will preserve the aspect ratio.
         * @param {Object} options Additional options for resizing.
         * @param {0|1|2|3|4|5|string} [options.resample] The resampling method to use.
         * @returns {Promise<RawImage>} `this` to support chaining.
         */
        async resize(W, j, {
          resample: q = 2
        } = {}) {
          if (this.width === W && this.height === j)
            return this;
          let A = b[q] ?? q;
          const S = (0, _.isNullishDimension)(W), w = (0, _.isNullishDimension)(j);
          if (S && w)
            return this;
          if (S ? W = j / this.height * this.width : w && (j = W / this.width * this.height), y) {
            const x = this.channels, F = this.toCanvas(), le = g(W, j).getContext("2d");
            return le.drawImage(F, 0, 0, W, j), new K(le.getImageData(0, 0, W, j).data, W, j, 4).convert(x);
          } else {
            let x = this.toSharp();
            switch (A) {
              case "box":
              case "hamming":
                (A === "box" || A === "hamming") && (console.warn(`Resampling method ${A} is not yet supported. Using bilinear instead.`), A = "bilinear");
              case "nearest":
              case "bilinear":
              case "bicubic":
                x = x.affine([W / this.width, 0, 0, j / this.height], {
                  interpolator: A
                });
                break;
              case "lanczos":
                x = x.resize({
                  width: W,
                  height: j,
                  fit: "fill",
                  kernel: "lanczos3"
                  // PIL Lanczos uses a kernel size of 3
                });
                break;
              default:
                throw new Error(`Resampling method ${A} is not supported.`);
            }
            return await M(x);
          }
        }
        async pad([W, j, q, A]) {
          if (W = Math.max(W, 0), j = Math.max(j, 0), q = Math.max(q, 0), A = Math.max(A, 0), W === 0 && j === 0 && q === 0 && A === 0)
            return this;
          if (y) {
            const S = this.channels, w = this.toCanvas(), x = this.width + W + j, F = this.height + q + A, le = g(x, F).getContext("2d");
            return le.drawImage(
              w,
              0,
              0,
              this.width,
              this.height,
              W,
              q,
              this.width,
              this.height
            ), new K(
              le.getImageData(0, 0, x, F).data,
              x,
              F,
              4
            ).convert(S);
          } else {
            const S = this.toSharp().extend({ left: W, right: j, top: q, bottom: A });
            return await M(S);
          }
        }
        async crop([W, j, q, A]) {
          if (W = Math.max(W, 0), j = Math.max(j, 0), q = Math.min(q, this.width - 1), A = Math.min(A, this.height - 1), W === 0 && j === 0 && q === this.width - 1 && A === this.height - 1)
            return this;
          const S = q - W + 1, w = A - j + 1;
          if (y) {
            const x = this.channels, F = this.toCanvas(), le = g(S, w).getContext("2d");
            return le.drawImage(
              F,
              W,
              j,
              S,
              w,
              0,
              0,
              S,
              w
            ), new K(le.getImageData(0, 0, S, w).data, S, w, 4).convert(x);
          } else {
            const x = this.toSharp().extract({
              left: W,
              top: j,
              width: S,
              height: w
            });
            return await M(x);
          }
        }
        async center_crop(W, j) {
          if (this.width === W && this.height === j)
            return this;
          const q = (this.width - W) / 2, A = (this.height - j) / 2;
          if (y) {
            const S = this.channels, w = this.toCanvas(), x = g(W, j).getContext("2d");
            let F = 0, le = 0, ne = 0, be = 0;
            return q >= 0 ? F = q : ne = -q, A >= 0 ? le = A : be = -A, x.drawImage(
              w,
              F,
              le,
              W,
              j,
              ne,
              be,
              W,
              j
            ), new K(x.getImageData(0, 0, W, j).data, W, j, 4).convert(S);
          } else {
            let S = this.toSharp();
            if (q >= 0 && A >= 0)
              S = S.extract({
                left: Math.floor(q),
                top: Math.floor(A),
                width: W,
                height: j
              });
            else if (q <= 0 && A <= 0) {
              const w = Math.floor(-A), x = Math.floor(-q);
              S = S.extend({
                top: w,
                left: x,
                // Ensures the resulting image has the desired dimensions
                right: W - this.width - x,
                bottom: j - this.height - w
              });
            } else {
              let w = [0, 0], x = 0;
              A < 0 ? (w[0] = Math.floor(-A), w[1] = j - this.height - w[0]) : x = Math.floor(A);
              let F = [0, 0], le = 0;
              q < 0 ? (F[0] = Math.floor(-q), F[1] = W - this.width - F[0]) : le = Math.floor(q), S = S.extend({
                top: w[0],
                bottom: w[1],
                left: F[0],
                right: F[1]
              }).extract({
                left: le,
                top: x,
                width: W,
                height: j
              });
            }
            return await M(S);
          }
        }
        async toBlob(W = "image/png", j = 1) {
          if (!y)
            throw new Error("toBlob() is only supported in browser environments.");
          return await this.toCanvas().convertToBlob({ type: W, quality: j });
        }
        toTensor(W = "CHW") {
          let j = new Y.Tensor(
            "uint8",
            new Uint8Array(this.data),
            [this.height, this.width, this.channels]
          );
          if (W !== "HWC") if (W === "CHW")
            j = j.permute(2, 0, 1);
          else
            throw new Error(`Unsupported channel format: ${W}`);
          return j;
        }
        toCanvas() {
          if (!y)
            throw new Error("toCanvas() is only supported in browser environments.");
          const W = this.clone().rgba(), j = g(W.width, W.height), q = new v(W.data, W.width, W.height);
          return j.getContext("2d").putImageData(q, 0, 0), j;
        }
        /**
         * Split this image into individual bands. This method returns an array of individual image bands from an image.
         * For example, splitting an "RGB" image creates three new images each containing a copy of one of the original bands (red, green, blue).
         * 
         * Inspired by PIL's `Image.split()` [function](https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.split).
         * @returns {RawImage[]} An array containing bands.
         */
        split() {
          const { data: W, width: j, height: q, channels: A } = this, S = (
            /** @type {any} */
            W.constructor
          ), w = W.length / A, x = Array.from(
            { length: A },
            () => new S(w)
          );
          for (let F = 0; F < w; ++F) {
            const le = A * F;
            for (let ne = 0; ne < A; ++ne)
              x[ne][F] = W[le + ne];
          }
          return x.map((F) => new K(F, j, q, 1));
        }
        /**
         * Helper method to update the image data.
         * @param {Uint8ClampedArray} data The new image data.
         * @param {number} width The new width of the image.
         * @param {number} height The new height of the image.
         * @param {1|2|3|4|null} [channels] The new number of channels of the image.
         * @private
         */
        _update(W, j, q, A = null) {
          return this.data = W, this.width = j, this.height = q, A !== null && (this.channels = A), this;
        }
        /**
         * Clone the image
         * @returns {RawImage} The cloned image
         */
        clone() {
          return new K(this.data.slice(), this.width, this.height, this.channels);
        }
        /**
         * Helper method for converting image to have a certain number of channels
         * @param {number} numChannels The number of channels. Must be 1, 3, or 4.
         * @returns {RawImage} `this` to support chaining.
         */
        convert(W) {
          if (this.channels === W) return this;
          switch (W) {
            case 1:
              this.grayscale();
              break;
            case 3:
              this.rgb();
              break;
            case 4:
              this.rgba();
              break;
            default:
              throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
          }
          return this;
        }
        /**
         * Save the image to the given path.
         * @param {string} path The path to save the image to.
         */
        async save(W) {
          if (y) {
            if (U.apis.IS_WEBWORKER_ENV)
              throw new Error("Unable to save an image from a Web Worker.");
            const j = W.split(".").pop().toLowerCase(), q = I.get(j) ?? "image/png", A = await this.toBlob(q);
            (0, _.saveBlob)(W, A);
          } else {
            if (U.apis.IS_FS_AVAILABLE)
              return await this.toSharp().toFile(W);
            throw new Error("Unable to save the image because filesystem is disabled in this environment.");
          }
        }
        toSharp() {
          if (y)
            throw new Error("toSharp() is only supported in server-side environments.");
          return R(this.data, {
            raw: {
              width: this.width,
              height: this.height,
              channels: this.channels
            }
          });
        }
      }
      const se = K.read.bind(K);
    }
  ),
  /***/
  "./src/utils/maths.js": (
    /*!****************************!*\
      !*** ./src/utils/maths.js ***!
      \****************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        FFT: () => (
          /* binding */
          se
        ),
        /* harmony export */
        bankers_round: () => (
          /* binding */
          j
        ),
        /* harmony export */
        cos_sim: () => (
          /* binding */
          g
        ),
        /* harmony export */
        dot: () => (
          /* binding */
          R
        ),
        /* harmony export */
        dynamic_time_warping: () => (
          /* binding */
          q
        ),
        /* harmony export */
        interpolate_data: () => (
          /* binding */
          _
        ),
        /* harmony export */
        log_softmax: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        magnitude: () => (
          /* binding */
          v
        ),
        /* harmony export */
        max: () => (
          /* binding */
          y
        ),
        /* harmony export */
        medianFilter: () => (
          /* binding */
          ie
        ),
        /* harmony export */
        min: () => (
          /* binding */
          M
        ),
        /* harmony export */
        permute_data: () => (
          /* binding */
          D
        ),
        /* harmony export */
        round: () => (
          /* binding */
          W
        ),
        /* harmony export */
        softmax: () => (
          /* binding */
          U
        )
        /* harmony export */
      });
      function _(A, [S, w, x], [F, le], ne = "bilinear", be = !1) {
        const _e = le / x, re = F / w, xe = new A.constructor(F * le * S), ce = w * x, ke = F * le;
        for (let Fe = 0; Fe < F; ++Fe)
          for (let Ee = 0; Ee < le; ++Ee) {
            const tt = Fe * le + Ee, Ge = (Ee + 0.5) / _e - 0.5, ye = (Fe + 0.5) / re - 0.5;
            let J = Math.floor(Ge), de = Math.floor(ye);
            const Ce = Math.min(J + 1, x - 1), Be = Math.min(de + 1, w - 1);
            J = Math.max(J, 0), de = Math.max(de, 0);
            const Ze = Ge - J, te = ye - de, Ke = (1 - Ze) * (1 - te), je = Ze * (1 - te), ae = (1 - Ze) * te, Te = Ze * te, Ue = de * x, Ve = Be * x, Ne = Ue + J, Re = Ue + Ce, st = Ve + J, dt = Ve + Ce;
            for (let ct = 0; ct < S; ++ct) {
              const lt = ct * ce;
              xe[ct * ke + tt] = Ke * A[lt + Ne] + je * A[lt + Re] + ae * A[lt + st] + Te * A[lt + dt];
            }
          }
        return xe;
      }
      function D(A, S, w) {
        const x = new Array(w.length), F = new Array(w.length);
        for (let be = w.length - 1, _e = 1; be >= 0; --be)
          F[be] = _e, x[be] = S[w[be]], _e *= x[be];
        const le = w.map((be, _e) => F[w.indexOf(_e)]), ne = new A.constructor(A.length);
        for (let be = 0; be < A.length; ++be) {
          let _e = 0;
          for (let re = S.length - 1, xe = be; re >= 0; --re)
            _e += xe % S[re] * le[re], xe = Math.floor(xe / S[re]);
          ne[_e] = A[be];
        }
        return [ne, x];
      }
      function U(A) {
        const S = y(A)[0], w = A.map((le) => Math.exp(le - S)), x = w.reduce((le, ne) => le + ne, 0);
        return w.map((le) => le / x);
      }
      function Y(A) {
        const S = y(A)[0];
        let w = 0;
        for (let le = 0; le < A.length; ++le)
          w += Math.exp(A[le] - S);
        const x = Math.log(w);
        return A.map((le) => le - S - x);
      }
      function R(A, S) {
        let w = 0;
        for (let x = 0; x < A.length; ++x)
          w += A[x] * S[x];
        return w;
      }
      function g(A, S) {
        const w = R(A, S), x = v(A), F = v(S);
        return w / (x * F);
      }
      function v(A) {
        return Math.sqrt(A.reduce((S, w) => S + w * w, 0));
      }
      function M(A) {
        if (A.length === 0) throw Error("Array must not be empty");
        let S = A[0], w = 0;
        for (let x = 1; x < A.length; ++x)
          A[x] < S && (S = A[x], w = x);
        return (
          /** @type {T extends bigint[]|BigTypedArray ? [bigint, number] : [number, number]} */
          [S, w]
        );
      }
      function y(A) {
        if (A.length === 0) throw Error("Array must not be empty");
        let S = A[0], w = 0;
        for (let x = 1; x < A.length; ++x)
          A[x] > S && (S = A[x], w = x);
        return (
          /** @type {T extends bigint[]|BigTypedArray ? [bigint, number] : [number, number]} */
          [S, w]
        );
      }
      function b(A) {
        return A > 0 && (A & A - 1) === 0;
      }
      class I {
        /**
         * @param {number} size The size of the input array. Must be a power of two larger than 1.
         * @throws {Error} FFT size must be a power of two larger than 1.
         */
        constructor(S) {
          if (this.size = S | 0, this.size <= 1 || !b(this.size))
            throw new Error("FFT size must be a power of two larger than 1");
          this._csize = S << 1, this.table = new Float64Array(this.size * 2);
          for (let x = 0; x < this.table.length; x += 2) {
            const F = Math.PI * x / this.size;
            this.table[x] = Math.cos(F), this.table[x + 1] = -Math.sin(F);
          }
          let w = 0;
          for (let x = 1; this.size > x; x <<= 1)
            ++w;
          this._width = w % 2 === 0 ? w - 1 : w, this._bitrev = new Int32Array(1 << this._width);
          for (let x = 0; x < this._bitrev.length; ++x) {
            this._bitrev[x] = 0;
            for (let F = 0; F < this._width; F += 2) {
              const le = this._width - F - 2;
              this._bitrev[x] |= (x >>> F & 3) << le;
            }
          }
        }
        /**
         * Create a complex number array with size `2 * size`
         *
         * @returns {Float64Array} A complex number array with size `2 * size`
         */
        createComplexArray() {
          return new Float64Array(this._csize);
        }
        /**
         * Converts a complex number representation stored in a Float64Array to an array of real numbers.
         * 
         * @param {Float64Array} complex The complex number representation to be converted.
         * @param {number[]} [storage] An optional array to store the result in.
         * @returns {number[]} An array of real numbers representing the input complex number representation.
         */
        fromComplexArray(S, w) {
          const x = w || new Array(S.length >>> 1);
          for (let F = 0; F < S.length; F += 2)
            x[F >>> 1] = S[F];
          return x;
        }
        /**
         * Convert a real-valued input array to a complex-valued output array.
         * @param {Float64Array} input The real-valued input array.
         * @param {Float64Array} [storage] Optional buffer to store the output array.
         * @returns {Float64Array} The complex-valued output array.
         */
        toComplexArray(S, w) {
          const x = w || this.createComplexArray();
          for (let F = 0; F < x.length; F += 2)
            x[F] = S[F >>> 1], x[F + 1] = 0;
          return x;
        }
        /**
         * Performs a Fast Fourier Transform (FFT) on the given input data and stores the result in the output buffer.
         * 
         * @param {Float64Array} out The output buffer to store the result.
         * @param {Float64Array} data The input data to transform.
         * 
         * @throws {Error} Input and output buffers must be different.
         * 
         * @returns {void}
         */
        transform(S, w) {
          if (S === w)
            throw new Error("Input and output buffers must be different");
          this._transform4(
            S,
            w,
            1
            /* DONE */
          );
        }
        /**
         * Performs a real-valued forward FFT on the given input buffer and stores the result in the given output buffer.
         * The input buffer must contain real values only, while the output buffer will contain complex values. The input and
         * output buffers must be different.
         *
         * @param {Float64Array} out The output buffer.
         * @param {Float64Array} data The input buffer containing real values.
         *
         * @throws {Error} If the input and output buffers are the same.
         */
        realTransform(S, w) {
          if (S === w)
            throw new Error("Input and output buffers must be different");
          this._realTransform4(
            S,
            w,
            1
            /* DONE */
          );
        }
        /**
         * Performs an inverse FFT transformation on the given `data` array, and stores the result in `out`.
         * The `out` array must be a different buffer than the `data` array. The `out` array will contain the
         * result of the transformation. The `data` array will not be modified.
         * 
         * @param {Float64Array} out The output buffer for the transformed data.
         * @param {Float64Array} data The input data to transform.
         * @throws {Error} If `out` and `data` refer to the same buffer.
         * @returns {void}
         */
        inverseTransform(S, w) {
          if (S === w)
            throw new Error("Input and output buffers must be different");
          this._transform4(
            S,
            w,
            -1
            /* DONE */
          );
          for (let x = 0; x < S.length; ++x)
            S[x] /= this.size;
        }
        /**
         * Performs a radix-4 implementation of a discrete Fourier transform on a given set of data.
         *
         * @param {Float64Array} out The output buffer for the transformed data.
         * @param {Float64Array} data The input buffer of data to be transformed.
         * @param {number} inv A scaling factor to apply to the transform.
         * @returns {void}
         */
        _transform4(S, w, x) {
          const F = this._csize;
          let ne = 1 << this._width, be = F / ne << 1, _e, re;
          const xe = this._bitrev;
          if (be === 4)
            for (_e = 0, re = 0; _e < F; _e += be, ++re) {
              const ke = xe[re];
              this._singleTransform2(w, S, _e, ke, ne);
            }
          else
            for (_e = 0, re = 0; _e < F; _e += be, ++re) {
              const ke = xe[re];
              this._singleTransform4(w, S, _e, ke, ne, x);
            }
          const ce = this.table;
          for (ne >>= 2; ne >= 2; ne >>= 2) {
            be = F / ne << 1;
            const ke = be >>> 2;
            for (_e = 0; _e < F; _e += be) {
              const Fe = _e + ke - 1;
              for (let Ee = _e, tt = 0; Ee < Fe; Ee += 2, tt += ne) {
                const Ge = Ee, ye = Ge + ke, J = ye + ke, de = J + ke, Ce = S[Ge], Be = S[Ge + 1], Ze = S[ye], te = S[ye + 1], Ke = S[J], je = S[J + 1], ae = S[de], Te = S[de + 1], Ue = ce[tt], Ve = x * ce[tt + 1], Ne = Ze * Ue - te * Ve, Re = Ze * Ve + te * Ue, st = ce[2 * tt], dt = x * ce[2 * tt + 1], ct = Ke * st - je * dt, lt = Ke * dt + je * st, ht = ce[3 * tt], L = x * ce[3 * tt + 1], oe = ae * ht - Te * L, H = ae * L + Te * ht, me = Ce + ct, Ae = Be + lt, We = Ce - ct, Je = Be - lt, ut = Ne + oe, mt = Re + H, vt = x * (Ne - oe), kt = x * (Re - H);
                S[Ge] = me + ut, S[Ge + 1] = Ae + mt, S[ye] = We + kt, S[ye + 1] = Je - vt, S[J] = me - ut, S[J + 1] = Ae - mt, S[de] = We - kt, S[de + 1] = Je + vt;
              }
            }
          }
        }
        /**
         * Performs a radix-2 implementation of a discrete Fourier transform on a given set of data.
         *
         * @param {Float64Array} data The input buffer of data to be transformed.
         * @param {Float64Array} out The output buffer for the transformed data.
         * @param {number} outOff The offset at which to write the output data.
         * @param {number} off The offset at which to begin reading the input data.
         * @param {number} step The step size for indexing the input data.
         * @returns {void}
         */
        _singleTransform2(S, w, x, F, le) {
          const ne = S[F], be = S[F + 1], _e = S[F + le], re = S[F + le + 1];
          w[x] = ne + _e, w[x + 1] = be + re, w[x + 2] = ne - _e, w[x + 3] = be - re;
        }
        /**
         * Performs radix-4 transformation on input data of length 8
         *
         * @param {Float64Array} data Input data array of length 8
         * @param {Float64Array} out Output data array of length 8
         * @param {number} outOff Index of output array to start writing from
         * @param {number} off Index of input array to start reading from
         * @param {number} step Step size between elements in input array
         * @param {number} inv Scaling factor for inverse transform
         * 
         * @returns {void}
         */
        _singleTransform4(S, w, x, F, le, ne) {
          const be = le * 2, _e = le * 3, re = S[F], xe = S[F + 1], ce = S[F + le], ke = S[F + le + 1], Fe = S[F + be], Ee = S[F + be + 1], tt = S[F + _e], Ge = S[F + _e + 1], ye = re + Fe, J = xe + Ee, de = re - Fe, Ce = xe - Ee, Be = ce + tt, Ze = ke + Ge, te = ne * (ce - tt), Ke = ne * (ke - Ge);
          w[x] = ye + Be, w[x + 1] = J + Ze, w[x + 2] = de + Ke, w[x + 3] = Ce - te, w[x + 4] = ye - Be, w[x + 5] = J - Ze, w[x + 6] = de - Ke, w[x + 7] = Ce + te;
        }
        /**
         * Real input radix-4 implementation
         * @param {Float64Array} out Output array for the transformed data
         * @param {Float64Array} data Input array of real data to be transformed
         * @param {number} inv The scale factor used to normalize the inverse transform
         */
        _realTransform4(S, w, x) {
          const F = this._csize;
          let ne = 1 << this._width, be = F / ne << 1, _e, re;
          const xe = this._bitrev;
          if (be === 4)
            for (_e = 0, re = 0; _e < F; _e += be, ++re) {
              const Fe = xe[re];
              this._singleRealTransform2(w, S, _e, Fe >>> 1, ne >>> 1);
            }
          else
            for (_e = 0, re = 0; _e < F; _e += be, ++re) {
              const Fe = xe[re];
              this._singleRealTransform4(w, S, _e, Fe >>> 1, ne >>> 1, x);
            }
          const ce = this.table;
          for (ne >>= 2; ne >= 2; ne >>= 2) {
            be = F / ne << 1;
            const Fe = be >>> 1, Ee = Fe >>> 1, tt = Ee >>> 1;
            for (_e = 0; _e < F; _e += be)
              for (let Ge = 0, ye = 0; Ge <= tt; Ge += 2, ye += ne) {
                const J = _e + Ge, de = J + Ee, Ce = de + Ee, Be = Ce + Ee, Ze = S[J], te = S[J + 1], Ke = S[de], je = S[de + 1], ae = S[Ce], Te = S[Ce + 1], Ue = S[Be], Ve = S[Be + 1], Ne = Ze, Re = te, st = ce[ye], dt = x * ce[ye + 1], ct = Ke * st - je * dt, lt = Ke * dt + je * st, ht = ce[2 * ye], L = x * ce[2 * ye + 1], oe = ae * ht - Te * L, H = ae * L + Te * ht, me = ce[3 * ye], Ae = x * ce[3 * ye + 1], We = Ue * me - Ve * Ae, Je = Ue * Ae + Ve * me, ut = Ne + oe, mt = Re + H, vt = Ne - oe, kt = Re - H, At = ct + We, is = lt + Je, ys = x * (ct - We), Cs = x * (lt - Je);
                if (S[J] = ut + At, S[J + 1] = mt + is, S[de] = vt + Cs, S[de + 1] = kt - ys, Ge === 0) {
                  S[Ce] = ut - At, S[Ce + 1] = mt - is;
                  continue;
                }
                if (Ge === tt)
                  continue;
                const Ds = _e + Ee - Ge, sr = _e + Fe - Ge;
                S[Ds] = vt - x * Cs, S[Ds + 1] = -kt - x * ys, S[sr] = ut - x * At, S[sr + 1] = -mt + x * is;
              }
          }
          const ke = F >>> 1;
          for (let Fe = 2; Fe < ke; Fe += 2)
            S[F - Fe] = S[Fe], S[F - Fe + 1] = -S[Fe + 1];
        }
        /**
         * Performs a single real input radix-2 transformation on the provided data
         * 
         * @param {Float64Array} data The input data array
         * @param {Float64Array} out The output data array
         * @param {number} outOff The output offset
         * @param {number} off The input offset
         * @param {number} step The step
         * 
         * @returns {void}
         */
        _singleRealTransform2(S, w, x, F, le) {
          const ne = S[F], be = S[F + le];
          w[x] = ne + be, w[x + 1] = 0, w[x + 2] = ne - be, w[x + 3] = 0;
        }
        /**
         * Computes a single real-valued transform using radix-4 algorithm.
         * This method is only called for len=8.
         *
         * @param {Float64Array} data The input data array.
         * @param {Float64Array} out The output data array.
         * @param {number} outOff The offset into the output array.
         * @param {number} off The offset into the input array.
         * @param {number} step The step size for the input array.
         * @param {number} inv The value of inverse.
         */
        _singleRealTransform4(S, w, x, F, le, ne) {
          const be = le * 2, _e = le * 3, re = S[F], xe = S[F + le], ce = S[F + be], ke = S[F + _e], Fe = re + ce, Ee = re - ce, tt = xe + ke, Ge = ne * (xe - ke);
          w[x] = Fe + tt, w[x + 1] = 0, w[x + 2] = Ee, w[x + 3] = -Ge, w[x + 4] = Fe - tt, w[x + 5] = 0, w[x + 6] = Ee, w[x + 7] = Ge;
        }
      }
      class K {
        /**
         * Constructs a new NP2FFT object.
         * @param {number} fft_length The length of the FFT
         */
        constructor(S) {
          const w = 2 * (S - 1), x = 2 * (2 * S - 1), F = 2 ** Math.ceil(Math.log2(x));
          this.bufferSize = F, this._a = w;
          const le = new Float64Array(x), ne = new Float64Array(F);
          this._chirpBuffer = new Float64Array(F), this._buffer1 = new Float64Array(F), this._buffer2 = new Float64Array(F), this._outBuffer1 = new Float64Array(F), this._outBuffer2 = new Float64Array(F);
          const be = -2 * Math.PI / S, _e = Math.cos(be), re = Math.sin(be);
          for (let xe = 0; xe < x >> 1; ++xe) {
            const ce = (xe + 1 - S) ** 2 / 2, ke = Math.sqrt(_e ** 2 + re ** 2) ** ce, Fe = ce * Math.atan2(re, _e), Ee = 2 * xe;
            le[Ee] = ke * Math.cos(Fe), le[Ee + 1] = ke * Math.sin(Fe), ne[Ee] = le[Ee], ne[Ee + 1] = -le[Ee + 1];
          }
          this._slicedChirpBuffer = le.subarray(w, x), this._f = new I(F >> 1), this._f.transform(this._chirpBuffer, ne);
        }
        _transform(S, w, x) {
          const F = this._buffer1, le = this._buffer2, ne = this._outBuffer1, be = this._outBuffer2, _e = this._chirpBuffer, re = this._slicedChirpBuffer, xe = this._a;
          if (x)
            for (let ce = 0; ce < re.length; ce += 2) {
              const ke = ce + 1, Fe = ce >> 1, Ee = w[Fe];
              F[ce] = Ee * re[ce], F[ke] = Ee * re[ke];
            }
          else
            for (let ce = 0; ce < re.length; ce += 2) {
              const ke = ce + 1;
              F[ce] = w[ce] * re[ce] - w[ke] * re[ke], F[ke] = w[ce] * re[ke] + w[ke] * re[ce];
            }
          this._f.transform(ne, F);
          for (let ce = 0; ce < _e.length; ce += 2) {
            const ke = ce + 1;
            le[ce] = ne[ce] * _e[ce] - ne[ke] * _e[ke], le[ke] = ne[ce] * _e[ke] + ne[ke] * _e[ce];
          }
          this._f.inverseTransform(be, le);
          for (let ce = 0; ce < be.length; ce += 2) {
            const ke = be[ce + xe], Fe = be[ce + xe + 1], Ee = re[ce], tt = re[ce + 1];
            S[ce] = ke * Ee - Fe * tt, S[ce + 1] = ke * tt + Fe * Ee;
          }
        }
        transform(S, w) {
          this._transform(S, w, !1);
        }
        realTransform(S, w) {
          this._transform(S, w, !0);
        }
      }
      class se {
        constructor(S) {
          this.fft_length = S, this.isPowerOfTwo = b(S), this.isPowerOfTwo ? (this.fft = new I(S), this.outputBufferSize = 2 * S) : (this.fft = new K(S), this.outputBufferSize = this.fft.bufferSize);
        }
        realTransform(S, w) {
          this.fft.realTransform(S, w);
        }
        transform(S, w) {
          this.fft.transform(S, w);
        }
      }
      function ie(A, S) {
        if (S % 2 === 0 || S <= 0)
          throw new Error("Window size must be a positive odd number");
        const w = new A.constructor(A.length), x = new A.constructor(S), F = Math.floor(S / 2);
        for (let le = 0; le < A.length; ++le) {
          let ne = 0;
          for (let be = -F; be <= F; ++be) {
            let _e = le + be;
            _e < 0 ? _e = Math.abs(_e) : _e >= A.length && (_e = 2 * (A.length - 1) - _e), x[ne++] = A[_e];
          }
          x.sort(), w[le] = x[F];
        }
        return w;
      }
      function W(A, S) {
        const w = Math.pow(10, S);
        return Math.round(A * w) / w;
      }
      function j(A) {
        const S = Math.round(A);
        return Math.abs(A) % 1 === 0.5 ? S % 2 === 0 ? S : S - 1 : S;
      }
      function q(A) {
        const S = A.length, w = A[0].length, x = [S + 1, w + 1], F = Array.from(
          { length: x[0] },
          () => Array(x[1]).fill(1 / 0)
        );
        F[0][0] = 0;
        const le = Array.from(
          { length: x[0] },
          () => Array(x[1]).fill(-1)
        );
        for (let xe = 1; xe < x[1]; ++xe)
          for (let ce = 1; ce < x[0]; ++ce) {
            const ke = F[ce - 1][xe - 1], Fe = F[ce - 1][xe], Ee = F[ce][xe - 1];
            let tt, Ge;
            ke < Fe && ke < Ee ? (tt = ke, Ge = 0) : Fe < ke && Fe < Ee ? (tt = Fe, Ge = 1) : (tt = Ee, Ge = 2), F[ce][xe] = A[ce - 1][xe - 1] + tt, le[ce][xe] = Ge;
          }
        for (let xe = 0; xe < x[1]; ++xe)
          le[0][xe] = 2;
        for (let xe = 0; xe < x[0]; ++xe)
          le[xe][0] = 1;
        let ne = S, be = w, _e = [], re = [];
        for (; ne > 0 || be > 0; )
          switch (_e.push(ne - 1), re.push(be - 1), le[ne][be]) {
            case 0:
              --ne, --be;
              break;
            case 1:
              --ne;
              break;
            case 2:
              --be;
              break;
            default:
              throw new Error(
                `Internal error in dynamic time warping. Unexpected trace[${ne}, ${be}]. Please file a bug report.`
              );
          }
        return _e.reverse(), re.reverse(), [_e, re];
      }
    }
  ),
  /***/
  "./src/utils/tensor.js": (
    /*!*****************************!*\
      !*** ./src/utils/tensor.js ***!
      \*****************************/
    /***/
    ($e, $, r) => {
      r.r($), r.d($, {
        /* harmony export */
        Tensor: () => (
          /* binding */
          R
        ),
        /* harmony export */
        cat: () => (
          /* binding */
          w
        ),
        /* harmony export */
        full: () => (
          /* binding */
          re
        ),
        /* harmony export */
        full_like: () => (
          /* binding */
          xe
        ),
        /* harmony export */
        interpolate: () => (
          /* binding */
          M
        ),
        /* harmony export */
        interpolate_4d: () => (
          /* binding */
          y
        ),
        /* harmony export */
        layer_norm: () => (
          /* binding */
          j
        ),
        /* harmony export */
        matmul: () => (
          /* binding */
          b
        ),
        /* harmony export */
        mean: () => (
          /* binding */
          ne
        ),
        /* harmony export */
        mean_pooling: () => (
          /* binding */
          W
        ),
        /* harmony export */
        ones: () => (
          /* binding */
          ce
        ),
        /* harmony export */
        ones_like: () => (
          /* binding */
          ke
        ),
        /* harmony export */
        permute: () => (
          /* binding */
          v
        ),
        /* harmony export */
        quantize_embeddings: () => (
          /* binding */
          Ge
        ),
        /* harmony export */
        rand: () => (
          /* binding */
          tt
        ),
        /* harmony export */
        rfft: () => (
          /* binding */
          I
        ),
        /* harmony export */
        slice: () => (
          /* binding */
          ie
        ),
        /* harmony export */
        stack: () => (
          /* binding */
          x
        ),
        /* harmony export */
        std_mean: () => (
          /* binding */
          le
        ),
        /* harmony export */
        topk: () => (
          /* binding */
          K
        ),
        /* harmony export */
        zeros: () => (
          /* binding */
          Fe
        ),
        /* harmony export */
        zeros_like: () => (
          /* binding */
          Ee
        )
        /* harmony export */
      });
      var _ = r(
        /*! ./maths.js */
        "./src/utils/maths.js"
      ), D = r(
        /*! ../backends/onnx.js */
        "./src/backends/onnx.js"
      ), U = r(
        /*! ../ops/registry.js */
        "./src/ops/registry.js"
      );
      const Y = Object.freeze({
        float32: Float32Array,
        float16: Uint16Array,
        float64: Float64Array,
        string: Array,
        // string[]
        int8: Int8Array,
        uint8: Uint8Array,
        int16: Int16Array,
        uint16: Uint16Array,
        int32: Int32Array,
        uint32: Uint32Array,
        int64: BigInt64Array,
        uint64: BigUint64Array,
        bool: Uint8Array,
        uint4: Uint8Array,
        int4: Int8Array
      });
      class R {
        /**
         * Create a new Tensor or copy an existing Tensor.
         * @param {[DataType, DataArray, number[]]|[ONNXTensor]} args
         */
        constructor(...J) {
          fe(this, "ort_tensor");
          return (0, D.isONNXTensor)(J[0]) ? this.ort_tensor = /** @type {ONNXTensor} */
          J[0] : this.ort_tensor = new D.Tensor(
            /** @type {DataType} */
            J[0],
            /** @type {Exclude<import('./maths.js').AnyTypedArray, Uint8ClampedArray>} */
            J[1],
            J[2]
          ), new Proxy(this, {
            get: (de, Ce) => {
              if (typeof Ce == "string") {
                let Be = Number(Ce);
                if (Number.isInteger(Be))
                  return de._getitem(Be);
              }
              return de[Ce];
            },
            set: (de, Ce, Be) => de[Ce] = Be
          });
        }
        /** @type {number[]} Dimensions of the tensor. */
        get dims() {
          return this.ort_tensor.dims;
        }
        set dims(J) {
          this.ort_tensor.dims = J;
        }
        /** @type {DataType} Type of the tensor. */
        get type() {
          return this.ort_tensor.type;
        }
        /** @type {DataArray} The data stored in the tensor. */
        get data() {
          return this.ort_tensor.data;
        }
        /** @type {number} The number of elements in the tensor. */
        get size() {
          return this.ort_tensor.size;
        }
        /** @type {string} The location of the tensor data. */
        get location() {
          return this.ort_tensor.location;
        }
        dispose() {
          this.ort_tensor.dispose();
        }
        /**
         * Returns an iterator object for iterating over the tensor data in row-major order.
         * If the tensor has more than one dimension, the iterator will yield subarrays.
         * @returns {Iterator} An iterator object for iterating over the tensor data in row-major order.
         */
        *[Symbol.iterator]() {
          const [J, ...de] = this.dims;
          if (de.length > 0) {
            const Ce = de.reduce((Be, Ze) => Be * Ze);
            for (let Be = 0; Be < J; ++Be)
              yield this._subarray(Be, Ce, de);
          } else
            yield* this.data;
        }
        /**
         * Index into a Tensor object.
         * @param {number} index The index to access.
         * @returns {Tensor} The data at the specified index.
         */
        _getitem(J) {
          const [de, ...Ce] = this.dims;
          if (J = S(J, de), Ce.length > 0) {
            const Be = Ce.reduce((Ze, te) => Ze * te);
            return this._subarray(J, Be, Ce);
          } else
            return new R(this.type, [this.data[J]], Ce);
        }
        /**
         * @param {number|bigint} item The item to search for in the tensor
         * @returns {number} The index of the first occurrence of item in the tensor data.
         */
        indexOf(J) {
          const de = this.data;
          for (let Ce = 0; Ce < de.length; ++Ce)
            if (de[Ce] == J)
              return Ce;
          return -1;
        }
        /**
         * @param {number} index
         * @param {number} iterSize
         * @param {any} iterDims
         * @returns {Tensor}
         */
        _subarray(J, de, Ce) {
          const Be = J * de, Ze = (J + 1) * de, te = "subarray" in this.data ? this.data.subarray(Be, Ze) : this.data.slice(Be, Ze);
          return new R(this.type, te, Ce);
        }
        /**
         * Returns the value of this tensor as a standard JavaScript Number. This only works
         * for tensors with one element. For other cases, see `Tensor.tolist()`.
         * @returns {number|bigint} The value of this tensor as a standard JavaScript Number.
         * @throws {Error} If the tensor has more than one element.
         */
        item() {
          const J = this.data;
          if (J.length !== 1)
            throw new Error(`a Tensor with ${J.length} elements cannot be converted to Scalar`);
          return J[0];
        }
        /**
         * Convert tensor data to a n-dimensional JS list
         * @returns {Array}
         */
        tolist() {
          return g(this.data, this.dims);
        }
        /**
         * Return a new Tensor with the sigmoid function applied to each element.
         * @returns {Tensor} The tensor with the sigmoid function applied.
         */
        sigmoid() {
          return this.clone().sigmoid_();
        }
        /**
         * Applies the sigmoid function to the tensor in place.
         * @returns {Tensor} Returns `this`.
         */
        sigmoid_() {
          const J = this.data;
          for (let de = 0; de < J.length; ++de)
            J[de] = 1 / (1 + Math.exp(-J[de]));
          return this;
        }
        /**
         * Return a new Tensor with a callback function applied to each element.
         * @param {Function} callback - The function to apply to each element. It should take three arguments:
         *                              the current element, its index, and the tensor's data array.
         * @returns {Tensor} A new Tensor with the callback function applied to each element.
         */
        map(J) {
          return this.clone().map_(J);
        }
        /**
         * Apply a callback function to each element of the tensor in place.
         * @param {Function} callback - The function to apply to each element. It should take three arguments:
         *                              the current element, its index, and the tensor's data array.
         * @returns {Tensor} Returns `this`.
         */
        map_(J) {
          const de = this.data;
          for (let Ce = 0; Ce < de.length; ++Ce)
            de[Ce] = J(de[Ce], Ce, de);
          return this;
        }
        /**
         * Return a new Tensor with every element multiplied by a constant.
         * @param {number} val The value to multiply by.
         * @returns {Tensor} The new tensor.
         */
        mul(J) {
          return this.clone().mul_(J);
        }
        /**
         * Multiply the tensor by a constant in place.
         * @param {number} val The value to multiply by.
         * @returns {Tensor} Returns `this`.
         */
        mul_(J) {
          const de = this.data;
          for (let Ce = 0; Ce < de.length; ++Ce)
            de[Ce] *= J;
          return this;
        }
        /**
         * Return a new Tensor with every element divided by a constant.
         * @param {number} val The value to divide by.
         * @returns {Tensor} The new tensor.
         */
        div(J) {
          return this.clone().div_(J);
        }
        /**
         * Divide the tensor by a constant in place.
         * @param {number} val The value to divide by.
         * @returns {Tensor} Returns `this`.
         */
        div_(J) {
          const de = this.data;
          for (let Ce = 0; Ce < de.length; ++Ce)
            de[Ce] /= J;
          return this;
        }
        /**
         * Return a new Tensor with every element added by a constant.
         * @param {number} val The value to add by.
         * @returns {Tensor} The new tensor.
         */
        add(J) {
          return this.clone().add_(J);
        }
        /**
         * Add the tensor by a constant in place.
         * @param {number} val The value to add by.
         * @returns {Tensor} Returns `this`.
         */
        add_(J) {
          const de = this.data;
          for (let Ce = 0; Ce < de.length; ++Ce)
            de[Ce] += J;
          return this;
        }
        /**
         * Return a new Tensor with every element subtracted by a constant.
         * @param {number} val The value to subtract by.
         * @returns {Tensor} The new tensor.
         */
        sub(J) {
          return this.clone().sub_(J);
        }
        /**
         * Subtract the tensor by a constant in place.
         * @param {number} val The value to subtract by.
         * @returns {Tensor} Returns `this`.
         */
        sub_(J) {
          const de = this.data;
          for (let Ce = 0; Ce < de.length; ++Ce)
            de[Ce] -= J;
          return this;
        }
        /**
         * Creates a deep copy of the current Tensor.
         * @returns {Tensor} A new Tensor with the same type, data, and dimensions as the original.
         */
        clone() {
          return new R(this.type, this.data.slice(), this.dims.slice());
        }
        /**
         * Performs a slice operation on the Tensor along specified dimensions.
         *
         * Consider a Tensor that has a dimension of [4, 7]:
         * ```
         * [ 1,  2,  3,  4,  5,  6,  7]
         * [ 8,  9, 10, 11, 12, 13, 14]
         * [15, 16, 17, 18, 19, 20, 21]
         * [22, 23, 24, 25, 26, 27, 28]
         * ```
         * We can slice against the two dims of row and column, for instance in this
         * case we can start at the second element, and return to the second last,
         * like this:
         * ```
         * tensor.slice([1, -1], [1, -1]);
         * ```
         * which would return:
         * ```
         * [  9, 10, 11, 12, 13 ]
         * [ 16, 17, 18, 19, 20 ]
         * ```
         *
         * @param {...(number|number[]|null)} slices The slice specifications for each dimension.
         * - If a number is given, then a single element is selected.
         * - If an array of two numbers is given, then a range of elements [start, end (exclusive)] is selected.
         * - If null is given, then the entire dimension is selected.
         * @returns {Tensor} A new Tensor containing the selected elements.
         * @throws {Error} If the slice input is invalid.
         */
        slice(...J) {
          const de = [], Ce = [];
          for (let ae = 0; ae < this.dims.length; ++ae) {
            let Te = J[ae];
            if (Te == null)
              Ce.push([0, this.dims[ae]]), de.push(this.dims[ae]);
            else if (typeof Te == "number")
              Te = S(Te, this.dims[ae], ae), Ce.push([Te, Te + 1]);
            else if (Array.isArray(Te) && Te.length === 2) {
              let [Ue, Ve] = Te;
              if (Ue = Ue === null ? 0 : S(Ue, this.dims[ae], ae, !1), Ve = Ve === null ? this.dims[ae] : S(Ve, this.dims[ae], ae, !1), Ue > Ve)
                throw new Error(`Invalid slice: ${Te}`);
              const Ne = [
                Math.max(Ue, 0),
                Math.min(Ve, this.dims[ae])
              ];
              Ce.push(Ne), de.push(Ne[1] - Ne[0]);
            } else
              throw new Error(`Invalid slice: ${Te}`);
          }
          const Be = Ce.map(([ae, Te]) => Te - ae), Ze = Be.reduce((ae, Te) => ae * Te), te = this.data, Ke = new te.constructor(Ze), je = this.stride();
          for (let ae = 0; ae < Ze; ++ae) {
            let Te = 0;
            for (let Ue = Be.length - 1, Ve = ae; Ue >= 0; --Ue) {
              const Ne = Be[Ue];
              Te += (Ve % Ne + Ce[Ue][0]) * je[Ue], Ve = Math.floor(Ve / Ne);
            }
            Ke[ae] = te[Te];
          }
          return new R(this.type, Ke, de);
        }
        /**
         * Return a permuted version of this Tensor, according to the provided dimensions.
         * @param  {...number} dims Dimensions to permute.
         * @returns {Tensor} The permuted tensor.
         */
        permute(...J) {
          return v(this, J);
        }
        // TODO: implement transpose. For now (backwards compatibility), it's just an alias for permute()
        transpose(...J) {
          return this.permute(...J);
        }
        /**
         * Returns the sum of each row of the input tensor in the given dimension dim.
         *
         * @param {number} [dim=null] The dimension or dimensions to reduce. If `null`, all dimensions are reduced.
         * @param {boolean} keepdim Whether the output tensor has `dim` retained or not.
         * @returns The summed tensor
         */
        sum(J = null, de = !1) {
          return this.norm(1, J, de);
        }
        /**
         * Returns the matrix norm or vector norm of a given tensor.
         * @param {number|string} [p='fro'] The order of norm
         * @param {number} [dim=null] Specifies which dimension of the tensor to calculate the norm across.
         * If dim is None, the norm will be calculated across all dimensions of input.
         * @param {boolean} [keepdim=false] Whether the output tensors have dim retained or not.
         * @returns {Tensor} The norm of the tensor.
         */
        norm(J = "fro", de = null, Ce = !1) {
          if (J === "fro")
            J = 2;
          else if (typeof J == "string")
            throw Error(`Unsupported norm: ${J}`);
          const Be = this.data, Ze = (ae, Te) => ae + Te ** J;
          if (de === null) {
            const ae = Be.reduce(Ze, 0) ** (1 / J);
            return new R(this.type, [ae], []);
          }
          const [te, Ke, je] = F(Ze, this, de, Ce);
          if (J !== 1)
            for (let ae = 0; ae < Ke.length; ++ae)
              Ke[ae] = Ke[ae] ** (1 / J);
          return new R(te, Ke, je);
        }
        /**
         * Performs `L_p` normalization of inputs over specified dimension. Operates in place.
         * @param {number} [p=2] The exponent value in the norm formulation
         * @param {number} [dim=1] The dimension to reduce
         * @returns {Tensor} `this` for operation chaining.
         */
        normalize_(J = 2, de = 1) {
          de = S(de, this.dims.length);
          const Ce = this.norm(J, de, !0), Be = this.data, Ze = Ce.data;
          for (let te = 0; te < Be.length; ++te) {
            let Ke = 0;
            for (let je = this.dims.length - 1, ae = te, Te = 1; je >= 0; --je) {
              const Ue = this.dims[je];
              if (je !== de) {
                const Ve = ae % Ue;
                Ke += Ve * Te, Te *= this.dims[je];
              }
              ae = Math.floor(ae / Ue);
            }
            Be[te] /= Ze[Ke];
          }
          return this;
        }
        /**
         * Performs `L_p` normalization of inputs over specified dimension.
         * @param {number} [p=2] The exponent value in the norm formulation
         * @param {number} [dim=1] The dimension to reduce
         * @returns {Tensor} The normalized tensor.
         */
        normalize(J = 2, de = 1) {
          return this.clone().normalize_(J, de);
        }
        /**
         * Compute and return the stride of this tensor.
         * Stride is the jump necessary to go from one element to the next one in the specified dimension dim.
         * @returns {number[]} The stride of this tensor.
         */
        stride() {
          return be(this.dims);
        }
        /**
         * Returns a tensor with all specified dimensions of input of size 1 removed.
         *
         * NOTE: The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.
         * If you would like a copy, use `tensor.clone()` before squeezing.
         *
         * @param {number|number[]} [dim=null] If given, the input will be squeezed only in the specified dimensions.
         * @returns {Tensor} The squeezed tensor
         */
        squeeze(J = null) {
          return new R(
            this.type,
            this.data,
            q(this.dims, J)
          );
        }
        /**
         * In-place version of @see {@link Tensor.squeeze}
         */
        squeeze_(J = null) {
          return this.dims = q(this.dims, J), this;
        }
        /**
         * Returns a new tensor with a dimension of size one inserted at the specified position.
         *
         * NOTE: The returned tensor shares the same underlying data with this tensor.
         *
         * @param {number} dim The index at which to insert the singleton dimension
         * @returns {Tensor} The unsqueezed tensor
         */
        unsqueeze(J = null) {
          return new R(
            this.type,
            this.data,
            A(this.dims, J)
          );
        }
        /**
         * In-place version of @see {@link Tensor.unsqueeze}
         */
        unsqueeze_(J = null) {
          return this.dims = A(this.dims, J), this;
        }
        /**
         * In-place version of @see {@link Tensor.flatten}
         */
        flatten_(J = 0, de = -1) {
          de = (de + this.dims.length) % this.dims.length;
          let Ce = this.dims.slice(0, J), Be = this.dims.slice(J, de + 1), Ze = this.dims.slice(de + 1);
          return this.dims = [...Ce, Be.reduce((te, Ke) => te * Ke, 1), ...Ze], this;
        }
        /**
         * Flattens input by reshaping it into a one-dimensional tensor.
         * If `start_dim` or `end_dim` are passed, only dimensions starting with `start_dim`
         * and ending with `end_dim` are flattened. The order of elements in input is unchanged.
         * @param {number} start_dim the first dim to flatten
         * @param {number} end_dim the last dim to flatten
         * @returns {Tensor} The flattened tensor.
         */
        flatten(J = 0, de = -1) {
          return this.clone().flatten_(J, de);
        }
        /**
         * Returns a new tensor with the same data as the `self` tensor but of a different `shape`.
         * @param  {...number} dims the desired size
         * @returns {Tensor} The tensor with the same data but different shape
         */
        view(...J) {
          let de = -1;
          for (let Be = 0; Be < J.length; ++Be)
            if (J[Be] === -1) {
              if (de !== -1)
                throw new Error("Only one dimension can be inferred");
              de = Be;
            }
          const Ce = this.data;
          if (de !== -1) {
            const Be = J.reduce((Ze, te, Ke) => Ke !== de ? Ze * te : Ze, 1);
            J[de] = Ce.length / Be;
          }
          return new R(this.type, Ce, J);
        }
        neg_() {
          const J = this.data;
          for (let de = 0; de < J.length; ++de)
            J[de] = -J[de];
          return this;
        }
        neg() {
          return this.clone().neg_();
        }
        /**
         * Computes input > val element-wise.
         * @param {number} val The value to compare with.
         * @returns {Tensor} A boolean tensor that is `true` where input is greater than other and `false` elsewhere.
         */
        gt(J) {
          const de = new Uint8Array(this.data.length), Ce = this.data;
          for (let Be = 0; Be < Ce.length; ++Be)
            de[Be] = Ce[Be] > J ? 1 : 0;
          return new R("bool", de, this.dims);
        }
        /**
         * Computes input < val element-wise.
         * @param {number} val The value to compare with.
         * @returns {Tensor} A boolean tensor that is `true` where input is less than other and `false` elsewhere.
         */
        lt(J) {
          const de = new Uint8Array(this.data.length), Ce = this.data;
          for (let Be = 0; Be < Ce.length; ++Be)
            de[Be] = Ce[Be] < J ? 1 : 0;
          return new R("bool", de, this.dims);
        }
        /**
         * In-place version of @see {@link Tensor.clamp}
         */
        clamp_(J, de) {
          const Ce = this.data;
          for (let Be = 0; Be < Ce.length; ++Be)
            Ce[Be] = Math.min(Math.max(Ce[Be], J), de);
          return this;
        }
        /**
         * Clamps all elements in input into the range [ min, max ]
         * @param {number} min lower-bound of the range to be clamped to
         * @param {number} max upper-bound of the range to be clamped to
         * @returns {Tensor} the output tensor.
         */
        clamp(J, de) {
          return this.clone().clamp_(J, de);
        }
        /**
         * In-place version of @see {@link Tensor.round}
         */
        round_() {
          const J = this.data;
          for (let de = 0; de < J.length; ++de)
            J[de] = Math.round(J[de]);
          return this;
        }
        /**
         * Rounds elements of input to the nearest integer.
         * @returns {Tensor} the output tensor.
         */
        round() {
          return this.clone().round_();
        }
        mean(J = null, de = !1) {
          return ne(this, J, de);
        }
        min(J = null, de = !1) {
          if (J === null) {
            const te = (0, _.min)(this.data)[0];
            return new R(this.type, [te], [
              /* scalar */
            ]);
          }
          const [Ce, Be, Ze] = F((te, Ke) => Math.min(te, Ke), this, J, de, 1 / 0);
          return new R(Ce, Be, Ze);
        }
        max(J = null, de = !1) {
          if (J === null) {
            const te = (0, _.max)(this.data)[0];
            return new R(this.type, [te], [
              /* scalar */
            ]);
          }
          const [Ce, Be, Ze] = F((te, Ke) => Math.max(te, Ke), this, J, de, -1 / 0);
          return new R(Ce, Be, Ze);
        }
        argmin(J = null, de = !1) {
          if (J !== null)
            throw new Error("`dim !== null` not yet implemented.");
          const Ce = (0, _.min)(this.data)[1];
          return new R("int64", [BigInt(Ce)], []);
        }
        argmax(J = null, de = !1) {
          if (J !== null)
            throw new Error("`dim !== null` not yet implemented.");
          const Ce = (0, _.max)(this.data)[1];
          return new R("int64", [BigInt(Ce)], []);
        }
        /**
         * Performs Tensor dtype conversion.
         * @param {DataType} type The desired data type.
         * @returns {Tensor} The converted tensor.
         */
        to(J) {
          if (this.type === J) return this;
          if (!Y.hasOwnProperty(J))
            throw new Error(`Unsupported type: ${J}`);
          let de;
          const Ce = ["int64", "uint64"].includes(this.type), Be = ["int64", "uint64"].includes(J);
          return Ce && !Be ? de = Number : !Ce && Be && (de = BigInt), new R(J, Y[J].from(this.data, de), this.dims);
        }
      }
      function g(ye, J) {
        const de = ye.length, Ce = J.reduce((Ze, te) => Ze * te);
        if (de !== Ce)
          throw Error(`cannot reshape array of size ${de} into shape (${J})`);
        let Be = ye;
        for (let Ze = J.length - 1; Ze >= 0; Ze--)
          Be = Be.reduce((te, Ke) => {
            let je = te[te.length - 1];
            return je.length < J[Ze] ? je.push(Ke) : te.push([Ke]), te;
          }, [[]]);
        return Be[0];
      }
      function v(ye, J) {
        const [de, Ce] = (0, _.permute_data)(ye.data, ye.dims, J);
        return new R(ye.type, de, Ce);
      }
      function M(ye, [J, de], Ce = "bilinear", Be = !1) {
        const Ze = ye.dims.at(-3) ?? 1, te = ye.dims.at(-2), Ke = ye.dims.at(-1);
        let je = (0, _.interpolate_data)(
          /** @type {import('./maths.js').TypedArray}*/
          ye.data,
          [Ze, te, Ke],
          [J, de],
          Ce,
          Be
        );
        return new R(ye.type, je, [Ze, J, de]);
      }
      async function y(ye, {
        size: J = null,
        mode: de = "bilinear"
      } = {}) {
        if (ye.dims.length !== 4)
          throw new Error("`interpolate_4d` currently only supports 4D input.");
        if (!J)
          throw new Error("`interpolate_4d` requires a `size` argument.");
        let Ce;
        if (J.length === 2)
          Ce = [...ye.dims.slice(0, 2), ...J];
        else if (J.length === 3)
          Ce = [ye.dims[0], ...J];
        else if (J.length === 4)
          Ce = J;
        else
          throw new Error("`size` must be of length 2, 3, or 4.");
        let Be;
        if (de === "nearest")
          Be = await U.TensorOpRegistry.nearest_interpolate_4d;
        else if (de === "bilinear")
          Be = await U.TensorOpRegistry.bilinear_interpolate_4d;
        else if (de === "bicubic")
          Be = await U.TensorOpRegistry.bicubic_interpolate_4d;
        else
          throw new Error(`Unsupported mode: ${de}`);
        const Ze = new R("int64", new BigInt64Array(Ce.map(BigInt)), [Ce.length]);
        return await Be({ x: ye, s: Ze });
      }
      async function b(ye, J) {
        return await (await U.TensorOpRegistry.matmul)({ a: ye, b: J });
      }
      async function I(ye, J) {
        return await (await U.TensorOpRegistry.rfft)({ x: ye, a: J });
      }
      async function K(ye, J) {
        const de = await U.TensorOpRegistry.top_k;
        return J == null ? J = ye.dims.at(-1) : J = Math.min(J, ye.dims.at(-1)), await de({
          x: ye,
          k: new R(
            "int64",
            [BigInt(J)],
            [1]
          )
        });
      }
      const se = (ye) => new R("int64", ye, [ye.length]);
      async function ie(ye, J, de, Ce, Be) {
        return await (await U.TensorOpRegistry.slice)({
          x: ye,
          s: se(J),
          e: se(de),
          a: se(Ce),
          t: se(Be ?? new Array(Ce.length).fill(1))
        });
      }
      function W(ye, J) {
        const de = ye.data, Ce = J.data, Be = [ye.dims[0], ye.dims[2]], Ze = new de.constructor(Be[0] * Be[1]), [te, Ke, je] = ye.dims;
        let ae = 0;
        for (let Te = 0; Te < te; ++Te) {
          const Ue = Te * je * Ke;
          for (let Ve = 0; Ve < je; ++Ve) {
            let Ne = 0, Re = 0;
            const st = Te * Ke, dt = Ue + Ve;
            for (let lt = 0; lt < Ke; ++lt) {
              const ht = Number(Ce[st + lt]);
              Re += ht, Ne += de[dt + lt * je] * ht;
            }
            const ct = Ne / Re;
            Ze[ae++] = ct;
          }
        }
        return new R(
          ye.type,
          Ze,
          Be
        );
      }
      function j(ye, J, {
        eps: de = 1e-5
      } = {}) {
        if (ye.dims.length !== 2)
          throw new Error("`layer_norm` currently only supports 2D input.");
        const [Ce, Be] = ye.dims;
        if (J.length !== 1 && J[0] !== Be)
          throw new Error("`normalized_shape` must be a 1D array with shape `[input.dims[1]]`.");
        const [Ze, te] = le(ye, 1, 0, !0), Ke = (
          /** @type {Float32Array} */
          Ze.data
        ), je = (
          /** @type {Float32Array} */
          te.data
        ), ae = (
          /** @type {Float32Array} */
          ye.data
        ), Te = new ae.constructor(ae.length);
        for (let Ue = 0; Ue < Ce; ++Ue) {
          const Ve = Ue * Be;
          for (let Ne = 0; Ne < Be; ++Ne) {
            const Re = Ve + Ne;
            Te[Re] = (ae[Re] - je[Ue]) / (Ke[Ue] + de);
          }
        }
        return new R(ye.type, Te, ye.dims);
      }
      function q(ye, J) {
        return ye = ye.slice(), J === null ? ye = ye.filter((de) => de !== 1) : typeof J == "number" ? ye[J] === 1 && ye.splice(J, 1) : Array.isArray(J) && (ye = ye.filter((de, Ce) => de !== 1 || !J.includes(Ce))), ye;
      }
      function A(ye, J) {
        return J = S(J, ye.length + 1), ye = ye.slice(), ye.splice(J, 0, 1), ye;
      }
      function S(ye, J, de = null, Ce = !0) {
        if (Ce && (ye < -J || ye >= J))
          throw new Error(`IndexError: index ${ye} is out of bounds for dimension${de === null ? "" : " " + de} with size ${J}`);
        return ye < 0 && (ye = (ye % J + J) % J), ye;
      }
      function w(ye, J = 0) {
        J = S(J, ye[0].dims.length);
        const de = ye[0].dims.slice();
        de[J] = ye.reduce((te, Ke) => te + Ke.dims[J], 0);
        const Ce = de.reduce((te, Ke) => te * Ke, 1), Be = new ye[0].data.constructor(Ce), Ze = ye[0].type;
        if (J === 0) {
          let te = 0;
          for (const Ke of ye) {
            const je = Ke.data;
            Be.set(je, te), te += je.length;
          }
        } else {
          let te = 0;
          for (let Ke = 0; Ke < ye.length; ++Ke) {
            const { data: je, dims: ae } = ye[Ke];
            for (let Te = 0; Te < je.length; ++Te) {
              let Ue = 0;
              for (let Ve = ae.length - 1, Ne = Te, Re = 1; Ve >= 0; --Ve) {
                const st = ae[Ve];
                let dt = Ne % st;
                Ve === J && (dt += te), Ue += dt * Re, Re *= de[Ve], Ne = Math.floor(Ne / st);
              }
              Be[Ue] = je[Te];
            }
            te += ae[J];
          }
        }
        return new R(Ze, Be, de);
      }
      function x(ye, J = 0) {
        return w(ye.map((de) => de.unsqueeze(J)), J);
      }
      function F(ye, J, de = null, Ce = !1, Be = null) {
        const Ze = J.data, te = J.dims;
        de = S(de, te.length);
        const Ke = te.slice();
        Ke[de] = 1;
        const je = new Ze.constructor(Ze.length / te[de]);
        Be !== null && je.fill(Be);
        for (let ae = 0; ae < Ze.length; ++ae) {
          let Te = 0;
          for (let Ue = te.length - 1, Ve = ae, Ne = 1; Ue >= 0; --Ue) {
            const Re = te[Ue];
            if (Ue !== de) {
              const st = Ve % Re;
              Te += st * Ne, Ne *= Ke[Ue];
            }
            Ve = Math.floor(Ve / Re);
          }
          je[Te] = ye(je[Te], Ze[ae], ae, Te);
        }
        return Ce || Ke.splice(de, 1), [J.type, je, Ke];
      }
      function le(ye, J = null, de = 1, Ce = !1) {
        const Be = (
          /** @type {Float32Array} */
          ye.data
        ), Ze = ye.dims;
        if (J === null) {
          const Ne = Be.reduce((ct, lt) => ct + lt, 0) / Be.length, Re = Math.sqrt(Be.reduce((ct, lt) => ct + (lt - Ne) ** 2, 0) / (Be.length - de)), st = new R(ye.type, [Ne], [
            /* scalar */
          ]);
          return [new R(ye.type, [Re], [
            /* scalar */
          ]), st];
        }
        J = S(J, Ze.length);
        const te = ne(ye, J, Ce), Ke = te.data, [je, ae, Te] = F((Ve, Ne, Re, st) => Ve + (Ne - Ke[st]) ** 2, ye, J, Ce);
        for (let Ve = 0; Ve < ae.length; ++Ve)
          ae[Ve] = Math.sqrt(ae[Ve] / (Ze[J] - de));
        return [new R(je, ae, Te), te];
      }
      function ne(ye, J = null, de = !1) {
        const Ce = ye.dims, Be = (
          /** @type {Float32Array} */
          ye.data
        );
        if (J === null) {
          const je = Be.reduce((ae, Te) => ae + Te, 0);
          return new R(ye.type, [je / Be.length], [
            /* scalar */
          ]);
        }
        J = S(J, Ce.length);
        const [Ze, te, Ke] = F((je, ae) => je + ae, ye, J, de);
        if (Ce[J] !== 1)
          for (let je = 0; je < te.length; ++je)
            te[je] /= Ce[J];
        return new R(Ze, te, Ke);
      }
      function be(ye) {
        const J = new Array(ye.length);
        for (let de = ye.length - 1, Ce = 1; de >= 0; --de)
          J[de] = Ce, Ce *= ye[de];
        return J;
      }
      function _e(ye, J, de, Ce) {
        const Be = ye.reduce((Ze, te) => Ze * te, 1);
        return new R(
          de,
          new Ce(Be).fill(J),
          ye
        );
      }
      function re(ye, J) {
        let de, Ce;
        if (typeof J == "number")
          de = "float32", Ce = Float32Array;
        else if (typeof J == "bigint")
          de = "int64", Ce = BigInt64Array;
        else if (typeof J == "boolean")
          de = "bool", Ce = Uint8Array;
        else
          throw new Error(`Unsupported data type: ${typeof J}`);
        return _e(ye, J, de, Ce);
      }
      function xe(ye, J) {
        return re(ye.dims, J);
      }
      function ce(ye) {
        return _e(ye, 1n, "int64", BigInt64Array);
      }
      function ke(ye) {
        return ce(ye.dims);
      }
      function Fe(ye) {
        return _e(ye, 0n, "int64", BigInt64Array);
      }
      function Ee(ye) {
        return Fe(ye.dims);
      }
      function tt(ye) {
        const J = ye.reduce((de, Ce) => de * Ce, 1);
        return new R(
          "float32",
          Float32Array.from({ length: J }, () => Math.random()),
          ye
        );
      }
      function Ge(ye, J) {
        if (ye.dims.length !== 2)
          throw new Error("The tensor must have 2 dimensions");
        if (ye.dims.at(-1) % 8 !== 0)
          throw new Error("The last dimension of the tensor must be a multiple of 8");
        if (!["binary", "ubinary"].includes(J))
          throw new Error("The precision must be either 'binary' or 'ubinary'");
        const de = J === "binary", Ce = de ? "int8" : "uint8", Be = de ? Int8Array : Uint8Array, Ze = ye.data, te = new Be(Ze.length / 8);
        for (let Ke = 0; Ke < Ze.length; ++Ke) {
          const je = Ze[Ke] > 0 ? 1 : 0, ae = Math.floor(Ke / 8), Te = Ke % 8;
          te[ae] |= je << 7 - Te, de && Te === 0 && (te[ae] -= 128);
        }
        return new R(Ce, te, [ye.dims[0], ye.dims[1] / 8]);
      }
    }
  )
  /******/
}, of = {};
function ws($e) {
  var $ = of[$e];
  if ($ !== void 0)
    return $.exports;
  var r = of[$e] = {
    /******/
    // no module.id needed
    /******/
    // no module.loaded needed
    /******/
    exports: {}
    /******/
  };
  return af[$e](r, r.exports, ws), r.exports;
}
ws.m = af;
(() => {
  var $e = Object.getPrototypeOf ? (r) => Object.getPrototypeOf(r) : (r) => r.__proto__, $;
  ws.t = function(r, _) {
    if (_ & 1 && (r = this(r)), _ & 8 || typeof r == "object" && r && (_ & 4 && r.__esModule || _ & 16 && typeof r.then == "function"))
      return r;
    var D = /* @__PURE__ */ Object.create(null);
    ws.r(D);
    var U = {};
    $ = $ || [null, $e({}), $e([]), $e($e)];
    for (var Y = _ & 2 && r; typeof Y == "object" && !~$.indexOf(Y); Y = $e(Y))
      Object.getOwnPropertyNames(Y).forEach((R) => U[R] = () => r[R]);
    return U.default = () => r, ws.d(D, U), D;
  };
})();
ws.d = ($e, $) => {
  for (var r in $)
    ws.o($, r) && !ws.o($e, r) && Object.defineProperty($e, r, { enumerable: !0, get: $[r] });
};
ws.o = ($e, $) => Object.prototype.hasOwnProperty.call($e, $);
ws.r = ($e) => {
  typeof Symbol < "u" && Symbol.toStringTag && Object.defineProperty($e, Symbol.toStringTag, { value: "Module" }), Object.defineProperty($e, "__esModule", { value: !0 });
};
(() => {
  var $e;
  if (typeof import.meta.url == "string" && ($e = import.meta.url), !$e) throw new Error("Automatic publicPath is not supported in this browser");
  $e = $e.replace(/#.*$/, "").replace(/\?.*$/, "").replace(/\/[^\/]+$/, "/"), ws.p = $e;
})();
ws.b = new URL(import.meta.url);
var c = {};
(() => {
  /*!*****************************!*\
    !*** ./src/transformers.js ***!
    \*****************************/
  ws.r(c), ws.d(c, {
    /* harmony export */
    ASTFeatureExtractor: () => (
      /* reexport safe */
      M.ASTFeatureExtractor
    ),
    /* harmony export */
    ASTForAudioClassification: () => (
      /* reexport safe */
      r.ASTForAudioClassification
    ),
    /* harmony export */
    ASTModel: () => (
      /* reexport safe */
      r.ASTModel
    ),
    /* harmony export */
    ASTPreTrainedModel: () => (
      /* reexport safe */
      r.ASTPreTrainedModel
    ),
    /* harmony export */
    AlbertForMaskedLM: () => (
      /* reexport safe */
      r.AlbertForMaskedLM
    ),
    /* harmony export */
    AlbertForQuestionAnswering: () => (
      /* reexport safe */
      r.AlbertForQuestionAnswering
    ),
    /* harmony export */
    AlbertForSequenceClassification: () => (
      /* reexport safe */
      r.AlbertForSequenceClassification
    ),
    /* harmony export */
    AlbertModel: () => (
      /* reexport safe */
      r.AlbertModel
    ),
    /* harmony export */
    AlbertPreTrainedModel: () => (
      /* reexport safe */
      r.AlbertPreTrainedModel
    ),
    /* harmony export */
    AlbertTokenizer: () => (
      /* reexport safe */
      _.AlbertTokenizer
    ),
    /* harmony export */
    AudioClassificationPipeline: () => (
      /* reexport safe */
      $.AudioClassificationPipeline
    ),
    /* harmony export */
    AutoConfig: () => (
      /* reexport safe */
      D.AutoConfig
    ),
    /* harmony export */
    AutoFeatureExtractor: () => (
      /* reexport safe */
      y.AutoFeatureExtractor
    ),
    /* harmony export */
    AutoImageProcessor: () => (
      /* reexport safe */
      K.AutoImageProcessor
    ),
    /* harmony export */
    AutoModel: () => (
      /* reexport safe */
      r.AutoModel
    ),
    /* harmony export */
    AutoModelForAudioClassification: () => (
      /* reexport safe */
      r.AutoModelForAudioClassification
    ),
    /* harmony export */
    AutoModelForAudioFrameClassification: () => (
      /* reexport safe */
      r.AutoModelForAudioFrameClassification
    ),
    /* harmony export */
    AutoModelForCTC: () => (
      /* reexport safe */
      r.AutoModelForCTC
    ),
    /* harmony export */
    AutoModelForCausalLM: () => (
      /* reexport safe */
      r.AutoModelForCausalLM
    ),
    /* harmony export */
    AutoModelForDepthEstimation: () => (
      /* reexport safe */
      r.AutoModelForDepthEstimation
    ),
    /* harmony export */
    AutoModelForDocumentQuestionAnswering: () => (
      /* reexport safe */
      r.AutoModelForDocumentQuestionAnswering
    ),
    /* harmony export */
    AutoModelForImageClassification: () => (
      /* reexport safe */
      r.AutoModelForImageClassification
    ),
    /* harmony export */
    AutoModelForImageFeatureExtraction: () => (
      /* reexport safe */
      r.AutoModelForImageFeatureExtraction
    ),
    /* harmony export */
    AutoModelForImageMatting: () => (
      /* reexport safe */
      r.AutoModelForImageMatting
    ),
    /* harmony export */
    AutoModelForImageSegmentation: () => (
      /* reexport safe */
      r.AutoModelForImageSegmentation
    ),
    /* harmony export */
    AutoModelForImageToImage: () => (
      /* reexport safe */
      r.AutoModelForImageToImage
    ),
    /* harmony export */
    AutoModelForMaskGeneration: () => (
      /* reexport safe */
      r.AutoModelForMaskGeneration
    ),
    /* harmony export */
    AutoModelForMaskedLM: () => (
      /* reexport safe */
      r.AutoModelForMaskedLM
    ),
    /* harmony export */
    AutoModelForNormalEstimation: () => (
      /* reexport safe */
      r.AutoModelForNormalEstimation
    ),
    /* harmony export */
    AutoModelForObjectDetection: () => (
      /* reexport safe */
      r.AutoModelForObjectDetection
    ),
    /* harmony export */
    AutoModelForPoseEstimation: () => (
      /* reexport safe */
      r.AutoModelForPoseEstimation
    ),
    /* harmony export */
    AutoModelForQuestionAnswering: () => (
      /* reexport safe */
      r.AutoModelForQuestionAnswering
    ),
    /* harmony export */
    AutoModelForSemanticSegmentation: () => (
      /* reexport safe */
      r.AutoModelForSemanticSegmentation
    ),
    /* harmony export */
    AutoModelForSeq2SeqLM: () => (
      /* reexport safe */
      r.AutoModelForSeq2SeqLM
    ),
    /* harmony export */
    AutoModelForSequenceClassification: () => (
      /* reexport safe */
      r.AutoModelForSequenceClassification
    ),
    /* harmony export */
    AutoModelForSpeechSeq2Seq: () => (
      /* reexport safe */
      r.AutoModelForSpeechSeq2Seq
    ),
    /* harmony export */
    AutoModelForTextToSpectrogram: () => (
      /* reexport safe */
      r.AutoModelForTextToSpectrogram
    ),
    /* harmony export */
    AutoModelForTextToWaveform: () => (
      /* reexport safe */
      r.AutoModelForTextToWaveform
    ),
    /* harmony export */
    AutoModelForTokenClassification: () => (
      /* reexport safe */
      r.AutoModelForTokenClassification
    ),
    /* harmony export */
    AutoModelForUniversalSegmentation: () => (
      /* reexport safe */
      r.AutoModelForUniversalSegmentation
    ),
    /* harmony export */
    AutoModelForVision2Seq: () => (
      /* reexport safe */
      r.AutoModelForVision2Seq
    ),
    /* harmony export */
    AutoModelForXVector: () => (
      /* reexport safe */
      r.AutoModelForXVector
    ),
    /* harmony export */
    AutoModelForZeroShotObjectDetection: () => (
      /* reexport safe */
      r.AutoModelForZeroShotObjectDetection
    ),
    /* harmony export */
    AutoProcessor: () => (
      /* reexport safe */
      W.AutoProcessor
    ),
    /* harmony export */
    AutoTokenizer: () => (
      /* reexport safe */
      _.AutoTokenizer
    ),
    /* harmony export */
    AutomaticSpeechRecognitionPipeline: () => (
      /* reexport safe */
      $.AutomaticSpeechRecognitionPipeline
    ),
    /* harmony export */
    BartForConditionalGeneration: () => (
      /* reexport safe */
      r.BartForConditionalGeneration
    ),
    /* harmony export */
    BartForSequenceClassification: () => (
      /* reexport safe */
      r.BartForSequenceClassification
    ),
    /* harmony export */
    BartModel: () => (
      /* reexport safe */
      r.BartModel
    ),
    /* harmony export */
    BartPretrainedModel: () => (
      /* reexport safe */
      r.BartPretrainedModel
    ),
    /* harmony export */
    BartTokenizer: () => (
      /* reexport safe */
      _.BartTokenizer
    ),
    /* harmony export */
    BaseModelOutput: () => (
      /* reexport safe */
      r.BaseModelOutput
    ),
    /* harmony export */
    BaseStreamer: () => (
      /* reexport safe */
      j.BaseStreamer
    ),
    /* harmony export */
    BeitFeatureExtractor: () => (
      /* reexport safe */
      I.BeitFeatureExtractor
    ),
    /* harmony export */
    BeitForImageClassification: () => (
      /* reexport safe */
      r.BeitForImageClassification
    ),
    /* harmony export */
    BeitModel: () => (
      /* reexport safe */
      r.BeitModel
    ),
    /* harmony export */
    BeitPreTrainedModel: () => (
      /* reexport safe */
      r.BeitPreTrainedModel
    ),
    /* harmony export */
    BertForMaskedLM: () => (
      /* reexport safe */
      r.BertForMaskedLM
    ),
    /* harmony export */
    BertForQuestionAnswering: () => (
      /* reexport safe */
      r.BertForQuestionAnswering
    ),
    /* harmony export */
    BertForSequenceClassification: () => (
      /* reexport safe */
      r.BertForSequenceClassification
    ),
    /* harmony export */
    BertForTokenClassification: () => (
      /* reexport safe */
      r.BertForTokenClassification
    ),
    /* harmony export */
    BertModel: () => (
      /* reexport safe */
      r.BertModel
    ),
    /* harmony export */
    BertPreTrainedModel: () => (
      /* reexport safe */
      r.BertPreTrainedModel
    ),
    /* harmony export */
    BertTokenizer: () => (
      /* reexport safe */
      _.BertTokenizer
    ),
    /* harmony export */
    BitImageProcessor: () => (
      /* reexport safe */
      I.BitImageProcessor
    ),
    /* harmony export */
    BlenderbotForConditionalGeneration: () => (
      /* reexport safe */
      r.BlenderbotForConditionalGeneration
    ),
    /* harmony export */
    BlenderbotModel: () => (
      /* reexport safe */
      r.BlenderbotModel
    ),
    /* harmony export */
    BlenderbotPreTrainedModel: () => (
      /* reexport safe */
      r.BlenderbotPreTrainedModel
    ),
    /* harmony export */
    BlenderbotSmallForConditionalGeneration: () => (
      /* reexport safe */
      r.BlenderbotSmallForConditionalGeneration
    ),
    /* harmony export */
    BlenderbotSmallModel: () => (
      /* reexport safe */
      r.BlenderbotSmallModel
    ),
    /* harmony export */
    BlenderbotSmallPreTrainedModel: () => (
      /* reexport safe */
      r.BlenderbotSmallPreTrainedModel
    ),
    /* harmony export */
    BlenderbotSmallTokenizer: () => (
      /* reexport safe */
      _.BlenderbotSmallTokenizer
    ),
    /* harmony export */
    BlenderbotTokenizer: () => (
      /* reexport safe */
      _.BlenderbotTokenizer
    ),
    /* harmony export */
    BloomForCausalLM: () => (
      /* reexport safe */
      r.BloomForCausalLM
    ),
    /* harmony export */
    BloomModel: () => (
      /* reexport safe */
      r.BloomModel
    ),
    /* harmony export */
    BloomPreTrainedModel: () => (
      /* reexport safe */
      r.BloomPreTrainedModel
    ),
    /* harmony export */
    BloomTokenizer: () => (
      /* reexport safe */
      _.BloomTokenizer
    ),
    /* harmony export */
    CLIPFeatureExtractor: () => (
      /* reexport safe */
      I.CLIPFeatureExtractor
    ),
    /* harmony export */
    CLIPImageProcessor: () => (
      /* reexport safe */
      I.CLIPImageProcessor
    ),
    /* harmony export */
    CLIPModel: () => (
      /* reexport safe */
      r.CLIPModel
    ),
    /* harmony export */
    CLIPPreTrainedModel: () => (
      /* reexport safe */
      r.CLIPPreTrainedModel
    ),
    /* harmony export */
    CLIPSegForImageSegmentation: () => (
      /* reexport safe */
      r.CLIPSegForImageSegmentation
    ),
    /* harmony export */
    CLIPSegModel: () => (
      /* reexport safe */
      r.CLIPSegModel
    ),
    /* harmony export */
    CLIPSegPreTrainedModel: () => (
      /* reexport safe */
      r.CLIPSegPreTrainedModel
    ),
    /* harmony export */
    CLIPTextModel: () => (
      /* reexport safe */
      r.CLIPTextModel
    ),
    /* harmony export */
    CLIPTextModelWithProjection: () => (
      /* reexport safe */
      r.CLIPTextModelWithProjection
    ),
    /* harmony export */
    CLIPTokenizer: () => (
      /* reexport safe */
      _.CLIPTokenizer
    ),
    /* harmony export */
    CLIPVisionModel: () => (
      /* reexport safe */
      r.CLIPVisionModel
    ),
    /* harmony export */
    CLIPVisionModelWithProjection: () => (
      /* reexport safe */
      r.CLIPVisionModelWithProjection
    ),
    /* harmony export */
    CamembertForMaskedLM: () => (
      /* reexport safe */
      r.CamembertForMaskedLM
    ),
    /* harmony export */
    CamembertForQuestionAnswering: () => (
      /* reexport safe */
      r.CamembertForQuestionAnswering
    ),
    /* harmony export */
    CamembertForSequenceClassification: () => (
      /* reexport safe */
      r.CamembertForSequenceClassification
    ),
    /* harmony export */
    CamembertForTokenClassification: () => (
      /* reexport safe */
      r.CamembertForTokenClassification
    ),
    /* harmony export */
    CamembertModel: () => (
      /* reexport safe */
      r.CamembertModel
    ),
    /* harmony export */
    CamembertPreTrainedModel: () => (
      /* reexport safe */
      r.CamembertPreTrainedModel
    ),
    /* harmony export */
    CamembertTokenizer: () => (
      /* reexport safe */
      _.CamembertTokenizer
    ),
    /* harmony export */
    CausalLMOutput: () => (
      /* reexport safe */
      r.CausalLMOutput
    ),
    /* harmony export */
    CausalLMOutputWithPast: () => (
      /* reexport safe */
      r.CausalLMOutputWithPast
    ),
    /* harmony export */
    ChineseCLIPFeatureExtractor: () => (
      /* reexport safe */
      I.ChineseCLIPFeatureExtractor
    ),
    /* harmony export */
    ChineseCLIPModel: () => (
      /* reexport safe */
      r.ChineseCLIPModel
    ),
    /* harmony export */
    ChineseCLIPPreTrainedModel: () => (
      /* reexport safe */
      r.ChineseCLIPPreTrainedModel
    ),
    /* harmony export */
    ClapAudioModelWithProjection: () => (
      /* reexport safe */
      r.ClapAudioModelWithProjection
    ),
    /* harmony export */
    ClapFeatureExtractor: () => (
      /* reexport safe */
      M.ClapFeatureExtractor
    ),
    /* harmony export */
    ClapModel: () => (
      /* reexport safe */
      r.ClapModel
    ),
    /* harmony export */
    ClapPreTrainedModel: () => (
      /* reexport safe */
      r.ClapPreTrainedModel
    ),
    /* harmony export */
    ClapTextModelWithProjection: () => (
      /* reexport safe */
      r.ClapTextModelWithProjection
    ),
    /* harmony export */
    ClassifierFreeGuidanceLogitsProcessor: () => (
      /* reexport safe */
      A.ClassifierFreeGuidanceLogitsProcessor
    ),
    /* harmony export */
    CodeGenForCausalLM: () => (
      /* reexport safe */
      r.CodeGenForCausalLM
    ),
    /* harmony export */
    CodeGenModel: () => (
      /* reexport safe */
      r.CodeGenModel
    ),
    /* harmony export */
    CodeGenPreTrainedModel: () => (
      /* reexport safe */
      r.CodeGenPreTrainedModel
    ),
    /* harmony export */
    CodeGenTokenizer: () => (
      /* reexport safe */
      _.CodeGenTokenizer
    ),
    /* harmony export */
    CodeLlamaTokenizer: () => (
      /* reexport safe */
      _.CodeLlamaTokenizer
    ),
    /* harmony export */
    CohereForCausalLM: () => (
      /* reexport safe */
      r.CohereForCausalLM
    ),
    /* harmony export */
    CohereModel: () => (
      /* reexport safe */
      r.CohereModel
    ),
    /* harmony export */
    CoherePreTrainedModel: () => (
      /* reexport safe */
      r.CoherePreTrainedModel
    ),
    /* harmony export */
    CohereTokenizer: () => (
      /* reexport safe */
      _.CohereTokenizer
    ),
    /* harmony export */
    ConvBertForMaskedLM: () => (
      /* reexport safe */
      r.ConvBertForMaskedLM
    ),
    /* harmony export */
    ConvBertForQuestionAnswering: () => (
      /* reexport safe */
      r.ConvBertForQuestionAnswering
    ),
    /* harmony export */
    ConvBertForSequenceClassification: () => (
      /* reexport safe */
      r.ConvBertForSequenceClassification
    ),
    /* harmony export */
    ConvBertForTokenClassification: () => (
      /* reexport safe */
      r.ConvBertForTokenClassification
    ),
    /* harmony export */
    ConvBertModel: () => (
      /* reexport safe */
      r.ConvBertModel
    ),
    /* harmony export */
    ConvBertPreTrainedModel: () => (
      /* reexport safe */
      r.ConvBertPreTrainedModel
    ),
    /* harmony export */
    ConvBertTokenizer: () => (
      /* reexport safe */
      _.ConvBertTokenizer
    ),
    /* harmony export */
    ConvNextFeatureExtractor: () => (
      /* reexport safe */
      I.ConvNextFeatureExtractor
    ),
    /* harmony export */
    ConvNextForImageClassification: () => (
      /* reexport safe */
      r.ConvNextForImageClassification
    ),
    /* harmony export */
    ConvNextImageProcessor: () => (
      /* reexport safe */
      I.ConvNextImageProcessor
    ),
    /* harmony export */
    ConvNextModel: () => (
      /* reexport safe */
      r.ConvNextModel
    ),
    /* harmony export */
    ConvNextPreTrainedModel: () => (
      /* reexport safe */
      r.ConvNextPreTrainedModel
    ),
    /* harmony export */
    ConvNextV2ForImageClassification: () => (
      /* reexport safe */
      r.ConvNextV2ForImageClassification
    ),
    /* harmony export */
    ConvNextV2Model: () => (
      /* reexport safe */
      r.ConvNextV2Model
    ),
    /* harmony export */
    ConvNextV2PreTrainedModel: () => (
      /* reexport safe */
      r.ConvNextV2PreTrainedModel
    ),
    /* harmony export */
    DPTFeatureExtractor: () => (
      /* reexport safe */
      I.DPTFeatureExtractor
    ),
    /* harmony export */
    DPTForDepthEstimation: () => (
      /* reexport safe */
      r.DPTForDepthEstimation
    ),
    /* harmony export */
    DPTImageProcessor: () => (
      /* reexport safe */
      I.DPTImageProcessor
    ),
    /* harmony export */
    DPTModel: () => (
      /* reexport safe */
      r.DPTModel
    ),
    /* harmony export */
    DPTPreTrainedModel: () => (
      /* reexport safe */
      r.DPTPreTrainedModel
    ),
    /* harmony export */
    DebertaForMaskedLM: () => (
      /* reexport safe */
      r.DebertaForMaskedLM
    ),
    /* harmony export */
    DebertaForQuestionAnswering: () => (
      /* reexport safe */
      r.DebertaForQuestionAnswering
    ),
    /* harmony export */
    DebertaForSequenceClassification: () => (
      /* reexport safe */
      r.DebertaForSequenceClassification
    ),
    /* harmony export */
    DebertaForTokenClassification: () => (
      /* reexport safe */
      r.DebertaForTokenClassification
    ),
    /* harmony export */
    DebertaModel: () => (
      /* reexport safe */
      r.DebertaModel
    ),
    /* harmony export */
    DebertaPreTrainedModel: () => (
      /* reexport safe */
      r.DebertaPreTrainedModel
    ),
    /* harmony export */
    DebertaTokenizer: () => (
      /* reexport safe */
      _.DebertaTokenizer
    ),
    /* harmony export */
    DebertaV2ForMaskedLM: () => (
      /* reexport safe */
      r.DebertaV2ForMaskedLM
    ),
    /* harmony export */
    DebertaV2ForQuestionAnswering: () => (
      /* reexport safe */
      r.DebertaV2ForQuestionAnswering
    ),
    /* harmony export */
    DebertaV2ForSequenceClassification: () => (
      /* reexport safe */
      r.DebertaV2ForSequenceClassification
    ),
    /* harmony export */
    DebertaV2ForTokenClassification: () => (
      /* reexport safe */
      r.DebertaV2ForTokenClassification
    ),
    /* harmony export */
    DebertaV2Model: () => (
      /* reexport safe */
      r.DebertaV2Model
    ),
    /* harmony export */
    DebertaV2PreTrainedModel: () => (
      /* reexport safe */
      r.DebertaV2PreTrainedModel
    ),
    /* harmony export */
    DebertaV2Tokenizer: () => (
      /* reexport safe */
      _.DebertaV2Tokenizer
    ),
    /* harmony export */
    DecisionTransformerModel: () => (
      /* reexport safe */
      r.DecisionTransformerModel
    ),
    /* harmony export */
    DecisionTransformerPreTrainedModel: () => (
      /* reexport safe */
      r.DecisionTransformerPreTrainedModel
    ),
    /* harmony export */
    DeiTFeatureExtractor: () => (
      /* reexport safe */
      I.DeiTFeatureExtractor
    ),
    /* harmony export */
    DeiTForImageClassification: () => (
      /* reexport safe */
      r.DeiTForImageClassification
    ),
    /* harmony export */
    DeiTImageProcessor: () => (
      /* reexport safe */
      I.DeiTImageProcessor
    ),
    /* harmony export */
    DeiTModel: () => (
      /* reexport safe */
      r.DeiTModel
    ),
    /* harmony export */
    DeiTPreTrainedModel: () => (
      /* reexport safe */
      r.DeiTPreTrainedModel
    ),
    /* harmony export */
    DepthAnythingForDepthEstimation: () => (
      /* reexport safe */
      r.DepthAnythingForDepthEstimation
    ),
    /* harmony export */
    DepthAnythingPreTrainedModel: () => (
      /* reexport safe */
      r.DepthAnythingPreTrainedModel
    ),
    /* harmony export */
    DepthEstimationPipeline: () => (
      /* reexport safe */
      $.DepthEstimationPipeline
    ),
    /* harmony export */
    DepthProForDepthEstimation: () => (
      /* reexport safe */
      r.DepthProForDepthEstimation
    ),
    /* harmony export */
    DepthProPreTrainedModel: () => (
      /* reexport safe */
      r.DepthProPreTrainedModel
    ),
    /* harmony export */
    DetrFeatureExtractor: () => (
      /* reexport safe */
      I.DetrFeatureExtractor
    ),
    /* harmony export */
    DetrForObjectDetection: () => (
      /* reexport safe */
      r.DetrForObjectDetection
    ),
    /* harmony export */
    DetrForSegmentation: () => (
      /* reexport safe */
      r.DetrForSegmentation
    ),
    /* harmony export */
    DetrImageProcessor: () => (
      /* reexport safe */
      I.DetrImageProcessor
    ),
    /* harmony export */
    DetrModel: () => (
      /* reexport safe */
      r.DetrModel
    ),
    /* harmony export */
    DetrObjectDetectionOutput: () => (
      /* reexport safe */
      r.DetrObjectDetectionOutput
    ),
    /* harmony export */
    DetrPreTrainedModel: () => (
      /* reexport safe */
      r.DetrPreTrainedModel
    ),
    /* harmony export */
    DetrSegmentationOutput: () => (
      /* reexport safe */
      r.DetrSegmentationOutput
    ),
    /* harmony export */
    Dinov2ForImageClassification: () => (
      /* reexport safe */
      r.Dinov2ForImageClassification
    ),
    /* harmony export */
    Dinov2Model: () => (
      /* reexport safe */
      r.Dinov2Model
    ),
    /* harmony export */
    Dinov2PreTrainedModel: () => (
      /* reexport safe */
      r.Dinov2PreTrainedModel
    ),
    /* harmony export */
    Dinov2WithRegistersForImageClassification: () => (
      /* reexport safe */
      r.Dinov2WithRegistersForImageClassification
    ),
    /* harmony export */
    Dinov2WithRegistersModel: () => (
      /* reexport safe */
      r.Dinov2WithRegistersModel
    ),
    /* harmony export */
    Dinov2WithRegistersPreTrainedModel: () => (
      /* reexport safe */
      r.Dinov2WithRegistersPreTrainedModel
    ),
    /* harmony export */
    DistilBertForMaskedLM: () => (
      /* reexport safe */
      r.DistilBertForMaskedLM
    ),
    /* harmony export */
    DistilBertForQuestionAnswering: () => (
      /* reexport safe */
      r.DistilBertForQuestionAnswering
    ),
    /* harmony export */
    DistilBertForSequenceClassification: () => (
      /* reexport safe */
      r.DistilBertForSequenceClassification
    ),
    /* harmony export */
    DistilBertForTokenClassification: () => (
      /* reexport safe */
      r.DistilBertForTokenClassification
    ),
    /* harmony export */
    DistilBertModel: () => (
      /* reexport safe */
      r.DistilBertModel
    ),
    /* harmony export */
    DistilBertPreTrainedModel: () => (
      /* reexport safe */
      r.DistilBertPreTrainedModel
    ),
    /* harmony export */
    DistilBertTokenizer: () => (
      /* reexport safe */
      _.DistilBertTokenizer
    ),
    /* harmony export */
    DocumentQuestionAnsweringPipeline: () => (
      /* reexport safe */
      $.DocumentQuestionAnsweringPipeline
    ),
    /* harmony export */
    DonutFeatureExtractor: () => (
      /* reexport safe */
      I.DonutFeatureExtractor
    ),
    /* harmony export */
    DonutImageProcessor: () => (
      /* reexport safe */
      I.DonutImageProcessor
    ),
    /* harmony export */
    DonutSwinModel: () => (
      /* reexport safe */
      r.DonutSwinModel
    ),
    /* harmony export */
    DonutSwinPreTrainedModel: () => (
      /* reexport safe */
      r.DonutSwinPreTrainedModel
    ),
    /* harmony export */
    EfficientNetForImageClassification: () => (
      /* reexport safe */
      r.EfficientNetForImageClassification
    ),
    /* harmony export */
    EfficientNetImageProcessor: () => (
      /* reexport safe */
      I.EfficientNetImageProcessor
    ),
    /* harmony export */
    EfficientNetModel: () => (
      /* reexport safe */
      r.EfficientNetModel
    ),
    /* harmony export */
    EfficientNetPreTrainedModel: () => (
      /* reexport safe */
      r.EfficientNetPreTrainedModel
    ),
    /* harmony export */
    ElectraForMaskedLM: () => (
      /* reexport safe */
      r.ElectraForMaskedLM
    ),
    /* harmony export */
    ElectraForQuestionAnswering: () => (
      /* reexport safe */
      r.ElectraForQuestionAnswering
    ),
    /* harmony export */
    ElectraForSequenceClassification: () => (
      /* reexport safe */
      r.ElectraForSequenceClassification
    ),
    /* harmony export */
    ElectraForTokenClassification: () => (
      /* reexport safe */
      r.ElectraForTokenClassification
    ),
    /* harmony export */
    ElectraModel: () => (
      /* reexport safe */
      r.ElectraModel
    ),
    /* harmony export */
    ElectraPreTrainedModel: () => (
      /* reexport safe */
      r.ElectraPreTrainedModel
    ),
    /* harmony export */
    ElectraTokenizer: () => (
      /* reexport safe */
      _.ElectraTokenizer
    ),
    /* harmony export */
    EosTokenCriteria: () => (
      /* reexport safe */
      q.EosTokenCriteria
    ),
    /* harmony export */
    EsmForMaskedLM: () => (
      /* reexport safe */
      r.EsmForMaskedLM
    ),
    /* harmony export */
    EsmForSequenceClassification: () => (
      /* reexport safe */
      r.EsmForSequenceClassification
    ),
    /* harmony export */
    EsmForTokenClassification: () => (
      /* reexport safe */
      r.EsmForTokenClassification
    ),
    /* harmony export */
    EsmModel: () => (
      /* reexport safe */
      r.EsmModel
    ),
    /* harmony export */
    EsmPreTrainedModel: () => (
      /* reexport safe */
      r.EsmPreTrainedModel
    ),
    /* harmony export */
    EsmTokenizer: () => (
      /* reexport safe */
      _.EsmTokenizer
    ),
    /* harmony export */
    ExaoneForCausalLM: () => (
      /* reexport safe */
      r.ExaoneForCausalLM
    ),
    /* harmony export */
    ExaoneModel: () => (
      /* reexport safe */
      r.ExaoneModel
    ),
    /* harmony export */
    ExaonePreTrainedModel: () => (
      /* reexport safe */
      r.ExaonePreTrainedModel
    ),
    /* harmony export */
    FFT: () => (
      /* reexport safe */
      g.FFT
    ),
    /* harmony export */
    FalconForCausalLM: () => (
      /* reexport safe */
      r.FalconForCausalLM
    ),
    /* harmony export */
    FalconModel: () => (
      /* reexport safe */
      r.FalconModel
    ),
    /* harmony export */
    FalconPreTrainedModel: () => (
      /* reexport safe */
      r.FalconPreTrainedModel
    ),
    /* harmony export */
    FalconTokenizer: () => (
      /* reexport safe */
      _.FalconTokenizer
    ),
    /* harmony export */
    FastViTForImageClassification: () => (
      /* reexport safe */
      r.FastViTForImageClassification
    ),
    /* harmony export */
    FastViTModel: () => (
      /* reexport safe */
      r.FastViTModel
    ),
    /* harmony export */
    FastViTPreTrainedModel: () => (
      /* reexport safe */
      r.FastViTPreTrainedModel
    ),
    /* harmony export */
    FeatureExtractionPipeline: () => (
      /* reexport safe */
      $.FeatureExtractionPipeline
    ),
    /* harmony export */
    FeatureExtractor: () => (
      /* reexport safe */
      v.FeatureExtractor
    ),
    /* harmony export */
    FillMaskPipeline: () => (
      /* reexport safe */
      $.FillMaskPipeline
    ),
    /* harmony export */
    Florence2ForConditionalGeneration: () => (
      /* reexport safe */
      r.Florence2ForConditionalGeneration
    ),
    /* harmony export */
    Florence2PreTrainedModel: () => (
      /* reexport safe */
      r.Florence2PreTrainedModel
    ),
    /* harmony export */
    Florence2Processor: () => (
      /* reexport safe */
      ie.Florence2Processor
    ),
    /* harmony export */
    ForcedBOSTokenLogitsProcessor: () => (
      /* reexport safe */
      A.ForcedBOSTokenLogitsProcessor
    ),
    /* harmony export */
    ForcedEOSTokenLogitsProcessor: () => (
      /* reexport safe */
      A.ForcedEOSTokenLogitsProcessor
    ),
    /* harmony export */
    GLPNFeatureExtractor: () => (
      /* reexport safe */
      I.GLPNFeatureExtractor
    ),
    /* harmony export */
    GLPNForDepthEstimation: () => (
      /* reexport safe */
      r.GLPNForDepthEstimation
    ),
    /* harmony export */
    GLPNModel: () => (
      /* reexport safe */
      r.GLPNModel
    ),
    /* harmony export */
    GLPNPreTrainedModel: () => (
      /* reexport safe */
      r.GLPNPreTrainedModel
    ),
    /* harmony export */
    GPT2LMHeadModel: () => (
      /* reexport safe */
      r.GPT2LMHeadModel
    ),
    /* harmony export */
    GPT2Model: () => (
      /* reexport safe */
      r.GPT2Model
    ),
    /* harmony export */
    GPT2PreTrainedModel: () => (
      /* reexport safe */
      r.GPT2PreTrainedModel
    ),
    /* harmony export */
    GPT2Tokenizer: () => (
      /* reexport safe */
      _.GPT2Tokenizer
    ),
    /* harmony export */
    GPTBigCodeForCausalLM: () => (
      /* reexport safe */
      r.GPTBigCodeForCausalLM
    ),
    /* harmony export */
    GPTBigCodeModel: () => (
      /* reexport safe */
      r.GPTBigCodeModel
    ),
    /* harmony export */
    GPTBigCodePreTrainedModel: () => (
      /* reexport safe */
      r.GPTBigCodePreTrainedModel
    ),
    /* harmony export */
    GPTJForCausalLM: () => (
      /* reexport safe */
      r.GPTJForCausalLM
    ),
    /* harmony export */
    GPTJModel: () => (
      /* reexport safe */
      r.GPTJModel
    ),
    /* harmony export */
    GPTJPreTrainedModel: () => (
      /* reexport safe */
      r.GPTJPreTrainedModel
    ),
    /* harmony export */
    GPTNeoForCausalLM: () => (
      /* reexport safe */
      r.GPTNeoForCausalLM
    ),
    /* harmony export */
    GPTNeoModel: () => (
      /* reexport safe */
      r.GPTNeoModel
    ),
    /* harmony export */
    GPTNeoPreTrainedModel: () => (
      /* reexport safe */
      r.GPTNeoPreTrainedModel
    ),
    /* harmony export */
    GPTNeoXForCausalLM: () => (
      /* reexport safe */
      r.GPTNeoXForCausalLM
    ),
    /* harmony export */
    GPTNeoXModel: () => (
      /* reexport safe */
      r.GPTNeoXModel
    ),
    /* harmony export */
    GPTNeoXPreTrainedModel: () => (
      /* reexport safe */
      r.GPTNeoXPreTrainedModel
    ),
    /* harmony export */
    GPTNeoXTokenizer: () => (
      /* reexport safe */
      _.GPTNeoXTokenizer
    ),
    /* harmony export */
    Gemma2ForCausalLM: () => (
      /* reexport safe */
      r.Gemma2ForCausalLM
    ),
    /* harmony export */
    Gemma2Model: () => (
      /* reexport safe */
      r.Gemma2Model
    ),
    /* harmony export */
    Gemma2PreTrainedModel: () => (
      /* reexport safe */
      r.Gemma2PreTrainedModel
    ),
    /* harmony export */
    GemmaForCausalLM: () => (
      /* reexport safe */
      r.GemmaForCausalLM
    ),
    /* harmony export */
    GemmaModel: () => (
      /* reexport safe */
      r.GemmaModel
    ),
    /* harmony export */
    GemmaPreTrainedModel: () => (
      /* reexport safe */
      r.GemmaPreTrainedModel
    ),
    /* harmony export */
    GemmaTokenizer: () => (
      /* reexport safe */
      _.GemmaTokenizer
    ),
    /* harmony export */
    GlmForCausalLM: () => (
      /* reexport safe */
      r.GlmForCausalLM
    ),
    /* harmony export */
    GlmModel: () => (
      /* reexport safe */
      r.GlmModel
    ),
    /* harmony export */
    GlmPreTrainedModel: () => (
      /* reexport safe */
      r.GlmPreTrainedModel
    ),
    /* harmony export */
    GraniteForCausalLM: () => (
      /* reexport safe */
      r.GraniteForCausalLM
    ),
    /* harmony export */
    GraniteModel: () => (
      /* reexport safe */
      r.GraniteModel
    ),
    /* harmony export */
    GranitePreTrainedModel: () => (
      /* reexport safe */
      r.GranitePreTrainedModel
    ),
    /* harmony export */
    Grok1Tokenizer: () => (
      /* reexport safe */
      _.Grok1Tokenizer
    ),
    /* harmony export */
    GroundingDinoForObjectDetection: () => (
      /* reexport safe */
      r.GroundingDinoForObjectDetection
    ),
    /* harmony export */
    GroundingDinoImageProcessor: () => (
      /* reexport safe */
      I.GroundingDinoImageProcessor
    ),
    /* harmony export */
    GroundingDinoPreTrainedModel: () => (
      /* reexport safe */
      r.GroundingDinoPreTrainedModel
    ),
    /* harmony export */
    GroundingDinoProcessor: () => (
      /* reexport safe */
      ie.GroundingDinoProcessor
    ),
    /* harmony export */
    GroupViTModel: () => (
      /* reexport safe */
      r.GroupViTModel
    ),
    /* harmony export */
    GroupViTPreTrainedModel: () => (
      /* reexport safe */
      r.GroupViTPreTrainedModel
    ),
    /* harmony export */
    HeliumForCausalLM: () => (
      /* reexport safe */
      r.HeliumForCausalLM
    ),
    /* harmony export */
    HeliumModel: () => (
      /* reexport safe */
      r.HeliumModel
    ),
    /* harmony export */
    HeliumPreTrainedModel: () => (
      /* reexport safe */
      r.HeliumPreTrainedModel
    ),
    /* harmony export */
    HerbertTokenizer: () => (
      /* reexport safe */
      _.HerbertTokenizer
    ),
    /* harmony export */
    HieraForImageClassification: () => (
      /* reexport safe */
      r.HieraForImageClassification
    ),
    /* harmony export */
    HieraModel: () => (
      /* reexport safe */
      r.HieraModel
    ),
    /* harmony export */
    HieraPreTrainedModel: () => (
      /* reexport safe */
      r.HieraPreTrainedModel
    ),
    /* harmony export */
    HubertForCTC: () => (
      /* reexport safe */
      r.HubertForCTC
    ),
    /* harmony export */
    HubertForSequenceClassification: () => (
      /* reexport safe */
      r.HubertForSequenceClassification
    ),
    /* harmony export */
    HubertModel: () => (
      /* reexport safe */
      r.HubertModel
    ),
    /* harmony export */
    HubertPreTrainedModel: () => (
      /* reexport safe */
      r.HubertPreTrainedModel
    ),
    /* harmony export */
    IJepaForImageClassification: () => (
      /* reexport safe */
      r.IJepaForImageClassification
    ),
    /* harmony export */
    IJepaModel: () => (
      /* reexport safe */
      r.IJepaModel
    ),
    /* harmony export */
    IJepaPreTrainedModel: () => (
      /* reexport safe */
      r.IJepaPreTrainedModel
    ),
    /* harmony export */
    Idefics3ForConditionalGeneration: () => (
      /* reexport safe */
      r.Idefics3ForConditionalGeneration
    ),
    /* harmony export */
    Idefics3ImageProcessor: () => (
      /* reexport safe */
      I.Idefics3ImageProcessor
    ),
    /* harmony export */
    Idefics3PreTrainedModel: () => (
      /* reexport safe */
      r.Idefics3PreTrainedModel
    ),
    /* harmony export */
    Idefics3Processor: () => (
      /* reexport safe */
      ie.Idefics3Processor
    ),
    /* harmony export */
    ImageClassificationPipeline: () => (
      /* reexport safe */
      $.ImageClassificationPipeline
    ),
    /* harmony export */
    ImageFeatureExtractionPipeline: () => (
      /* reexport safe */
      $.ImageFeatureExtractionPipeline
    ),
    /* harmony export */
    ImageFeatureExtractor: () => (
      /* reexport safe */
      M.ImageFeatureExtractor
    ),
    /* harmony export */
    ImageMattingOutput: () => (
      /* reexport safe */
      r.ImageMattingOutput
    ),
    /* harmony export */
    ImageProcessor: () => (
      /* reexport safe */
      b.ImageProcessor
    ),
    /* harmony export */
    ImageSegmentationPipeline: () => (
      /* reexport safe */
      $.ImageSegmentationPipeline
    ),
    /* harmony export */
    ImageToImagePipeline: () => (
      /* reexport safe */
      $.ImageToImagePipeline
    ),
    /* harmony export */
    ImageToTextPipeline: () => (
      /* reexport safe */
      $.ImageToTextPipeline
    ),
    /* harmony export */
    InterruptableStoppingCriteria: () => (
      /* reexport safe */
      q.InterruptableStoppingCriteria
    ),
    /* harmony export */
    JAISLMHeadModel: () => (
      /* reexport safe */
      r.JAISLMHeadModel
    ),
    /* harmony export */
    JAISModel: () => (
      /* reexport safe */
      r.JAISModel
    ),
    /* harmony export */
    JAISPreTrainedModel: () => (
      /* reexport safe */
      r.JAISPreTrainedModel
    ),
    /* harmony export */
    JinaCLIPImageProcessor: () => (
      /* reexport safe */
      I.JinaCLIPImageProcessor
    ),
    /* harmony export */
    JinaCLIPModel: () => (
      /* reexport safe */
      r.JinaCLIPModel
    ),
    /* harmony export */
    JinaCLIPPreTrainedModel: () => (
      /* reexport safe */
      r.JinaCLIPPreTrainedModel
    ),
    /* harmony export */
    JinaCLIPProcessor: () => (
      /* reexport safe */
      ie.JinaCLIPProcessor
    ),
    /* harmony export */
    JinaCLIPTextModel: () => (
      /* reexport safe */
      r.JinaCLIPTextModel
    ),
    /* harmony export */
    JinaCLIPVisionModel: () => (
      /* reexport safe */
      r.JinaCLIPVisionModel
    ),
    /* harmony export */
    LlamaForCausalLM: () => (
      /* reexport safe */
      r.LlamaForCausalLM
    ),
    /* harmony export */
    LlamaModel: () => (
      /* reexport safe */
      r.LlamaModel
    ),
    /* harmony export */
    LlamaPreTrainedModel: () => (
      /* reexport safe */
      r.LlamaPreTrainedModel
    ),
    /* harmony export */
    LlamaTokenizer: () => (
      /* reexport safe */
      _.LlamaTokenizer
    ),
    /* harmony export */
    LlavaForConditionalGeneration: () => (
      /* reexport safe */
      r.LlavaForConditionalGeneration
    ),
    /* harmony export */
    LlavaOnevisionForConditionalGeneration: () => (
      /* reexport safe */
      r.LlavaOnevisionForConditionalGeneration
    ),
    /* harmony export */
    LlavaOnevisionImageProcessor: () => (
      /* reexport safe */
      I.LlavaOnevisionImageProcessor
    ),
    /* harmony export */
    LlavaPreTrainedModel: () => (
      /* reexport safe */
      r.LlavaPreTrainedModel
    ),
    /* harmony export */
    LogitsProcessor: () => (
      /* reexport safe */
      A.LogitsProcessor
    ),
    /* harmony export */
    LogitsProcessorList: () => (
      /* reexport safe */
      A.LogitsProcessorList
    ),
    /* harmony export */
    LogitsWarper: () => (
      /* reexport safe */
      A.LogitsWarper
    ),
    /* harmony export */
    LongT5ForConditionalGeneration: () => (
      /* reexport safe */
      r.LongT5ForConditionalGeneration
    ),
    /* harmony export */
    LongT5Model: () => (
      /* reexport safe */
      r.LongT5Model
    ),
    /* harmony export */
    LongT5PreTrainedModel: () => (
      /* reexport safe */
      r.LongT5PreTrainedModel
    ),
    /* harmony export */
    M2M100ForConditionalGeneration: () => (
      /* reexport safe */
      r.M2M100ForConditionalGeneration
    ),
    /* harmony export */
    M2M100Model: () => (
      /* reexport safe */
      r.M2M100Model
    ),
    /* harmony export */
    M2M100PreTrainedModel: () => (
      /* reexport safe */
      r.M2M100PreTrainedModel
    ),
    /* harmony export */
    M2M100Tokenizer: () => (
      /* reexport safe */
      _.M2M100Tokenizer
    ),
    /* harmony export */
    MBart50Tokenizer: () => (
      /* reexport safe */
      _.MBart50Tokenizer
    ),
    /* harmony export */
    MBartForCausalLM: () => (
      /* reexport safe */
      r.MBartForCausalLM
    ),
    /* harmony export */
    MBartForConditionalGeneration: () => (
      /* reexport safe */
      r.MBartForConditionalGeneration
    ),
    /* harmony export */
    MBartForSequenceClassification: () => (
      /* reexport safe */
      r.MBartForSequenceClassification
    ),
    /* harmony export */
    MBartModel: () => (
      /* reexport safe */
      r.MBartModel
    ),
    /* harmony export */
    MBartPreTrainedModel: () => (
      /* reexport safe */
      r.MBartPreTrainedModel
    ),
    /* harmony export */
    MBartTokenizer: () => (
      /* reexport safe */
      _.MBartTokenizer
    ),
    /* harmony export */
    MPNetForMaskedLM: () => (
      /* reexport safe */
      r.MPNetForMaskedLM
    ),
    /* harmony export */
    MPNetForQuestionAnswering: () => (
      /* reexport safe */
      r.MPNetForQuestionAnswering
    ),
    /* harmony export */
    MPNetForSequenceClassification: () => (
      /* reexport safe */
      r.MPNetForSequenceClassification
    ),
    /* harmony export */
    MPNetForTokenClassification: () => (
      /* reexport safe */
      r.MPNetForTokenClassification
    ),
    /* harmony export */
    MPNetModel: () => (
      /* reexport safe */
      r.MPNetModel
    ),
    /* harmony export */
    MPNetPreTrainedModel: () => (
      /* reexport safe */
      r.MPNetPreTrainedModel
    ),
    /* harmony export */
    MPNetTokenizer: () => (
      /* reexport safe */
      _.MPNetTokenizer
    ),
    /* harmony export */
    MT5ForConditionalGeneration: () => (
      /* reexport safe */
      r.MT5ForConditionalGeneration
    ),
    /* harmony export */
    MT5Model: () => (
      /* reexport safe */
      r.MT5Model
    ),
    /* harmony export */
    MT5PreTrainedModel: () => (
      /* reexport safe */
      r.MT5PreTrainedModel
    ),
    /* harmony export */
    MarianMTModel: () => (
      /* reexport safe */
      r.MarianMTModel
    ),
    /* harmony export */
    MarianModel: () => (
      /* reexport safe */
      r.MarianModel
    ),
    /* harmony export */
    MarianPreTrainedModel: () => (
      /* reexport safe */
      r.MarianPreTrainedModel
    ),
    /* harmony export */
    MarianTokenizer: () => (
      /* reexport safe */
      _.MarianTokenizer
    ),
    /* harmony export */
    Mask2FormerImageProcessor: () => (
      /* reexport safe */
      I.Mask2FormerImageProcessor
    ),
    /* harmony export */
    MaskFormerFeatureExtractor: () => (
      /* reexport safe */
      I.MaskFormerFeatureExtractor
    ),
    /* harmony export */
    MaskFormerForInstanceSegmentation: () => (
      /* reexport safe */
      r.MaskFormerForInstanceSegmentation
    ),
    /* harmony export */
    MaskFormerImageProcessor: () => (
      /* reexport safe */
      I.MaskFormerImageProcessor
    ),
    /* harmony export */
    MaskFormerModel: () => (
      /* reexport safe */
      r.MaskFormerModel
    ),
    /* harmony export */
    MaskFormerPreTrainedModel: () => (
      /* reexport safe */
      r.MaskFormerPreTrainedModel
    ),
    /* harmony export */
    MaskedLMOutput: () => (
      /* reexport safe */
      r.MaskedLMOutput
    ),
    /* harmony export */
    MaxLengthCriteria: () => (
      /* reexport safe */
      q.MaxLengthCriteria
    ),
    /* harmony export */
    MgpstrForSceneTextRecognition: () => (
      /* reexport safe */
      r.MgpstrForSceneTextRecognition
    ),
    /* harmony export */
    MgpstrModelOutput: () => (
      /* reexport safe */
      r.MgpstrModelOutput
    ),
    /* harmony export */
    MgpstrPreTrainedModel: () => (
      /* reexport safe */
      r.MgpstrPreTrainedModel
    ),
    /* harmony export */
    MgpstrProcessor: () => (
      /* reexport safe */
      ie.MgpstrProcessor
    ),
    /* harmony export */
    MgpstrTokenizer: () => (
      /* reexport safe */
      _.MgpstrTokenizer
    ),
    /* harmony export */
    MinLengthLogitsProcessor: () => (
      /* reexport safe */
      A.MinLengthLogitsProcessor
    ),
    /* harmony export */
    MinNewTokensLengthLogitsProcessor: () => (
      /* reexport safe */
      A.MinNewTokensLengthLogitsProcessor
    ),
    /* harmony export */
    MistralForCausalLM: () => (
      /* reexport safe */
      r.MistralForCausalLM
    ),
    /* harmony export */
    MistralModel: () => (
      /* reexport safe */
      r.MistralModel
    ),
    /* harmony export */
    MistralPreTrainedModel: () => (
      /* reexport safe */
      r.MistralPreTrainedModel
    ),
    /* harmony export */
    MobileBertForMaskedLM: () => (
      /* reexport safe */
      r.MobileBertForMaskedLM
    ),
    /* harmony export */
    MobileBertForQuestionAnswering: () => (
      /* reexport safe */
      r.MobileBertForQuestionAnswering
    ),
    /* harmony export */
    MobileBertForSequenceClassification: () => (
      /* reexport safe */
      r.MobileBertForSequenceClassification
    ),
    /* harmony export */
    MobileBertModel: () => (
      /* reexport safe */
      r.MobileBertModel
    ),
    /* harmony export */
    MobileBertPreTrainedModel: () => (
      /* reexport safe */
      r.MobileBertPreTrainedModel
    ),
    /* harmony export */
    MobileBertTokenizer: () => (
      /* reexport safe */
      _.MobileBertTokenizer
    ),
    /* harmony export */
    MobileLLMForCausalLM: () => (
      /* reexport safe */
      r.MobileLLMForCausalLM
    ),
    /* harmony export */
    MobileLLMModel: () => (
      /* reexport safe */
      r.MobileLLMModel
    ),
    /* harmony export */
    MobileLLMPreTrainedModel: () => (
      /* reexport safe */
      r.MobileLLMPreTrainedModel
    ),
    /* harmony export */
    MobileNetV1FeatureExtractor: () => (
      /* reexport safe */
      I.MobileNetV1FeatureExtractor
    ),
    /* harmony export */
    MobileNetV1ForImageClassification: () => (
      /* reexport safe */
      r.MobileNetV1ForImageClassification
    ),
    /* harmony export */
    MobileNetV1ImageProcessor: () => (
      /* reexport safe */
      I.MobileNetV1ImageProcessor
    ),
    /* harmony export */
    MobileNetV1Model: () => (
      /* reexport safe */
      r.MobileNetV1Model
    ),
    /* harmony export */
    MobileNetV1PreTrainedModel: () => (
      /* reexport safe */
      r.MobileNetV1PreTrainedModel
    ),
    /* harmony export */
    MobileNetV2FeatureExtractor: () => (
      /* reexport safe */
      I.MobileNetV2FeatureExtractor
    ),
    /* harmony export */
    MobileNetV2ForImageClassification: () => (
      /* reexport safe */
      r.MobileNetV2ForImageClassification
    ),
    /* harmony export */
    MobileNetV2ImageProcessor: () => (
      /* reexport safe */
      I.MobileNetV2ImageProcessor
    ),
    /* harmony export */
    MobileNetV2Model: () => (
      /* reexport safe */
      r.MobileNetV2Model
    ),
    /* harmony export */
    MobileNetV2PreTrainedModel: () => (
      /* reexport safe */
      r.MobileNetV2PreTrainedModel
    ),
    /* harmony export */
    MobileNetV3FeatureExtractor: () => (
      /* reexport safe */
      I.MobileNetV3FeatureExtractor
    ),
    /* harmony export */
    MobileNetV3ForImageClassification: () => (
      /* reexport safe */
      r.MobileNetV3ForImageClassification
    ),
    /* harmony export */
    MobileNetV3ImageProcessor: () => (
      /* reexport safe */
      I.MobileNetV3ImageProcessor
    ),
    /* harmony export */
    MobileNetV3Model: () => (
      /* reexport safe */
      r.MobileNetV3Model
    ),
    /* harmony export */
    MobileNetV3PreTrainedModel: () => (
      /* reexport safe */
      r.MobileNetV3PreTrainedModel
    ),
    /* harmony export */
    MobileNetV4FeatureExtractor: () => (
      /* reexport safe */
      I.MobileNetV4FeatureExtractor
    ),
    /* harmony export */
    MobileNetV4ForImageClassification: () => (
      /* reexport safe */
      r.MobileNetV4ForImageClassification
    ),
    /* harmony export */
    MobileNetV4ImageProcessor: () => (
      /* reexport safe */
      I.MobileNetV4ImageProcessor
    ),
    /* harmony export */
    MobileNetV4Model: () => (
      /* reexport safe */
      r.MobileNetV4Model
    ),
    /* harmony export */
    MobileNetV4PreTrainedModel: () => (
      /* reexport safe */
      r.MobileNetV4PreTrainedModel
    ),
    /* harmony export */
    MobileViTFeatureExtractor: () => (
      /* reexport safe */
      I.MobileViTFeatureExtractor
    ),
    /* harmony export */
    MobileViTForImageClassification: () => (
      /* reexport safe */
      r.MobileViTForImageClassification
    ),
    /* harmony export */
    MobileViTImageProcessor: () => (
      /* reexport safe */
      I.MobileViTImageProcessor
    ),
    /* harmony export */
    MobileViTModel: () => (
      /* reexport safe */
      r.MobileViTModel
    ),
    /* harmony export */
    MobileViTPreTrainedModel: () => (
      /* reexport safe */
      r.MobileViTPreTrainedModel
    ),
    /* harmony export */
    MobileViTV2ForImageClassification: () => (
      /* reexport safe */
      r.MobileViTV2ForImageClassification
    ),
    /* harmony export */
    MobileViTV2Model: () => (
      /* reexport safe */
      r.MobileViTV2Model
    ),
    /* harmony export */
    MobileViTV2PreTrainedModel: () => (
      /* reexport safe */
      r.MobileViTV2PreTrainedModel
    ),
    /* harmony export */
    ModelOutput: () => (
      /* reexport safe */
      r.ModelOutput
    ),
    /* harmony export */
    ModernBertForMaskedLM: () => (
      /* reexport safe */
      r.ModernBertForMaskedLM
    ),
    /* harmony export */
    ModernBertForSequenceClassification: () => (
      /* reexport safe */
      r.ModernBertForSequenceClassification
    ),
    /* harmony export */
    ModernBertForTokenClassification: () => (
      /* reexport safe */
      r.ModernBertForTokenClassification
    ),
    /* harmony export */
    ModernBertModel: () => (
      /* reexport safe */
      r.ModernBertModel
    ),
    /* harmony export */
    ModernBertPreTrainedModel: () => (
      /* reexport safe */
      r.ModernBertPreTrainedModel
    ),
    /* harmony export */
    Moondream1ForConditionalGeneration: () => (
      /* reexport safe */
      r.Moondream1ForConditionalGeneration
    ),
    /* harmony export */
    MoonshineFeatureExtractor: () => (
      /* reexport safe */
      M.MoonshineFeatureExtractor
    ),
    /* harmony export */
    MoonshineForConditionalGeneration: () => (
      /* reexport safe */
      r.MoonshineForConditionalGeneration
    ),
    /* harmony export */
    MoonshineModel: () => (
      /* reexport safe */
      r.MoonshineModel
    ),
    /* harmony export */
    MoonshinePreTrainedModel: () => (
      /* reexport safe */
      r.MoonshinePreTrainedModel
    ),
    /* harmony export */
    MoonshineProcessor: () => (
      /* reexport safe */
      ie.MoonshineProcessor
    ),
    /* harmony export */
    MptForCausalLM: () => (
      /* reexport safe */
      r.MptForCausalLM
    ),
    /* harmony export */
    MptModel: () => (
      /* reexport safe */
      r.MptModel
    ),
    /* harmony export */
    MptPreTrainedModel: () => (
      /* reexport safe */
      r.MptPreTrainedModel
    ),
    /* harmony export */
    MultiModalityCausalLM: () => (
      /* reexport safe */
      r.MultiModalityCausalLM
    ),
    /* harmony export */
    MultiModalityPreTrainedModel: () => (
      /* reexport safe */
      r.MultiModalityPreTrainedModel
    ),
    /* harmony export */
    MusicgenForCausalLM: () => (
      /* reexport safe */
      r.MusicgenForCausalLM
    ),
    /* harmony export */
    MusicgenForConditionalGeneration: () => (
      /* reexport safe */
      r.MusicgenForConditionalGeneration
    ),
    /* harmony export */
    MusicgenModel: () => (
      /* reexport safe */
      r.MusicgenModel
    ),
    /* harmony export */
    MusicgenPreTrainedModel: () => (
      /* reexport safe */
      r.MusicgenPreTrainedModel
    ),
    /* harmony export */
    NllbTokenizer: () => (
      /* reexport safe */
      _.NllbTokenizer
    ),
    /* harmony export */
    NoBadWordsLogitsProcessor: () => (
      /* reexport safe */
      A.NoBadWordsLogitsProcessor
    ),
    /* harmony export */
    NoRepeatNGramLogitsProcessor: () => (
      /* reexport safe */
      A.NoRepeatNGramLogitsProcessor
    ),
    /* harmony export */
    NomicBertModel: () => (
      /* reexport safe */
      r.NomicBertModel
    ),
    /* harmony export */
    NomicBertPreTrainedModel: () => (
      /* reexport safe */
      r.NomicBertPreTrainedModel
    ),
    /* harmony export */
    NougatImageProcessor: () => (
      /* reexport safe */
      I.NougatImageProcessor
    ),
    /* harmony export */
    NougatTokenizer: () => (
      /* reexport safe */
      _.NougatTokenizer
    ),
    /* harmony export */
    OPTForCausalLM: () => (
      /* reexport safe */
      r.OPTForCausalLM
    ),
    /* harmony export */
    OPTModel: () => (
      /* reexport safe */
      r.OPTModel
    ),
    /* harmony export */
    OPTPreTrainedModel: () => (
      /* reexport safe */
      r.OPTPreTrainedModel
    ),
    /* harmony export */
    ObjectDetectionPipeline: () => (
      /* reexport safe */
      $.ObjectDetectionPipeline
    ),
    /* harmony export */
    Olmo2ForCausalLM: () => (
      /* reexport safe */
      r.Olmo2ForCausalLM
    ),
    /* harmony export */
    Olmo2Model: () => (
      /* reexport safe */
      r.Olmo2Model
    ),
    /* harmony export */
    Olmo2PreTrainedModel: () => (
      /* reexport safe */
      r.Olmo2PreTrainedModel
    ),
    /* harmony export */
    OlmoForCausalLM: () => (
      /* reexport safe */
      r.OlmoForCausalLM
    ),
    /* harmony export */
    OlmoModel: () => (
      /* reexport safe */
      r.OlmoModel
    ),
    /* harmony export */
    OlmoPreTrainedModel: () => (
      /* reexport safe */
      r.OlmoPreTrainedModel
    ),
    /* harmony export */
    OpenELMForCausalLM: () => (
      /* reexport safe */
      r.OpenELMForCausalLM
    ),
    /* harmony export */
    OpenELMModel: () => (
      /* reexport safe */
      r.OpenELMModel
    ),
    /* harmony export */
    OpenELMPreTrainedModel: () => (
      /* reexport safe */
      r.OpenELMPreTrainedModel
    ),
    /* harmony export */
    OwlViTFeatureExtractor: () => (
      /* reexport safe */
      I.OwlViTFeatureExtractor
    ),
    /* harmony export */
    OwlViTForObjectDetection: () => (
      /* reexport safe */
      r.OwlViTForObjectDetection
    ),
    /* harmony export */
    OwlViTImageProcessor: () => (
      /* reexport safe */
      I.OwlViTImageProcessor
    ),
    /* harmony export */
    OwlViTModel: () => (
      /* reexport safe */
      r.OwlViTModel
    ),
    /* harmony export */
    OwlViTPreTrainedModel: () => (
      /* reexport safe */
      r.OwlViTPreTrainedModel
    ),
    /* harmony export */
    OwlViTProcessor: () => (
      /* reexport safe */
      ie.OwlViTProcessor
    ),
    /* harmony export */
    Owlv2ForObjectDetection: () => (
      /* reexport safe */
      r.Owlv2ForObjectDetection
    ),
    /* harmony export */
    Owlv2ImageProcessor: () => (
      /* reexport safe */
      I.Owlv2ImageProcessor
    ),
    /* harmony export */
    Owlv2Model: () => (
      /* reexport safe */
      r.Owlv2Model
    ),
    /* harmony export */
    Owlv2PreTrainedModel: () => (
      /* reexport safe */
      r.Owlv2PreTrainedModel
    ),
    /* harmony export */
    PaliGemmaForConditionalGeneration: () => (
      /* reexport safe */
      r.PaliGemmaForConditionalGeneration
    ),
    /* harmony export */
    PaliGemmaPreTrainedModel: () => (
      /* reexport safe */
      r.PaliGemmaPreTrainedModel
    ),
    /* harmony export */
    PaliGemmaProcessor: () => (
      /* reexport safe */
      ie.PaliGemmaProcessor
    ),
    /* harmony export */
    PatchTSMixerForPrediction: () => (
      /* reexport safe */
      r.PatchTSMixerForPrediction
    ),
    /* harmony export */
    PatchTSMixerModel: () => (
      /* reexport safe */
      r.PatchTSMixerModel
    ),
    /* harmony export */
    PatchTSMixerPreTrainedModel: () => (
      /* reexport safe */
      r.PatchTSMixerPreTrainedModel
    ),
    /* harmony export */
    PatchTSTForPrediction: () => (
      /* reexport safe */
      r.PatchTSTForPrediction
    ),
    /* harmony export */
    PatchTSTModel: () => (
      /* reexport safe */
      r.PatchTSTModel
    ),
    /* harmony export */
    PatchTSTPreTrainedModel: () => (
      /* reexport safe */
      r.PatchTSTPreTrainedModel
    ),
    /* harmony export */
    Phi3ForCausalLM: () => (
      /* reexport safe */
      r.Phi3ForCausalLM
    ),
    /* harmony export */
    Phi3Model: () => (
      /* reexport safe */
      r.Phi3Model
    ),
    /* harmony export */
    Phi3PreTrainedModel: () => (
      /* reexport safe */
      r.Phi3PreTrainedModel
    ),
    /* harmony export */
    Phi3VForCausalLM: () => (
      /* reexport safe */
      r.Phi3VForCausalLM
    ),
    /* harmony export */
    Phi3VImageProcessor: () => (
      /* reexport safe */
      I.Phi3VImageProcessor
    ),
    /* harmony export */
    Phi3VPreTrainedModel: () => (
      /* reexport safe */
      r.Phi3VPreTrainedModel
    ),
    /* harmony export */
    Phi3VProcessor: () => (
      /* reexport safe */
      ie.Phi3VProcessor
    ),
    /* harmony export */
    PhiForCausalLM: () => (
      /* reexport safe */
      r.PhiForCausalLM
    ),
    /* harmony export */
    PhiModel: () => (
      /* reexport safe */
      r.PhiModel
    ),
    /* harmony export */
    PhiPreTrainedModel: () => (
      /* reexport safe */
      r.PhiPreTrainedModel
    ),
    /* harmony export */
    Pipeline: () => (
      /* reexport safe */
      $.Pipeline
    ),
    /* harmony export */
    PreTrainedModel: () => (
      /* reexport safe */
      r.PreTrainedModel
    ),
    /* harmony export */
    PreTrainedTokenizer: () => (
      /* reexport safe */
      _.PreTrainedTokenizer
    ),
    /* harmony export */
    PretrainedConfig: () => (
      /* reexport safe */
      D.PretrainedConfig
    ),
    /* harmony export */
    PretrainedMixin: () => (
      /* reexport safe */
      r.PretrainedMixin
    ),
    /* harmony export */
    Processor: () => (
      /* reexport safe */
      se.Processor
    ),
    /* harmony export */
    PvtForImageClassification: () => (
      /* reexport safe */
      r.PvtForImageClassification
    ),
    /* harmony export */
    PvtImageProcessor: () => (
      /* reexport safe */
      I.PvtImageProcessor
    ),
    /* harmony export */
    PvtModel: () => (
      /* reexport safe */
      r.PvtModel
    ),
    /* harmony export */
    PvtPreTrainedModel: () => (
      /* reexport safe */
      r.PvtPreTrainedModel
    ),
    /* harmony export */
    PyAnnoteFeatureExtractor: () => (
      /* reexport safe */
      M.PyAnnoteFeatureExtractor
    ),
    /* harmony export */
    PyAnnoteForAudioFrameClassification: () => (
      /* reexport safe */
      r.PyAnnoteForAudioFrameClassification
    ),
    /* harmony export */
    PyAnnoteModel: () => (
      /* reexport safe */
      r.PyAnnoteModel
    ),
    /* harmony export */
    PyAnnotePreTrainedModel: () => (
      /* reexport safe */
      r.PyAnnotePreTrainedModel
    ),
    /* harmony export */
    PyAnnoteProcessor: () => (
      /* reexport safe */
      ie.PyAnnoteProcessor
    ),
    /* harmony export */
    QuestionAnsweringModelOutput: () => (
      /* reexport safe */
      r.QuestionAnsweringModelOutput
    ),
    /* harmony export */
    QuestionAnsweringPipeline: () => (
      /* reexport safe */
      $.QuestionAnsweringPipeline
    ),
    /* harmony export */
    Qwen2ForCausalLM: () => (
      /* reexport safe */
      r.Qwen2ForCausalLM
    ),
    /* harmony export */
    Qwen2Model: () => (
      /* reexport safe */
      r.Qwen2Model
    ),
    /* harmony export */
    Qwen2PreTrainedModel: () => (
      /* reexport safe */
      r.Qwen2PreTrainedModel
    ),
    /* harmony export */
    Qwen2Tokenizer: () => (
      /* reexport safe */
      _.Qwen2Tokenizer
    ),
    /* harmony export */
    Qwen2VLForConditionalGeneration: () => (
      /* reexport safe */
      r.Qwen2VLForConditionalGeneration
    ),
    /* harmony export */
    Qwen2VLImageProcessor: () => (
      /* reexport safe */
      I.Qwen2VLImageProcessor
    ),
    /* harmony export */
    Qwen2VLPreTrainedModel: () => (
      /* reexport safe */
      r.Qwen2VLPreTrainedModel
    ),
    /* harmony export */
    Qwen2VLProcessor: () => (
      /* reexport safe */
      ie.Qwen2VLProcessor
    ),
    /* harmony export */
    RTDetrForObjectDetection: () => (
      /* reexport safe */
      r.RTDetrForObjectDetection
    ),
    /* harmony export */
    RTDetrImageProcessor: () => (
      /* reexport safe */
      I.RTDetrImageProcessor
    ),
    /* harmony export */
    RTDetrModel: () => (
      /* reexport safe */
      r.RTDetrModel
    ),
    /* harmony export */
    RTDetrObjectDetectionOutput: () => (
      /* reexport safe */
      r.RTDetrObjectDetectionOutput
    ),
    /* harmony export */
    RTDetrPreTrainedModel: () => (
      /* reexport safe */
      r.RTDetrPreTrainedModel
    ),
    /* harmony export */
    RawAudio: () => (
      /* reexport safe */
      U.RawAudio
    ),
    /* harmony export */
    RawImage: () => (
      /* reexport safe */
      Y.RawImage
    ),
    /* harmony export */
    RepetitionPenaltyLogitsProcessor: () => (
      /* reexport safe */
      A.RepetitionPenaltyLogitsProcessor
    ),
    /* harmony export */
    ResNetForImageClassification: () => (
      /* reexport safe */
      r.ResNetForImageClassification
    ),
    /* harmony export */
    ResNetModel: () => (
      /* reexport safe */
      r.ResNetModel
    ),
    /* harmony export */
    ResNetPreTrainedModel: () => (
      /* reexport safe */
      r.ResNetPreTrainedModel
    ),
    /* harmony export */
    RoFormerForMaskedLM: () => (
      /* reexport safe */
      r.RoFormerForMaskedLM
    ),
    /* harmony export */
    RoFormerForQuestionAnswering: () => (
      /* reexport safe */
      r.RoFormerForQuestionAnswering
    ),
    /* harmony export */
    RoFormerForSequenceClassification: () => (
      /* reexport safe */
      r.RoFormerForSequenceClassification
    ),
    /* harmony export */
    RoFormerForTokenClassification: () => (
      /* reexport safe */
      r.RoFormerForTokenClassification
    ),
    /* harmony export */
    RoFormerModel: () => (
      /* reexport safe */
      r.RoFormerModel
    ),
    /* harmony export */
    RoFormerPreTrainedModel: () => (
      /* reexport safe */
      r.RoFormerPreTrainedModel
    ),
    /* harmony export */
    RoFormerTokenizer: () => (
      /* reexport safe */
      _.RoFormerTokenizer
    ),
    /* harmony export */
    RobertaForMaskedLM: () => (
      /* reexport safe */
      r.RobertaForMaskedLM
    ),
    /* harmony export */
    RobertaForQuestionAnswering: () => (
      /* reexport safe */
      r.RobertaForQuestionAnswering
    ),
    /* harmony export */
    RobertaForSequenceClassification: () => (
      /* reexport safe */
      r.RobertaForSequenceClassification
    ),
    /* harmony export */
    RobertaForTokenClassification: () => (
      /* reexport safe */
      r.RobertaForTokenClassification
    ),
    /* harmony export */
    RobertaModel: () => (
      /* reexport safe */
      r.RobertaModel
    ),
    /* harmony export */
    RobertaPreTrainedModel: () => (
      /* reexport safe */
      r.RobertaPreTrainedModel
    ),
    /* harmony export */
    RobertaTokenizer: () => (
      /* reexport safe */
      _.RobertaTokenizer
    ),
    /* harmony export */
    SamImageProcessor: () => (
      /* reexport safe */
      I.SamImageProcessor
    ),
    /* harmony export */
    SamImageSegmentationOutput: () => (
      /* reexport safe */
      r.SamImageSegmentationOutput
    ),
    /* harmony export */
    SamModel: () => (
      /* reexport safe */
      r.SamModel
    ),
    /* harmony export */
    SamPreTrainedModel: () => (
      /* reexport safe */
      r.SamPreTrainedModel
    ),
    /* harmony export */
    SamProcessor: () => (
      /* reexport safe */
      ie.SamProcessor
    ),
    /* harmony export */
    SapiensForDepthEstimation: () => (
      /* reexport safe */
      r.SapiensForDepthEstimation
    ),
    /* harmony export */
    SapiensForNormalEstimation: () => (
      /* reexport safe */
      r.SapiensForNormalEstimation
    ),
    /* harmony export */
    SapiensForSemanticSegmentation: () => (
      /* reexport safe */
      r.SapiensForSemanticSegmentation
    ),
    /* harmony export */
    SapiensPreTrainedModel: () => (
      /* reexport safe */
      r.SapiensPreTrainedModel
    ),
    /* harmony export */
    SeamlessM4TFeatureExtractor: () => (
      /* reexport safe */
      M.SeamlessM4TFeatureExtractor
    ),
    /* harmony export */
    SegformerFeatureExtractor: () => (
      /* reexport safe */
      I.SegformerFeatureExtractor
    ),
    /* harmony export */
    SegformerForImageClassification: () => (
      /* reexport safe */
      r.SegformerForImageClassification
    ),
    /* harmony export */
    SegformerForSemanticSegmentation: () => (
      /* reexport safe */
      r.SegformerForSemanticSegmentation
    ),
    /* harmony export */
    SegformerImageProcessor: () => (
      /* reexport safe */
      I.SegformerImageProcessor
    ),
    /* harmony export */
    SegformerModel: () => (
      /* reexport safe */
      r.SegformerModel
    ),
    /* harmony export */
    SegformerPreTrainedModel: () => (
      /* reexport safe */
      r.SegformerPreTrainedModel
    ),
    /* harmony export */
    Seq2SeqLMOutput: () => (
      /* reexport safe */
      r.Seq2SeqLMOutput
    ),
    /* harmony export */
    SequenceClassifierOutput: () => (
      /* reexport safe */
      r.SequenceClassifierOutput
    ),
    /* harmony export */
    SiglipImageProcessor: () => (
      /* reexport safe */
      I.SiglipImageProcessor
    ),
    /* harmony export */
    SiglipModel: () => (
      /* reexport safe */
      r.SiglipModel
    ),
    /* harmony export */
    SiglipPreTrainedModel: () => (
      /* reexport safe */
      r.SiglipPreTrainedModel
    ),
    /* harmony export */
    SiglipTextModel: () => (
      /* reexport safe */
      r.SiglipTextModel
    ),
    /* harmony export */
    SiglipTokenizer: () => (
      /* reexport safe */
      _.SiglipTokenizer
    ),
    /* harmony export */
    SiglipVisionModel: () => (
      /* reexport safe */
      r.SiglipVisionModel
    ),
    /* harmony export */
    SpeechT5FeatureExtractor: () => (
      /* reexport safe */
      M.SpeechT5FeatureExtractor
    ),
    /* harmony export */
    SpeechT5ForSpeechToText: () => (
      /* reexport safe */
      r.SpeechT5ForSpeechToText
    ),
    /* harmony export */
    SpeechT5ForTextToSpeech: () => (
      /* reexport safe */
      r.SpeechT5ForTextToSpeech
    ),
    /* harmony export */
    SpeechT5HifiGan: () => (
      /* reexport safe */
      r.SpeechT5HifiGan
    ),
    /* harmony export */
    SpeechT5Model: () => (
      /* reexport safe */
      r.SpeechT5Model
    ),
    /* harmony export */
    SpeechT5PreTrainedModel: () => (
      /* reexport safe */
      r.SpeechT5PreTrainedModel
    ),
    /* harmony export */
    SpeechT5Processor: () => (
      /* reexport safe */
      ie.SpeechT5Processor
    ),
    /* harmony export */
    SpeechT5Tokenizer: () => (
      /* reexport safe */
      _.SpeechT5Tokenizer
    ),
    /* harmony export */
    SqueezeBertForMaskedLM: () => (
      /* reexport safe */
      r.SqueezeBertForMaskedLM
    ),
    /* harmony export */
    SqueezeBertForQuestionAnswering: () => (
      /* reexport safe */
      r.SqueezeBertForQuestionAnswering
    ),
    /* harmony export */
    SqueezeBertForSequenceClassification: () => (
      /* reexport safe */
      r.SqueezeBertForSequenceClassification
    ),
    /* harmony export */
    SqueezeBertModel: () => (
      /* reexport safe */
      r.SqueezeBertModel
    ),
    /* harmony export */
    SqueezeBertPreTrainedModel: () => (
      /* reexport safe */
      r.SqueezeBertPreTrainedModel
    ),
    /* harmony export */
    SqueezeBertTokenizer: () => (
      /* reexport safe */
      _.SqueezeBertTokenizer
    ),
    /* harmony export */
    StableLmForCausalLM: () => (
      /* reexport safe */
      r.StableLmForCausalLM
    ),
    /* harmony export */
    StableLmModel: () => (
      /* reexport safe */
      r.StableLmModel
    ),
    /* harmony export */
    StableLmPreTrainedModel: () => (
      /* reexport safe */
      r.StableLmPreTrainedModel
    ),
    /* harmony export */
    Starcoder2ForCausalLM: () => (
      /* reexport safe */
      r.Starcoder2ForCausalLM
    ),
    /* harmony export */
    Starcoder2Model: () => (
      /* reexport safe */
      r.Starcoder2Model
    ),
    /* harmony export */
    Starcoder2PreTrainedModel: () => (
      /* reexport safe */
      r.Starcoder2PreTrainedModel
    ),
    /* harmony export */
    StoppingCriteria: () => (
      /* reexport safe */
      q.StoppingCriteria
    ),
    /* harmony export */
    StoppingCriteriaList: () => (
      /* reexport safe */
      q.StoppingCriteriaList
    ),
    /* harmony export */
    StyleTextToSpeech2Model: () => (
      /* reexport safe */
      r.StyleTextToSpeech2Model
    ),
    /* harmony export */
    StyleTextToSpeech2PreTrainedModel: () => (
      /* reexport safe */
      r.StyleTextToSpeech2PreTrainedModel
    ),
    /* harmony export */
    SummarizationPipeline: () => (
      /* reexport safe */
      $.SummarizationPipeline
    ),
    /* harmony export */
    SuppressTokensAtBeginLogitsProcessor: () => (
      /* reexport safe */
      A.SuppressTokensAtBeginLogitsProcessor
    ),
    /* harmony export */
    Swin2SRForImageSuperResolution: () => (
      /* reexport safe */
      r.Swin2SRForImageSuperResolution
    ),
    /* harmony export */
    Swin2SRImageProcessor: () => (
      /* reexport safe */
      I.Swin2SRImageProcessor
    ),
    /* harmony export */
    Swin2SRModel: () => (
      /* reexport safe */
      r.Swin2SRModel
    ),
    /* harmony export */
    Swin2SRPreTrainedModel: () => (
      /* reexport safe */
      r.Swin2SRPreTrainedModel
    ),
    /* harmony export */
    SwinForImageClassification: () => (
      /* reexport safe */
      r.SwinForImageClassification
    ),
    /* harmony export */
    SwinModel: () => (
      /* reexport safe */
      r.SwinModel
    ),
    /* harmony export */
    SwinPreTrainedModel: () => (
      /* reexport safe */
      r.SwinPreTrainedModel
    ),
    /* harmony export */
    T5ForConditionalGeneration: () => (
      /* reexport safe */
      r.T5ForConditionalGeneration
    ),
    /* harmony export */
    T5Model: () => (
      /* reexport safe */
      r.T5Model
    ),
    /* harmony export */
    T5PreTrainedModel: () => (
      /* reexport safe */
      r.T5PreTrainedModel
    ),
    /* harmony export */
    T5Tokenizer: () => (
      /* reexport safe */
      _.T5Tokenizer
    ),
    /* harmony export */
    TableTransformerForObjectDetection: () => (
      /* reexport safe */
      r.TableTransformerForObjectDetection
    ),
    /* harmony export */
    TableTransformerModel: () => (
      /* reexport safe */
      r.TableTransformerModel
    ),
    /* harmony export */
    TableTransformerObjectDetectionOutput: () => (
      /* reexport safe */
      r.TableTransformerObjectDetectionOutput
    ),
    /* harmony export */
    TableTransformerPreTrainedModel: () => (
      /* reexport safe */
      r.TableTransformerPreTrainedModel
    ),
    /* harmony export */
    TemperatureLogitsWarper: () => (
      /* reexport safe */
      A.TemperatureLogitsWarper
    ),
    /* harmony export */
    Tensor: () => (
      /* reexport safe */
      R.Tensor
    ),
    /* harmony export */
    Text2TextGenerationPipeline: () => (
      /* reexport safe */
      $.Text2TextGenerationPipeline
    ),
    /* harmony export */
    TextClassificationPipeline: () => (
      /* reexport safe */
      $.TextClassificationPipeline
    ),
    /* harmony export */
    TextGenerationPipeline: () => (
      /* reexport safe */
      $.TextGenerationPipeline
    ),
    /* harmony export */
    TextStreamer: () => (
      /* reexport safe */
      j.TextStreamer
    ),
    /* harmony export */
    TextToAudioPipeline: () => (
      /* reexport safe */
      $.TextToAudioPipeline
    ),
    /* harmony export */
    TokenClassificationPipeline: () => (
      /* reexport safe */
      $.TokenClassificationPipeline
    ),
    /* harmony export */
    TokenClassifierOutput: () => (
      /* reexport safe */
      r.TokenClassifierOutput
    ),
    /* harmony export */
    TokenizerModel: () => (
      /* reexport safe */
      _.TokenizerModel
    ),
    /* harmony export */
    TopKLogitsWarper: () => (
      /* reexport safe */
      A.TopKLogitsWarper
    ),
    /* harmony export */
    TopPLogitsWarper: () => (
      /* reexport safe */
      A.TopPLogitsWarper
    ),
    /* harmony export */
    TrOCRForCausalLM: () => (
      /* reexport safe */
      r.TrOCRForCausalLM
    ),
    /* harmony export */
    TrOCRPreTrainedModel: () => (
      /* reexport safe */
      r.TrOCRPreTrainedModel
    ),
    /* harmony export */
    TranslationPipeline: () => (
      /* reexport safe */
      $.TranslationPipeline
    ),
    /* harmony export */
    UniSpeechForCTC: () => (
      /* reexport safe */
      r.UniSpeechForCTC
    ),
    /* harmony export */
    UniSpeechForSequenceClassification: () => (
      /* reexport safe */
      r.UniSpeechForSequenceClassification
    ),
    /* harmony export */
    UniSpeechModel: () => (
      /* reexport safe */
      r.UniSpeechModel
    ),
    /* harmony export */
    UniSpeechPreTrainedModel: () => (
      /* reexport safe */
      r.UniSpeechPreTrainedModel
    ),
    /* harmony export */
    UniSpeechSatForAudioFrameClassification: () => (
      /* reexport safe */
      r.UniSpeechSatForAudioFrameClassification
    ),
    /* harmony export */
    UniSpeechSatForCTC: () => (
      /* reexport safe */
      r.UniSpeechSatForCTC
    ),
    /* harmony export */
    UniSpeechSatForSequenceClassification: () => (
      /* reexport safe */
      r.UniSpeechSatForSequenceClassification
    ),
    /* harmony export */
    UniSpeechSatModel: () => (
      /* reexport safe */
      r.UniSpeechSatModel
    ),
    /* harmony export */
    UniSpeechSatPreTrainedModel: () => (
      /* reexport safe */
      r.UniSpeechSatPreTrainedModel
    ),
    /* harmony export */
    VLChatProcessor: () => (
      /* reexport safe */
      ie.VLChatProcessor
    ),
    /* harmony export */
    VLMImageProcessor: () => (
      /* reexport safe */
      I.VLMImageProcessor
    ),
    /* harmony export */
    ViTFeatureExtractor: () => (
      /* reexport safe */
      I.ViTFeatureExtractor
    ),
    /* harmony export */
    ViTForImageClassification: () => (
      /* reexport safe */
      r.ViTForImageClassification
    ),
    /* harmony export */
    ViTImageProcessor: () => (
      /* reexport safe */
      I.ViTImageProcessor
    ),
    /* harmony export */
    ViTMAEModel: () => (
      /* reexport safe */
      r.ViTMAEModel
    ),
    /* harmony export */
    ViTMAEPreTrainedModel: () => (
      /* reexport safe */
      r.ViTMAEPreTrainedModel
    ),
    /* harmony export */
    ViTMSNForImageClassification: () => (
      /* reexport safe */
      r.ViTMSNForImageClassification
    ),
    /* harmony export */
    ViTMSNModel: () => (
      /* reexport safe */
      r.ViTMSNModel
    ),
    /* harmony export */
    ViTMSNPreTrainedModel: () => (
      /* reexport safe */
      r.ViTMSNPreTrainedModel
    ),
    /* harmony export */
    ViTModel: () => (
      /* reexport safe */
      r.ViTModel
    ),
    /* harmony export */
    ViTPreTrainedModel: () => (
      /* reexport safe */
      r.ViTPreTrainedModel
    ),
    /* harmony export */
    VisionEncoderDecoderModel: () => (
      /* reexport safe */
      r.VisionEncoderDecoderModel
    ),
    /* harmony export */
    VitMatteForImageMatting: () => (
      /* reexport safe */
      r.VitMatteForImageMatting
    ),
    /* harmony export */
    VitMatteImageProcessor: () => (
      /* reexport safe */
      I.VitMatteImageProcessor
    ),
    /* harmony export */
    VitMattePreTrainedModel: () => (
      /* reexport safe */
      r.VitMattePreTrainedModel
    ),
    /* harmony export */
    VitPoseForPoseEstimation: () => (
      /* reexport safe */
      r.VitPoseForPoseEstimation
    ),
    /* harmony export */
    VitPoseImageProcessor: () => (
      /* reexport safe */
      I.VitPoseImageProcessor
    ),
    /* harmony export */
    VitPosePreTrainedModel: () => (
      /* reexport safe */
      r.VitPosePreTrainedModel
    ),
    /* harmony export */
    VitsModel: () => (
      /* reexport safe */
      r.VitsModel
    ),
    /* harmony export */
    VitsModelOutput: () => (
      /* reexport safe */
      r.VitsModelOutput
    ),
    /* harmony export */
    VitsPreTrainedModel: () => (
      /* reexport safe */
      r.VitsPreTrainedModel
    ),
    /* harmony export */
    VitsTokenizer: () => (
      /* reexport safe */
      _.VitsTokenizer
    ),
    /* harmony export */
    Wav2Vec2BertForCTC: () => (
      /* reexport safe */
      r.Wav2Vec2BertForCTC
    ),
    /* harmony export */
    Wav2Vec2BertForSequenceClassification: () => (
      /* reexport safe */
      r.Wav2Vec2BertForSequenceClassification
    ),
    /* harmony export */
    Wav2Vec2BertModel: () => (
      /* reexport safe */
      r.Wav2Vec2BertModel
    ),
    /* harmony export */
    Wav2Vec2BertPreTrainedModel: () => (
      /* reexport safe */
      r.Wav2Vec2BertPreTrainedModel
    ),
    /* harmony export */
    Wav2Vec2CTCTokenizer: () => (
      /* reexport safe */
      _.Wav2Vec2CTCTokenizer
    ),
    /* harmony export */
    Wav2Vec2FeatureExtractor: () => (
      /* reexport safe */
      M.Wav2Vec2FeatureExtractor
    ),
    /* harmony export */
    Wav2Vec2ForAudioFrameClassification: () => (
      /* reexport safe */
      r.Wav2Vec2ForAudioFrameClassification
    ),
    /* harmony export */
    Wav2Vec2ForCTC: () => (
      /* reexport safe */
      r.Wav2Vec2ForCTC
    ),
    /* harmony export */
    Wav2Vec2ForSequenceClassification: () => (
      /* reexport safe */
      r.Wav2Vec2ForSequenceClassification
    ),
    /* harmony export */
    Wav2Vec2Model: () => (
      /* reexport safe */
      r.Wav2Vec2Model
    ),
    /* harmony export */
    Wav2Vec2PreTrainedModel: () => (
      /* reexport safe */
      r.Wav2Vec2PreTrainedModel
    ),
    /* harmony export */
    Wav2Vec2Processor: () => (
      /* reexport safe */
      ie.Wav2Vec2Processor
    ),
    /* harmony export */
    Wav2Vec2ProcessorWithLM: () => (
      /* reexport safe */
      ie.Wav2Vec2ProcessorWithLM
    ),
    /* harmony export */
    WavLMForAudioFrameClassification: () => (
      /* reexport safe */
      r.WavLMForAudioFrameClassification
    ),
    /* harmony export */
    WavLMForCTC: () => (
      /* reexport safe */
      r.WavLMForCTC
    ),
    /* harmony export */
    WavLMForSequenceClassification: () => (
      /* reexport safe */
      r.WavLMForSequenceClassification
    ),
    /* harmony export */
    WavLMForXVector: () => (
      /* reexport safe */
      r.WavLMForXVector
    ),
    /* harmony export */
    WavLMModel: () => (
      /* reexport safe */
      r.WavLMModel
    ),
    /* harmony export */
    WavLMPreTrainedModel: () => (
      /* reexport safe */
      r.WavLMPreTrainedModel
    ),
    /* harmony export */
    WeSpeakerFeatureExtractor: () => (
      /* reexport safe */
      M.WeSpeakerFeatureExtractor
    ),
    /* harmony export */
    WeSpeakerResNetModel: () => (
      /* reexport safe */
      r.WeSpeakerResNetModel
    ),
    /* harmony export */
    WeSpeakerResNetPreTrainedModel: () => (
      /* reexport safe */
      r.WeSpeakerResNetPreTrainedModel
    ),
    /* harmony export */
    WhisperFeatureExtractor: () => (
      /* reexport safe */
      M.WhisperFeatureExtractor
    ),
    /* harmony export */
    WhisperForConditionalGeneration: () => (
      /* reexport safe */
      r.WhisperForConditionalGeneration
    ),
    /* harmony export */
    WhisperModel: () => (
      /* reexport safe */
      r.WhisperModel
    ),
    /* harmony export */
    WhisperPreTrainedModel: () => (
      /* reexport safe */
      r.WhisperPreTrainedModel
    ),
    /* harmony export */
    WhisperProcessor: () => (
      /* reexport safe */
      ie.WhisperProcessor
    ),
    /* harmony export */
    WhisperTextStreamer: () => (
      /* reexport safe */
      j.WhisperTextStreamer
    ),
    /* harmony export */
    WhisperTimeStampLogitsProcessor: () => (
      /* reexport safe */
      A.WhisperTimeStampLogitsProcessor
    ),
    /* harmony export */
    WhisperTokenizer: () => (
      /* reexport safe */
      _.WhisperTokenizer
    ),
    /* harmony export */
    XLMForQuestionAnswering: () => (
      /* reexport safe */
      r.XLMForQuestionAnswering
    ),
    /* harmony export */
    XLMForSequenceClassification: () => (
      /* reexport safe */
      r.XLMForSequenceClassification
    ),
    /* harmony export */
    XLMForTokenClassification: () => (
      /* reexport safe */
      r.XLMForTokenClassification
    ),
    /* harmony export */
    XLMModel: () => (
      /* reexport safe */
      r.XLMModel
    ),
    /* harmony export */
    XLMPreTrainedModel: () => (
      /* reexport safe */
      r.XLMPreTrainedModel
    ),
    /* harmony export */
    XLMRobertaForMaskedLM: () => (
      /* reexport safe */
      r.XLMRobertaForMaskedLM
    ),
    /* harmony export */
    XLMRobertaForQuestionAnswering: () => (
      /* reexport safe */
      r.XLMRobertaForQuestionAnswering
    ),
    /* harmony export */
    XLMRobertaForSequenceClassification: () => (
      /* reexport safe */
      r.XLMRobertaForSequenceClassification
    ),
    /* harmony export */
    XLMRobertaForTokenClassification: () => (
      /* reexport safe */
      r.XLMRobertaForTokenClassification
    ),
    /* harmony export */
    XLMRobertaModel: () => (
      /* reexport safe */
      r.XLMRobertaModel
    ),
    /* harmony export */
    XLMRobertaPreTrainedModel: () => (
      /* reexport safe */
      r.XLMRobertaPreTrainedModel
    ),
    /* harmony export */
    XLMRobertaTokenizer: () => (
      /* reexport safe */
      _.XLMRobertaTokenizer
    ),
    /* harmony export */
    XLMTokenizer: () => (
      /* reexport safe */
      _.XLMTokenizer
    ),
    /* harmony export */
    XLMWithLMHeadModel: () => (
      /* reexport safe */
      r.XLMWithLMHeadModel
    ),
    /* harmony export */
    XVectorOutput: () => (
      /* reexport safe */
      r.XVectorOutput
    ),
    /* harmony export */
    YolosFeatureExtractor: () => (
      /* reexport safe */
      I.YolosFeatureExtractor
    ),
    /* harmony export */
    YolosForObjectDetection: () => (
      /* reexport safe */
      r.YolosForObjectDetection
    ),
    /* harmony export */
    YolosImageProcessor: () => (
      /* reexport safe */
      I.YolosImageProcessor
    ),
    /* harmony export */
    YolosModel: () => (
      /* reexport safe */
      r.YolosModel
    ),
    /* harmony export */
    YolosObjectDetectionOutput: () => (
      /* reexport safe */
      r.YolosObjectDetectionOutput
    ),
    /* harmony export */
    YolosPreTrainedModel: () => (
      /* reexport safe */
      r.YolosPreTrainedModel
    ),
    /* harmony export */
    ZeroShotAudioClassificationPipeline: () => (
      /* reexport safe */
      $.ZeroShotAudioClassificationPipeline
    ),
    /* harmony export */
    ZeroShotClassificationPipeline: () => (
      /* reexport safe */
      $.ZeroShotClassificationPipeline
    ),
    /* harmony export */
    ZeroShotImageClassificationPipeline: () => (
      /* reexport safe */
      $.ZeroShotImageClassificationPipeline
    ),
    /* harmony export */
    ZeroShotObjectDetectionPipeline: () => (
      /* reexport safe */
      $.ZeroShotObjectDetectionPipeline
    ),
    /* harmony export */
    bankers_round: () => (
      /* reexport safe */
      g.bankers_round
    ),
    /* harmony export */
    cat: () => (
      /* reexport safe */
      R.cat
    ),
    /* harmony export */
    cos_sim: () => (
      /* reexport safe */
      g.cos_sim
    ),
    /* harmony export */
    dot: () => (
      /* reexport safe */
      g.dot
    ),
    /* harmony export */
    dynamic_time_warping: () => (
      /* reexport safe */
      g.dynamic_time_warping
    ),
    /* harmony export */
    env: () => (
      /* reexport safe */
      $e.env
    ),
    /* harmony export */
    full: () => (
      /* reexport safe */
      R.full
    ),
    /* harmony export */
    full_like: () => (
      /* reexport safe */
      R.full_like
    ),
    /* harmony export */
    getKeyValueShapes: () => (
      /* reexport safe */
      D.getKeyValueShapes
    ),
    /* harmony export */
    hamming: () => (
      /* reexport safe */
      U.hamming
    ),
    /* harmony export */
    hanning: () => (
      /* reexport safe */
      U.hanning
    ),
    /* harmony export */
    interpolate: () => (
      /* reexport safe */
      R.interpolate
    ),
    /* harmony export */
    interpolate_4d: () => (
      /* reexport safe */
      R.interpolate_4d
    ),
    /* harmony export */
    interpolate_data: () => (
      /* reexport safe */
      g.interpolate_data
    ),
    /* harmony export */
    is_chinese_char: () => (
      /* reexport safe */
      _.is_chinese_char
    ),
    /* harmony export */
    layer_norm: () => (
      /* reexport safe */
      R.layer_norm
    ),
    /* harmony export */
    load_image: () => (
      /* reexport safe */
      Y.load_image
    ),
    /* harmony export */
    log_softmax: () => (
      /* reexport safe */
      g.log_softmax
    ),
    /* harmony export */
    magnitude: () => (
      /* reexport safe */
      g.magnitude
    ),
    /* harmony export */
    matmul: () => (
      /* reexport safe */
      R.matmul
    ),
    /* harmony export */
    max: () => (
      /* reexport safe */
      g.max
    ),
    /* harmony export */
    mean: () => (
      /* reexport safe */
      R.mean
    ),
    /* harmony export */
    mean_pooling: () => (
      /* reexport safe */
      R.mean_pooling
    ),
    /* harmony export */
    medianFilter: () => (
      /* reexport safe */
      g.medianFilter
    ),
    /* harmony export */
    mel_filter_bank: () => (
      /* reexport safe */
      U.mel_filter_bank
    ),
    /* harmony export */
    min: () => (
      /* reexport safe */
      g.min
    ),
    /* harmony export */
    ones: () => (
      /* reexport safe */
      R.ones
    ),
    /* harmony export */
    ones_like: () => (
      /* reexport safe */
      R.ones_like
    ),
    /* harmony export */
    permute: () => (
      /* reexport safe */
      R.permute
    ),
    /* harmony export */
    permute_data: () => (
      /* reexport safe */
      g.permute_data
    ),
    /* harmony export */
    pipeline: () => (
      /* reexport safe */
      $.pipeline
    ),
    /* harmony export */
    quantize_embeddings: () => (
      /* reexport safe */
      R.quantize_embeddings
    ),
    /* harmony export */
    rand: () => (
      /* reexport safe */
      R.rand
    ),
    /* harmony export */
    read_audio: () => (
      /* reexport safe */
      U.read_audio
    ),
    /* harmony export */
    rfft: () => (
      /* reexport safe */
      R.rfft
    ),
    /* harmony export */
    round: () => (
      /* reexport safe */
      g.round
    ),
    /* harmony export */
    slice: () => (
      /* reexport safe */
      R.slice
    ),
    /* harmony export */
    softmax: () => (
      /* reexport safe */
      g.softmax
    ),
    /* harmony export */
    spectrogram: () => (
      /* reexport safe */
      U.spectrogram
    ),
    /* harmony export */
    stack: () => (
      /* reexport safe */
      R.stack
    ),
    /* harmony export */
    std_mean: () => (
      /* reexport safe */
      R.std_mean
    ),
    /* harmony export */
    topk: () => (
      /* reexport safe */
      R.topk
    ),
    /* harmony export */
    window_function: () => (
      /* reexport safe */
      U.window_function
    ),
    /* harmony export */
    zeros: () => (
      /* reexport safe */
      R.zeros
    ),
    /* harmony export */
    zeros_like: () => (
      /* reexport safe */
      R.zeros_like
    )
    /* harmony export */
  });
  var $e = ws(
    /*! ./env.js */
    "./src/env.js"
  ), $ = ws(
    /*! ./pipelines.js */
    "./src/pipelines.js"
  ), r = ws(
    /*! ./models.js */
    "./src/models.js"
  ), _ = ws(
    /*! ./tokenizers.js */
    "./src/tokenizers.js"
  ), D = ws(
    /*! ./configs.js */
    "./src/configs.js"
  ), U = ws(
    /*! ./utils/audio.js */
    "./src/utils/audio.js"
  ), Y = ws(
    /*! ./utils/image.js */
    "./src/utils/image.js"
  ), R = ws(
    /*! ./utils/tensor.js */
    "./src/utils/tensor.js"
  ), g = ws(
    /*! ./utils/maths.js */
    "./src/utils/maths.js"
  ), v = ws(
    /*! ./base/feature_extraction_utils.js */
    "./src/base/feature_extraction_utils.js"
  ), M = ws(
    /*! ./models/feature_extractors.js */
    "./src/models/feature_extractors.js"
  ), y = ws(
    /*! ./models/auto/feature_extraction_auto.js */
    "./src/models/auto/feature_extraction_auto.js"
  ), b = ws(
    /*! ./base/image_processors_utils.js */
    "./src/base/image_processors_utils.js"
  ), I = ws(
    /*! ./models/image_processors.js */
    "./src/models/image_processors.js"
  ), K = ws(
    /*! ./models/auto/image_processing_auto.js */
    "./src/models/auto/image_processing_auto.js"
  ), se = ws(
    /*! ./base/processing_utils.js */
    "./src/base/processing_utils.js"
  ), ie = ws(
    /*! ./models/processors.js */
    "./src/models/processors.js"
  ), W = ws(
    /*! ./models/auto/processing_auto.js */
    "./src/models/auto/processing_auto.js"
  ), j = ws(
    /*! ./generation/streamers.js */
    "./src/generation/streamers.js"
  ), q = ws(
    /*! ./generation/stopping_criteria.js */
    "./src/generation/stopping_criteria.js"
  ), A = ws(
    /*! ./generation/logits_process.js */
    "./src/generation/logits_process.js"
  );
})();
c.ASTFeatureExtractor;
c.ASTForAudioClassification;
c.ASTModel;
c.ASTPreTrainedModel;
c.AlbertForMaskedLM;
c.AlbertForQuestionAnswering;
c.AlbertForSequenceClassification;
c.AlbertModel;
c.AlbertPreTrainedModel;
c.AlbertTokenizer;
c.AudioClassificationPipeline;
c.AutoConfig;
c.AutoFeatureExtractor;
c.AutoImageProcessor;
c.AutoModel;
c.AutoModelForAudioClassification;
c.AutoModelForAudioFrameClassification;
c.AutoModelForCTC;
c.AutoModelForCausalLM;
c.AutoModelForDepthEstimation;
c.AutoModelForDocumentQuestionAnswering;
c.AutoModelForImageClassification;
c.AutoModelForImageFeatureExtraction;
c.AutoModelForImageMatting;
c.AutoModelForImageSegmentation;
c.AutoModelForImageToImage;
c.AutoModelForMaskGeneration;
c.AutoModelForMaskedLM;
c.AutoModelForNormalEstimation;
c.AutoModelForObjectDetection;
c.AutoModelForPoseEstimation;
c.AutoModelForQuestionAnswering;
c.AutoModelForSemanticSegmentation;
c.AutoModelForSeq2SeqLM;
c.AutoModelForSequenceClassification;
c.AutoModelForSpeechSeq2Seq;
c.AutoModelForTextToSpectrogram;
c.AutoModelForTextToWaveform;
c.AutoModelForTokenClassification;
c.AutoModelForUniversalSegmentation;
c.AutoModelForVision2Seq;
c.AutoModelForXVector;
c.AutoModelForZeroShotObjectDetection;
c.AutoProcessor;
c.AutoTokenizer;
c.AutomaticSpeechRecognitionPipeline;
c.BartForConditionalGeneration;
c.BartForSequenceClassification;
c.BartModel;
c.BartPretrainedModel;
c.BartTokenizer;
c.BaseModelOutput;
c.BaseStreamer;
c.BeitFeatureExtractor;
c.BeitForImageClassification;
c.BeitModel;
c.BeitPreTrainedModel;
c.BertForMaskedLM;
c.BertForQuestionAnswering;
c.BertForSequenceClassification;
c.BertForTokenClassification;
c.BertModel;
c.BertPreTrainedModel;
c.BertTokenizer;
c.BitImageProcessor;
c.BlenderbotForConditionalGeneration;
c.BlenderbotModel;
c.BlenderbotPreTrainedModel;
c.BlenderbotSmallForConditionalGeneration;
c.BlenderbotSmallModel;
c.BlenderbotSmallPreTrainedModel;
c.BlenderbotSmallTokenizer;
c.BlenderbotTokenizer;
c.BloomForCausalLM;
c.BloomModel;
c.BloomPreTrainedModel;
c.BloomTokenizer;
c.CLIPFeatureExtractor;
c.CLIPImageProcessor;
c.CLIPModel;
c.CLIPPreTrainedModel;
c.CLIPSegForImageSegmentation;
c.CLIPSegModel;
c.CLIPSegPreTrainedModel;
c.CLIPTextModel;
c.CLIPTextModelWithProjection;
c.CLIPTokenizer;
c.CLIPVisionModel;
c.CLIPVisionModelWithProjection;
c.CamembertForMaskedLM;
c.CamembertForQuestionAnswering;
c.CamembertForSequenceClassification;
c.CamembertForTokenClassification;
c.CamembertModel;
c.CamembertPreTrainedModel;
c.CamembertTokenizer;
c.CausalLMOutput;
c.CausalLMOutputWithPast;
c.ChineseCLIPFeatureExtractor;
c.ChineseCLIPModel;
c.ChineseCLIPPreTrainedModel;
c.ClapAudioModelWithProjection;
c.ClapFeatureExtractor;
c.ClapModel;
c.ClapPreTrainedModel;
c.ClapTextModelWithProjection;
c.ClassifierFreeGuidanceLogitsProcessor;
c.CodeGenForCausalLM;
c.CodeGenModel;
c.CodeGenPreTrainedModel;
c.CodeGenTokenizer;
c.CodeLlamaTokenizer;
c.CohereForCausalLM;
c.CohereModel;
c.CoherePreTrainedModel;
c.CohereTokenizer;
c.ConvBertForMaskedLM;
c.ConvBertForQuestionAnswering;
c.ConvBertForSequenceClassification;
c.ConvBertForTokenClassification;
c.ConvBertModel;
c.ConvBertPreTrainedModel;
c.ConvBertTokenizer;
c.ConvNextFeatureExtractor;
c.ConvNextForImageClassification;
c.ConvNextImageProcessor;
c.ConvNextModel;
c.ConvNextPreTrainedModel;
c.ConvNextV2ForImageClassification;
c.ConvNextV2Model;
c.ConvNextV2PreTrainedModel;
c.DPTFeatureExtractor;
c.DPTForDepthEstimation;
c.DPTImageProcessor;
c.DPTModel;
c.DPTPreTrainedModel;
c.DebertaForMaskedLM;
c.DebertaForQuestionAnswering;
c.DebertaForSequenceClassification;
c.DebertaForTokenClassification;
c.DebertaModel;
c.DebertaPreTrainedModel;
c.DebertaTokenizer;
c.DebertaV2ForMaskedLM;
c.DebertaV2ForQuestionAnswering;
c.DebertaV2ForSequenceClassification;
c.DebertaV2ForTokenClassification;
c.DebertaV2Model;
c.DebertaV2PreTrainedModel;
c.DebertaV2Tokenizer;
c.DecisionTransformerModel;
c.DecisionTransformerPreTrainedModel;
c.DeiTFeatureExtractor;
c.DeiTForImageClassification;
c.DeiTImageProcessor;
c.DeiTModel;
c.DeiTPreTrainedModel;
c.DepthAnythingForDepthEstimation;
c.DepthAnythingPreTrainedModel;
c.DepthEstimationPipeline;
c.DepthProForDepthEstimation;
c.DepthProPreTrainedModel;
c.DetrFeatureExtractor;
c.DetrForObjectDetection;
c.DetrForSegmentation;
c.DetrImageProcessor;
c.DetrModel;
c.DetrObjectDetectionOutput;
c.DetrPreTrainedModel;
c.DetrSegmentationOutput;
c.Dinov2ForImageClassification;
c.Dinov2Model;
c.Dinov2PreTrainedModel;
c.Dinov2WithRegistersForImageClassification;
c.Dinov2WithRegistersModel;
c.Dinov2WithRegistersPreTrainedModel;
c.DistilBertForMaskedLM;
c.DistilBertForQuestionAnswering;
c.DistilBertForSequenceClassification;
c.DistilBertForTokenClassification;
c.DistilBertModel;
c.DistilBertPreTrainedModel;
c.DistilBertTokenizer;
c.DocumentQuestionAnsweringPipeline;
c.DonutFeatureExtractor;
c.DonutImageProcessor;
c.DonutSwinModel;
c.DonutSwinPreTrainedModel;
c.EfficientNetForImageClassification;
c.EfficientNetImageProcessor;
c.EfficientNetModel;
c.EfficientNetPreTrainedModel;
c.ElectraForMaskedLM;
c.ElectraForQuestionAnswering;
c.ElectraForSequenceClassification;
c.ElectraForTokenClassification;
c.ElectraModel;
c.ElectraPreTrainedModel;
c.ElectraTokenizer;
c.EosTokenCriteria;
c.EsmForMaskedLM;
c.EsmForSequenceClassification;
c.EsmForTokenClassification;
c.EsmModel;
c.EsmPreTrainedModel;
c.EsmTokenizer;
c.ExaoneForCausalLM;
c.ExaoneModel;
c.ExaonePreTrainedModel;
c.FFT;
c.FalconForCausalLM;
c.FalconModel;
c.FalconPreTrainedModel;
c.FalconTokenizer;
c.FastViTForImageClassification;
c.FastViTModel;
c.FastViTPreTrainedModel;
c.FeatureExtractionPipeline;
c.FeatureExtractor;
c.FillMaskPipeline;
c.Florence2ForConditionalGeneration;
c.Florence2PreTrainedModel;
c.Florence2Processor;
c.ForcedBOSTokenLogitsProcessor;
c.ForcedEOSTokenLogitsProcessor;
c.GLPNFeatureExtractor;
c.GLPNForDepthEstimation;
c.GLPNModel;
c.GLPNPreTrainedModel;
c.GPT2LMHeadModel;
c.GPT2Model;
c.GPT2PreTrainedModel;
c.GPT2Tokenizer;
c.GPTBigCodeForCausalLM;
c.GPTBigCodeModel;
c.GPTBigCodePreTrainedModel;
c.GPTJForCausalLM;
c.GPTJModel;
c.GPTJPreTrainedModel;
c.GPTNeoForCausalLM;
c.GPTNeoModel;
c.GPTNeoPreTrainedModel;
c.GPTNeoXForCausalLM;
c.GPTNeoXModel;
c.GPTNeoXPreTrainedModel;
c.GPTNeoXTokenizer;
c.Gemma2ForCausalLM;
c.Gemma2Model;
c.Gemma2PreTrainedModel;
c.GemmaForCausalLM;
c.GemmaModel;
c.GemmaPreTrainedModel;
c.GemmaTokenizer;
c.GlmForCausalLM;
c.GlmModel;
c.GlmPreTrainedModel;
c.GraniteForCausalLM;
c.GraniteModel;
c.GranitePreTrainedModel;
c.Grok1Tokenizer;
c.GroundingDinoForObjectDetection;
c.GroundingDinoImageProcessor;
c.GroundingDinoPreTrainedModel;
c.GroundingDinoProcessor;
c.GroupViTModel;
c.GroupViTPreTrainedModel;
c.HeliumForCausalLM;
c.HeliumModel;
c.HeliumPreTrainedModel;
c.HerbertTokenizer;
c.HieraForImageClassification;
c.HieraModel;
c.HieraPreTrainedModel;
c.HubertForCTC;
c.HubertForSequenceClassification;
c.HubertModel;
c.HubertPreTrainedModel;
c.IJepaForImageClassification;
c.IJepaModel;
c.IJepaPreTrainedModel;
c.Idefics3ForConditionalGeneration;
c.Idefics3ImageProcessor;
c.Idefics3PreTrainedModel;
c.Idefics3Processor;
c.ImageClassificationPipeline;
c.ImageFeatureExtractionPipeline;
c.ImageFeatureExtractor;
c.ImageMattingOutput;
c.ImageProcessor;
c.ImageSegmentationPipeline;
c.ImageToImagePipeline;
c.ImageToTextPipeline;
c.InterruptableStoppingCriteria;
c.JAISLMHeadModel;
c.JAISModel;
c.JAISPreTrainedModel;
c.JinaCLIPImageProcessor;
c.JinaCLIPModel;
c.JinaCLIPPreTrainedModel;
c.JinaCLIPProcessor;
c.JinaCLIPTextModel;
c.JinaCLIPVisionModel;
c.LlamaForCausalLM;
c.LlamaModel;
c.LlamaPreTrainedModel;
c.LlamaTokenizer;
c.LlavaForConditionalGeneration;
c.LlavaOnevisionForConditionalGeneration;
c.LlavaOnevisionImageProcessor;
c.LlavaPreTrainedModel;
c.LogitsProcessor;
c.LogitsProcessorList;
c.LogitsWarper;
c.LongT5ForConditionalGeneration;
c.LongT5Model;
c.LongT5PreTrainedModel;
c.M2M100ForConditionalGeneration;
c.M2M100Model;
c.M2M100PreTrainedModel;
c.M2M100Tokenizer;
c.MBart50Tokenizer;
c.MBartForCausalLM;
c.MBartForConditionalGeneration;
c.MBartForSequenceClassification;
c.MBartModel;
c.MBartPreTrainedModel;
c.MBartTokenizer;
c.MPNetForMaskedLM;
c.MPNetForQuestionAnswering;
c.MPNetForSequenceClassification;
c.MPNetForTokenClassification;
c.MPNetModel;
c.MPNetPreTrainedModel;
c.MPNetTokenizer;
c.MT5ForConditionalGeneration;
c.MT5Model;
c.MT5PreTrainedModel;
c.MarianMTModel;
c.MarianModel;
c.MarianPreTrainedModel;
c.MarianTokenizer;
c.Mask2FormerImageProcessor;
c.MaskFormerFeatureExtractor;
c.MaskFormerForInstanceSegmentation;
c.MaskFormerImageProcessor;
c.MaskFormerModel;
c.MaskFormerPreTrainedModel;
c.MaskedLMOutput;
c.MaxLengthCriteria;
c.MgpstrForSceneTextRecognition;
c.MgpstrModelOutput;
c.MgpstrPreTrainedModel;
c.MgpstrProcessor;
c.MgpstrTokenizer;
c.MinLengthLogitsProcessor;
c.MinNewTokensLengthLogitsProcessor;
c.MistralForCausalLM;
c.MistralModel;
c.MistralPreTrainedModel;
c.MobileBertForMaskedLM;
c.MobileBertForQuestionAnswering;
c.MobileBertForSequenceClassification;
c.MobileBertModel;
c.MobileBertPreTrainedModel;
c.MobileBertTokenizer;
c.MobileLLMForCausalLM;
c.MobileLLMModel;
c.MobileLLMPreTrainedModel;
c.MobileNetV1FeatureExtractor;
c.MobileNetV1ForImageClassification;
c.MobileNetV1ImageProcessor;
c.MobileNetV1Model;
c.MobileNetV1PreTrainedModel;
c.MobileNetV2FeatureExtractor;
c.MobileNetV2ForImageClassification;
c.MobileNetV2ImageProcessor;
c.MobileNetV2Model;
c.MobileNetV2PreTrainedModel;
c.MobileNetV3FeatureExtractor;
c.MobileNetV3ForImageClassification;
c.MobileNetV3ImageProcessor;
c.MobileNetV3Model;
c.MobileNetV3PreTrainedModel;
c.MobileNetV4FeatureExtractor;
c.MobileNetV4ForImageClassification;
c.MobileNetV4ImageProcessor;
c.MobileNetV4Model;
c.MobileNetV4PreTrainedModel;
c.MobileViTFeatureExtractor;
c.MobileViTForImageClassification;
c.MobileViTImageProcessor;
c.MobileViTModel;
c.MobileViTPreTrainedModel;
c.MobileViTV2ForImageClassification;
c.MobileViTV2Model;
c.MobileViTV2PreTrainedModel;
c.ModelOutput;
c.ModernBertForMaskedLM;
c.ModernBertForSequenceClassification;
c.ModernBertForTokenClassification;
c.ModernBertModel;
c.ModernBertPreTrainedModel;
c.Moondream1ForConditionalGeneration;
c.MoonshineFeatureExtractor;
c.MoonshineForConditionalGeneration;
c.MoonshineModel;
c.MoonshinePreTrainedModel;
c.MoonshineProcessor;
c.MptForCausalLM;
c.MptModel;
c.MptPreTrainedModel;
c.MultiModalityCausalLM;
c.MultiModalityPreTrainedModel;
c.MusicgenForCausalLM;
c.MusicgenForConditionalGeneration;
c.MusicgenModel;
c.MusicgenPreTrainedModel;
c.NllbTokenizer;
c.NoBadWordsLogitsProcessor;
c.NoRepeatNGramLogitsProcessor;
c.NomicBertModel;
c.NomicBertPreTrainedModel;
c.NougatImageProcessor;
c.NougatTokenizer;
c.OPTForCausalLM;
c.OPTModel;
c.OPTPreTrainedModel;
c.ObjectDetectionPipeline;
c.Olmo2ForCausalLM;
c.Olmo2Model;
c.Olmo2PreTrainedModel;
c.OlmoForCausalLM;
c.OlmoModel;
c.OlmoPreTrainedModel;
c.OpenELMForCausalLM;
c.OpenELMModel;
c.OpenELMPreTrainedModel;
c.OwlViTFeatureExtractor;
c.OwlViTForObjectDetection;
c.OwlViTImageProcessor;
c.OwlViTModel;
c.OwlViTPreTrainedModel;
c.OwlViTProcessor;
c.Owlv2ForObjectDetection;
c.Owlv2ImageProcessor;
c.Owlv2Model;
c.Owlv2PreTrainedModel;
c.PaliGemmaForConditionalGeneration;
c.PaliGemmaPreTrainedModel;
c.PaliGemmaProcessor;
c.PatchTSMixerForPrediction;
c.PatchTSMixerModel;
c.PatchTSMixerPreTrainedModel;
c.PatchTSTForPrediction;
c.PatchTSTModel;
c.PatchTSTPreTrainedModel;
c.Phi3ForCausalLM;
c.Phi3Model;
c.Phi3PreTrainedModel;
c.Phi3VForCausalLM;
c.Phi3VImageProcessor;
c.Phi3VPreTrainedModel;
c.Phi3VProcessor;
c.PhiForCausalLM;
c.PhiModel;
c.PhiPreTrainedModel;
c.Pipeline;
c.PreTrainedModel;
c.PreTrainedTokenizer;
c.PretrainedConfig;
c.PretrainedMixin;
c.Processor;
c.PvtForImageClassification;
c.PvtImageProcessor;
c.PvtModel;
c.PvtPreTrainedModel;
c.PyAnnoteFeatureExtractor;
c.PyAnnoteForAudioFrameClassification;
c.PyAnnoteModel;
c.PyAnnotePreTrainedModel;
c.PyAnnoteProcessor;
c.QuestionAnsweringModelOutput;
c.QuestionAnsweringPipeline;
c.Qwen2ForCausalLM;
c.Qwen2Model;
c.Qwen2PreTrainedModel;
c.Qwen2Tokenizer;
c.Qwen2VLForConditionalGeneration;
c.Qwen2VLImageProcessor;
c.Qwen2VLPreTrainedModel;
c.Qwen2VLProcessor;
c.RTDetrForObjectDetection;
c.RTDetrImageProcessor;
c.RTDetrModel;
c.RTDetrObjectDetectionOutput;
c.RTDetrPreTrainedModel;
c.RawAudio;
c.RawImage;
c.RepetitionPenaltyLogitsProcessor;
c.ResNetForImageClassification;
c.ResNetModel;
c.ResNetPreTrainedModel;
c.RoFormerForMaskedLM;
c.RoFormerForQuestionAnswering;
c.RoFormerForSequenceClassification;
c.RoFormerForTokenClassification;
c.RoFormerModel;
c.RoFormerPreTrainedModel;
c.RoFormerTokenizer;
c.RobertaForMaskedLM;
c.RobertaForQuestionAnswering;
c.RobertaForSequenceClassification;
c.RobertaForTokenClassification;
c.RobertaModel;
c.RobertaPreTrainedModel;
c.RobertaTokenizer;
c.SamImageProcessor;
c.SamImageSegmentationOutput;
c.SamModel;
c.SamPreTrainedModel;
c.SamProcessor;
c.SapiensForDepthEstimation;
c.SapiensForNormalEstimation;
c.SapiensForSemanticSegmentation;
c.SapiensPreTrainedModel;
c.SeamlessM4TFeatureExtractor;
c.SegformerFeatureExtractor;
c.SegformerForImageClassification;
c.SegformerForSemanticSegmentation;
c.SegformerImageProcessor;
c.SegformerModel;
c.SegformerPreTrainedModel;
c.Seq2SeqLMOutput;
c.SequenceClassifierOutput;
c.SiglipImageProcessor;
c.SiglipModel;
c.SiglipPreTrainedModel;
c.SiglipTextModel;
c.SiglipTokenizer;
c.SiglipVisionModel;
c.SpeechT5FeatureExtractor;
c.SpeechT5ForSpeechToText;
c.SpeechT5ForTextToSpeech;
c.SpeechT5HifiGan;
c.SpeechT5Model;
c.SpeechT5PreTrainedModel;
c.SpeechT5Processor;
c.SpeechT5Tokenizer;
c.SqueezeBertForMaskedLM;
c.SqueezeBertForQuestionAnswering;
c.SqueezeBertForSequenceClassification;
c.SqueezeBertModel;
c.SqueezeBertPreTrainedModel;
c.SqueezeBertTokenizer;
c.StableLmForCausalLM;
c.StableLmModel;
c.StableLmPreTrainedModel;
c.Starcoder2ForCausalLM;
c.Starcoder2Model;
c.Starcoder2PreTrainedModel;
c.StoppingCriteria;
c.StoppingCriteriaList;
c.StyleTextToSpeech2Model;
c.StyleTextToSpeech2PreTrainedModel;
c.SummarizationPipeline;
c.SuppressTokensAtBeginLogitsProcessor;
c.Swin2SRForImageSuperResolution;
c.Swin2SRImageProcessor;
c.Swin2SRModel;
c.Swin2SRPreTrainedModel;
c.SwinForImageClassification;
c.SwinModel;
c.SwinPreTrainedModel;
c.T5ForConditionalGeneration;
c.T5Model;
c.T5PreTrainedModel;
c.T5Tokenizer;
c.TableTransformerForObjectDetection;
c.TableTransformerModel;
c.TableTransformerObjectDetectionOutput;
c.TableTransformerPreTrainedModel;
c.TemperatureLogitsWarper;
c.Tensor;
c.Text2TextGenerationPipeline;
c.TextClassificationPipeline;
c.TextGenerationPipeline;
c.TextStreamer;
c.TextToAudioPipeline;
c.TokenClassificationPipeline;
c.TokenClassifierOutput;
c.TokenizerModel;
c.TopKLogitsWarper;
c.TopPLogitsWarper;
c.TrOCRForCausalLM;
c.TrOCRPreTrainedModel;
c.TranslationPipeline;
c.UniSpeechForCTC;
c.UniSpeechForSequenceClassification;
c.UniSpeechModel;
c.UniSpeechPreTrainedModel;
c.UniSpeechSatForAudioFrameClassification;
c.UniSpeechSatForCTC;
c.UniSpeechSatForSequenceClassification;
c.UniSpeechSatModel;
c.UniSpeechSatPreTrainedModel;
c.VLChatProcessor;
c.VLMImageProcessor;
c.ViTFeatureExtractor;
c.ViTForImageClassification;
c.ViTImageProcessor;
c.ViTMAEModel;
c.ViTMAEPreTrainedModel;
c.ViTMSNForImageClassification;
c.ViTMSNModel;
c.ViTMSNPreTrainedModel;
c.ViTModel;
c.ViTPreTrainedModel;
c.VisionEncoderDecoderModel;
c.VitMatteForImageMatting;
c.VitMatteImageProcessor;
c.VitMattePreTrainedModel;
c.VitPoseForPoseEstimation;
c.VitPoseImageProcessor;
c.VitPosePreTrainedModel;
c.VitsModel;
c.VitsModelOutput;
c.VitsPreTrainedModel;
c.VitsTokenizer;
c.Wav2Vec2BertForCTC;
c.Wav2Vec2BertForSequenceClassification;
c.Wav2Vec2BertModel;
c.Wav2Vec2BertPreTrainedModel;
c.Wav2Vec2CTCTokenizer;
c.Wav2Vec2FeatureExtractor;
c.Wav2Vec2ForAudioFrameClassification;
c.Wav2Vec2ForCTC;
c.Wav2Vec2ForSequenceClassification;
c.Wav2Vec2Model;
c.Wav2Vec2PreTrainedModel;
c.Wav2Vec2Processor;
c.Wav2Vec2ProcessorWithLM;
c.WavLMForAudioFrameClassification;
c.WavLMForCTC;
c.WavLMForSequenceClassification;
c.WavLMForXVector;
c.WavLMModel;
c.WavLMPreTrainedModel;
c.WeSpeakerFeatureExtractor;
c.WeSpeakerResNetModel;
c.WeSpeakerResNetPreTrainedModel;
c.WhisperFeatureExtractor;
c.WhisperForConditionalGeneration;
c.WhisperModel;
c.WhisperPreTrainedModel;
c.WhisperProcessor;
c.WhisperTextStreamer;
c.WhisperTimeStampLogitsProcessor;
c.WhisperTokenizer;
c.XLMForQuestionAnswering;
c.XLMForSequenceClassification;
c.XLMForTokenClassification;
c.XLMModel;
c.XLMPreTrainedModel;
c.XLMRobertaForMaskedLM;
c.XLMRobertaForQuestionAnswering;
c.XLMRobertaForSequenceClassification;
c.XLMRobertaForTokenClassification;
c.XLMRobertaModel;
c.XLMRobertaPreTrainedModel;
c.XLMRobertaTokenizer;
c.XLMTokenizer;
c.XLMWithLMHeadModel;
c.XVectorOutput;
c.YolosFeatureExtractor;
c.YolosForObjectDetection;
c.YolosImageProcessor;
c.YolosModel;
c.YolosObjectDetectionOutput;
c.YolosPreTrainedModel;
c.ZeroShotAudioClassificationPipeline;
c.ZeroShotClassificationPipeline;
c.ZeroShotImageClassificationPipeline;
c.ZeroShotObjectDetectionPipeline;
c.bankers_round;
c.cat;
c.cos_sim;
c.dot;
c.dynamic_time_warping;
c.env;
c.full;
c.full_like;
c.getKeyValueShapes;
c.hamming;
c.hanning;
c.interpolate;
c.interpolate_4d;
c.interpolate_data;
c.is_chinese_char;
c.layer_norm;
c.load_image;
c.log_softmax;
c.magnitude;
c.matmul;
c.max;
c.mean;
c.mean_pooling;
c.medianFilter;
c.mel_filter_bank;
c.min;
c.ones;
c.ones_like;
c.permute;
c.permute_data;
var O_ = c.pipeline;
c.quantize_embeddings;
c.rand;
c.read_audio;
c.rfft;
c.round;
c.slice;
c.softmax;
c.spectrogram;
c.stack;
c.std_mean;
c.topk;
c.window_function;
c.zeros;
c.zeros_like;
var Bc, Rc, Bi;
class D_ {
  constructor({ task: $ = "text-classification", model: r = "Cohee/distilbert-base-uncased-go-emotions-onnx" } = {}) {
    Gp(this, Bc, null);
    Gp(this, Rc, null);
    Gp(this, Bi, null);
    Lc(this, Bc, $), Lc(this, Rc, r);
  }
  destroy() {
    Lc(this, Bi, null);
  }
  async loadPipeline() {
    return Dc(this, Bi) || Lc(this, Bi, await O_(Dc(this, Bc), Dc(this, Rc))), Promise.resolve(Dc(this, Bi));
  }
  async generate($) {
    return this.loadPipeline().then(async (r) => r($)).then((r) => r[0]);
  }
}
Bc = new WeakMap(), Rc = new WeakMap(), Bi = new WeakMap();
let Kp = null;
self.onmessage = ({ data: { type: $e, data: $ } }) => {
  switch ($e) {
    case "constructor":
      Kp = new D_($ || {});
      break;
    case "destroy":
      Kp.destroy();
      break;
    case "loadPipeline":
      Kp.loadPipeline(...$ || []).then(() => self.postMessage(null));
      break;
    case "generate":
      Kp.generate(...$ || []).then(self.postMessage);
      break;
    default:
      throw new Error("Unknown type " + $e);
  }
};
